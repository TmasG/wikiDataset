

A rocket (from Italian: rocchetto, lit. 'bobbin/spool') is a spacecraft, aircraft, vehicle or projectile that obtains thrust from a rocket engine. Rocket engine exhaust is formed entirely from propellant carried within the rocket. Rocket engines work by action and reaction and push rockets forward simply by expelling their exhaust in the opposite direction at high speed, and can therefore work in the vacuum of space.
In fact, rockets work more efficiently in the vacuum of space than in an atmosphere. Multistage rockets are capable of attaining escape velocity from Earth and therefore can achieve unlimited maximum altitude. Compared with airbreathing engines, rockets are lightweight and powerful and capable of generating large accelerations. To control their flight, rockets rely on momentum, airfoils, auxiliary reaction engines, gimballed thrust, momentum wheels, deflection of the exhaust stream, propellant flow, spin, or gravity.
Rockets for military and recreational uses date back to at least 13th-century China. Significant scientific, interplanetary and industrial use did not occur until the 20th century, when rocketry was the enabling technology for the Space Age, including setting foot on the Moon. Rockets are now used for fireworks, missiles and other weaponry, ejection seats, launch vehicles for artificial satellites, human spaceflight, and space exploration.
Chemical rockets are the most common type of high power rocket, typically creating a high speed exhaust by the combustion of fuel with an oxidizer. The stored propellant can be a simple pressurized gas or a single liquid fuel that disassociates in the presence of a catalyst (monopropellant), two liquids that spontaneously react on contact (hypergolic propellants), two liquids that must be ignited to react (like kerosene (RP1) and liquid oxygen, used in most liquid-propellant rockets), a solid combination of fuel with oxidizer (solid fuel), or solid fuel with liquid or gaseous oxidizer (hybrid propellant system). Chemical rockets store a large amount of energy in an easily released form, and can be very dangerous. However, careful design, testing, construction and use minimizes risks.
The first gunpowder-powered rockets evolved in medieval China under the Song dynasty by the 13th century. They also developed an early form of MLRS during this time. The Mongols adopted Chinese rocket technology and the invention spread via the Mongol invasions to the Middle East and to Europe in the mid-13th century. Rockets are recorded in use  by the Song navy in a military exercise dated to 1245. Internal-combustion rocket propulsion is mentioned in a reference to 1264, recording that the "ground-rat", a type of firework, had frightened the Empress-Mother Gongsheng at a feast held in her honor by her son the Emperor Lizong. Subsequently, rockets are included in the military treatise Huolongjing, also known as the Fire Drake Manual, written by the Chinese artillery officer Jiao Yu in the mid-14th century. This text mentions the first known multistage rocket, the 'fire-dragon issuing from the water' (Huo long chu shui), thought to have been used by the Chinese navy.
Medieval and early modern rockets were used militarily as incendiary weapons in sieges. Between 1270 and 1280, Hasan al-Rammah wrote al-furusiyyah wa al-manasib al-harbiyya (The Book of Military Horsemanship and Ingenious War Devices), which included 107 gunpowder recipes, 22 of them for rockets. 
In Europe, Konrad Kyeser described rockets in his military treatise Bellifortis around 1405.
The name "rocket" comes from the Italian rocchetta, meaning "bobbin" or "little spindle", given due to the similarity in shape to the bobbin or spool  used to hold the  thread from a spinning wheel.
Leonhard Fronsperger and Conrad Haas adopted the Italian term into German in the mid-16th century; "rocket" appears in English by the early 17th century.
Artis Magnae Artilleriae pars prima, an important early modern work on rocket artillery, by Casimir Siemienowicz,  was first printed in Amsterdam in 1650.
The Mysorean rockets were the first successful iron-cased rockets, developed in the late 18th century in the Kingdom of Mysore (part of present-day India) under the rule of Hyder Ali.
The Congreve rocket was a British weapon designed and developed by Sir William Congreve in 1804. This rocket was based directly on the Mysorean rockets, used compressed powder and was fielded in the Napoleonic Wars. It was Congreve rockets to which Francis Scott Key was referring, when he wrote of the "rockets’ red glare" while held captive on a British ship that was laying siege to Fort McHenry in 1814. Together, the Mysorean and British innovations increased the effective range of military rockets from 100 to 2,000 yards.
The first mathematical treatment of the dynamics of rocket propulsion is due to William Moore (1813). In 1814 Congreve published a book in which he discussed the use of multiple rocket launching apparatus. In 1815 Alexander Dmitrievich Zasyadko constructed rocket-launching platforms, which allowed rockets to be fired in salvos (6 rockets at a time), and gun-laying devices. William Hale in 1844 greatly increased the accuracy of rocket artillery. Edward Mounier Boxer further improved the Congreve rocket in 1865.
William Leitch first proposed the concept of using rockets to enable human spaceflight in 1861. Leitch's rocket spaceflight description was first provided in his 1861 essay "A Journey Through Space", which was later published in his book God's Glory in the Heavens (1862). Konstantin Tsiolkovsky later (in 1903) also conceived this idea, and extensively developed a body of theory that has provided the foundation for subsequent spaceflight development.
The British Royal Flying Corps designed a guided rocket during World War I. Archibald Low stated “...in 1917 the Experimental Works designed an electrically steered rocket…  Rocket experiments were conducted under my own patents with the help of Cdr. Brock” The patent “Improvements in Rockets” was raised in July 1918 but not published until February 1923 for security reasons. Firing and Guidance controls could be either wire or wireless. The propulsion and guidance rocket eflux emerged from the deflecting cowl at the nose.
In 1920, Professor Robert Goddard of Clark University published proposed improvements to rocket technology in A Method of Reaching Extreme Altitudes.  In 1923, Hermann Oberth (1894–1989) published Die Rakete zu den Planetenräumen ("The Rocket into Planetary Space"). Modern rockets originated in 1926 when Goddard attached a supersonic (de Laval) nozzle to a high pressure combustion chamber. These nozzles turn the hot gas from the combustion chamber into a cooler, hypersonic, highly directed jet of gas, more than doubling the thrust and raising the engine efficiency from 2% to 64%. His use of liquid propellants instead of gunpowder greatly lowered the weight and increased the effectiveness of rockets.
In 1921 the Soviet research and development laboratory Gas Dynamics Laboratory began developing solid-propellant rockets, which resulted in the first launch in 1928, which flew for approximately 1,300 metres. These rockets were used in 1931 for the world's first successful use of rockets for ] of aircraft and became the prototypes for the Katyusha rocket launcher, which were used during World War II.
In 1943 production of the V-2 rocket began in Germany. It was designed by the Peenemünde Army Research Center with Wernher von Braun serving as the technical director. The V-2 became the first artificial object to travel into space by crossing the Kármán line with the vertical launch of MW 18014 on 20 June 1944. In parallel with the German guided-missile programme,  rockets were also used on aircraft, either for assisting horizontal take-off (RATO), vertical take-off (Bachem Ba 349 "Natter") or for powering them (Me 163, see list of World War II guided missiles of Germany). The Allies' rocket programs were less technological, relying mostly on unguided missiles like the Soviet Katyusha rocket in the artillery role, and the American anti tank bazooka projectile. These used solid chemical propellants.
The Americans captured a large number of German rocket scientists, including Wernher von Braun, in 1945, and brought them to the United States as part of Operation Paperclip. After World War II scientists used rockets to study high-altitude conditions, by radio telemetry of temperature and pressure of the atmosphere, detection of cosmic rays, and further techniques; note too the Bell X-1, the first crewed vehicle to break the sound barrier (1947). Independently, in the  Soviet Union's space program research continued under the leadership of the chief designer Sergei Korolev (1907–1966).
During the Cold War rockets became extremely important militarily with the development of modern intercontinental ballistic missiles (ICBMs).
The 1960s saw rapid development of rocket technology, particularly in the Soviet Union (Vostok, Soyuz, Proton) and in the United States (e.g. the X-15). Rockets came into use for space exploration. American crewed programs (Project Mercury, Project Gemini and later the Apollo programme) culminated in 1969 with the first crewed  landing on the Moon – using equipment launched by the Saturn V rocket.
Rocket vehicles are often constructed in the archetypal tall thin "rocket" shape that takes off vertically, but there are actually many different types of rockets including:
A rocket design can be as simple as a cardboard tube filled with black powder, but to make an efficient, accurate rocket or missile involves overcoming a number of difficult problems. The main difficulties include cooling the combustion chamber, pumping the fuel (in the case of a liquid fuel), and controlling and correcting the direction of motion.
Rockets consist of a propellant, a place to put propellant (such as a propellant tank), and a nozzle. They may also have one or more rocket engines, directional stabilization device(s) (such as fins, vernier engines or engine gimbals for thrust vectoring, gyroscopes) and a structure (typically monocoque) to hold these components together. Rockets intended for high speed atmospheric use also have an aerodynamic fairing such as a nose cone, which usually holds the payload.
As well as these components, rockets can have any number of other components, such as wings (rocketplanes), parachutes, wheels (rocket cars), even, in a sense, a person (rocket belt). Vehicles frequently possess navigation systems and guidance systems that typically use satellite navigation and inertial navigation systems.
Rocket engines employ the principle of jet propulsion. The rocket engines powering rockets come in a great variety of different types; a comprehensive list can be found in the main article, Rocket engine. Most current rockets are chemically powered rockets (usually internal combustion engines, but some employ a decomposing monopropellant) that emit a hot exhaust gas. A rocket engine can use gas propellants, solid propellant, liquid propellant, or a hybrid mixture of both solid and liquid. Some rockets use heat or pressure that is supplied from a source other than the chemical reaction of propellant(s), such as steam rockets, solar thermal rockets, nuclear thermal rocket engines or simple pressurized rockets such as water rocket or cold gas thrusters. With combustive propellants a chemical reaction is initiated between the fuel and the oxidizer in the combustion chamber, and the resultant hot gases accelerate out of a rocket engine nozzle (or nozzles) at the rearward-facing end of the rocket. The acceleration of these gases through the engine exerts force ("thrust") on the combustion chamber and nozzle, propelling the vehicle (according to Newton's Third Law). This actually happens because the force (pressure times area) on the combustion chamber wall is unbalanced by the nozzle opening; this is not the case in any other direction. The shape of the nozzle also generates force by directing the exhaust gas along the axis of the rocket.
Rocket propellant is mass that is stored, usually in some form of propellant tank or casing, prior to being used as the propulsive mass that is ejected from a rocket engine in the form of a fluid jet to produce thrust. For chemical rockets often the propellants are a fuel such as liquid hydrogen or kerosene burned with an oxidizer such as liquid oxygen or nitric acid to produce large volumes of very hot gas. The oxidiser is either kept separate and mixed in the combustion chamber, or comes premixed, as with solid rockets.
Sometimes the propellant is not burned but still undergoes a chemical reaction, and can be a 'monopropellant' such as hydrazine, nitrous oxide or hydrogen peroxide that can be catalytically decomposed to hot gas.
Alternatively, an inert propellant can be used that can be externally heated, such as in steam rocket, solar thermal rocket or nuclear thermal rockets.
For smaller, low performance rockets such as attitude control thrusters where high performance is less necessary, a pressurised fluid is used as propellant that simply escapes the spacecraft through a propelling nozzle.
The first liquid-fuel rocket, constructed by Robert H. Goddard, differed significantly from modern rockets. The rocket engine was at the top and the fuel tank at the bottom of the rocket, based on Goddard's belief that the rocket would achieve stability by "hanging" from the engine like a pendulum in flight. However, the rocket veered off course and crashed 184 feet (56 m) away from the launch site, indicating that the rocket was no more stable than one with the rocket engine at the base.
Rockets or other similar reaction devices carrying their own propellant must be used when there is no other substance (land, water, or air) or force (gravity, magnetism, light) that a vehicle may usefully employ for propulsion, such as in space. In these circumstances, it is necessary to carry all the propellant to be used.
However, they are also useful in other situations:
Some military weapons use rockets to propel warheads to their targets. A rocket and its payload together are generally referred to as a missile when the weapon has a guidance system (not all missiles use rocket engines, some use other engines such as jets) or as a rocket if it is unguided. Anti-tank and anti-aircraft missiles use rocket engines to engage targets at high speed at a range of several miles, while intercontinental ballistic missiles can be used to deliver multiple nuclear warheads from thousands of miles, and anti-ballistic missiles try to stop them. Rockets have also been tested for reconnaissance, such as the Ping-Pong rocket, which was launched to surveil enemy targets, however, recon rockets have never come into wide use in the military.
Sounding rockets are commonly used to carry instruments that take readings from 50 kilometers (31 mi) to 1,500 kilometers (930 mi) above the surface of the Earth.
The first images of Earth from space were obtained from a V-2 rocket in 1946 (flight #13).
Rocket engines are also used to propel rocket sleds along a rail at extremely high speed. The world record for this is Mach 8.5.
Larger rockets are normally launched from a launch pad that provides stable support until a few seconds after ignition. Due to their high exhaust velocity—2,500 to 4,500 m/s (9,000 to 16,200 km/h; 5,600 to 10,100 mph)—rockets are particularly useful when very high speeds are required, such as orbital speed at approximately 7,800 m/s (28,000 km/h; 17,000 mph). Spacecraft delivered into orbital trajectories become artificial satellites, which are used for many commercial purposes. Indeed, rockets remain the only way to launch spacecraft into orbit and beyond. They are also used to rapidly accelerate spacecraft when they change orbits or de-orbit for landing. Also, a rocket may be used to soften a hard parachute landing immediately before touchdown (see retrorocket).
Rockets were used to propel a line to a stricken ship so that a Breeches buoy can be used to rescue those on board. Rockets are also used to launch emergency flares.
Some crewed rockets, notably the Saturn V and Soyuz, have launch escape systems. This is a small, usually solid rocket that is capable of pulling the crewed capsule away from the main vehicle towards safety at a moments notice. These types of systems have been operated several times, both in testing and in flight, and operated correctly each time.
This was the case when the Safety Assurance System (Soviet nomenclature) successfully pulled away the L3 capsule during three of the four failed launches of the Soviet moon rocket, N1 vehicles 3L, 5L and 7L. In all three cases the capsule, albeit uncrewed, was saved from destruction. Only the three aforementioned N1 rockets had functional Safety Assurance Systems. The outstanding vehicle, 6L, had dummy upper stages and therefore no escape system giving the N1 booster a 100% success rate for egress from a failed launch.
A successful escape of a crewed capsule occurred when Soyuz T-10, on a mission to the Salyut 7 space station, exploded on the pad.
Solid rocket propelled ejection seats are used in many military aircraft to propel crew away to safety from a vehicle when flight control is lost.
A model rocket is a small rocket designed to reach low altitudes (e.g., 100–500 m (330–1,640 ft) for 30 g (1.1 oz) model) and be recovered by a variety of means.
According to the United States National Association of Rocketry (nar) Safety Code, model rockets are constructed of paper, wood, plastic and other lightweight materials. The code also provides guidelines for motor use, launch site selection, launch methods, launcher placement, recovery system design and deployment and more. Since the early 1960s, a copy of the Model Rocket Safety Code has been provided with most model rocket kits and motors. Despite its inherent association with extremely flammable substances and objects with a pointed tip traveling at high speeds, model rocketry historically has proven to be a very safe hobby and has been credited as a significant source of inspiration for children who eventually become scientists and engineers.
Hobbyists build and fly a wide variety of model rockets. Many companies produce model rocket kits and parts but due to their inherent simplicity some hobbyists have been known to make rockets out of almost anything. Rockets are also used in some types of consumer and professional fireworks. A water rocket is a type of model rocket using water as its reaction mass. The pressure vessel (the engine of the rocket) is usually a used plastic soft drink bottle. The water is forced out by a pressurized gas, typically compressed air. It is an example of Newton's third law of motion.
The scale of amateur rocketry can range from a small rocket launched in one's own backyard to a rocket that reached space. Amateur rocketry is split into three categories according to total engine impulse: low-power, mid-power, and high-power.
Hydrogen peroxide rockets are used to power jet packs, and have been used to power cars and a rocket car holds the all time (albeit unofficial) drag racing record.
Corpulent Stump is the most powerful non-commercial rocket ever launched on an Aerotech engine in the United Kingdom.
Launches for orbital spaceflights, or into interplanetary space, are usually from a fixed location on the ground, but would also be possible from an aircraft or ship.
Rocket launch technologies include the entire set of systems needed to successfully launch a vehicle, not just the vehicle itself, but also the firing control systems, mission control center, launch pad, ground stations, and tracking stations needed for a successful launch or recovery or both. These are often collectively referred to as the "ground segment".
Orbital launch vehicles commonly take off vertically, and then begin to progressively lean over, usually following a gravity turn trajectory.
Once above the majority of the atmosphere, the vehicle then angles the rocket jet, pointing it largely horizontally but somewhat downwards, which permits the vehicle to gain and then maintain altitude while increasing horizontal speed. As the speed grows, the vehicle will become more and more horizontal until at orbital speed, the engine will cut off.
All current vehicles stage, that is, jettison hardware on the way to orbit. Although vehicles have been proposed which would be able to reach orbit without staging, none have ever been constructed, and, if powered only by rockets,  the exponentially increasing fuel requirements of such a vehicle would make its useful payload tiny or nonexistent. Most current and historical launch vehicles "expend" their jettisoned hardware, typically by allowing it to crash into the ocean, but some have recovered and reused jettisoned hardware, either by parachute or by propulsive landing.
When launching a spacecraft to orbit, a ".mw-parser-output .vanchor>:target~.vanchor-text{background-color:#b1d2ff}dogleg" is a guided, powered turn during ascent phase that causes a rocket's flight path to deviate from a "straight" path. A dogleg is necessary if the desired launch azimuth, to reach a desired orbital inclination, would take the ground track over land (or over a populated area, e.g. Russia usually does launch over land, but over unpopulated areas), or if the rocket is trying to reach an orbital plane that does not reach the latitude of the launch site. Doglegs are undesirable due to extra onboard fuel required, causing heavier load, and a reduction of vehicle performance.
Rocket exhaust generates a significant amount of acoustic energy. As the supersonic exhaust collides with the ambient air, shock waves are formed. The sound intensity from these shock waves depends on the size of the rocket as well as the exhaust velocity. The sound intensity of large, high performance rockets could potentially kill at close range.
The Space Shuttle generated 180 dB of noise around its base. To combat this, NASA developed a sound suppression system which can flow water at rates up to 900,000 gallons per minute (57 m3/s) onto the launch pad. The water reduces the noise level from 180 dB down to 142 dB (the design requirement is 145 dB). Without the sound suppression system, acoustic waves would reflect off of the launch pad towards the rocket, vibrating the sensitive payload and crew. These acoustic waves can be so severe as to damage or destroy the rocket.
Noise is generally most intense when a rocket is close to the ground, since the noise from the engines radiates up away from the jet, as well as reflecting off the ground. This noise can be reduced somewhat by flame trenches with roofs, by water injection around the jet and by deflecting the jet at an angle.
For crewed rockets various methods are used to reduce the sound intensity for the passengers, and typically the placement of the astronauts far away from the rocket engines helps significantly. For the passengers and crew, when a vehicle goes supersonic the sound cuts off as the sound waves are no longer able to keep up with the vehicle.
The effect of the combustion of propellant in the rocket engine is to increase the internal energy of the resulting gases, utilizing the stored chemical energy in the fuel. As the internal energy increases, pressure increases, and a nozzle is utilized to convert this energy into a directed kinetic energy. This produces thrust against the ambient environment to which these gases are released. The ideal direction of motion of the exhaust is in the direction so as to cause thrust. At the top end of the combustion chamber the hot, energetic gas fluid cannot move forward, and so, it pushes upward against the top of the rocket engine's combustion chamber. As the combustion gases approach the exit of the combustion chamber, they increase in speed. The effect of the convergent part of the rocket engine nozzle on the high pressure fluid of combustion gases, is to cause the gases to accelerate to high speed. The higher the speed of the gases, the lower the pressure of the gas (Bernoulli's principle or conservation of energy) acting on that part of the combustion chamber. In a properly designed engine, the flow will reach Mach 1 at the throat of the nozzle. At which point the speed of the flow increases. Beyond the throat of the nozzle, a bell shaped expansion part of the engine allows the gases that are expanding to push against that part of the rocket engine. Thus, the bell part of the nozzle gives additional thrust. Simply expressed, for every action there is an equal and opposite reaction, according to Newton's third law with the result that the exiting gases produce the reaction of a force on the rocket causing it to accelerate the rocket.
In a closed chamber, the pressures are equal in each direction and no acceleration occurs. If an opening is provided in the bottom of the chamber then the pressure is no longer acting on the missing section. This opening permits the exhaust to escape. The remaining pressures give a resultant thrust on the side opposite the opening, and these pressures are what push the rocket along.
The shape of the nozzle is important. Consider a balloon propelled by air coming out of a tapering nozzle. In such a case the combination of air pressure and viscous friction is such that the nozzle does not push the balloon but is pulled by it. Using a convergent/divergent nozzle gives more force since the exhaust also presses on it as it expands outwards, roughly doubling the total force. If propellant gas is continuously added to the chamber then these pressures can be maintained for as long as propellant remains. Note that in the case of liquid propellant engines, the pumps moving the propellant into the combustion chamber must maintain a pressure larger than the combustion chamber – typically on the order of 100 atmospheres.
As a side effect, these pressures on the rocket also act on the exhaust in the opposite direction and accelerate this exhaust to very high speeds (according to Newton's Third Law). From the principle of conservation of momentum the speed of the exhaust of a rocket determines how much momentum increase is created for a given amount of propellant. This is called the rocket's specific impulse. Because a rocket, propellant and exhaust in flight, without any external perturbations, may be considered as a closed system, the total momentum is always constant. Therefore, the faster the net speed of the exhaust in one direction, the greater the speed of the rocket can achieve in the opposite direction. This is especially true since the rocket body's mass is typically far lower than the final total exhaust mass.
The general study of the forces on a rocket is part of the field of ballistics. Spacecraft are further studied in the subfield of astrodynamics.
Flying rockets are primarily affected by the following:
In addition, the inertia and centrifugal pseudo-force can be significant due to the path of the rocket around the center of a celestial body; when high enough speeds in the right direction and altitude are achieved a stable orbit or escape velocity is obtained.
These forces, with a stabilizing tail (the empennage) present will, unless deliberate control efforts are made, naturally cause the vehicle to follow a roughly parabolic trajectory termed a gravity turn, and this trajectory is often used at least during the initial part of a launch. (This is true even if the rocket engine is mounted at the nose.) Vehicles can thus maintain low or even zero angle of attack, which minimizes transverse stress on the launch vehicle, permitting a weaker, and hence lighter, launch vehicle.
Drag is a force opposite to the direction of the rocket's motion relative to any air it is moving through. This slows the speed of the vehicle and produces structural loads. The deceleration forces for fast-moving rockets are calculated using the drag equation.
Drag can be minimised by an aerodynamic nose cone and by using a shape with a high ballistic coefficient (the "classic" rocket shape—long and thin), and by keeping the rocket's angle of attack as low as possible.
During a launch, as the vehicle speed increases, and the atmosphere thins, there is a point of maximum aerodynamic drag called max Q. This determines the minimum aerodynamic strength of the vehicle, as the rocket must avoid buckling under these forces.
A typical rocket engine can handle a significant fraction of its own mass in propellant each second, with the propellant leaving the nozzle at several kilometres per second. This means that the thrust-to-weight ratio of a rocket engine, and often the entire vehicle can be very high, in extreme cases over 100. This compares with other jet propulsion engines that can exceed 5 for some of the better engines.
It can be shown that the net thrust of a rocket is:
where:
The effective exhaust velocity 




v

e




{\displaystyle v_{e}}

 is more or less the speed the exhaust leaves the vehicle, and in the vacuum of space, the effective exhaust velocity is often equal to the actual average exhaust speed along the thrust axis. However, the effective exhaust velocity allows for various losses, and notably, is reduced when operated within an atmosphere.
The rate of propellant flow through a rocket engine is often deliberately varied over a flight, to provide a way to control the thrust and thus the airspeed of the vehicle. This, for example, allows minimization of aerodynamic losses and can limit the increase of g-forces due to the reduction in propellant load.
Impulse is defined as a force acting on an object over time, which in the absence of opposing forces (gravity and aerodynamic drag), changes the momentum (integral of mass and velocity) of the object. As such, it is the best performance class (payload mass and terminal velocity capability) indicator of a rocket, rather than takeoff thrust, mass, or "power". The total impulse of a rocket (stage) burning its propellant is:: 27 
When there is fixed thrust, this is simply:
The total impulse of a multi-stage rocket is the sum of the impulses of the individual stages.
As can be seen from the thrust equation, the effective speed of the exhaust controls the amount of thrust produced from a particular quantity of fuel burnt per second.
An equivalent measure, the net impulse per weight unit of propellant expelled, is called specific Impulse, 




I

s
p




{\displaystyle I_{sp}}

, and this is one of the most important figures that describes a rocket's performance. It is defined such that it is related to the effective exhaust velocity by:
where:
Thus, the greater the specific impulse, the greater the net thrust and performance of the engine. 




I

s
p




{\displaystyle I_{sp}}

 is determined by measurement while testing the engine. In practice the effective exhaust velocities of rockets varies but can be extremely high, ~4500 m/s, about 15 times the sea level speed of sound in air.
The delta-v capacity of a rocket is the theoretical total change in velocity that a rocket can achieve without any external interference (without air drag or gravity or other forces).
When 




v

e




{\displaystyle v_{e}}

 is constant, the delta-v that a rocket vehicle can provide can be calculated from the Tsiolkovsky rocket equation:
where:
When launched from the Earth practical delta-vs for a single rockets carrying payloads can be a few km/s. Some theoretical designs have rockets with delta-vs over 9 km/s.
The required delta-v can also be calculated for a particular manoeuvre; for example the delta-v to launch from the surface of the Earth to low Earth orbit is about 9.7 km/s, which leaves the vehicle with a sideways speed of about 7.8 km/s at an altitude of around 200 km. In this manoeuvre about 1.9 km/s is lost in air drag, gravity drag and gaining altitude.
The ratio 






m

0



m

1






{\displaystyle {\frac {m_{0}}{m_{1}}}}

 is sometimes called the mass ratio.
Almost all of a launch vehicle's mass consists of propellant. Mass ratio is, for any 'burn', the ratio between the rocket's initial mass and its final mass. Everything else being equal, a high mass ratio is desirable for good performance, since it indicates that the rocket is lightweight and hence performs better, for essentially the same reasons that low weight is desirable in sports cars.
Rockets as a group have the highest thrust-to-weight ratio of any type of engine; and this helps vehicles achieve high mass ratios, which improves the performance of flights. The higher the ratio, the less engine mass is needed to be carried. This permits the carrying of even more propellant, enormously improving the delta-v. Alternatively, some rockets such as for rescue scenarios or racing carry relatively little propellant and payload and thus need only a lightweight structure and instead achieve high accelerations. For example, the Soyuz escape system can produce 20 g.
Achievable mass ratios are highly dependent on many factors such as propellant type, the design of engine the vehicle uses, structural safety margins and construction techniques.
The highest mass ratios are generally achieved with liquid rockets, and these types are usually used for orbital launch vehicles, a situation which calls for a high delta-v. Liquid propellants generally have densities similar to water (with the notable exceptions of liquid hydrogen and liquid methane), and these types are able to use lightweight, low pressure tanks and typically run high-performance turbopumps to force the propellant into the combustion chamber.
Some notable mass fractions are found in the following table (some aircraft are included for comparison purposes):
Thus far, the required velocity (delta-v) to achieve orbit has been unattained by any single rocket because the propellant, tankage, structure, guidance, valves and engines and so on, take a particular minimum percentage of take-off mass that is too great for the propellant it carries to achieve that delta-v carrying reasonable payloads. Since Single-stage-to-orbit has so far not been achievable, orbital rockets always have more than one stage.
For example, the first stage of the Saturn V, carrying the weight of the upper stages, was able to achieve a mass ratio of about 10, and achieved a specific impulse of 263 seconds. This gives a delta-v of around 5.9 km/s whereas around 9.4 km/s delta-v is needed to achieve orbit with all losses allowed for.
This problem is frequently solved by staging—the rocket sheds excess weight (usually empty tankage and associated engines) during launch. Staging is either serial where the rockets light after the previous stage has fallen away, or parallel, where rockets are burning together and then detach when they burn out.
The maximum speeds that can be achieved with staging is theoretically limited only by the speed of light. However the payload that can be carried goes down geometrically with each extra stage needed, while the additional delta-v for each stage is simply additive.
From Newton's second law, the acceleration, 



a


{\displaystyle a}

, of a vehicle is simply:
where m is the instantaneous mass of the vehicle and 




F

n




{\displaystyle F_{n}}

 is the net force acting on the rocket (mostly thrust, but air drag and other forces can play a part).
As the remaining propellant decreases, rocket vehicles become lighter and their acceleration tends to increase until the propellant is exhausted. This means that much of the speed change occurs towards the end of the burn when the vehicle is much lighter. However, the thrust can be throttled to offset or vary this if needed. Discontinuities in acceleration also occur when stages burn out, often starting at a lower acceleration with each new stage firing.
Peak accelerations can be increased by designing the vehicle with a reduced mass, usually achieved by a reduction in the fuel load and tankage and associated structures, but obviously this reduces range, delta-v and burn time. Still, for some applications that rockets are used for, a high peak acceleration applied for just a short time is highly desirable.
The minimal mass of vehicle consists of a rocket engine with minimal fuel and structure to carry it. In that case the thrust-to-weight ratio of the rocket engine limits the maximum acceleration that can be designed. It turns out that rocket engines generally have truly excellent thrust to weight ratios (137 for the NK-33 engine; some solid rockets are over 1000: 442 ), and nearly all really high-g vehicles employ or have employed rockets.
The high accelerations that rockets naturally possess means that rocket vehicles are often capable of vertical takeoff, and in some cases, with suitable guidance and control of the engines, also vertical landing. For these operations to be done it is necessary for a vehicle's engines to provide more than the local gravitational acceleration.
The energy density of a typical rocket propellant is often around one-third that of conventional hydrocarbon fuels; the bulk of the mass is (often relatively inexpensive) oxidizer. Nevertheless, at take-off the rocket has a great deal of energy in the fuel and oxidizer stored within the vehicle. It is of course desirable that as much of the energy of the propellant end up as kinetic or potential energy of the body of the rocket as possible.
Energy from the fuel is lost in air drag and gravity drag and is used for the rocket to gain altitude and speed. However, much of the lost energy ends up in the exhaust.: 37–38 
In a chemical propulsion device, the engine efficiency is simply the ratio of the kinetic power of the exhaust gases and the power available from the chemical reaction:: 37–38 
100% efficiency within the engine (engine efficiency 




η

c


=
100
%


{\displaystyle \eta _{c}=100\%}

) would mean that all the heat energy of the combustion products is converted into kinetic energy of the jet. This is not possible, but the near-adiabatic high expansion ratio nozzles that can be used with rockets come surprisingly close: when the nozzle expands the gas, the gas is cooled and accelerated, and an energy efficiency of up to 70% can be achieved. Most of the rest is heat energy in the exhaust that is not recovered.: 37–38  The high efficiency is a consequence of the fact that rocket combustion can be performed at very high temperatures and the gas is finally released at much lower temperatures, and so giving good Carnot efficiency.
However, engine efficiency is not the whole story. In common with the other jet-based engines, but particularly in rockets due to their high and typically fixed exhaust speeds, rocket vehicles are extremely inefficient at low speeds irrespective of the engine efficiency. The problem is that at low speeds, the exhaust carries away a huge amount of kinetic energy rearward. This phenomenon is termed propulsive efficiency (




η

p




{\displaystyle \eta _{p}}

).: 37–38 
However, as speeds rise, the resultant exhaust speed goes down, and the overall vehicle energetic efficiency rises, reaching a peak of around 100% of the engine efficiency when the vehicle is travelling exactly at the same speed that the exhaust is emitted. In this case the exhaust would ideally stop dead in space behind the moving vehicle, taking away zero energy, and from conservation of energy, all the energy would end up in the vehicle. The efficiency then drops off again at even higher speeds as the exhaust ends up traveling forwards – trailing behind the vehicle.
From these principles it can be shown that the propulsive efficiency 




η

p




{\displaystyle \eta _{p}}

 for a rocket moving at speed 



u


{\displaystyle u}

 with an exhaust velocity 



c


{\displaystyle c}

 is:
And the overall (instantaneous) energy efficiency 



η


{\displaystyle \eta }

 is:
For example, from the equation, with an 




η

c




{\displaystyle \eta _{c}}

 of 0.7, a rocket flying at Mach 0.85 (which most aircraft cruise at) with an exhaust velocity of Mach 10, would have a predicted overall energy efficiency of 5.9%, whereas a conventional, modern, air-breathing jet engine achieves closer to 35% efficiency. Thus a rocket would need about 6x more energy; and allowing for the specific energy of rocket propellant being around one third that of conventional air fuel, roughly 18x more mass of propellant would need to be carried for the same journey. This is why rockets are rarely if ever used for general aviation.
Since the energy ultimately comes from fuel, these considerations mean that rockets are mainly useful when a very high speed is required, such as ICBMs or orbital launch. For example, NASA's Space Shuttle fired its engines for around 8.5 minutes, consuming 1,000 tonnes of solid propellant (containing 16% aluminium) and an additional 2,000,000 litres of liquid propellant (106,261 kg of liquid hydrogen fuel) to lift the 100,000 kg vehicle (including the 25,000 kg payload) to an altitude of 111 km and an orbital velocity of 30,000 km/h. At this altitude and velocity, the vehicle had a kinetic energy of about 3 TJ and a potential energy of roughly 200 GJ. Given the initial energy of 20 TJ, the Space Shuttle was about 16% energy efficient at launching the orbiter.
Thus jet engines, with a better match between speed and jet exhaust speed (such as turbofans—in spite of their worse 




η

c




{\displaystyle \eta _{c}}

)—dominate for subsonic and supersonic atmospheric use, while rockets work best at hypersonic speeds. On the other hand, rockets serve in many short-range relatively low speed military applications where their low-speed inefficiency is outweighed by their extremely high thrust and hence high accelerations.
One subtle feature of rockets relates to energy. A rocket stage, while carrying a given load, is capable of giving a particular delta-v. This delta-v means that the speed increases (or decreases) by a particular amount, independent of the initial speed. However, because kinetic energy is a square law on speed, this means that the faster the rocket is travelling before the burn the more orbital energy it gains or loses.
This fact is used in interplanetary travel. It means that the amount of delta-v to reach other planets, over and above that to reach escape velocity can be much less if the delta-v is applied when the rocket is travelling at high speeds, close to the Earth or other planetary surface; whereas waiting until the rocket has slowed at altitude multiplies up the effort required to achieve the desired trajectory.
The reliability of rockets, as for all physical systems, is dependent on the quality of engineering design and construction.
Because of the enormous chemical energy in rocket propellants (greater energy by weight than explosives, but lower than gasoline), consequences of accidents can be severe. Most space missions have some problems. In 1986, following the Space Shuttle Challenger disaster, American physicist Richard Feynman, having served on the Rogers Commission, estimated that the chance of an unsafe condition for a launch of the Shuttle was very roughly 1%; more recently the historical per person-flight risk in orbital spaceflight has been calculated to be around 2% or 4%.
In May, 2003 the astronaut office made clear its position on the need and feasibility of improving crew safety for future NASA crewed missions indicating their "consensus that an order of magnitude reduction in the risk of human life during ascent, compared to the Space Shuttle, is both achievable with current technology and consistent with NASA's focus on steadily improving rocket reliability".
The costs of rockets can be roughly divided into propellant costs, the costs of obtaining and/or producing the 'dry mass' of the rocket, and the costs of any required support equipment and facilities.
Most of the takeoff mass of a rocket is normally propellant. However propellant is seldom more than a few times more expensive than gasoline per kilogram (as of 2009 gasoline was about $1/kg  or less), and although substantial amounts are needed, for all but the very cheapest rockets, it turns out that the propellant costs are usually comparatively small, although not completely negligible. With liquid oxygen costing $0.15 per kilogram ($0.068/lb) and liquid hydrogen $2.20/kg ($1.00/lb), the Space Shuttle in 2009 had a liquid propellant expense of approximately $1.4 million for each launch that cost $450 million from other expenses (with 40% of the mass of propellants used by it being liquids in the external fuel tank, 60% solids in the SRBs).
Even though a rocket's non-propellant, dry mass is often only between 5–20% of total mass, nevertheless this cost dominates. For hardware with the performance used in orbital launch vehicles, expenses of $2000–$10,000+ per kilogram of dry weight are common, primarily from engineering, fabrication, and testing; raw materials amount to typically around 2% of total expense. For most rockets except reusable ones (shuttle engines) the engines need not function more than a few minutes, which simplifies design.
Extreme performance requirements for rockets reaching orbit correlate with high cost, including intensive quality control to ensure reliability despite the limited safety factors allowable for weight reasons. Components produced in small numbers if not individually machined can prevent amortization of R&amp;D and facility costs over mass production to the degree seen in more pedestrian manufacturing. Amongst liquid-fueled rockets, complexity can be influenced by how much hardware must be lightweight, like pressure-fed engines can have two orders of magnitude lesser part count than pump-fed engines but lead to more weight by needing greater tank pressure, most often used in just small maneuvering thrusters as a consequence.
To change the preceding factors for orbital launch vehicles, proposed methods have included mass-producing simple rockets in large quantities or on large scale, or developing reusable rockets meant to fly very frequently to amortize their up-front expense over many payloads, or reducing rocket performance requirements by constructing a non-rocket spacelaunch system for part of the velocity to orbit (or all of it but with most methods involving some rocket use).
The costs of support equipment, range costs and launch pads generally scale up with the size of the rocket, but vary less with launch rate, and so may be considered to be approximately a fixed cost.
Rockets in applications other than launch to orbit (such as military rockets and rocket-assisted take off), commonly not needing comparable performance and sometimes mass-produced, are often relatively inexpensive.
Since the early 2010s, new private options for obtaining spaceflight services emerged, bringing substantial price pressure into the existing market.
Lists
General rocketry
Rocket propulsion
Recreational rocketry
Weaponry
Rockets for research
Miscellaneous
Governing agencies
Information sites

Italian (italiano  (listen) or lingua italiana ) is a Romance language of the Indo-European language family that evolved from the Vulgar Latin of the Roman Empire. About 85 million people speak this language (2022). Italian is credited as the most direct descendant of Latin, being the closest to it among the national languages and the least divergent from it together with Sardinian when regional and minority languages are also taken into account. Italian is an official language in Italy, Switzerland (Ticino and the Grisons), San Marino, and Vatican City. It has an official minority status in western Istria (Croatia and Slovenia).
It formerly had official status in Albania (due to the annexation of the country to the Kingdom of Italy), Malta, Monaco, Montenegro (because of the Venetian Albania), parts of France (Nice, Savoy and Corsica), parts of Slovenia and Croatia (because of the Venetian Istria and Venetian Dalmatia), parts of Greece (because of the Venetian rule in the Ionian Islands and by the Kingdom of Italy in the Dodecanese), and is generally understood in Corsica by the population resident therein who speak Corsican, which is an Italo-Romance idiom similar to Tuscan. It used to be an official language in the former colonial areas of Italian East Africa and Italian North Africa, where it still has a significant role in various sectors.
Italian is also spoken by large immigrant and expatriate communities in the Americas and Australia. Italian is included under the languages covered by the European Charter for Regional or Minority Languages in Bosnia and Herzegovina and in Romania, although Italian is neither a co-official nor a protected language in these countries. Many speakers of Italian are native bilinguals of both Italian (either in its standard form or regional varieties) and another regional language of Italy.
Italian is a major European language, being one of the official languages of the Organization for Security and Co-operation in Europe and one of the working languages of the Council of Europe. It is the second-most-widely spoken native language in the European Union with 67 million speakers (15% of the EU population) and it is spoken as a second language by 13.4 million EU citizens (3%). Including Italian speakers in non-EU European countries (such as Switzerland, Albania and the United Kingdom) and on other continents, the total number of speakers is approximately 85 million. Italian is the main working language of the Holy See, serving as the lingua franca (common language) in the Roman Catholic hierarchy as well as the official language of the Sovereign Military Order of Malta. Italian is known as the language of music because of its use in musical terminology and opera; numerous Italian words referring to music have become international terms taken into various languages worldwide. Its influence is also widespread in the arts and in the food and luxury goods markets.
Italian was adopted by the state after the Unification of Italy, having previously been a literary language based on Tuscan as spoken mostly by the upper class of Florentine society. Its development was also influenced by other Italian languages and, to some minor extent, by the Germanic languages of the post-Roman invaders. The incorporation into Italian of learned words from its own ancestor language, Latin, is another form of lexical borrowing through the influence of written language, scientific terminology and the liturgical language of the Church. Throughout the Middle Ages and into the early modern period, most literate Italians were also literate in Latin and thus they easily adopted Latin words into their writing—and eventually speech—in Italian. Almost all native Italian words end with vowels, a factor that makes Italian words extremely easy to use in rhyming. Italian has a 7-vowel sound system ('e' and 'o' have mid-low and mid-high sounds); Classical Latin had 10, 5 with short and 5 with long sounds. Unlike most other Romance languages, Italian retains Latin's contrast between short and long consonants. Gemination (doubling) of consonants is a characteristic feature of Italian.

During the Middle Ages, the established written language in Europe was Latin, though the great majority of people were illiterate, and only a handful were well versed in the language. In the Italian peninsula, as in most of Europe, most would instead speak a local vernacular. These dialects, as they are commonly referred to, evolved from Vulgar Latin over the course of centuries, unaffected by formal standards and teachings. They are not in any sense "dialects" of standard Italian, which itself started off as one of these local tongues, but sister languages of Italian. Mutual intelligibility with Italian varies widely, as it does with Romance languages in general. The Romance languages of Italy can differ greatly from Italian at all levels (phonology, morphology, syntax, lexicon, pragmatics) and are classified typologically as distinct languages.
The standard Italian language has a poetic and literary origin in the writings of Tuscan and Sicilian writers of the 12th century, and, even though the grammar and core lexicon are basically unchanged from those used in Florence in the 13th century, the modern standard of the language was largely shaped by relatively recent events. However, Romance vernacular as language spoken in the Apennine peninsula has a longer history. In fact, the earliest surviving texts that can definitely be called vernacular (as distinct from its predecessor Vulgar Latin) are legal formulae known as the Placiti Cassinesi from the Province of Benevento that date from 960 to 963, although the Veronese Riddle, probably from the 8th or early 9th century, contains a late form of Vulgar Latin that can be seen as a very early sample of a vernacular dialect of Italy. The Commodilla catacomb inscription is also a similar case.
The Italian language has progressed through a long and slow process, which started after the Western Roman Empire's fall in the 5th century.
The language that came to be thought of as Italian developed in central Tuscany and was first formalized in the early 14th century through the works of Tuscan writer Dante Alighieri, written in his native Florentine. Dante's epic poems, known collectively as the Commedia, to which another Tuscan poet Giovanni Boccaccio later affixed the title Divina, were read throughout the peninsula and his written dialect became the "canonical standard" that all educated Italians could understand. Dante is still credited with standardizing the Italian language. In addition to the widespread exposure gained through literature, the Florentine dialect also gained prestige due to the political and cultural significance of Florence at the time and the fact that it was linguistically an intermediate between the northern and the southern Italian dialects.: 22  Thus the dialect of Florence became the basis for what would become the official language of Italy.
Italian was progressively made an official language of most of the Italian states predating unification, slowly replacing Latin, even when ruled by foreign powers (like Spain in the Kingdom of Naples, or Austria in the Kingdom of Lombardy–Venetia), even though the masses kept speaking primarily their local vernaculars. Italian was also one of the many recognised languages in the Austro-Hungarian Empire.
Italy has always had a distinctive dialect for each city because the cities, until recently, were thought of as city-states. Those dialects now have considerable variety. As Tuscan-derived Italian came to be used throughout Italy, features of local speech were naturally adopted, producing various versions of Regional Italian. The most characteristic differences, for instance, between Roman Italian and Milanese Italian are syntactic gemination of initial consonants in some contexts and the pronunciation of stressed "e", and of "s" between vowels in many words: e.g. va bene "all right" is pronounced  by a Roman (and by any standard Italian speaker),  by a Milanese (and by any speaker whose native dialect lies to the north of the La Spezia–Rimini Line); a casa "at home" is  for Roman,  or  for standard,  for Milanese and generally northern.
In contrast to the Gallo-Italic linguistic panorama of northern Italy, the Italo-Dalmatian, Neapolitan and its related dialects were largely unaffected by the Franco-Occitan influences introduced to Italy mainly by bards from France during the Middle Ages, but after the Norman conquest of southern Italy, Sicily became the first Italian land to adopt Occitan lyric moods (and words) in poetry. Even in the case of Northern Italian languages, however, scholars are careful not to overstate the effects of outsiders on the natural indigenous developments of the languages.
The economic might and relatively advanced development of Tuscany at the time (Late Middle Ages) gave its language weight, though Venetian remained widespread in medieval Italian commercial life, and Ligurian (or Genoese) remained in use in maritime trade alongside the Mediterranean. The increasing political and cultural relevance of Florence during the periods of the rise of the Banco Medici, Humanism, and the Renaissance made its dialect, or rather a refined version of it, a standard in the arts.
The Renaissance era, known as il Rinascimento in Italian, was seen as a time of rebirth, which is the literal meaning of both renaissance (from French) and rinascimento (Italian).
During this time, long-existing beliefs stemming from the teachings of the Roman Catholic Church began to be understood from new perspectives as humanists—individuals who placed emphasis on the human body and its full potential—began to shift focus from the church to human beings themselves. The continual advancements in technology plays a crucial role in the diffusion of languages. After the invention of the printing press in the fifteenth century, the number of printing presses in Italy grew rapidly and by the year 1500 reached a total of 56, the biggest number of printing presses in all of Europe. This enabled the production of more pieces of literature at a lower cost and as the dominant language, Italian spread.
Italian became the language used in the courts of every state in the Italian peninsula, as well as the prestige variety used in the island of Corsica (but not in the neighboring Sardinia, which on the contrary underwent Italianization well into the late 18th century, under Savoyard sway: the island's linguistic composition, roofed by the prestige of Spanish among the Sardinians, would therein make for a rather slow process of assimilation to the Italian cultural sphere). The rediscovery of Dante's De vulgari eloquentia, as well as a renewed interest in linguistics in the 16th century, sparked a debate that raged throughout Italy concerning the criteria that should govern the establishment of a modern Italian literary and spoken language. This discussion, known as questione della lingua (i. e., the problem of the language), ran through the Italian culture until the end of the 19th century, often linked to the political debate on achieving a united Italian state. Renaissance scholars divided into three main factions:
A fourth faction claimed that the best Italian was the one that the papal court adopted, which was a mixture of the Tuscan and Roman dialects. Eventually, Bembo's ideas prevailed, and the foundation of the Accademia della Crusca in Florence (1582–1583), the official legislative body of the Italian language, led to publication of Agnolo Monosini's Latin tome Floris italicae linguae libri novem in 1604 followed by the first Italian dictionary in 1612.
An important event that helped the diffusion of Italian was the conquest and occupation of Italy by Napoleon in the early 19th century (who was himself of Italian-Corsican descent). This conquest propelled the unification of Italy some decades after and pushed the Italian language into a lingua franca used not only among clerks, nobility, and functionaries in the Italian courts but also by the bourgeoisie.
Italian literature's first modern novel, I promessi sposi (The Betrothed) by Alessandro Manzoni, further defined the standard by "rinsing" his Milanese "in the waters of the Arno" (Florence's river), as he states in the preface to his 1840 edition.
After unification, a huge number of civil servants and soldiers recruited from all over the country introduced many more words and idioms from their home languages—ciao is derived from the Venetian word s-ciao ("slave"), panettone comes from the Lombard word panetton, etc. Only 2.5% of Italy's population could speak the Italian standardized language properly when the nation was unified in 1861.
Italian is a Romance language, a descendant of Vulgar Latin (colloquial spoken Latin). Standard Italian is based on Tuscan, especially its Florentine dialect, and is therefore an Italo-Dalmatian language, a classification that includes most other central and southern Italian languages and the extinct Dalmatian.
According to many sources, Italian is the closest language to Latin in terms of vocabulary. According to the Ethnologue, Lexical similarity is 89% with French, 87% with Catalan, 85% with Sardinian, 82% with Spanish, 80% with Portuguese, 78% with Ladin, 77% with Romanian. Estimates may differ according to sources.
One study (analyzing the degree of differentiation of Romance languages in comparison to Latin (comparing phonology, inflection, discourse, syntax, vocabulary, and intonation) estimated that distance between Italian and Latin is higher than that between Sardinian and Latin. In particular, its vowels are the second-closest to Latin after Sardinian. As in most Romance languages, stress is distinctive.
Italian is an official language of Italy and San Marino and is spoken fluently by the majority of the countries' populations. Italian is the third most spoken language in Switzerland (after German and French), though its use there has moderately declined since the 1970s. It is official both on the national level and on regional level in two cantons: Ticino and the Grisons. In the latter canton, however, it is only spoken by a small minority, in the Italian Grisons. Ticino, which includes Lugano, the largest Italian-speaking city outside Italy, is the only canton where Italian is predominant. Italian is also used in administration and official documents in Vatican City.
Italian is also spoken by a minority in Monaco and France, especially in the southeastern part of the country. Italian was the official language in Savoy and in Nice until 1860, when they were both annexed by France under the Treaty of Turin, a development that triggered the "Niçard exodus", the emigration of a quarter of the Niçard Italians to Italy. Italian was the official language of Corsica until 1859. Italian is generally understood in Corsica by the population resident therein who speak Corsican, which is an Italo-Romance idiom similar to Tuscan. Italian was the official language in Monaco until 1860, when it was replaced by the French. This was due to the annexation of the surrounding County of Nice to France following the Treaty of Turin (1860).
It formerly had official status in Montenegro (because of the Venetian Albania), parts of Slovenia and Croatia (because of the Venetian Istria and Venetian Dalmatia), parts of Greece (because of the Venetian rule in the Ionian Islands and by the Kingdom of Italy in the Dodecanese). Italian is widely spoken in Malta, where nearly two-thirds of the population can speak it fluently. Italian served as Malta's official language until 1934, when it was abolished by the British colonial administration amid strong local opposition. Italian language in Slovenia is an officially recognized minority language in the country. The official census, carried out in 2002, reported 2,258 ethnic Italians (Istrian Italians) in Slovenia (0.11% of the total population). Italian language in Croatia is an official minority language in the country, with many schools and public announcements published in both languages. The 2001 census in Croatia reported 19,636 ethnic Italians (Istrian Italians and Dalmatian Italians) in the country (some 0.42% of the total population). Their numbers dropped dramatically after World War II following the Istrian–Dalmatian exodus, which caused the emigration of between 230,000 and 350,000 Istrian Italians and Dalmatian Italians. Italian was the official language of the Republic of Ragusa from 1492 to 1807.
It formerly had official status in Albania due to the annexation of the country to the Kingdom of Italy (1939–1943). Albania have large populations of non-native speakers, with over half of the population having some knowledge of the Italian language. The Albanian government has pushed to make Italian a compulsory second language in schools. The Italian language is well-known and studied in Albania, due to its historical ties and geographical proximity to Italy and to the diffusion of Italian television in the country.
Due to heavy Italian influence during the Italian colonial period, Italian is still understood by some in former colonies. Although it was the primary language in Libya since colonial rule, Italian greatly declined under the rule of Muammar Gaddafi, who expelled the Italian Libyan population and made Arabic the sole official language of the country. A few hundred Italian settlers returned to Libya in the 2000s.
Italian was the official language of Eritrea during Italian colonisation. Italian is today used in commerce and it is still spoken especially among elders; besides that, Italian words are incorporated as loan words in the main language spoken in the country (Tigrinya). The capital city of Eritrea, Asmara, still has several Italian schools, established during the colonial period. In the early 19th century, Eritrea was the country with the highest number of Italians abroad, and the Italian Eritreans grew from 4,000 during World War I to nearly 100,000 at the beginning of World War II. In Asmara there are two Italian schools, the Italian School of Asmara (Italian primary school with a Montessori department) and the Liceo Sperimentale "G. Marconi" (Italian international senior high school).
Italian was also introduced to Somalia through colonialism and was the sole official language of administration and education during the colonial period but fell out of use after government, educational and economic infrastructure were destroyed in the Somali Civil War.
Italian is also spoken by large immigrant and expatriate communities in the Americas and Australia. Although over 17 million Americans are of Italian descent, only a little over one million people in the United States speak Italian at home. Nevertheless, an Italian language media market does exist in the country. In Canada, Italian is the second most spoken non-official language when varieties of Chinese are not grouped together, with 375,645 claiming Italian as their mother tongue in 2016.
Italian immigrants to South America have also brought a presence of the language to that continent. According to some sources, Italian is the second most spoken language in Argentina after the official language of Spanish, although its number of speakers, mainly of the older generation, is decreasing. Italian bilingual speakers can be found in the Southeast of Brazil as well as in the South, corresponding to 2.07% of the total population of the country. In Venezuela, Italian is the most spoken language after Spanish and Portuguese, with around 200,000 speakers. In Uruguay, people that speak Italian as their home language is 1.1% of the total population of the country. In Australia, Italian is the second most spoken foreign language after Chinese, with 1.4% of the population speaking it as their home language.
The main Italian-language newspapers published outside Italy are the L'Osservatore Romano (Vatican City), the L'Informazione di San Marino (San Marino), the Corriere del Ticino and the laRegione Ticino (Switzerland), the La Voce del Popolo (Croatia), the Corriere d'Italia (Germany), the L'italoeuropeo (United Kingdom), the Passaparola (Luxembourg), the America Oggi (United States), the Corriere Canadese and the Corriere Italiano (Canada), the Il punto d'incontro (Mexico), the L'Italia del Popolo (Argentina), the Fanfulla (Brazil), the Gente d'Italia (Uruguay), the La Voce d'Italia (Venezuela), the Il Globo (Australia) and the La gazzetta del Sud Africa (South Africa).
Italian is widely taught in many schools around the world, but rarely as the first foreign language. In the 21st century, technology also allows for the continual spread of the Italian language, as people have new ways to learn how to speak, read, and write languages at their own pace and at any given time. For example, the free website and application Duolingo has 4.94 million English speakers learning the Italian language.
According to the Italian Ministry of Foreign Affairs, every year there are more than 200,000 foreign students who study the Italian language; they are distributed among the 90 Institutes of Italian Culture that are located around the world, in the 179 Italian schools located abroad, or in the 111 Italian lecturer sections belonging to foreign schools where Italian is taught as a language of culture.
From the late 19th to the mid-20th century, thousands of Italians settled in Argentina, Uruguay, Southern Brazil and Venezuela, as well as in Canada and the United States, where they formed a physical and cultural presence.
In some cases, colonies were established where variants of regional languages of Italy were used, and some continue to use this regional language. Examples are Rio Grande do Sul, Brazil, where Talian is used, and the town of Chipilo near Puebla, Mexico; each continues to use a derived form of Venetian dating back to the nineteenth century. Another example is Cocoliche, an Italian–Spanish pidgin once spoken in Argentina and especially in Buenos Aires, and Lunfardo.
Starting in late medieval times in much of Europe and the Mediterranean, Latin was replaced as the primary commercial language by Italian language variants (especially Tuscan and Venetian). These variants were consolidated during the Renaissance with the strength of Italy and the rise of humanism and the arts.
During that period, Italy held artistic sway over the rest of Europe. It was the norm for all educated gentlemen to make the Grand Tour, visiting Italy to see its great historical monuments and works of art. It thus became expected to learn at least some Italian. In England, while the classical languages Latin and Greek were the first to be learned, Italian became the second most common modern language after French, a position it held until the late 18th century when it tended to be replaced by German. John Milton, for instance, wrote some of his early poetry in Italian.
Within the Catholic Church, Italian is known by a large part of the ecclesiastical hierarchy and is used in substitution for Latin in some official documents.
Italian loanwords continue to be used in most languages in matters of art and music (especially classical music including opera), in the design and fashion industries, in some sports like football and especially in culinary terms.
In Italy, almost all the other languages spoken as the vernacular—other than standard Italian and some languages spoken among immigrant communities—are often called "Italian dialects", a label that can be very misleading if it is understood to mean "dialects of Italian". The Romance dialects of Italy are local evolutions of spoken Latin that pre-date the establishment of Italian, and as such are sister languages to the Tuscan that was the historical source of Italian. They can be quite different from Italian and from each other, with some belonging to different linguistic branches of Romance. The only exceptions to this are twelve groups considered "historical language minorities", which are officially recognized as distinct minority languages by the law. On the other hand, Corsican (a language spoken on the French island of Corsica) is closely related to medieval Tuscan, from which Standard Italian derives and evolved.
The differences in the evolution of Latin in the different regions of Italy can be attributed to the natural changes that all languages in regular use are subject to, and to some extent to the presence of three other types of languages: substrata, superstrata, and adstrata. The most prevalent were substrata (the language of the original inhabitants), as the Italian dialects were most likely simply Latin as spoken by native cultural groups. Superstrata and adstrata were both less important. Foreign conquerors of Italy that dominated different regions at different times left behind little to no influence on the dialects. Foreign cultures with which Italy engaged in peaceful relations with, such as trade, had no significant influence either.: 19-20 
Throughout Italy, regional variations of Standard Italian, called Regional Italian, are spoken. Regional differences can be recognized by various factors: the openness of vowels, the length of the consonants, and influence of the local language (for example, in informal situations andà, annà and nare replace the standard Italian andare in the area of Tuscany, Rome and Venice respectively for the infinitive "to go").
There is no definitive date when the various Italian variants of Latin—including varieties that contributed to modern Standard Italian—began to be distinct enough from Latin to be considered separate languages. One criterion for determining that two language variants are to be considered separate languages rather than variants of a single language is that they have evolved so that they are no longer mutually intelligible; this diagnostic is effective if mutual intelligibility is minimal or absent (e.g. in Romance, Romanian and Portuguese), but it fails in cases such as Spanish-Portuguese or Spanish-Italian, as native speakers of either pairing can understand each other well if they choose to do so. Nevertheless, on the basis of accumulated differences in morphology, syntax, phonology, and to some extent lexicon, it is not difficult to identify that for the Romance varieties of Italy, the first extant written evidence of languages that can no longer be considered Latin comes from the ninth and tenth centuries C.E. These written sources demonstrate certain vernacular characteristics and sometimes explicitly mention the use of the vernacular in Italy. Full literary manifestations of the vernacular began to surface around the 13th century in the form of various religious texts and poetry.: 21 Although these are the first written records of Italian varieties separate from Latin, the spoken language had likely diverged long before the first written records appear, since those who were literate generally wrote in Latin even if they spoke other Romance varieties in person.
Throughout the 19th and 20th centuries, the use of Standard Italian became increasingly widespread and was mirrored by a decline in the use of the dialects. An increase in literacy was one of the main driving factors (one can assume that only literates were capable of learning Standard Italian, whereas those who were illiterate had access only to their native dialect). The percentage of literates rose from 25% in 1861 to 60% in 1911, and then on to 78.1% in 1951. Tullio De Mauro, an Italian linguist, has asserted that in 1861 only 2.5% of the population of Italy could speak Standard Italian. He reports that in 1951 that percentage had risen to 87%. The ability to speak Italian did not necessarily mean it was in everyday use, and most people (63.5%) still usually spoke their native dialects. In addition, other factors such as mass emigration, industrialization, and urbanization, and internal migrations after World War II, contributed to the proliferation of Standard Italian. The Italians who emigrated during the Italian diaspora beginning in 1861 were often of the uneducated lower class, and thus the emigration had the effect of increasing the percentage of literates, who often knew and understood the importance of Standard Italian, back home in Italy. A large percentage of those who had emigrated also eventually returned to Italy, often more educated than when they had left.: 35 
The Italian dialects have declined in the modern era, as Italy unified under Standard Italian and continues to do so aided by mass media, from newspapers to radio to television.: 37 
Notes:
Italian has a seven-vowel system, consisting of /a, ɛ, e, i, ɔ, o, u/, as well as 23 consonants. Compared with most other Romance languages, Italian phonology is conservative, preserving many words nearly unchanged from Vulgar Latin. Some examples:
The conservative nature of Italian phonology is partly explained by its origin. Italian stems from a literary language that is derived from the 13th-century speech of the city of Florence in the region of Tuscany, and has changed little in the last 700 years or so. Furthermore, the Tuscan dialect is the most conservative of all Italian dialects, radically different from the Gallo-Italian languages less than 160 kilometres (100 mi) to the north (across the La Spezia–Rimini Line).
The following are some of the conservative phonological features of Italian, as compared with the common Western Romance languages (French, Spanish, Portuguese, Galician, Catalan). Some of these features are also present in Romanian.
Compared with most other Romance languages, Italian has many inconsistent outcomes, where the same underlying sound produces different results in different words, e.g. laxāre &gt; lasciare and lassare, captiāre &gt; cacciare and cazzare, (ex)dēroteolāre &gt; sdrucciolare, druzzolare and ruzzolare, rēgīna &gt; regina and reina. Although in all these examples the second form has fallen out of usage, the dimorphism is thought to reflect the several-hundred-year period during which Italian developed as a literary language divorced from any native-speaking population, with an origin in 12th/13th-century Tuscan but with many words borrowed from languages farther to the north, with different sound outcomes. (The La Spezia–Rimini Line, the most important isogloss in the entire Romance-language area, passes only about 30 kilometres or 20 miles north of Florence.) Dual outcomes of Latin /p t k/ between vowels, such as lŏcum &gt; luogo but fŏcum &gt; fuoco, was once thought to be due to borrowing of northern voiced forms, but is now generally viewed as the result of early phonetic variation within Tuscany.
Some other features that distinguish Italian from the Western Romance languages:
Standard Italian also differs in some respects from most nearby Italian languages:
Italian phonotactics do not usually permit verbs and polysyllabic nouns to end with consonants, except in poetry and song, so foreign words may receive extra terminal vowel sounds.
Italian has a shallow orthography, meaning very regular spelling with an almost one-to-one correspondence between letters and sounds. In linguistic terms, the writing system is close to being a phonemic orthography. The most important of the few exceptions are the following (see below for more details): 
The Italian alphabet is typically considered to consist of 21 letters. The letters j, k, w, x, y are traditionally excluded, though they appear in loanwords such as jeans, whisky, taxi, xenofobo, xilofono. The letter ⟨x⟩ has become common in standard Italian with the prefix extra-, although (e)stra- is traditionally used; it is also common to use the Latin particle ex(-) to mean "former(ly)" as in: la mia ex ("my ex-girlfriend"), "Ex-Jugoslavia" ("Former Yugoslavia"). The letter ⟨j⟩ appears in the first name Jacopo and in some Italian place-names, such as Bajardo, Bojano, Joppolo, Jerzu, Jesolo, Jesi, Ajaccio, among others, and in Mar Jonio, an alternative spelling of Mar Ionio (the Ionian Sea). The letter ⟨j⟩ may appear in dialectal words, but its use is discouraged in contemporary standard Italian. Letters used in foreign words can be replaced with phonetically equivalent native Italian letters and digraphs: ⟨gi⟩, ⟨ge⟩, or ⟨i⟩ for ⟨j⟩; ⟨c⟩ or ⟨ch⟩ for ⟨k⟩ (including in the standard prefix kilo-); ⟨o⟩, ⟨u⟩ or ⟨v⟩ for ⟨w⟩; ⟨s⟩, ⟨ss⟩, ⟨z⟩, ⟨zz⟩ or ⟨cs⟩ for ⟨x⟩; and ⟨e⟩ or ⟨i⟩ for ⟨y⟩.
Italian has geminate, or double, consonants, which are distinguished by length and intensity. Length is distinctive for all consonants except for /ʃ/, /dz/, /ts/, /ʎ/, /ɲ/, which are always geminate when between vowels, and /z/, which is always single.
Geminate plosives and affricates are realized as lengthened closures. Geminate fricatives, nasals, and /l/ are realized as lengthened continuants. There is only one vibrant phoneme /r/ but the actual pronunciation depends on context and regional accent. Generally one can find a flap consonant  in unstressed position whereas  is more common in stressed syllables, but there may be exceptions. Especially people from the Northern part of Italy (Parma, Aosta Valley, South Tyrol) may pronounce /r/ as , , or .
Of special interest to the linguistic study of Regional Italian is the gorgia toscana, or "Tuscan Throat", the weakening or lenition of intervocalic /p/, /t/, and /k/ in the Tuscan language.
The voiced postalveolar fricative /ʒ/ is present as a phoneme only in loanwords: for example, garage . Phonetic  is common in Central and Southern Italy as an intervocalic allophone of /dʒ/: gente  'people' but la gente  'the people', ragione  'reason'.
Italian grammar is typical of the grammar of Romance languages in general. Cases exist for personal pronouns (nominative, oblique, accusative, dative), but not for nouns.
There are two basic classes of nouns in Italian, referred to as genders, masculine and feminine. Gender may be natural (ragazzo 'boy', ragazza 'girl') or simply grammatical with no possible reference to biological gender (masculine costo 'cost', feminine costa 'coast'). Masculine nouns typically end in -o (ragazzo 'boy'), with plural marked by -i (ragazzi 'boys'), and feminine nouns typically end in -a, with plural marked by -e (ragazza 'girl', ragazze 'girls'). For a group composed of boys and girls, ragazzi is the plural, suggesting that -i is a general neutral plural. A third category of nouns is unmarked for gender, ending in -e in the singular and -i in the plural: legge 'law, f. sg.', leggi 'laws, f. pl.'; fiume 'river, m. sg.', fiumi 'rivers, m. pl.', thus assignment of gender is arbitrary in terms of form, enough so that terms may be identical but of distinct genders: fine meaning 'aim', 'purpose' is masculine, while fine meaning 'end, ending' (e.g. of a movie) is feminine, and both are fini in the plural, a clear instance of -i as a non-gendered default plural marker. These nouns often, but not always, denote inanimates. There are a number of nouns that have a masculine singular and a feminine plural, most commonly of the pattern m. sg. -o, f. pl. -a (miglio 'mile, m. sg.', miglia 'miles, f. pl.'; paio 'pair, m. sg., paia 'pairs, f. pl.'), and thus are sometimes considered neuter (these are usually derived from neuter Latin nouns). An instance of neuter gender also exists in pronouns of the third person singular.
Examples:
Nouns, adjectives, and articles inflect for gender and number (singular and plural).
Like in English, common nouns are capitalized when occurring at the beginning of a sentence. Unlike English, nouns referring to languages (e.g. Italian), speakers of languages, or inhabitants of an area (e.g. Italians) are not capitalized.
There are three types of adjectives: descriptive, invariable and form-changing. Descriptive adjectives are the most common, and their endings change to match the number and gender of the noun they modify. Invariable adjectives are adjectives whose endings do not change. The form changing adjectives "buono (good), bello (beautiful), grande (big), and santo (saint)" change in form when placed before different types of nouns. Italian has three degrees for comparison of adjectives: positive, comparative, and superlative.
The order of words in the phrase is relatively free compared to most European languages. The position of the verb in the phrase is highly mobile. Word order often has a lesser grammatical function in Italian than in English. Adjectives are sometimes placed before their noun and sometimes after. Subject nouns generally come before the verb. Italian is a null-subject language, so that nominative pronouns are usually absent, with subject indicated by verbal inflections (e.g. amo 'I love', ama '(s)he loves', amano 'they love'). Noun objects normally come after the verb, as do pronoun objects after imperative verbs, infinitives and gerunds, but otherwise pronoun objects come before the verb.
There are both indefinite and definite articles in Italian. There are four indefinite articles, selected by the gender of the noun they modify and by the phonological structure of the word that immediately follows the article. Uno is masculine singular, used before z (/ts/ or /dz/), s+consonant, gn (/ɲ/), or ps, while masculine singular un is used before a word beginning with any other sound. The noun zio 'uncle' selects masculine singular, thus uno zio 'an uncle' or uno zio anziano 'an old uncle,' but un mio zio 'an uncle of mine'. The feminine singular indefinite articles are una, used before any consonant sound, and its abbreviated form, written un', used before vowels: una camicia 'a shirt', una camicia bianca 'a white shirt', un'altra camicia 'a different shirt'. There are seven forms for definite articles, both singular and plural. In the singular: lo, which corresponds to the uses of uno; il, which corresponds to the uses with consonant of un; la, which corresponds to the uses of una; l', used for both masculine and feminine singular before vowels. In the plural: gli is the masculine plural of lo and l'; i is the plural of il; and le is the plural of feminine la and l'.
There are numerous contractions of prepositions with subsequent articles. There are numerous productive suffixes for diminutive, augmentative, pejorative, attenuating, etc., which are also used to create neologisms.
There are 27 pronouns, grouped in clitic and tonic pronouns. Personal pronouns are separated into three groups: subject, object (which take the place of both direct and indirect objects), and reflexive. Second person subject pronouns have both a polite and a familiar form. These two different types of address are very important in Italian social distinctions. All object pronouns have two forms: stressed and unstressed (clitics). Unstressed object pronouns are much more frequently used, and come before the verb (Lo vedo. 'I see him.'). Stressed object pronouns come after the verb, and are used when emphasis is required, for contrast, or to avoid ambiguity (Vedo lui, ma non lei. 'I see him, but not her'). Aside from personal pronouns, Italian also has demonstrative, interrogative, possessive, and relative pronouns. There are two types of demonstrative pronouns: relatively near (this) and relatively far (that). Demonstratives in Italian are repeated before each noun, unlike in English.
There are three regular sets of verbal conjugations, and various verbs are irregularly conjugated. Within each of these sets of conjugations, there are four simple (one-word) verbal conjugations by person/number in the indicative mood (present tense; past tense with imperfective aspect, past tense with perfective aspect, and future tense), two simple conjugations in the subjunctive mood (present tense and past tense), one simple conjugation in the conditional mood, and one simple conjugation in the imperative mood. Corresponding to each of the simple conjugations, there is a compound conjugation involving a simple conjugation of "to be" or "to have" followed by a past participle. "To have" is used to form compound conjugation when the verb is transitive ("Ha detto", "ha fatto": he/she has said, he/she has made/done), while "to be" is used in the case of verbs of motion and some other intransitive verbs ("È andato", "è stato": he has gone, he has been). "To be" may be used with transitive verbs, but in such a case it makes the verb passive ("È detto", "è fatto": it is said, it is made/done). This rule is not absolute, and some exceptions do exist.
Note: the plural form of verbs could also be used as an extremely formal (for example to noble people in monarchies) singular form (see royal we).



Literal translation, direct translation or word-for-word translation, is a translation of a text done by translating each word separately, without looking at how the words are used together in a phrase or sentence.
In translation theory, another term for "literal translation" is metaphrase (as opposed to paraphrase for an analogous translation).
Literal translation leads to mistranslating of idioms, which is a serious problem for machine translation.
The term "literal translation" often appeared in the titles of 19th-century English translations of classical, Bible and other texts.
Word-for-word translations ("cribs," "ponies" or "trots") are sometimes prepared for a writer who is translating a work written in a language they do not know. For example, Robert Pinsky is reported to have used a literal translation in preparing his translation of Dante's Inferno (1994), as he does not know Italian. Similarly, Richard Pevear worked from literal translations provided by his wife, Larissa Volokhonsky, in their translations of several Russian novels.
Literal translation can also denote a translation that represents the precise meaning of the original text but does not attempt to convey its style, beauty, or poetry. There is, however, a great deal of difference between a literal translation of a poetic work and a prose translation. A literal translation of poetry may be in prose rather than verse, but also be error free. Charles Singleton's translation of The Divine Comedy (1975) is regarded as a prose translation.
"Literal" translation implies that it is probably full of errors, since the translator has made no effort to convey, for example, correct idioms or shades of meaning, but it might be also useful in seeing how words are used to convey meaning in the source language.
A literal English translation of the German word "Kindergarten" would be "children's garden," but also in (mainly US) English, the expression refers to the preschool institution. Literal translations in which individual components within words or compounds are translated to create new lexical items in the target language (a process also known as “loan translation”) are called calques, e.g., “beer garden” from German “Biergarten.”
The literal translation of the Italian sentence, "So che questo non va bene" ("I know that this is not good"), produces "Know(I) that this not goes(it) well," which has English words and Italian grammar.
Early machine translations (as of 1962 at least) were notorious for this type of translation as they simply employed a database of words and their translations. Later attempts utilized common phrases which resulted in better grammatical structure and capture of idioms but with many words left in the original language. For translating synthetic languages, a morphosyntactic analyzer and synthesizer is required.
The best systems today use a combination of the above technologies and apply algorithms to correct the "natural" sound of the translation. In the end though, professional translation firms that employ machine translation use it as a tool to create a rough translation that is then tweaked by a human, professional translator.
Douglas Hofstadter gave an example for the failures of a machine translation: The English sentence "In their house, everything comes in pairs. There's his car and her car, his towels and her towels, and his library and hers." is translated into French as "Dans leur maison, tout vient en paires. Il y a sa voiture et sa voiture, ses serviettes et ses serviettes, sa bibliothèque et les siennes." That does not make sense, because the literal translation of both "his" and "hers" into French is "sa" in case of singular, and "ses" in case of plural, therefore the French version is not understandable.
Often, first-generation immigrants create something of a literal translation in how they speak their parents' native language. This results in a mix of the two languages in something of a pidgin. Many such mixes have specific names, e.g. Spanglish or Germish. For example, American children of German immigrants are heard using "rockingstool" from the German word "Schaukelstuhl" instead of "rocking chair".
Literal translation of idioms is a source of translators' jokes and apocrypha. The following has often been told in relation to inexperienced translators or to machine translations: When the sentence, "The spirit is willing, but the flesh is weak" ("дух бодр, плоть же немощна", an allusion to Mark 14:38) was translated into Russian and then back into English, the result was "The vodka is good, but the meat is rotten" ("водка хорошая, но мясо протухло"). This is generally believed an amusing apocrypha rather than a reference to an actual machine-translation error.

A spacecraft is a vehicle or machine designed to fly in outer space. A type of artificial satellite, spacecraft are used for a variety of purposes, including communications, Earth observation, meteorology, navigation, space colonization, planetary exploration, and transportation of humans and cargo. All spacecraft except single-stage-to-orbit vehicles cannot get into space on their own, and require a launch vehicle (carrier rocket).
On a sub-orbital spaceflight, a space vehicle enters space and then returns to the surface without having gained sufficient energy or velocity to make a full Earth orbit. For orbital spaceflights, spacecraft enter closed orbits around the Earth or around other celestial bodies. Spacecraft used for human spaceflight carry people on board as crew or passengers from start or on orbit (space stations) only, whereas those used for robotic space missions operate either autonomously or telerobotically. Robotic spacecraft used to support scientific research are space probes. Robotic spacecraft that remain in orbit around a planetary body are artificial satellites. To date, only a handful of interstellar probes, such as Pioneer 10 and 11, Voyager 1 and 2, and New Horizons, are on trajectories that leave the Solar System.
Orbital spacecraft may be recoverable or not. Most are not.  Recoverable spacecraft may be subdivided by a method of reentry to Earth into non-winged space capsules and winged spaceplanes. Recoverable spacecraft may be reusable (can be launched again or several times, like the SpaceX Dragon and the Space Shuttle orbiters) or expendable (like the Soyuz). In recent years, more space agencies are tending towards reusable spacecraft.
Humanity has achieved space flight, but only a few nations have the technology for orbital launches: Russia (RSA or "Roscosmos"), the United States (NASA), the member states of the European Space Agency (ESA), Japan (JAXA), China (CNSA), India (ISRO), Taiwan (National Chung-Shan Institute of Science and Technology, Taiwan National Space Organization (NSPO), Israel (ISA), Iran (ISA), and North Korea (NADA). In addition, several private companies have developed or are developing the technology for orbital launches independently from government agencies. The most prominent examples of such companies are SpaceX and Blue Origin.
A German V-2 became the first spacecraft when it reached an altitude of 189 km in June 1944 in Peenemünde, Germany. Sputnik 1 was the first artificial satellite. It was launched into an elliptical low Earth orbit (LEO) by the Soviet Union on 4 October 1957. The launch ushered in new political, military, technological, and scientific developments;  while the Sputnik launch was a single event, it marked the start of the Space Age. Apart from its value as a technological first, Sputnik 1 also helped to identify the upper atmospheric layer's density, through measuring the satellite's orbital changes. It also provided data on radio-signal distribution in the ionosphere. Pressurized nitrogen in the satellite's false body provided the first opportunity for meteoroid detection. Sputnik 1 was launched during the International Geophysical Year from Site No.1/5, at the 5th Tyuratam range, in Kazakh SSR (now at the Baikonur Cosmodrome). The satellite traveled at 29,000 kilometres per hour (18,000 mph), taking 96.2 minutes to complete an orbit, and emitted radio signals at 20.005 and 40.002 MHz
While Sputnik 1 was the first spacecraft to orbit the Earth, other man-made objects had previously reached an altitude of 100 km, which is the height required by the international organization Fédération Aéronautique Internationale to count as a spaceflight. This altitude is called the Kármán line. In particular, in the 1940s there were several test launches of the V-2 rocket, some of which reached altitudes well over 100 km.
As of 2016, only three nations have flown crewed spacecraft: USSR/Russia, USA, and China.
The first crewed spacecraft was Vostok 1, which carried Soviet cosmonaut Yuri Gagarin into space in 1961, and completed a full Earth orbit. There were five other crewed missions which used a Vostok spacecraft. The second crewed spacecraft was named Freedom 7, and it performed a sub-orbital spaceflight in 1961 carrying American astronaut Alan Shepard to an altitude of just over 187 kilometers (116 mi). There were five other crewed missions using Mercury spacecraft.
Other Soviet crewed spacecraft include the Voskhod, Soyuz, flown uncrewed as Zond/L1, L3, TKS, and the Salyut and Mir crewed space stations. Other American crewed spacecraft include the Gemini spacecraft, the Apollo spacecraft including the Apollo Lunar Module, the Skylab space station, the Space Shuttle with undetached European Spacelab and private US Spacehab space stations-modules, and the SpaceX Crew Dragon configuration of their Dragon 2. US company Boeing also developed and flown a spacecraft of their own, the CST-100, commonly referred to as Starliner, but a crewed flight is yet to occur. China developed, but did not fly Shuguang, and is currently using Shenzhou (its first crewed mission was in 2003).
Except for the Space Shuttle, all of the recoverable crewed orbital spacecraft were space capsules.
American Mercury, Gemini, and Apollo spacecraft
Soviet Voskhod (variant of Vostok)
1967 Soviet/Russian Soyuz spacecraft
Chinese Shenzhou
Line drawing of Vostok capsule
The International Space Station, crewed since November 2000, is a joint venture between Russia, the United States, Canada and several other countries.
Spaceplanes are spacecraft are built in the shape of, and function as, airplanes. The first example of such was the North American X-15 spaceplane, which conducted two crewed flights which reached an altitude of over 100 km in the 1960s.  This first reusable spacecraft was air-launched on a suborbital trajectory on July 19, 1963.
The first partially reusable orbital spacecraft, a winged non-capsule, the Space Shuttle, was launched by the USA on the 20th anniversary of Yuri Gagarin's flight, on April 12, 1981. During the Shuttle era, six orbiters were built, all of which have flown in the atmosphere and five of which have flown in space. Enterprise was used only for approach and landing tests, launching from the back of a Boeing 747 SCA and gliding to deadstick landings at Edwards AFB, California. The first Space Shuttle to fly into space was Columbia, followed by Challenger, Discovery, Atlantis, and Endeavour. Endeavour was built to replace Challenger when it was lost in January 1986. Columbia broke up during reentry in February 2003.
The first automatic partially reusable spacecraft was the Buran-class shuttle, launched by the USSR on November 15, 1988, although it made only one flight and this was uncrewed. This spaceplane was designed for a crew and strongly resembled the U.S. Space Shuttle, although its drop-off boosters used liquid propellants and its main engines were located at the base of what would be the external tank in the American Shuttle. Lack of funding, complicated by the dissolution of the USSR, prevented any further flights of Buran. The Space Shuttle was subsequently modified to allow for autonomous re-entry in case of necessity.
Per the Vision for Space Exploration, the Space Shuttle was retired in 2011 mainly due to its old age and high cost of program reaching over a billion dollars per flight. The Shuttle's human transport role is to be replaced by SpaceX's SpaceX Dragon 2 and Boeing's CST-100 Starliner. Dragon 2's first crewed flight occurred on May 30, 2020. The Shuttle's heavy cargo transport role is to be replaced by expendable rockets such as the Space Launch System and ULA's Vulcan rocket, as well as the commercial launch vehicles.
Scaled Composites' SpaceShipOne was a reusable suborbital spaceplane that carried pilots Mike Melvill and Brian Binnie on consecutive flights in 2004 to win the Ansari X Prize. The Spaceship Company will build its successor SpaceShipTwo. A fleet of SpaceShipTwos operated by Virgin Galactic was planned to begin reusable private spaceflight carrying paying passengers in 2014, but was delayed after the crash of VSS Enterprise.
A spacecraft astrionics system comprises different subsystems, depending on the mission profile. Spacecraft subsystems comprise the spacecraft's "bus" and may include attitude determination and control (variously called ADAC, ADC, or ACS), guidance, navigation and control (GNC or GN&amp;C), communications (comms), command and data handling (CDH or C&amp;DH), power (EPS), thermal control (TCS), propulsion, and structures. Attached to the bus are typically payloads.

An aircraft is a vehicle or machine that is able to fly by gaining support from the air. It counters the force of gravity by using either static lift or by using the dynamic lift of an airfoil, or in a few cases the downward thrust from jet engines. Common examples of aircraft include airplanes, helicopters, airships (including blimps), gliders, paramotors, and hot air balloons.
The human activity that surrounds aircraft is called aviation. The science of aviation, including designing and building aircraft, is called aeronautics. Crewed aircraft are flown by an onboard pilot, but unmanned aerial vehicles may be remotely controlled or self-controlled by onboard computers. Aircraft may be classified by different criteria, such as lift type, aircraft propulsion, usage and others.
Flying model craft and stories of manned flight go back many centuries; however, the first manned ascent — and safe descent — in modern times took place by larger hot-air balloons developed in the 18th century. Each of the two World Wars led to great technical advances. Consequently, the history of aircraft can be divided into five eras:
Aerostats use buoyancy to float in the air in much the same way that ships float on the water. They are characterized by one or more large cells or canopies, filled with a relatively low-density gas such as helium, hydrogen, or hot air, which is less dense than the surrounding air. When the weight of this is added to the weight of the aircraft structure, it adds up to the same weight as the air that the craft displaces.
Small hot-air balloons, called sky lanterns, were first invented in ancient China prior to the 3rd century BC and used primarily in cultural celebrations, and were only the second type of aircraft to fly, the first being kites, which were first invented in ancient China over two thousand years ago. (See Han Dynasty)
A balloon was originally any aerostat, while the term airship was used for large, powered aircraft designs — usually fixed-wing. In 1919, Frederick Handley Page was reported as referring to "ships of the air," with smaller passenger types as "Air yachts." In the 1930s, large intercontinental flying boats were also sometimes referred to as "ships of the air" or "flying-ships". — though none had yet been built. The advent of powered balloons, called dirigible balloons, and later of rigid hulls allowing a great increase in size, began to change the way these words were used. Huge powered aerostats, characterized by a rigid outer framework and separate aerodynamic skin surrounding the gas bags, were produced, the Zeppelins being the largest and most famous. There were still no fixed-wing aircraft or non-rigid balloons large enough to be called airships, so "airship" came to be synonymous with these aircraft. Then several accidents, such as the Hindenburg disaster in 1937, led to the demise of these airships. Nowadays a "balloon" is an unpowered aerostat and an "airship" is a powered one.
A powered, steerable aerostat is called a dirigible. Sometimes this term is applied only to non-rigid balloons, and sometimes dirigible balloon is regarded as the definition of an airship (which may then be rigid or non-rigid). Non-rigid dirigibles are characterized by a moderately aerodynamic gasbag with stabilizing fins at the back. These soon became known as blimps. During World War II, this shape was widely adopted for tethered balloons; in windy weather, this both reduces the strain on the tether and stabilizes the balloon. The nickname blimp was adopted along with the shape. In modern times, any small dirigible or airship is called a blimp, though a blimp may be unpowered as well as powered.
Heavier-than-air aircraft, such as airplanes, must find some way to push air or gas downwards so that a reaction occurs (by Newton's laws of motion) to push the aircraft upwards. This dynamic movement through the air is the origin of the term. There are two ways to produce dynamic upthrust — aerodynamic lift, and powered lift in the form of engine thrust.
Aerodynamic lift involving wings is the most common, with fixed-wing aircraft being kept in the air by the forward movement of wings, and rotorcraft by spinning wing-shaped rotors sometimes called rotary wings. A wing is a flat, horizontal surface, usually shaped in cross-section as an aerofoil. To fly, air must flow over the wing and generate lift. A flexible wing is a wing made of fabric or thin sheet material, often stretched over a rigid frame. A kite is tethered to the ground and relies on the speed of the wind over its wings, which may be flexible or rigid, fixed, or rotary.
With powered lift, the aircraft directs its engine thrust vertically downward. V/STOL aircraft, such as the Harrier Jump Jet and Lockheed Martin F-35B take off and land vertically using powered lift and transfer to aerodynamic lift in steady flight.
A pure rocket is not usually regarded as an aerodyne because it does not depend on the air for its lift (and can even fly into space); however, many aerodynamic lift vehicles have been powered or assisted by rocket motors. Rocket-powered missiles that obtain aerodynamic lift at very high speed due to airflow over their bodies are a marginal case.
The forerunner of the fixed-wing aircraft is the kite. Whereas a fixed-wing aircraft relies on its forward speed to create airflow over the wings, a kite is tethered to the ground and relies on the wind blowing over its wings to provide lift. Kites were the first kind of aircraft to fly and were invented in China around 500 BC. Much aerodynamic research was done with kites before test aircraft, wind tunnels, and computer modelling programs became available.
The first heavier-than-air craft capable of controlled free-flight were gliders. A glider designed by George Cayley carried out the first true manned, controlled flight in 1853.
The practical, powered, fixed-wing aircraft (the airplane or aeroplane) was invented by Wilbur and Orville Wright. Besides the method of propulsion, fixed-wing aircraft are in general characterized by their wing configuration. The most important wing characteristics are:
A variable geometry aircraft can change its wing configuration during flight.
A flying wing has no fuselage, though it may have small blisters or pods. The opposite of this is a lifting body, which has no wings, though it may have small stabilizing and control surfaces.
Wing-in-ground-effect vehicles are generally not considered aircraft. They "fly" efficiently close to the surface of the ground or water, like conventional aircraft during takeoff. An example is the Russian ekranoplan nicknamed the "Caspian Sea Monster". Man-powered aircraft also rely on ground effect to remain airborne with minimal pilot power, but this is only because they are so underpowered—in fact, the airframe is capable of flying higher.
Rotorcraft, or rotary-wing aircraft, use a spinning rotor with aerofoil section blades (a rotary wing) to provide lift. Types include helicopters, autogyros, and various hybrids such as gyrodynes and compound rotorcraft.
Helicopters have a rotor turned by an engine-driven shaft. The rotor pushes air downward to create lift. By tilting the rotor forward, the downward flow is tilted backward, producing thrust for forward flight. Some helicopters have more than one rotor and a few have rotors turned by gas jets at the tips.
Autogyros have unpowered rotors, with a separate power plant to provide thrust. The rotor is tilted backward. As the autogyro moves forward, air blows upward across the rotor, making it spin. This spinning increases the speed of airflow over the rotor, to provide lift. Rotor kites are unpowered autogyros, which are towed to give them forward speed or tethered to a static anchor in high-wind for kited flight.
Cyclogyros rotate their wings about a horizontal axis.
Compound rotorcraft have wings that provide some or all of the lift in forward flight. They are nowadays classified as powered lift types and not as rotorcraft. Tiltrotor aircraft (such as the Bell Boeing V-22 Osprey), tiltwing, tail-sitter, and coleopter aircraft have their rotors/propellers horizontal for vertical flight and vertical for forward flight.
The smallest aircraft are toys/recreational items, and nano aircraft.
The largest aircraft by dimensions and volume (as of 2016) is the 302 ft (92 m) long British Airlander 10, a hybrid blimp, with helicopter and fixed-wing features, and reportedly capable of speeds up to 90 mph (140 km/h; 78 kn), and an airborne endurance of two weeks with a payload of up to 22,050 lb (10,000 kg).
The largest aircraft by weight and largest regular fixed-wing aircraft ever built, as of 2016, is the Antonov An-225 Mriya. That Ukrainian-built six-engine Russian transport of the 1980s is 84 m (276 ft) long, with an 88 m (289 ft) wingspan. It holds the world payload record, after transporting 428,834 lb (194,516 kg) of goods, and has recently flown 100 t (220,000 lb) loads commercially. With a maximum loaded weight of 550–700 t (1,210,000–1,540,000 lb), it is also the heaviest aircraft built to date. It can cruise at 500 mph (800 km/h; 430 kn).
The largest military airplanes are the Ukrainian Antonov An-124 Ruslan (world's second-largest airplane, also used as a civilian transport), and American Lockheed C-5 Galaxy transport, weighing, loaded, over 380 t (840,000 lb). The 8-engine, piston/propeller Hughes H-4 Hercules "Spruce Goose" — an American World War II wooden flying boat transport with a greater wingspan (94m/260ft) than any current aircraft and a tail height equal to the tallest (Airbus A380-800 at 24.1m/78ft) — flew only one short hop in the late 1940s and never flew out of ground effect.
The largest civilian airplanes, apart from the above-noted An-225 and An-124, are the Airbus Beluga cargo transport derivative of the Airbus A300 jet airliner, the Boeing Dreamlifter cargo transport derivative of the Boeing 747 jet airliner/transport (the 747-200B was, at its creation in the 1960s, the heaviest aircraft ever built, with a maximum weight of over 400 t (880,000 lb)), and the double-decker Airbus A380 "super-jumbo" jet airliner (the world's largest passenger airliner).
The fastest recorded powered aircraft flight and fastest recorded aircraft flight of an air-breathing powered aircraft was of the NASA X-43A Pegasus, a scramjet-powered, hypersonic, lifting body experimental research aircraft, at Mach 9.6, exactly 3,292.8 m/s (11,854 km/h; 6,400.7 kn; 7,366 mph). The X-43A set that new mark, and broke its own world record of Mach 6.3, exactly 2,160.9 m/s (7,779 km/h; 4,200.5 kn; 4,834 mph), set in March 2004, on its third and final flight on 16 November 2004.
Prior to the X-43A, the fastest recorded powered airplane flight (and still the record for the fastest manned, powered airplane / fastest manned, non-spacecraft aircraft) was of the North American X-15A-2, rocket-powered airplane at Mach 6.72, or 2,304.96 m/s (8,297.9 km/h; 4,480.48 kn; 5,156.0 mph), on 3 October 1967. On one flight it reached an altitude of 354,300 ft (108,000 m).
The fastest known, production aircraft (other than rockets and missiles) currently or formerly operational (as of 2016) are:
Gliders are heavier-than-air aircraft that do not employ propulsion once airborne. Take-off may be by launching forward and downward from a high location, or by pulling into the air on a tow-line, either by a ground-based winch or vehicle, or by a powered "tug" aircraft. For a glider to maintain its forward air speed and lift, it must descend in relation to the air (but not necessarily in relation to the ground). Many gliders can "soar", i.e., gain height from updrafts such as thermal currents. The first practical, controllable example was designed and built by the British scientist and pioneer George Cayley, whom many recognise as the first aeronautical engineer. Common examples of gliders are sailplanes, hang gliders and paragliders.
Balloons drift with the wind, though normally the pilot can control the altitude, either by heating the air or by releasing ballast, giving some directional control (since the wind direction changes with altitude). A wing-shaped hybrid balloon can glide directionally when rising or falling; but a spherically shaped balloon does not have such directional control.
Kites are aircraft that are tethered to the ground or other object (fixed or mobile) that maintains tension in the tether or kite line; they rely on virtual or real wind blowing over and under them to generate lift and drag. Kytoons are balloon-kite hybrids that are shaped and tethered to obtain kiting deflections, and can be lighter-than-air, neutrally buoyant, or heavier-than-air.
Powered aircraft have one or more onboard sources of mechanical power, typically aircraft engines although rubber and manpower have also been used. Most aircraft engines are either lightweight reciprocating engines or gas turbines. Engine fuel is stored in tanks, usually in the wings but larger aircraft also have additional fuel tanks in the fuselage.
Propeller aircraft use one or more propellers (airscrews) to create thrust in a forward direction. The propeller is usually mounted in front of the power source in tractor configuration but can be mounted behind in pusher configuration. Variations of propeller layout include contra-rotating propellers and ducted fans.
Many kinds of power plant have been used to drive propellers. Early airships used man power or steam engines. The more practical internal combustion piston engine was used for virtually all fixed-wing aircraft until World War II and is still used in many smaller aircraft. Some types use turbine engines to drive a propeller in the form of a turboprop or propfan. Human-powered flight has been achieved, but has not become a practical means of transport. Unmanned aircraft and models have also used power sources such as electric motors and rubber bands.
Jet aircraft use airbreathing jet engines, which take in air, burn fuel with it in a combustion chamber, and accelerate the exhaust rearwards to provide thrust.
Different jet engine configurations include the turbojet and turbofan, sometimes with the addition of an afterburner. Those with no rotating turbomachinery include the pulsejet and ramjet. These mechanically simple engines produce no thrust when stationary, so the aircraft must be launched to flying speed using a catapult, like the V-1 flying bomb, or a rocket, for example. Other engine types include the motorjet and the dual-cycle Pratt &amp; Whitney J58.
Compared to engines using propellers, jet engines can provide much higher thrust, higher speeds and, above about 40,000 ft (12,000 m), greater efficiency. They are also much more fuel-efficient than rockets. As a consequence nearly all large, high-speed or high-altitude aircraft use jet engines.
Some rotorcraft, such as helicopters, have a powered rotary wing or rotor, where the rotor disc can be angled slightly forward so that a proportion of its lift is directed forwards. The rotor may, like a propeller, be powered by a variety of methods such as a piston engine or turbine. Experiments have also used jet nozzles at the rotor blade tips.
Aircraft are designed according to many factors such as customer and manufacturer demand, safety protocols and physical and economic constraints. For many types of aircraft the design process is regulated by national airworthiness authorities.
The key parts of an aircraft are generally divided into three categories:
The approach to structural design varies widely between different types of aircraft. Some, such as paragliders, comprise only flexible materials that act in tension and rely on aerodynamic pressure to hold their shape. A balloon similarly relies on internal gas pressure, but may have a rigid basket or gondola slung below it to carry its payload. Early aircraft, including airships, often employed flexible doped aircraft fabric covering to give a reasonably smooth aeroshell stretched over a rigid frame. Later aircraft employed semi-monocoque techniques, where the skin of the aircraft is stiff enough to share much of the flight loads. In a true monocoque design there is no internal structure left. With the recent emphasis on sustainability hemp has picked up some attention, having a way smaller carbon foot print and 10 times stronger than steel, hemp could become the standard of manufacturing in the future. 
The key structural parts of an aircraft depend on what type it is.
Lighter-than-air types are characterised by one or more gasbags, typically with a supporting structure of flexible cables or a rigid framework called its hull. Other elements such as engines or a gondola may also be attached to the supporting structure.
Heavier-than-air types are characterised by one or more wings and a central fuselage. The fuselage typically also carries a tail or empennage for stability and control, and an undercarriage for takeoff and landing. Engines may be located on the fuselage or wings. On a fixed-wing aircraft the wings are rigidly attached to the fuselage, while on a rotorcraft the wings are attached to a rotating vertical shaft. Smaller designs sometimes use flexible materials for part or all of the structure, held in place either by a rigid frame or by air pressure. The fixed parts of the structure comprise the airframe.
The avionics comprise the aircraft flight control systems and related equipment, including the cockpit instrumentation, navigation, radar, monitoring, and communications systems.
The flight envelope of an aircraft refers to its approved design capabilities in terms of airspeed, load factor and altitude. The term can also refer to other assessments of aircraft performance such as maneuverability. When an aircraft is abused, for instance by diving it at too-high a speed, it is said to be flown outside the envelope, something considered foolhardy since it has been taken beyond the design limits which have been established by the manufacturer. Going beyond the envelope may have a known outcome such as flutter or entry to a non-recoverable spin (possible reasons for the boundary).
The range is the distance an aircraft can fly between takeoff and landing, as limited by the time it can remain airborne.
For a powered aircraft the time limit is determined by the fuel load and rate of consumption.
For an unpowered aircraft, the maximum flight time is limited by factors such as weather conditions and pilot endurance. Many aircraft types are restricted to daylight hours, while balloons are limited by their supply of lifting gas. The range can be seen as the average ground speed multiplied by the maximum time in the air.
The Airbus A350-900ULR is now the longest range airliner.
Flight dynamics is the science of air vehicle orientation and control in three dimensions. The three critical flight dynamics parameters are the angles of rotation around three axes which pass through the vehicle's center of gravity, known as pitch, roll, and yaw.
Flight dynamics is concerned with the stability and control of an aircraft's rotation about each of these axes.
An aircraft that is unstable tends to diverge from its intended flight path and so is difficult to fly. A very stable aircraft tends to stay on its flight path and is difficult to maneuver. Therefore, it is important for any design to achieve the desired degree of stability. Since the widespread use of digital computers, it is increasingly common for designs to be inherently unstable and rely on computerised control systems to provide artificial stability.
A fixed wing is typically unstable in pitch, roll, and yaw. Pitch and yaw stabilities of conventional fixed wing designs require horizontal and vertical stabilisers, which act similarly to the feathers on an arrow. These stabilizing surfaces allow equilibrium of aerodynamic forces and to stabilise the flight dynamics of pitch and yaw. They are usually mounted on the tail section (empennage), although in the canard layout, the main aft wing replaces the canard foreplane as pitch stabilizer. Tandem wing and tailless aircraft rely on the same general rule to achieve stability, the aft surface being the stabilising one.
A rotary wing is typically unstable in yaw, requiring a vertical stabiliser.
A balloon is typically very stable in pitch and roll due to the way the payload is slung underneath the center of lift.
Flight control surfaces enable the pilot to control an aircraft's flight attitude and are usually part of the wing or mounted on, or integral with, the associated stabilizing surface. Their development was a critical advance in the history of aircraft, which had until that point been uncontrollable in flight.
Aerospace engineers develop control systems for a vehicle's orientation (attitude) about its center of mass. The control systems include actuators, which exert forces in various directions, and generate rotational forces or moments about the aerodynamic center of the aircraft, and thus rotate the aircraft in pitch, roll, or yaw. For example, a pitching moment is a vertical force applied at a distance forward or aft from the aerodynamic center of the aircraft, causing the aircraft to pitch up or down. Control systems are also sometimes used to increase or decrease drag, for example to slow the aircraft to a safe speed for landing.
The two main aerodynamic forces acting on any aircraft are lift supporting it in the air and drag opposing its motion. Control surfaces or other techniques may also be used to affect these forces directly, without inducing any rotation.
Aircraft permit long distance, high speed travel and may be a more fuel efficient mode of transportation in some circumstances. Aircraft have environmental and climate impacts beyond fuel efficiency considerations, however. They are also relatively noisy compared to other forms of travel and high altitude aircraft generate contrails, which experimental evidence suggests may alter weather patterns.
Aircraft are produced in several different types optimized for various uses; military aircraft, which includes not just combat types but many types of supporting aircraft, and civil aircraft, which include all non-military types, experimental and model.
A military aircraft is any aircraft that is operated by a legal or insurrectionary armed service of any type. Military aircraft can be either combat or non-combat:
Most military aircraft are powered heavier-than-air types. Other types, such as gliders and balloons, have also been used as military aircraft; for example, balloons were used for observation during the American Civil War and World War I, and military gliders were used during World War II to land troops.
Civil aircraft divide into commercial and general types, however there are some overlaps.
Commercial aircraft include types designed for scheduled and charter airline flights, carrying passengers, mail and other cargo. The larger passenger-carrying types are the airliners, the largest of which are wide-body aircraft. Some of the smaller types are also used in general aviation, and some of the larger types are used as VIP aircraft.
General aviation is a catch-all covering other kinds of private (where the pilot is not paid for time or expenses) and commercial use, and involving a wide range of aircraft types such as business jets (bizjets), trainers, homebuilt, gliders, warbirds and hot air balloons to name a few. The vast majority of aircraft today are general aviation types.
An experimental aircraft is one that has not been fully proven in flight, or that carries a Special Airworthiness Certificate, called an Experimental Certificate in United States parlance. This often implies that the aircraft is testing new aerospace technologies, though the term also refers to amateur-built and kit-built aircraft, many of which are based on proven designs.
A model aircraft is a small unmanned type made to fly for fun, for static display, for aerodynamic research or for other purposes. A scale model is a replica of some larger design.

A vehicle (from Latin: vehiculum) is a machine that transports people or cargo.  Vehicles include wagons, bicycles, motor vehicles (motorcycles, cars, trucks, buses), railed vehicles (trains, trams), watercraft (ships, boats, underwater vehicles), amphibious vehicles (screw-propelled vehicles, hovercraft),  aircraft (airplanes, helicopters, aerostats) and spacecraft.
Land vehicles are classified broadly by what is used to apply steering and drive forces against the ground: wheeled, tracked, railed or skied. ISO 3833-1977 is the standard, also internationally used in legislation, for road vehicles types, terms and definitions.
There are over 1 billion bicycles in use worldwide. In 2002 there were an estimated 590 million cars and 205 million motorcycles in service in the world. At least 500 million Chinese Flying Pigeon bicycles have been made, more than any other single model of vehicle. The most-produced model of motor vehicle is the Honda Super Cub motorcycle, having passed 60 million units in 2008. The most-produced car model is the Toyota Corolla, with at least 35 million made by 2010. The most common fixed-wing airplane is the Cessna 172, with about 44,000 having been made as of 2017. The Soviet Mil Mi-8, at 17,000, is the most-produced helicopter. The top commercial jet airliner is the Boeing 737, at about 10,000 in 2018. At around 14,000 for both, the most produced trams are the KTM-5 and Tatra T3. The most common trolleybus is ZiU-9.
Locomotion consists of a means that allows displacement with little opposition, a power source to provide the required kinetic energy and a means to control the motion, such as a brake and steering system. By far, most vehicles use wheels which employ the principle of rolling to enable displacement with very little rolling friction.
It is essential that a vehicle have a source of energy to drive it. Energy can be extracted from external sources, as in the cases of a sailboat, a solar-powered car, or an electric streetcar that uses overhead lines. Energy can also be stored, provided it can be converted on demand and the storing medium's energy density and power density are sufficient to meet the vehicle's needs.
Human power is a simple source of energy that requires nothing more than humans. Despite the fact that humans cannot exceed 500 W (0.67 hp) for meaningful amounts of time, the land speed record for human-powered vehicles (unpaced) is 133 km/h (83 mph), as of 2009 on a recumbent bicycle.
The most common type of energy source is fuel. External combustion engines can use almost anything that burns as fuel, whilst internal combustion engines and rocket engines are designed to burn a specific fuel, typically gasoline, diesel or ethanol.
Another common medium for storing energy is batteries, which have the advantages of being responsive, useful in a wide range of power levels, environmentally friendly, efficient, simple to install, and easy to maintain. Batteries also facilitate the use of electric motors, which have their own advantages. On the other hand, batteries have low energy densities, short service life, poor performance at extreme temperatures, long charging times, and difficulties with disposal (although they can usually be recycled). Like fuel, batteries store chemical energy and can cause burns and poisoning in event of an accident. Batteries also lose effectiveness with time. The issue of charge time can be resolved by swapping discharged batteries with charged ones; however, this incurs additional hardware costs and may be impractical for larger batteries. Moreover, there must be standard batteries for battery swapping to work at a gas station. Fuel cells are similar to batteries in that they convert from chemical to electrical energy, but have their own advantages and disadvantages.
Electrified rails and overhead cables are a common source of electrical energy on subways, railways, trams, and trolleybuses.
Solar energy is a more modern development, and several solar vehicles have been successfully built and tested, including Helios, a solar-powered aircraft.
Nuclear power is a more exclusive form of energy storage, currently limited to large ships and submarines, mostly military. Nuclear energy can be released by a nuclear reactor, nuclear battery, or repeatedly detonating nuclear bombs. There have been two experiments with nuclear-powered aircraft, the Tupolev Tu-119 and the Convair X-6.
Mechanical strain is another method of storing energy, whereby an elastic band or metal spring is deformed and releases energy as it is allowed to return to its ground state. Systems employing elastic materials suffer from hysteresis, and metal springs are too dense to be useful in many cases.
Flywheels store energy in a spinning mass. Because a light and fast rotor is energetically favorable, flywheels can pose a significant safety hazard. Moreover, flywheels leak energy fairly quickly and affect a vehicle's steering through the gyroscopic effect. They have been used experimentally in gyrobuses.
Wind energy is used by sailboats and land yachts as the primary source of energy. It is very cheap and fairly easy to use, the main issues being dependence on weather and upwind performance. Balloons also rely on the wind to move horizontally. Aircraft flying in the jet stream may get a boost from high altitude winds.
Compressed gas is currently an experimental method of storing energy. In this case, compressed gas is simply stored in a tank and released when necessary. Like elastics, they have hysteresis losses when gas heats up during compression.
Gravitational potential energy is a form of energy used in gliders, skis, bobsleds and numerous other vehicles that go down hill. Regenerative braking is an example of capturing kinetic energy where the brakes of a vehicle are augmented with a generator or other means of extracting energy.
When needed, the energy is taken from the source and consumed by one or more motors or engines. Sometimes there is an intermediate medium, such as the batteries of a diesel submarine.
Most motor vehicles have internal combustion engines. They are fairly cheap, easy to maintain, reliable, safe and small. Since these engines burn fuel, they have long ranges but pollute the environment. A related engine is the external combustion engine. An example of this is the steam engine. Aside from fuel, steam engines also need water, making them impractical for some purposes. Steam engines also need time to warm up, whereas IC engines can usually run right after being started, although this may not be recommended in cold conditions. Steam engines burning coal release sulfur into the air, causing harmful acid rain.
While intermittent internal combustion engines were once the primary means of aircraft propulsion, they have been largely superseded by continuous internal combustion engines: gas turbines. Turbine engines are light and, particularly when used on aircraft, efficient. On the other hand, they cost more and require careful maintenance. They can also be damaged by ingesting foreign objects, and they produce a hot exhaust. Trains using turbines are called gas turbine-electric locomotives. Examples of surface vehicles using turbines are M1 Abrams, MTT Turbine SUPERBIKE and the Millennium. Pulse jet engines are similar in many ways to turbojets, but have almost no moving parts. For this reason, they were very appealing to vehicle designers in the past; however their noise, heat and inefficiency has led to their abandonment. A historical example of the use of a pulse jet was the V-1 flying bomb. Pulse jets are still occasionally used in amateur experiments. With the advent of modern technology, the pulse detonation engine has become practical and was successfully tested on a Rutan VariEze. While the pulse detonation engine is much more efficient than the pulse jet and even turbine engines, it still suffers from extreme noise and vibration levels. Ramjets also have few moving parts, but they only work at high speed, so that their use is restricted to tip jet helicopters and high speed aircraft such as the Lockheed SR-71 Blackbird.
Rocket engines are primarily used on rockets, rocket sleds and experimental aircraft. Rocket engines are extremely powerful. The heaviest vehicle ever to leave the ground, the Saturn V rocket, was powered by five F-1 rocket engines generating a combined 180 million horsepower (134.2 gigawatt). Rocket engines also have no need to "push off" anything, a fact that the New York Times denied in error. Rocket engines can be particularly simple, sometimes consisting of nothing more than a catalyst, as in the case of a hydrogen peroxide rocket. This makes them an attractive option for vehicles such as jet packs. Despite their simplicity, rocket engines are often dangerous and susceptible to explosions. The fuel they run off may be flammable, poisonous, corrosive or cryogenic. They also suffer from poor efficiency. For these reasons, rocket engines are only used when absolutely necessary.
Electric motors are used in electric vehicles such as electric bicycles, electric scooters, small boats, subways, trains, trolleybuses, trams and experimental aircraft. Electric motors can be very efficient: over 90% efficiency is common. Electric motors can also be built to be powerful, reliable, low-maintenance and of any size. Electric motors can deliver a range of speeds and torques without necessarily using a gearbox (although it may be more economical to use one). Electric motors are limited in their use chiefly by the difficulty of supplying electricity.
Compressed gas motors have been used on some vehicles experimentally. They are simple, efficient, safe, cheap, reliable and operate in a variety of conditions. One of the difficulties met when using gas motors is the cooling effect of expanding gas. These engines are limited by how quickly they absorb heat from their surroundings. The cooling effect can, however, double as air conditioning. Compressed gas motors also lose effectiveness with falling gas pressure.
Ion thrusters are used on some satellites and spacecraft. They are only effective in a vacuum, which limits their use to spaceborne vehicles. Ion thrusters run primarily off electricity, but they also need a propellant such as caesium, or more recently xenon. Ion thrusters can achieve extremely high speeds and use little propellant; however they are power-hungry.
The mechanical energy that motors and engines produce must be converted to work by wheels, propellers, nozzles, or similar means.
Aside from converting mechanical energy into motion, wheels allow a vehicle to roll along a surface and, with the exception of railed vehicles, to be steered. Wheels are ancient technology, with specimens being discovered from over 5000 years ago. Wheels are used in a plethora of vehicles, including motor vehicles, armoured personnel carriers, amphibious vehicles, airplanes, trains, skateboards and wheelbarrows.
Nozzles are used in conjunction with almost all reaction engines. Vehicles using nozzles include jet aircraft, rockets and personal watercraft.  While most nozzles take the shape of a cone or bell, some unorthodox designs have been created such as the aerospike. Some nozzles are intangible, such as the electromagnetic field nozzle of a vectored ion thruster.
Continuous track is sometimes used instead of wheels to power land vehicles. Continuous track has the advantages of a larger contact area, easy repairs on small damage, and high maneuverability. Examples of vehicles using continuous track are tanks, snowmobiles and excavators. Two continuous tracks used together allow for steering. The largest vehicle in the world, the Bagger 288, is propelled by continuous tracks.
Propellers (as well as screws, fans and rotors) are used to move through a fluid. Propellers have been used as toys since ancient times, however it was Leonardo da Vinci who devised what was one of the earliest propeller driven vehicles, the "aerial-screw". In 1661, Toogood &amp; Hays adopted the screw for use as a ship propeller. Since then, the propeller has been tested on many terrestrial vehicles, including the Schienenzeppelin train and numerous cars. In modern times, propellers are most prevalent on watercraft and aircraft, as well as some amphibious vehicles such as hovercraft and ground-effect vehicles. Intuitively, propellers cannot work in space as there is no working fluid, however some sources have suggested that since space is never empty, a propeller could be made to work in space.
Similarly to propeller vehicles, some vehicles use wings for propulsion. Sailboats and sailplanes are propelled by the forward component of lift generated by their sails/wings. Ornithopters also produce thrust aerodynamically. Ornithopters with large rounded leading edges produce lift by leading-edge suction forces.
Paddle wheels are used on some older watercraft and their reconstructions. These ships were known as paddle steamers. Because paddle wheels simply push against the water, their design and construction is very simple. The oldest such ship in scheduled service is the Skibladner. Many pedalo boats also use paddle wheels for propulsion.
Screw-propelled vehicles are propelled by auger-like cylinders fitted with helical flanges. Because they can produce thrust on both land and water, they are commonly used on all-terrain vehicles. The ZiL-2906 was a Soviet-designed screw-propelled vehicle designed to retrieve cosmonauts from the Siberian wilderness.
All or almost all of the useful energy produced by the engine is usually dissipated as friction; so minimising frictional losses is very important in many vehicles. The main sources of friction are rolling friction and fluid drag (air drag or water drag).
Wheels have low bearing friction and pneumatic tyres give low rolling friction. Steel wheels on steel tracks are lower still.
Aerodynamic drag can be reduced by streamlined design features.
Friction is desirable and important in supplying traction to facilitate motion on land. Most land vehicles rely on friction for accelerating, decelerating and changing direction. Sudden reductions in traction can cause loss of control and accidents.
Most vehicles, with the notable exception of railed vehicles, have at least one steering mechanism. Wheeled vehicles steer by angling their front or rear wheels. The B-52 Stratofortress has a special arrangement in which all four main wheels can be angled. Skids can also be used to steer by angling them, as in the case of a snowmobile. Ships, boats, submarines, dirigibles and aeroplanes usually have a rudder for steering. On an airplane, ailerons are used to bank the airplane for directional control, sometimes assisted by the rudder.
With no power applied, most vehicles come to a stop due to friction. But it is often required to stop a vehicle faster than by friction alone: so almost all vehicles are equipped with a braking system. Wheeled vehicles are typically equipped with friction brakes, which use the friction between brake pads (stators) and brake rotors to slow the vehicle. Many airplanes have high performance versions of the same system in their landing gear for use on the ground. A Boeing 757 brake, for example, has 3 stators and 4 rotors. The Space Shuttle also uses frictional brakes on its wheels. As well as frictional brakes, hybrid/electric cars, trolleybuses and electric bicycles can also use regenerative brakes to recycle some of the vehicle's potential energy. High-speed trains sometimes use frictionless Eddy-current brakes; however widespread application of the technology has been limited by overheating and interference issues.
Aside from landing gear brakes, most large aircraft have other ways of decelerating. In aircraft, air brakes are aerodynamic surfaces that provide braking force by increasing the frontal cross section thus aerodynamic drag of the aircraft. These are usually implemented as flaps that oppose air flow when extended and are flush with aircraft when retracted. Reverse thrust is also used in many aeroplane engines. Propeller aircraft achieve reverse thrust by reversing the pitch of the propellers, while jet aircraft do so by redirecting their engine exhaust forwards. On aircraft carriers, arresting gears are used to stop an aircraft. Pilots may even apply full forward throttle on touchdown, in case the arresting gear does not catch and a go around is needed.
Parachutes are used to slow down vehicles travelling very fast. Parachutes have been used in land, air and space vehicles such as the ThrustSSC, Eurofighter Typhoon and Apollo Command Module. Some older Soviet passenger jets had braking parachutes for emergency landings. Boats use similar devices called sea anchors to maintain stability in rough seas.
To further increase the rate of deceleration or where the brakes have failed, several mechanisms can be used to stop a vehicle. Cars and rolling stock usually have hand brakes that, while designed to secure an already parked vehicle, can provide limited braking should the primary brakes fail. A secondary procedure called forward-slip is sometimes used to slow airplanes by flying at an angle, causing more drag.
Motor vehicle and trailer categories are defined according to the following international classification:
In the European Union the classifications for vehicle types are defined by:
European Community, is based on the Community's WVTA (whole vehicle type-approval) system. Under this system, manufacturers can obtain certification for a vehicle type in one Member State if it meets the EC technical requirements and then market it EU-wide with no need for further tests. Total technical harmonization already has been achieved in three vehicle categories (passenger cars, motorcycles, and tractors) and soon will extend to other vehicle categories (coaches and utility vehicles). It is essential that European car manufacturers be ensured access to as large a market as possible.
While the Community type-approval system allows manufacturers to benefit fully from internal market opportunities, worldwide technical harmonization in the context of the United Nations Economic Commission for Europe (UNECE) offers a market beyond European borders.
In many cases, it is unlawful to operate a vehicle without a license or certification. The least strict form of regulation usually limits what passengers the driver may carry or prohibits them completely (e.g., a Canadian ultra-light license without endorsements). The next level of licensing may allow passengers, but without any form of compensation or payment. A private driver's license usually has these conditions. Commercial licenses that allow the transport of passengers and cargo are more tightly regulated. The most strict form of licensing is generally reserved for school buses, hazardous materials transports and emergency vehicles.
The driver of a motor vehicle is typically required to hold a valid driver's license while driving on public lands, whereas the pilot of an aircraft must have a license at all times, regardless of where in the jurisdiction the aircraft is flying.
Vehicles are often required to be registered. Registration may be for purely legal reasons, for insurance reasons or to help law enforcement recover stolen vehicles. Toronto Police Service, for example, offers free and optional bicycle registration online. On motor vehicles, registration often takes the form of a vehicle registration plate, which makes it easy to identify a vehicle. In Russia, trucks and buses have their licence plate numbers repeated in large black letters on the back. On aircraft, a similar system is used where a tail number is painted on various surfaces. Like motor vehicles and aircraft, watercraft also have registration numbers in most jurisdictions, however the vessel name is still the primary means of identification  as has been the case since ancient times. For this reason, duplicate registration names are generally rejected. In Canada, boats with an engine power of 10 hp (7.5 kW) or greater require registration, leading to the ubiquitous "9.9 hp (7.4 kW)" engine.
Registration may be conditional on the vehicle being approved for use on public highways, as in the case of the UK and Ontario. Many US states also have requirements for vehicles operating on public highways. Aircraft have more stringent requirements, as they pose a high risk of damage to people and property in event of an accident. In the US, the FAA requires aircraft to have an airworthiness certificate. Because US aircraft must be flown for some time before they are certified, there is a provision for an experimental airworthiness certificate. FAA experimental aircraft are restricted in operation, including no overflights of populated areas, in busy airspace or with unessential passengers. Materials and parts used in FAA certified aircraft must meet the criteria set forth by the technical standard orders.
In many jurisdictions, the operator of a vehicle is legally obligated to carry safety equipment with or on them. Common examples include seat belts in cars, helmets on motorcycles and bicycles, fire extinguishers on boats, buses and airplanes and life jackets on boats and commercial aircraft. Passenger aircraft carry a great deal of safety equipment including inflatable slides are rafts, oxygen masks, oxygen tanks, life jackets, satellite beacons and first aid kits. Some equipment such as life jackets has led to debate regarding their usefulness. In the case of Ethiopian Airlines Flight 961, the life jackets saved many people but also led to many deaths when passengers inflated their vests prematurely.
There are specific real-estate arrangements made to allow vehicles to travel from one place to another. The most common arrangements are public highways, where appropriately licensed vehicles can navigate without hindrance. These highways are on public land and are maintained by the government. Similarly, toll routes are open to the public after paying a toll. These routes and the land they rest on may be government or privately owned or a combination of both. Some routes are privately owned but grant access to the public. These routes often have a warning sign stating that the government does not maintain the way. An example of this are byways in England and Wales. In Scotland, land is open to un-motorised vehicles if the land meets certain criteria. Public land is sometimes open to use by off-road vehicles. On US public land, the Bureau of Land Management (BLM) decides where vehicles may be used. Railways often pass over land not owned by the railway company. The right to this land is granted to the railway company through mechanisms such as easement. Watercraft are generally allowed to navigate public waters without restriction as long as they do not cause a disturbance. Passing through a lock, however, may require paying a toll. Despite the common law tradition Cuius est solum, eius est usque ad coelum et ad inferos of owning all the air above one's property, the US Supreme Court ruled that aircraft in the US have the right to use air above someone else's property without their consent. While the same rule generally applies in all jurisdictions, some countries such as Cuba and Russia have taken advantage of air rights on a national level to earn money. There are some areas that aircraft are barred from overflying. This is called prohibited airspace. Prohibited airspace is usually strictly enforced due to potential damage from espionage or attack. In the case of Korean Air Lines Flight 007, the airliner entered prohibited airspace over Soviet territory and was shot down as it was leaving.
For a comparison of transportation fatality rates, see: Air safety statistics.
Several different metrics used to compare and evaluate the safety of different vehicles. The main three are deaths per billion passenger-journeys, deaths per billion passenger-hours and deaths per billion passenger-kilometers.
A projectile is an object that is propelled by the application of an external force and then moves freely under the influence of gravity and air resistance. Although any objects in motion through space are projectiles, they are commonly found in warfare and sports (for example, a thrown baseball, kicked football, fired bullet, shot arrow, stone released from catapult).
In ballistics mathematical equations of motion are used to analyze projectile trajectories through launch, flight, and impact.
Blowguns and pneumatic rifles use compressed gases, while most other guns and cannons utilize expanding gases liberated by sudden chemical reactions by propellants like smokeless powder. Light-gas guns use a combination of these mechanisms.
Railguns utilize electromagnetic fields to provide a constant acceleration along the entire length of the device, greatly increasing the muzzle velocity.
Some projectiles provide propulsion during flight by means of a rocket engine or jet engine. In military terminology, a rocket is unguided, while a missile is guided. Note the two meanings of "rocket" (weapon and engine): an ICBM is a guided missile with a rocket engine.
An explosion, whether or not by a weapon, causes the debris to act as multiple high velocity projectiles. An explosive weapon or device may also be designed to produce many high velocity projectiles by the break-up of its casing; these are correctly termed fragments.
Many projectiles, e.g. shells, may carry an explosive charge or another chemical or biological substance. Aside from explosive payload, a projectile can be designed to cause special damage, e.g. fire (see also early thermal weapons), or poisoning (see also arrow poison).
In projectile motion the most important force applied to the ‘projectile’ is the propelling force, in this case the propelling forces are the muscles that act upon the ball to make it move, and the stronger the force applied, the more propelling force, which means the projectile (the ball) will travel farther. See pitching, bowling.
A projectile that does not contain an explosive charge or any other kind of payload is termed a kinetic projectile, kinetic energy weapon, kinetic energy warhead, kinetic warhead, kinetic kill vehicle or kinetic penetrator. Typical kinetic energy weapons are blunt projectiles such as rocks and round shots, pointed ones such as arrows, and somewhat pointed ones such as bullets. Among projectiles that do not contain explosives are those launched from railguns, coilguns, and mass drivers, as well as kinetic energy penetrators. All of these weapons work by attaining a high muzzle velocity, or initial velocity, generally up to hypervelocity, and collide with their targets, converting the kinetic energy associated with the relative velocity between the two objects into destructive shock waves and heat. Other types of kinetic weapons are accelerated over time by a rocket engine, or by gravity. In either case, it is this kinetic energy that destroys its target.
Some kinetic weapons for targeting objects in spaceflight are anti-satellite weapons and anti-ballistic missiles. Since in order to reach an object in orbit it is necessary to attain an extremely high velocity, their released kinetic energy alone is enough to destroy their target; explosives are not necessary. For example: the energy of TNT is 4.6 MJ/kg, and the energy of a kinetic kill vehicle with a closing speed of 10 km/s (22,000 mph) is 50 MJ/kg. For comparison, 50 MJ is equivalent to the kinetic energy of a school bus weighing 5 metric tons, traveling at 509 km/h (316 mph; 141 m/s). This saves costly weight and there is no detonation to be precisely timed. This method, however, requires direct contact with the target, which requires a more accurate trajectory. Some hit-to-kill warheads are additionally equipped with an explosive directional warhead to enhance the kill probability (e.g. Israeli Arrow missile or U.S. Patriot PAC-3).
With regard to anti-missile weapons, the Arrow missile and MIM-104 Patriot PAC-2 have explosives, while the Kinetic Energy Interceptor (KEI), Lightweight Exo-Atmospheric Projectile (LEAP, used in Aegis BMDS), and THAAD do not (see Missile Defense Agency).
A kinetic projectile can also be dropped from aircraft. This is applied by replacing the explosives of a regular bomb with a non-explosive material (e.g. concrete), for a precision hit with less collateral damage. A typical bomb has a mass of 900 kg (2,000 lb) and a speed of impact of 800 km/h (500 mph). It is also applied for training the act of dropping a bomb with explosives. This method has been used in Operation Iraqi Freedom and the subsequent military operations in Iraq by mating concrete-filled training bombs with JDAM GPS guidance kits, to attack vehicles and other relatively "soft" targets located too close to civilian structures for the use of conventional high explosive bombs.
A Prompt Global Strike may use a kinetic weapon. A kinetic bombardment may involve a projectile dropped from Earth orbit.
A hypothetical kinetic weapon that travels at a significant fraction of the speed of light, usually found in science fiction, is termed a relativistic kill vehicle (RKV).
Some projectiles stay connected by a cable to the launch equipment after launching it:
An object projected at an angle to the horizontal has both the vertical and horizontal components of velocity. The vertical component of the velocity on the y-axis is given as 




V

y


=
U
sin
⁡
θ


{\displaystyle V_{y}=U\sin \theta }

 while the horizontal component of the velocity is 




V

x


=
U
cos
⁡
θ


{\displaystyle V_{x}=U\cos \theta }

. There are various calculations for projectiles at a specific angle 



θ


{\displaystyle \theta }

:
1. Time to reach maximum height. It is symbolized as (



t


{\displaystyle t}

), which is the time taken for the projectile to reach the maximum height from the plane of projection. Mathematically, it is given as 



t
=
U
sin
⁡
θ

/

g


{\displaystyle t=U\sin \theta /g}

 where 



g


{\displaystyle g}

 = acceleration due to gravity (app 9.81 m/s²), 



U


{\displaystyle U}

 = initial velocity (m/s) and 



θ


{\displaystyle \theta }

 = angle made by the projectile with the horizontal axis.
2. Time of flight (



T


{\displaystyle T}

): this is the total time taken for the projectile to fall back to the same plane from which it was projected. Mathematically it is given as 



T
=
2
U
sin
⁡
θ

/

g


{\displaystyle T=2U\sin \theta /g}

.
3. Maximum Height (



H


{\displaystyle H}

): this is the maximum height attained by the projectile OR the maximum displacement on the vertical axis (y-axis) covered by the projectile. It is given as 



H
=

U

2



sin

2


⁡
θ

/

2
g


{\displaystyle H=U^{2}\sin ^{2}\theta /2g}

.
4. Range (



R


{\displaystyle R}

): The Range of a projectile is the horizontal distance covered (on the x-axis) by the projectile. Mathematically, 



R
=

U

2


sin
⁡
2
θ

/

g


{\displaystyle R=U^{2}\sin 2\theta /g}

. The Range is maximum when angle 



θ


{\displaystyle \theta }

 = 45°, i.e. 



sin
⁡
2
θ
=
1


{\displaystyle \sin 2\theta =1}

.

Thrust is a reaction force described quantitatively by Newton's third law. When a system expels or accelerates mass in one direction, the accelerated mass will cause a force of equal magnitude but opposite direction to be applied to that system.
The force applied on a surface in a direction perpendicular or normal to the surface is also called thrust. Force, and thus thrust, is measured using the International System of Units (SI) in newtons (symbol: N), and represents the amount needed to accelerate 1 kilogram of mass at the rate of 1 meter per second per second. In mechanical engineering, force orthogonal to the main load (such as in parallel helical gears) is referred to as static thrust.
A fixed-wing aircraft propulsion system generates forward thrust when air is pushed in the direction opposite to flight. This can be done by different means such as the spinning blades of a propeller, the propelling jet of a jet engine, or by ejecting hot gases from a rocket engine. Reverse thrust can be generated to aid braking after landing by reversing the pitch of variable-pitch propeller blades, or using a thrust reverser on a jet engine. Rotary wing aircraft use rotors and thrust vectoring V/STOL aircraft use propellers or engine thrust to support the weight of the aircraft and to provide forward propulsion.
A motorboat propeller generates thrust when it rotates and forces water backwards.
A rocket is propelled forward by a thrust equal in magnitude, but opposite in direction, to the time-rate of momentum change of the exhaust gas accelerated from the combustion chamber through the rocket engine nozzle. This is the exhaust velocity with respect to the rocket, times the time-rate at which the mass is expelled, or in mathematical terms:
Where T is the thrust generated (force), 







d

m



d

t





{\displaystyle {\frac {\mathrm {d} m}{\mathrm {d} t}}}

 is the rate of change of mass with respect to time (mass flow rate of exhaust), and v is the velocity of the exhaust gases measured relative to the rocket.
For vertical launch of a rocket the initial thrust at liftoff must be more than the weight.
Each of the three Space Shuttle Main Engines could produce a thrust of 1.8 meganewton, and each of the Space Shuttle's two Solid Rocket Boosters 14.7 MN (3,300,000 lbf), together 29.4 MN.
By contrast, the simplified Aid For EVA Rescue (SAFER) has 24 thrusters of 3.56 N (0.80 lbf) each.
In the air-breathing category, the AMT-USA AT-180 jet engine developed for radio-controlled aircraft produce 90 N (20 lbf) of thrust. The GE90-115B engine fitted on the Boeing 777-300ER, recognized by the Guinness Book of World Records as the "World's Most Powerful Commercial Jet Engine," has a thrust of 569 kN (127,900 lbf) until it was surpassed by the GE9X, fitted on the upcoming Boeing 777X, at 609 kN (134,300 lbf).
The power needed to generate thrust and the force of the thrust can be related in a non-linear way. In general, 





P


2


∝


T


3




{\displaystyle \mathbf {P} ^{2}\propto \mathbf {T} ^{3}}

. The proportionality constant varies, and can be solved for a uniform flow, where 




v

∞




{\displaystyle v_{\infty }}

 is the incoming air velocity, 




v

d




{\displaystyle v_{d}}

 is the velocity at the actuator disc, and 




v

f




{\displaystyle v_{f}}

 is the final exit velocity:
Solving for the velocity at the disc, 




v

d




{\displaystyle v_{d}}

, we then have:
When incoming air is accelerated from a standstill – for example when hovering – then 




v

∞


=
0


{\displaystyle v_{\infty }=0}

, and we can find:
From here we can see the 





P


2


∝


T


3




{\displaystyle \mathbf {P} ^{2}\propto \mathbf {T} ^{3}}

 relationship, finding:
The inverse of the proportionality constant, the "efficiency" of an otherwise-perfect thruster, is proportional to the area of the cross section of the propelled volume of fluid (



A


{\displaystyle A}

) and the density of the fluid (



ρ


{\displaystyle \rho }

). This helps to explain why moving through water is easier and why aircraft have much larger propellers than watercraft.
A very common question is how to compare the thrust rating of a jet engine with the power rating of a piston engine. Such comparison is difficult, as these quantities are not equivalent. A piston engine does not move the aircraft by itself (the propeller does that), so piston engines are usually rated by how much power they deliver to the propeller. Except for changes in temperature and air pressure, this quantity depends basically on the throttle setting.
A jet engine has no propeller, so the propulsive power of a jet engine is determined from its thrust as follows. Power is the force (F) it takes to move something over some distance (d) divided by the time (t) it takes to move that distance:
In case of a rocket or a jet aircraft, the force is exactly the thrust (T) produced by the engine. If the rocket or aircraft is moving at about a constant speed, then distance divided by time is just speed, so power is thrust times speed:
This formula looks very surprising, but it is correct: the propulsive power (or power available ) of a jet engine increases with its speed. If the speed is zero, then the propulsive power is zero. If a jet aircraft is at full throttle but attached to a static test stand, then the jet engine produces no propulsive power, however thrust is still produced. The combination piston engine–propeller also has a propulsive power with exactly the same formula, and it will also be zero at zero speed – but that is for the engine–propeller set. The engine alone will continue to produce its rated power at a constant rate, whether the aircraft is moving or not.
Now, imagine the strong chain is broken, and the jet and the piston aircraft start to move. At low speeds:
The piston engine will have constant 100% power, and the propeller's thrust will vary with speed
The jet engine will have constant 100% thrust, and the engine's power will vary with speed
If a powered aircraft is generating thrust T and experiencing drag D, the difference between the two, T − D, is termed the excess thrust. The instantaneous performance of the aircraft is mostly dependent on the excess thrust.
Excess thrust is a vector and is determined as the vector difference between the thrust vector and the drag vector.
The thrust axis for an airplane is the line of action of the total thrust at any instant. It depends on the location, number, and characteristics of the jet engines or propellers. It usually differs from the drag axis. If so, the distance between the thrust axis and the drag axis will cause a moment that must be resisted by a change in the aerodynamic force on the horizontal stabiliser. Notably, the Boeing 737 MAX, with larger, lower-slung engines than previous 737 models, had a greater distance between the thrust axis and the drag axis, causing the nose to rise up in some flight regimes, necessitating a pitch-control system, MCAS. Early versions of MCAS malfunctioned in flight with catastrophic consequences, leading to the deaths of over 300 people in 2018 and 2019.

A rocket engine uses stored rocket propellants as the reaction mass for forming a high-speed propulsive jet of fluid, usually high-temperature gas. Rocket engines are reaction engines, producing thrust by ejecting mass rearward, in accordance with Newton's third law. Most rocket engines use the combustion of reactive chemicals to supply the necessary energy, but non-combusting forms such as cold gas thrusters and nuclear thermal rockets also exist. Vehicles propelled by rocket engines are commonly called rockets. Rocket vehicles carry their own oxidiser, unlike most combustion engines, so rocket engines can be used in a vacuum to propel spacecraft and ballistic missiles.
Compared to other types of jet engine, rocket engines are the lightest and have the highest thrust, but are the least propellant-efficient (they have the lowest specific impulse). The ideal exhaust is hydrogen, the lightest of all elements, but chemical rockets produce a mix of heavier species, reducing the exhaust velocity.
Rocket engines become more efficient at high speeds, due to the Oberth effect.
Here, "rocket" is used as an abbreviation for "rocket engine".
Thermal rockets use an inert propellant, heated by electricity (electrothermal propulsion) or a nuclear reactor (nuclear thermal rocket).
Chemical rockets are powered by exothermic reduction-oxidation chemical reactions of the propellant:
Rocket engines produce thrust by the expulsion of an exhaust fluid that has been accelerated to high speed through a propelling nozzle. The fluid is usually a gas created by high pressure (150-to-4,350-pound-per-square-inch (10 to 300 bar)) combustion of solid or liquid propellants, consisting of fuel and oxidiser components, within a combustion chamber. As the gases expand through the nozzle, they are accelerated to very high (supersonic) speed, and the reaction to this pushes the engine in the opposite direction. Combustion is most frequently used for practical rockets, as the laws of thermodynamics (specifically Carnot's theorem) dictate that high temperatures and pressures are desirable for the best thermal efficiency. Nuclear thermal rockets are capable of higher efficiencies, but currently have environmental problems which preclude their routine use in the Earth's atmosphere and cislunar space.
For model rocketry, an available alternative to combustion is the water rocket pressurized by compressed air, carbon dioxide, nitrogen, or any other readily available, inert gas.
Rocket propellant is mass that is stored, usually in some form of tank, or within the combustion chamber itself, prior to being ejected from a rocket engine in the form of a fluid jet to produce thrust.
Chemical rocket propellants are the most commonly used. These undergo exothermic chemical reactions  producing a hot gas jet for propulsion. Alternatively, a chemically inert reaction mass can be heated by a high-energy power source through a heat exchanger in lieu of a combustion chamber.
Solid rocket propellants are prepared in a mixture of fuel and oxidising components called grain, and the propellant storage casing effectively becomes the combustion chamber.
Liquid-fuelled rockets force separate fuel and oxidiser components into the combustion chamber, where they mix and burn. Hybrid rocket engines use a combination of solid and liquid or gaseous propellants. Both liquid and hybrid rockets use injectors to introduce the propellant into the chamber. These are often an array of simple jets – holes through which the propellant escapes under pressure; but sometimes may be more complex spray nozzles. When two or more propellants are injected, the jets usually deliberately cause the propellants to collide as this breaks up the flow into smaller droplets that burn more easily.
For chemical rockets the combustion chamber is typically cylindrical, and flame holders, used to hold a part of the combustion in a slower-flowing portion of the combustion chamber, are not needed. The dimensions of the cylinder are such that the propellant is able to combust thoroughly; different rocket propellants require different combustion chamber sizes for this to occur.
This leads to a number called 




L

∗




{\displaystyle L^{*}}

, the characteristic length:
where:
L* is typically in the range of 64–152 centimetres (25–60 in).
The temperatures and pressures typically reached in a rocket combustion chamber in order to achieve practical thermal efficiency are extreme compared to a non-afterburning airbreathing jet engine. No atmospheric nitrogen is present to dilute and cool the combustion, so the propellant mixture can reach true stoichiometric ratios. This, in combination with the high pressures, means that the rate of heat conduction through the walls is very high.
In order for fuel and oxidiser to flow into the chamber, the pressure of the propellants entering the combustion chamber must exceed the pressure inside the combustion chamber itself.  This may be accomplished by a variety of design approaches including turbopumps or, in simpler engines, via sufficient tank pressure to advance fluid flow. Tank pressure may be maintained by several means, including a high-pressure helium pressurization system common to many large rocket engines or, in some newer rocket systems, by a bleed-off of high-pressure gas from the engine cycle to autogenously pressurize the propellant tanks  For example, the self-pressurization gas system of the SpaceX Starship is a critical part of SpaceX strategy to reduce launch vehicle fluids from five in their legacy Falcon 9 vehicle family to just two in Starship, eliminating not only the helium tank pressurant but all hypergolic propellants as well as nitrogen for cold-gas reaction-control thrusters.
The hot gas produced in the combustion chamber is permitted to escape through an opening (the "throat"), and then through a diverging expansion section. When sufficient pressure is provided to the nozzle (about 2.5–3 times ambient pressure), the nozzle chokes and a supersonic jet is formed, dramatically accelerating the gas, converting most of the thermal energy into kinetic energy. Exhaust speeds vary, depending on the expansion ratio the nozzle is designed for, but exhaust speeds as high as ten times the speed of sound in air at sea level are not uncommon. About half of the rocket engine's thrust comes from the unbalanced pressures inside the combustion chamber, and the rest comes from the pressures acting against the inside of the nozzle (see diagram). As the gas expands (adiabatically) the pressure against the nozzle's walls forces the rocket engine in one direction while accelerating the gas in the other.

The most commonly used nozzle is the de Laval nozzle, a fixed geometry nozzle with a high expansion-ratio. The large bell- or cone-shaped nozzle extension beyond the throat gives the rocket engine its characteristic shape.
The exit static pressure of the exhaust jet depends on the chamber pressure and the ratio of exit to throat area of the nozzle. As exit pressure varies from the ambient (atmospheric) pressure, a choked nozzle is said to be
In practice, perfect expansion is only achievable with a variable-exit area nozzle (since ambient pressure decreases as altitude increases), and is not possible above a certain altitude as ambient pressure approaches zero. If the nozzle is not perfectly expanded, then loss of efficiency occurs. Grossly over-expanded nozzles lose less efficiency, but can cause mechanical problems with the nozzle. Fixed-area nozzles become progressively more under-expanded as they gain altitude. Almost all de Laval nozzles will be momentarily grossly over-expanded during startup in an atmosphere.
Nozzle efficiency is affected by operation in the atmosphere because atmospheric pressure changes with altitude; but due to the supersonic speeds of the gas exiting from a rocket engine, the pressure of the jet may be either below or above ambient, and equilibrium between the two is not reached at all altitudes (see diagram).
For optimal performance, the pressure of the gas at the end of the nozzle should just equal the ambient pressure: if the exhaust's pressure is lower than the ambient pressure, then the vehicle will be slowed by the difference in pressure between the top of the engine and the exit; on the other hand, if the exhaust's pressure is higher, then exhaust pressure that could have been converted into thrust is not converted, and energy is wasted.
To maintain this ideal of equality between the exhaust's exit pressure and the ambient pressure, the diameter of the nozzle would need to increase with altitude, giving the pressure a longer nozzle to act on (and reducing the exit pressure and temperature). This increase is difficult to arrange in a lightweight fashion, although is routinely done with other forms of jet engines. In rocketry a lightweight compromise nozzle is generally used and some reduction in atmospheric performance occurs when used at other than the 'design altitude' or when throttled. To improve on this, various exotic nozzle designs such as the plug nozzle, stepped nozzles, the expanding nozzle and the aerospike have been proposed, each providing some way to adapt to changing ambient air pressure and each allowing the gas to expand further against the nozzle, giving extra thrust at higher altitudes.
When exhausting into a sufficiently low ambient pressure (vacuum) several issues arise. One is the sheer weight of the nozzle—beyond a certain point, for a particular vehicle, the extra weight of the nozzle outweighs any performance gained. Secondly, as the exhaust gases adiabatically expand within the nozzle they cool, and eventually some of the chemicals can freeze, producing 'snow' within the jet. This causes instabilities in the jet and must be avoided.
On a de Laval nozzle, exhaust gas flow detachment will occur in a grossly over-expanded nozzle.  As the detachment point will not be uniform around the axis of the engine, a side force may be imparted to the engine. This side force may change over time and result in control problems with the launch vehicle.
Advanced altitude-compensating designs, such as the aerospike or plug nozzle, attempt to minimize performance losses by adjusting to varying expansion ratio caused by changing altitude.
For a rocket engine to be propellant efficient, it is important that the maximum pressures possible be created on the walls of the chamber and nozzle by a specific amount of propellant; as this is the source of the thrust. This can be achieved by all of:
Since all of these things minimise the mass of the propellant used, and since pressure is proportional to the mass of propellant present to be accelerated as it pushes on the engine, and since from Newton's third law the pressure that acts on the engine also reciprocally acts on the propellant, it turns out that for any given engine, the speed that the propellant leaves the chamber is unaffected by the chamber pressure (although the thrust is proportional). However, speed is significantly affected by all three of the above factors and the exhaust speed is an excellent measure of the engine propellant efficiency. This is termed exhaust velocity, and after allowance is made for factors that can reduce it, the effective exhaust velocity is one of the most important parameters of a rocket engine (although weight, cost, ease of manufacture etc. are usually also very important).
For aerodynamic reasons the flow goes sonic ("chokes") at the narrowest part of the nozzle, the 'throat'. Since the speed of sound in gases increases with the square root of temperature, the use of hot exhaust gas greatly improves performance. By comparison, at room temperature the speed of sound in air is about 340 m/s while the speed of sound in the hot gas of a rocket engine can be over 1700 m/s; much of this performance is due to the higher temperature, but additionally rocket propellants are chosen to be of low molecular mass, and this also gives a higher velocity compared to air.
Expansion in the rocket nozzle then further multiplies the speed, typically between 1.5 and 2 times, giving a highly collimated hypersonic exhaust jet. The speed increase of a rocket nozzle is mostly determined by its area expansion ratio—the ratio of the area of the exit to the area of the throat, but detailed properties of the gas are also important. Larger ratio nozzles are more massive but are able to extract more heat from the combustion gases, increasing the exhaust velocity.
Vehicles typically require the overall thrust to change direction over the length of the burn. A number of different ways to achieve this have been flown:
Rocket technology can combine very high thrust (meganewtons), very high exhaust speeds (around 10 times the speed of sound in air at sea level) and very high thrust/weight ratios (&gt;100) simultaneously as well as being able to operate outside the atmosphere, and while permitting the use of low pressure and hence lightweight tanks and structure.
Rockets can be further optimised to even more extreme performance along one or more of these axes at the expense of the others.
The most important metric for the efficiency of a rocket engine is impulse per unit of propellant, this is called specific impulse (usually written 




I

s
p




{\displaystyle I_{sp}}

). This is either measured as a speed (the effective exhaust velocity 




v

e




{\displaystyle v_{e}}

 in metres/second or ft/s) or as a time (seconds). For example, if an engine producing 100 pounds of thrust runs for 320 seconds and burns 100 pounds of propellant, then the specific impulse is 320 seconds. The higher the specific impulse, the less propellant is required to provide the desired impulse.
The specific impulse that can be achieved is primarily a function of the propellant mix (and ultimately would limit the specific impulse), but practical limits on chamber pressures and the nozzle expansion ratios reduce the performance that can be achieved.
Below is an approximate equation for calculating the net thrust of a rocket engine:
Since, unlike a jet engine, a conventional rocket motor lacks an air intake, there is no 'ram drag' to deduct from the gross thrust. Consequently, the net thrust of a rocket motor is equal to the gross thrust (apart from static back pressure).
The 






m
˙





v

e
−
o
p
t





{\displaystyle {\dot {m}}\;v_{e-opt}\,}

 term represents the momentum thrust, which remains constant at a given throttle setting, whereas the 




A

e


(

p

e


−

p

a
m
b


)



{\displaystyle A_{e}(p_{e}-p_{amb})\,}

 term represents the pressure thrust term. At full throttle, the net thrust of a rocket motor improves slightly with increasing altitude, because as atmospheric pressure decreases with altitude, the pressure thrust term increases. At the surface of the Earth the pressure thrust may be reduced by up to 30%, depending on the engine design. This reduction drops roughly exponentially to zero with increasing altitude.
Maximum efficiency for a rocket engine is achieved by maximising the momentum contribution of the equation without incurring penalties from over expanding the exhaust. This occurs when 




p

e


=

p

a
m
b




{\displaystyle p_{e}=p_{amb}}

. Since ambient pressure changes with altitude, most rocket engines spend very little time operating at peak efficiency.
Since specific impulse is force divided by the rate of mass flow, this equation means that the specific impulse varies with altitude.
Due to the specific impulse varying with pressure, a quantity that is easy to compare and calculate with is useful. Because rockets choke at the throat, and because the supersonic exhaust prevents external pressure influences travelling upstream, it turns out that the pressure at the exit is ideally exactly proportional to the propellant flow 






m
˙





{\displaystyle {\dot {m}}}

, provided the mixture ratios and combustion efficiencies are maintained. It is thus quite usual to rearrange the above equation slightly:
and so define the vacuum Isp to be:
where:
And hence:
Rockets can be throttled by controlling the propellant combustion rate 






m
˙





{\displaystyle {\dot {m}}}

 (usually measured in kg/s or lb/s). In liquid and hybrid rockets, the propellant flow entering the chamber is controlled using valves, in solid rockets it is controlled by changing the area of propellant that is burning and this can be designed into the propellant grain (and hence cannot be controlled in real-time).
Rockets can usually be throttled down to an exit pressure of about one-third of ambient pressure (often limited by flow separation in nozzles) and up to a maximum limit determined only by the mechanical strength of the engine.
In practice, the degree to which rockets can be throttled varies greatly, but most rockets can be throttled by a factor of 2 without great difficulty; the typical limitation is combustion stability, as for example, injectors need a minimum pressure to avoid triggering damaging oscillations (chugging or combustion instabilities); but injectors can be optimised and tested for wider ranges.
For example, some more recent liquid-propellant engine designs that have been optimised for greater throttling capability (BE-3, Raptor) can be throttled to as low as 18–20 percent of rated thrust.
Solid rockets can be throttled by using shaped grains that will vary their surface area over the course of the burn.
Rocket engine nozzles are surprisingly efficient heat engines for generating a high speed jet, as a consequence of the high combustion temperature and high compression ratio. Rocket nozzles give an excellent approximation to adiabatic expansion which is a reversible process, and hence they give efficiencies which are very close to that of the Carnot cycle. Given the temperatures reached, over 60% efficiency can be achieved with chemical rockets.
For a vehicle employing a rocket engine the energetic efficiency is very good if the vehicle speed approaches or somewhat exceeds the exhaust velocity (relative to launch); but at low speeds the energy efficiency goes to 0% at zero speed (as with all jet propulsion). See Rocket energy efficiency for more details.
Rockets, of all the jet engines, indeed of essentially all engines, have the highest thrust to weight ratio. This is especially true for liquid rocket engines.
This high performance is due to the small volume of pressure vessels that make up the engine—the pumps, pipes and combustion chambers involved. The lack of inlet duct and the use of dense liquid propellant allows the pressurisation system to be small and lightweight, whereas duct engines have to deal with air which has around three orders of magnitude lower density.
Of the liquid propellants used, density is lowest for liquid hydrogen. Although this propellant has the highest specific impulse, its very low density (about one fourteenth that of water) requires larger and heavier turbopumps and pipework, which decreases the engine's thrust-to-weight ratio (for example the RS-25) compared to those that do not (NK-33).
For efficiency reasons, higher temperatures are desirable, but materials lose their strength if the temperature becomes too high. Rockets run with combustion temperatures that can reach 3,500 K (3,200 °C; 5,800 °F).
Most other jet engines have gas turbines in the hot exhaust. Due to their larger surface area, they are harder to cool and hence there is a need to run the combustion processes at much lower temperatures, losing efficiency. In addition, duct engines use air as an oxidant, which contains 78% largely unreactive nitrogen, which dilutes the reaction and lowers the temperatures. Rockets have none of these inherent combustion temperature limiters.
The temperatures reached by combustion in rocket engines often substantially exceed the melting points of the nozzle and combustion chamber materials (about 1,200 K for copper). Most construction materials will also combust if exposed to high temperature oxidiser, which leads to a number of design challenges. The nozzle and combustion chamber walls must not be allowed to combust, melt, or vaporize (sometimes facetiously termed an "engine-rich exhaust").
Rockets that use the common construction materials such as aluminium, steel, nickel or copper alloys must employ cooling systems to limit the temperatures that engine structures experience. Regenerative cooling, where the propellant is passed through tubes around the combustion chamber or nozzle, and other techniques, such as curtain cooling or film cooling, are employed to give longer nozzle and chamber life. These techniques ensure that a gaseous thermal boundary layer touching the material is kept below the temperature which would cause the material to catastrophically fail.
Two material exceptions that can directly sustain rocket combustion temperatures are graphite and tungsten, although both are subject to oxidation if not protected. Materials technology, combined with the engine design, is a limiting factor in chemical rockets.
In rockets, the heat fluxes that can pass through the wall are among the highest in engineering; fluxes are generally in the range of 100–200 MW/m2. The strongest heat fluxes are found at the throat, which often sees twice that found in the associated chamber and nozzle. This is due to the combination of high speeds (which gives a very thin boundary layer), and although lower than the chamber, the high temperatures seen there. (See § Nozzle above for temperatures in nozzle).
In rockets the coolant methods include
In all cases the cooling effect that prevents the wall from being destroyed is caused by a thin layer of insulating fluid (a boundary layer) that is in contact with the walls that is far cooler than the combustion temperature. Provided this boundary layer is intact the wall will not be damaged.
Disruption of the boundary layer may occur during cooling failures or combustion instabilities, and wall failure typically occurs soon after.
With regenerative cooling a second boundary layer is found in the coolant channels around the chamber. This boundary layer thickness needs to be as small as possible, since the boundary layer acts as an insulator between the wall and the coolant. This may be achieved by making the coolant velocity in the channels as high as possible.
In practice, regenerative cooling is nearly always used in conjunction with curtain cooling and/or film cooling.
Liquid-fuelled engines are often run fuel-rich, which lowers combustion temperatures.  This reduces heat loads on the engine and allows lower cost materials and a simplified cooling system. This can also increase performance by lowering the average molecular weight of the exhaust and increasing the efficiency with which combustion heat is converted to kinetic exhaust energy.
Rocket combustion chambers are normally operated at fairly high pressure, typically 10–200 bar (1–20 MPa, 150–3,000 psi). When operated within significant atmospheric pressure, higher combustion chamber pressures give better performance by permitting a larger and more efficient nozzle to be fitted without it being grossly overexpanded.
However, these high pressures cause the outermost part of the chamber to be under very large hoop stresses – rocket engines are pressure vessels.
Worse, due to the high temperatures created in rocket engines the materials used tend to have a significantly lowered working tensile strength.
In addition, significant temperature gradients are set up in the walls of the chamber and nozzle, these cause differential expansion of the inner liner that create internal stresses.
The extreme vibration and acoustic environment inside a rocket motor commonly result in peak stresses well above mean values, especially in the presence of organ pipe-like resonances and gas turbulence.
The combustion may display undesired instabilities, of sudden or periodic nature. The pressure in the injection chamber may increase until the propellant flow through the injector plate decreases; a moment later the pressure drops and the flow increases, injecting more propellant in the combustion chamber which burns a moment later, and again increases the chamber pressure, repeating the cycle. This may lead to high-amplitude pressure oscillations, often in ultrasonic range, which may damage the motor. Oscillations of ±200 psi at 25 kHz were the cause of failures of early versions of the Titan II missile second stage engines. The other failure mode is a deflagration to detonation transition; the supersonic pressure wave formed in the combustion chamber may destroy the engine.
Combustion instability was also a problem during Atlas development. The Rocketdyne engines used in the Atlas family were found to suffer from this effect in several static firing tests, and three missile launches exploded on the pad due to rough combustion in the booster engines. In most cases, it occurred while attempting to start the engines with a "dry start" method whereby the igniter mechanism would be activated prior to propellant injection. During the process of man-rating Atlas for Project Mercury, solving combustion instability was a high priority, and the final two Mercury flights sported an upgraded propulsion system with baffled injectors and a hypergolic igniter.
The problem affecting Atlas vehicles was mainly the so-called "racetrack" phenomenon, where burning propellant would swirl around in a circle at faster and faster speeds, eventually producing vibration strong enough to rupture the engine, leading to complete destruction of the rocket. It was eventually solved by adding several baffles around the injector face to break up swirling propellant.
More significantly, combustion instability was a problem with the Saturn F-1 engines. Some of the early units tested exploded during static firing, which led to the addition of injector baffles.
In the Soviet space program, combustion instability also proved a problem on some rocket engines, including the RD-107 engine used in the R-7 family and the RD-216 used in the R-14 family, and several failures of these vehicles occurred before the problem was solved. Soviet engineering and manufacturing processes never satisfactorily resolved combustion instability in larger RP-1/LOX engines, so the RD-171 engine used to power the Zenit family still used four smaller thrust chambers fed by a common engine mechanism.
The combustion instabilities can be provoked by remains of cleaning solvents in the engine (e.g. the first attempted launch of a Titan II in 1962), reflected shock wave, initial instability after ignition, explosion near the nozzle that reflects into the combustion chamber, and many more factors. In stable engine designs the oscillations are quickly suppressed; in unstable designs they persist for prolonged periods. Oscillation suppressors are commonly used.
Periodic variations of thrust, caused by combustion instability or longitudinal vibrations of structures between the tanks and the engines which modulate the propellant flow, are known as "pogo oscillations" or "pogo", named after the pogo stick.
Three different types of combustion instabilities occur:
This is a low frequency oscillation at a few Hertz in chamber pressure usually caused by pressure variations in feed lines due to variations in acceleration of the vehicle.: 261  
This can cause cyclic variation in thrust, and the effects can vary from merely annoying to actually damaging the payload or vehicle. Chugging can be minimised by using gas-filled damping tubes on feed lines of high density propellants.
This can be caused due to insufficient pressure drop across the injectors.: 261  
It generally is mostly annoying, rather than being damaging. However, in extreme cases combustion can end up being forced backwards through the injectors – this can cause explosions with monopropellants.
This is the most immediately damaging, and the hardest to control. It is due to acoustics within the combustion chamber that often couples to the chemical combustion processes that are the primary drivers of the energy release, and can lead to unstable resonant "screeching" that commonly leads to catastrophic failure due to thinning of the insulating thermal boundary layer. Acoustic oscillations can be excited by thermal processes, such as the flow of hot air through a pipe or combustion in a chamber. Specifically, standing acoustic waves inside a chamber can be intensified if combustion occurs more intensely in regions where the pressure of the acoustic wave is maximal. 
Such effects are very difficult to predict analytically during the design process, and have usually been addressed by expensive, time-consuming and extensive testing, combined with trial and error remedial correction measures.
Screeching is often dealt with by detailed changes to injectors, or changes in the propellant chemistry, or vaporising the propellant before injection, or use of Helmholtz dampers within the combustion chambers to change the resonant modes of the chamber.
Testing for the possibility of screeching is sometimes done by exploding small explosive charges outside the combustion chamber with a tube set tangentially to the combustion chamber near the injectors to determine the engine's impulse response and then evaluating the time response of the chamber pressure- a fast recovery indicates a stable system.
For all but the very smallest sizes, rocket exhaust compared to other engines is generally very noisy. As the hypersonic exhaust mixes with the ambient air, shock waves are formed. The Space Shuttle generated over 200 dB(A) of noise around its base. To reduce this, and the risk of payload damage or injury to the crew atop the stack, the mobile launcher platform was fitted with a Sound Suppression System that sprayed 1.1 million litres (290,000 US gal) of water around the base of the rocket in 41 seconds at launch time. Using this system kept sound levels within the payload bay to 142 dB.
The sound intensity from the shock waves generated depends on the size of the rocket and on the exhaust velocity. Such shock waves seem to account for the characteristic crackling and popping sounds produced by large rocket engines when heard live. These noise peaks typically overload microphones and audio electronics, and so are generally weakened or entirely absent in recorded or broadcast audio reproductions. For large rockets at close range, the acoustic effects could actually kill.
More worryingly for space agencies, such sound levels can also damage the launch structure, or worse, be reflected back at the comparatively delicate rocket above. This is why so much water is typically used at launches. The water spray changes the acoustic qualities of the air and reduces or deflects the sound energy away from the rocket.
Generally speaking, noise is most intense when a rocket is close to the ground, since the noise from the engines radiates up away from the jet, as well as reflecting off the ground. Also, when the vehicle is moving slowly, little of the chemical energy input to the engine can go into increasing the kinetic energy of the rocket (since useful power P transmitted to the vehicle is 



P
=
F
∗
V


{\displaystyle P=F*V}

 for thrust F and speed V). Then the largest portion of the energy is dissipated in the exhaust's interaction with the ambient air, producing noise. This noise can be reduced somewhat by flame trenches with roofs, by water injection around the jet and by deflecting the jet at an angle.
Rocket engines are usually statically tested at a test facility before being put into production. For high altitude engines, either a shorter nozzle must be used, or the rocket must be tested in a large vacuum chamber.
Rocket vehicles have a reputation for unreliability and danger; especially catastrophic failures. Contrary to this reputation, carefully designed rockets can be made arbitrarily reliable. In military use, rockets are not unreliable. However, one of the main non-military uses of rockets is for orbital launch. In this application, the premium has typically been placed on minimum weight, and it is difficult to achieve high reliability and low weight simultaneously. In addition, if the number of flights launched is low, there is a very high chance of a design, operations or manufacturing error causing destruction of the vehicle.
The Rocketdyne H-1 engine, used in a cluster of eight in the first stage of the Saturn I and Saturn IB launch vehicles, had no catastrophic failures in 152 engine-flights. The Pratt and Whitney RL10 engine, used in a cluster of six in the Saturn I second stage, had no catastrophic failures in 36 engine-flights. The Rocketdyne F-1 engine, used in a cluster of five in the first stage of the Saturn V, had no failures in 65 engine-flights. The Rocketdyne J-2 engine, used in a cluster of five in the Saturn V second stage, and singly in the Saturn IB second stage and Saturn V third stage, had no catastrophic failures in 86 engine-flights.
The Space Shuttle Solid Rocket Booster, used in pairs, caused one notable catastrophic failure in 270 engine-flights.
The RS-25, used in a cluster of three, flew in 46 refurbished engine units. These made a total of 405 engine-flights with no catastrophic in-flight failures. A single in-flight RS-25 engine failure occurred during Space Shuttle Challenger's STS-51-F mission. This failure had no effect on mission objectives or duration.
Rocket propellants require a high energy per unit mass (specific energy), which must be balanced against the tendency of highly energetic propellants to spontaneously explode. Assuming that the chemical potential energy of the propellants can be safely stored, the combustion process results in a great deal of heat being released. A significant fraction of this heat is transferred to kinetic energy in the engine nozzle, propelling the rocket forward in combination with the mass of combustion products released.
Ideally all the reaction energy appears as kinetic energy of the exhaust gases, as exhaust velocity is the single most important performance parameter of an engine. However, real exhaust species are molecules, which typically have translation, vibrational, and rotational modes with which to dissipate energy. Of these, only translation can do useful work to the vehicle, and while energy does transfer between modes this process occurs on a timescale far in excess of the time required for the exhaust to leave the nozzle.
The more chemical bonds an exhaust molecule has, the more rotational and vibrational modes it will have. Consequently, it is generally desirable for the exhaust species to be as simple as possible, with a diatomic molecule composed of light, abundant atoms such as H2 being ideal in practical terms. However, in the case of a chemical rocket, hydrogen is a reactant and reducing agent, not a product. An oxidizing agent, most typically oxygen or an oxygen-rich species, must be introduced into the combustion process, adding mass and chemical bonds to the exhaust species.
An additional advantage of light molecules is that they may be accelerated to high velocity at temperatures that can be contained by currently available materials - the high gas temperatures in rocket engines pose serious problems for the engineering of survivable motors.
Liquid hydrogen (LH2) and oxygen (LOX, or LO2), are the most effective propellants in terms of exhaust velocity that have been widely used to date, though a few exotic combinations involving boron or liquid ozone are potentially somewhat better in theory if various practical problems could be solved.
It is important to note that, when computing the specific reaction energy of a given propellant combination, the entire mass of the propellants (both fuel and oxidiser) must be included. The exception is in the case of air-breathing engines, which use atmospheric oxygen and consequently have to carry less mass for a given energy output. Fuels for car or turbojet engines have a much better effective energy output per unit mass of propellant that must be carried, but are similar per unit mass of fuel.
Computer programs that predict the performance of propellants in rocket engines are available.
With liquid and hybrid rockets, immediate ignition of the propellants as they first enter the combustion chamber is essential.
With liquid propellants (but not gaseous), failure to ignite within milliseconds usually causes too much liquid propellant to be inside the chamber, and if/when ignition occurs the amount of hot gas created can exceed the maximum design pressure of the chamber, causing a catastrophic failure of the pressure vessel.
This is sometimes called a hard start or a rapid unscheduled disassembly (RUD).
Ignition can be achieved by a number of different methods; a pyrotechnic charge can be used, a plasma torch can be used, or electric spark ignition may be employed. Some fuel/oxidiser combinations ignite on contact (hypergolic), and non-hypergolic fuels can be "chemically ignited" by priming the fuel lines with hypergolic propellants (popular in Russian engines).
Gaseous propellants generally will not cause hard starts, with rockets the total injector area is less than the throat thus the chamber pressure tends to ambient prior to ignition and high pressures cannot form even if the entire chamber is full of flammable gas at ignition.
Solid propellants are usually ignited with one-shot pyrotechnic devices and combustion usually proceeds through total consumption of the propellants.
Once ignited, rocket chambers are self-sustaining and igniters are not needed and combustion usually proceeds through total consumption of the propellants.  Indeed, chambers often spontaneously reignite if they are restarted after being shut down for a few seconds. Unless designed for re-ignition, when cooled, many rockets cannot be restarted without at least minor maintenance, such as replacement of the pyrotechnic igniter or even refueling of the propellants.
Rocket jets vary depending on the rocket engine, design altitude, altitude, thrust and other factors.
Carbon-rich exhausts from kerosene-based fuels such as RP-1 are often orange in colour due to the black-body radiation of the unburnt particles, in addition to the blue Swan bands. Peroxide oxidiser-based rockets and hydrogen rocket jets contain largely steam and are nearly invisible to the naked eye but shine brightly in the ultraviolet and infrared ranges. Jets from solid-propellant rockets can be highly visible, as the propellant frequently contains metals such as elemental aluminium which burns with an orange-white flame and adds energy to the combustion process. Rocket engines which burn liquid hydrogen and oxygen will exhibit a nearly transparent exhaust, due to it being mostly superheated steam (water vapour), plus some unburned hydrogen.
The nozzle is usually over-expanded at sea level, and the exhaust can exhibit visible shock diamonds through a schlieren effect caused by the incandescence of the exhaust gas.
The shape of the jet varies for a fixed-area nozzle as the expansion ratio varies with altitude: at high altitude all rockets are grossly under-expanded, and a quite small percentage of exhaust gases actually end up expanding forwards.
The solar thermal rocket would make use of solar power to directly heat reaction mass, and therefore does not require an electrical generator as most other forms of solar-powered propulsion do. A solar thermal rocket only has to carry the means of capturing solar energy, such as concentrators and mirrors. The heated propellant is fed through a conventional rocket nozzle to produce thrust. The engine thrust is directly related to the surface area of the solar collector and to the local intensity of the solar radiation and inversely proportional to the Isp.
Nuclear propulsion includes a wide variety of propulsion methods that use some form of nuclear reaction as their primary power source. Various types of nuclear propulsion have been proposed, and some of them tested, for spacecraft applications:
According to the writings of the Roman Aulus Gellius, the earliest known example of jet propulsion was in c. 400 BC, when a Greek Pythagorean named Archytas, propelled a wooden bird along wires using steam. However, it was not powerful enough to take off under its own thrust.
The aeolipile described in the first century BC, often known as Hero's engine, consisted of a pair of steam rocket nozzles mounted on a bearing. It was created almost two millennia before the Industrial Revolution but the principles behind it were not well understood, and it was not developed into a practical power source.
The availability of black powder to propel projectiles was a precursor to the development of the first solid rocket. Ninth Century Chinese Taoist alchemists discovered black powder in a search for the elixir of life; this accidental discovery led to fire arrows which were the first rocket engines to leave the ground.
It is stated that "the reactive forces of incendiaries were probably not applied to the propulsion of projectiles prior to the 13th century". A turning point in rocket technology emerged with a short manuscript entitled Liber Ignium ad Comburendos Hostes (abbreviated as The Book of Fires). The manuscript is composed of recipes for creating incendiary weapons from the mid-eighth to the end of the thirteenth centuries—two of which are rockets.  The first recipe calls for one part of colophonium and sulfur added to six parts of saltpeter (potassium nitrate) dissolved in laurel oil, then inserted into hollow wood and lit to "fly away suddenly to whatever place you wish and burn up everything".  The second recipe combines one pound of sulfur, two pounds of charcoal, and six pounds of saltpeter—all finely powdered on a marble slab.  This powder mixture is packed firmly into a long and narrow case.  The introduction of saltpeter into pyrotechnic mixtures connected the shift from hurled Greek fire into self-propelled rocketry. .
Articles and books on the subject of rocketry appeared increasingly from the fifteenth through seventeenth centuries.  In the sixteenth century, German military engineer Conrad Haas (1509–1576) wrote a manuscript which introduced the construction of multi-staged rockets.
Rocket engines were also put in use by Tippu Sultan, the king of Mysore. These usually consisted of a tube of soft hammered iron about 8 in (20 cm) long and .mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}1+1⁄2–3 in (3.8–7.6 cm) diameter, closed at one end, packed with black powder propellant and strapped to a shaft of bamboo about 4 ft (120 cm) long. A rocket carrying about one pound of powder could travel almost 1,000 yards (910 m). These 'rockets', fitted with swords, would travel several meters in the air before coming down with sword edges facing the enemy. These were used very effectively against the British empire.
Slow development of this technology continued up to the later 19th century, when Russian Konstantin Tsiolkovsky first wrote about liquid-fueled rocket engines. He was the first to develop the Tsiolkovsky rocket equation, though it was not published widely for some years.
The modern solid- and liquid-fueled engines became realities early in the 20th century, thanks to the American physicist Robert Goddard. Goddard was the first to use a De Laval nozzle on a solid-propellant (gunpowder) rocket engine, doubling the thrust and increasing the efficiency by a factor of about twenty-five. This was the birth of the modern rocket engine. He calculated from his independently derived rocket equation that a reasonably sized rocket, using solid fuel, could place a one-pound payload on the Moon.
Fritz von Opel was instrumental in popularizing rockets as means of propulsion. In the 1920s, he initiated together with Max Valier, co-founder of the "Verein für Raumschiffahrt", the world's first rocket program, Opel-RAK, leading to speed records for automobiles, rail vehicles and the first manned rocket-powered flight in September of 1929. Months earlier in 1928, one of his rocket-powered prototypes, the Opel RAK2, reached piloted by von Opel himself at the AVUS speedway in Berlin a record speed of 238 km/h, watched by 3000 spectators and world media. A world record for rail vehicles was reached with RAK3 and a top speed of 256 km/h. After these successes, von Opel piloted the world's first public rocket-powered flight using Opel RAK.1, a rocket plane designed by Julius Hatry. World media reported on these efforts, including UNIVERSAL Newsreel of the US, causing as "Raketen-Rummel" or "Rocket Rumble" immense global public excitement, and in particular in Germany, where inter alia Wernher von Braun was highly influenced. The Great Depression led to an end of the Opel-RAK program, but Max Valier continued the efforts. After switching from solid-fuel to liquid-fuel rockets, he died while testing and is considered the first fatality of the dawning space age.
Goddard began to use liquid propellants in 1921, and in 1926 became the first to launch a liquid-propellant rocket. Goddard pioneered the use of the De Laval nozzle, lightweight propellant tanks, small light turbopumps, thrust vectoring, the smoothly-throttled liquid fuel engine, regenerative cooling, and curtain cooling.: 247–266 
During the late 1930s, German scientists, such as Wernher von Braun and Hellmuth Walter, investigated installing liquid-fueled rockets in military aircraft (Heinkel He 112, He 111, He 176 and Messerschmitt Me 163).
The turbopump was employed by German scientists in World War II. Until then cooling the nozzle had been problematic, and the A4 ballistic missile used dilute alcohol for the fuel, which reduced the combustion temperature sufficiently.
Staged combustion (Замкнутая схема) was first proposed by Alexey Isaev in 1949. The first staged combustion engine was the S1.5400 used in the Soviet planetary rocket, designed by Melnikov, a former assistant to Isaev. About the same time (1959), Nikolai Kuznetsov began work on the closed cycle engine NK-9 for Korolev's orbital ICBM, GR-1. Kuznetsov later evolved that design into the NK-15 and NK-33 engines for the unsuccessful Lunar N1 rocket.
In the West, the first laboratory staged-combustion test engine was built in Germany in 1963, by Ludwig Boelkow.
Hydrogen peroxide / kerosene fueled engines such as the British Gamma of the 1950s used a closed-cycle process by catalytically decomposing the peroxide to drive turbines before combustion with the kerosene in the combustion chamber proper. This gave the efficiency advantages of staged combustion, without the major engineering problems.
Liquid hydrogen engines were first successfully developed in America: the RL-10 engine first flew in 1962. Its successor, the Rocketdyne J-2, was used in the Apollo program's Saturn V rocket to send humans to the Moon. The high specific impulse and low density of liquid hydrogen lowered the upper stage mass and the overall size and cost of the vehicle.
The record for most engines on one rocket flight is 44, set by NASA in 2016 on a Black Brant.
Rocket propellant is the reaction mass of a rocket. This reaction mass is ejected at the highest achievable velocity from a rocket engine to produce thrust. The energy required can either come from the propellants themselves, as with a chemical rocket, or from an external source, as with ion engines.
Rockets create thrust by expelling mass rear-ward, at high velocity. The thrust produced can be calculated by multiplying the mass flow rate of the propellants by their exhaust velocity relative to the rocket (specific impulse). A rocket can be thought of as being accelerated by the pressure of the combusting gases against the combustion chamber and nozzle, not by "pushing" against the air behind or below it. Rocket engines perform best in outer space because of the lack of air pressure on the outside of the engine. In space it is also possible to fit a longer nozzle without suffering from flow separation.
Most chemical propellants release energy through redox chemistry, more specifically combustion. As such, both an oxidizing agent and a reducing agent (fuel) must be present in the mixture. Decomposition, such as that of highly unstable peroxide bonds in monopropellant rockets, can also be the source of energy.
In the case of bipropellant liquid rockets, a mixture of reducing fuel and oxidizing oxidizer is introduced into a combustion chamber, typically using a turbopump to overcome the pressure. As combustion takes place, the liquid propellant mass is converted into a huge volume of gas at high temperature and pressure. This exhaust stream is ejected from the engine nozzle at high velocity, creating an opposing force that propels the rocket forward in accordance with Newton's laws of motion.
Chemical rockets can be grouped by phase. Solid rockets use propellant in the solid phase, liquid fuel rockets use propellant in the liquid phase, gas fuel rockets use propellant in the gas phase, and hybrid rockets use a combination of solid and liquid or gaseous propellants.
In the case of solid rocket motors, the fuel and oxidizer are combined when the motor is cast. Propellant combustion occurs inside the motor casing, which must contain the pressures developed. Solid rockets typically have higher thrust, less specific impulse, shorter burn times, and a higher mass than liquid rockets, and additionally cannot be stopped once lit.
In space, the maximum change in velocity that a rocket stage can impart on its payload is primarily a function of its mass ratio and its exhaust velocity. This relationship is described by the rocket equation. Exhaust velocity is dependent on the propellant and engine used and closely related to specific impulse, the total energy delivered to the rocket vehicle per unit of propellant mass consumed. Mass ratio can also be affected by the choice of a given propellant.
Rocket stages that fly through the atmosphere usually use lower performing, high molecular mass, high-density propellants due to the smaller and lighter tankage required. Upper stages, which mostly or only operate in the vacuum of space, tend to use the high energy, high performance, low density liquid hydrogen fuel.
Solid propellants come in two main types. "Composites" are composed mostly of a mixture of granules of solid oxidizer, such as ammonium nitrate, ammonium dinitramide, ammonium perchlorate, or potassium nitrate in a polymer binding agent, with flakes or powders of energetic fuel compounds (examples: RDX, HMX, aluminium, beryllium). Plasticizers, stabilizers, and/or burn rate modifiers (iron oxide, copper oxide) can also be added.
Single-, double-, or triple-bases (depending on the number of primary ingredients) are homogeneous mixtures of one to three primary ingredients. These primary ingredients must include fuel and oxidizer and often also include binders and plasticizers. All components are macroscopically indistinguishable and often blended as liquids and cured in a single batch. Ingredients can often have multiple roles. For example, RDX is both a fuel and oxidizer while nitrocellulose is a fuel, oxidizer, and structural polymer.
Further complicating categorization, there are many propellants that contain elements of double-base and composite propellants, which often contain some amount of energetic additives homogeneously mixed into the binder. In the case of gunpowder (a pressed composite without a polymeric binder) the fuel is charcoal, the oxidizer is potassium nitrate, and sulphur serves as a reaction catalyst while also being consumed to form a variety of reaction products such as potassium sulfide.
The newest nitramine solid propellants based on CL-20 (HNIW) can match the performance of NTO/UDMH storable liquid propellants, but cannot be throttled or restarted.
Solid propellant rockets are much easier to store and handle than liquid propellant rockets. High propellant density makes for compact size as well. These features plus simplicity and low cost make solid propellant rockets ideal for military and space applications.
Their simplicity also makes solid rockets a good choice whenever large amounts of thrust are needed and the cost is an issue.  The Space Shuttle and many other orbital launch vehicles use solid-fueled rockets in their boost stages (solid rocket boosters) for this reason.
Solid fuel rockets have lower specific impulse, a measure of propellant efficiency, than liquid fuel rockets. As a result, the overall performance of solid upper stages is less than liquid stages even though the solid mass ratios are usually in the .91 to .93 range, as good as or better than most liquid propellant upper stages. The high mass ratios possible with these unsegmented solid upper stages is a result of high propellant density and very high strength-to-weight ratio filament-wound motor casings.
A drawback to solid rockets is that they cannot be throttled in real time, although a programmed thrust schedule can be created by adjusting the interior propellant geometry. Solid rockets can be vented to extinguish combustion or reverse thrust as a means of controlling range or accommodating stage separation. Casting large amounts of propellant requires consistency and repeatability to avoid cracks and voids in the completed motor. The blending and casting take place under computer control in a vacuum, and the propellant blend is spread thin and scanned to assure no large gas bubbles are introduced into the motor.
Solid fuel rockets are intolerant to cracks and voids and require post-processing such as X-ray scans to identify faults. The combustion process is dependent on the surface area of the fuel. Voids and cracks represent local increases in burning surface area, increasing the local temperature, which increases the local rate of combustion. This positive feedback loop can easily lead to catastrophic failure of the case or nozzle.
Solid rocket propellant was first developed during the 13th century under the Chinese Song dynasty. The Song Chinese first used gunpowder in 1232 during the military siege of Kaifeng.
During the 1950s and 60s, researchers in the United States developed ammonium perchlorate composite propellant (APCP).  This mixture is typically 69-70% finely ground ammonium perchlorate (an oxidizer), combined with 16-20% fine aluminium powder (a fuel), held together in a base of 11-14% polybutadiene acrylonitrile (PBAN) or Hydroxyl-terminated polybutadiene (polybutadiene rubber fuel).  The mixture is formed as a thickened liquid and then cast into the correct shape and cured into a firm but flexible load-bearing solid. Historically, the tally of APCP solid propellants is relatively small. The military, however, uses a wide variety of different types of solid propellants, some of which exceed the performance of APCP. A comparison of the highest specific impulses achieved with the various solid and liquid propellant combinations used in current launch vehicles is given in the article on solid-fuel rockets.
In the 1970s and 1980s, the U.S. switched entirely to solid-fueled ICBMs: the LGM-30 Minuteman and LG-118A Peacekeeper (MX).  In the 1980s and 1990s, the USSR/Russia also deployed solid-fueled ICBMs (RT-23, RT-2PM, and RT-2UTTH), but retains two liquid-fueled ICBMs (R-36 and UR-100N).  All solid-fueled ICBMs on both sides had three initial solid stages, and those with multiple independently targeted warheads had a precision maneuverable bus used to fine tune the trajectory of the re-entry vehicles.
The main types of liquid propellants are storable propellants, which tend to be hypergolic, and cryogenic propellants.
Liquid-fueled rockets have higher specific impulse than solid rockets and are capable of being throttled, shut down, and restarted.  Only the combustion chamber of a liquid-fueled rocket needs to withstand high combustion pressures and temperatures. Cooling can be done regeneratively with the liquid propellant.  On vehicles employing turbopumps, the propellant tanks are at a lower pressure than the combustion chamber, decreasing tank mass.  For these reasons, most orbital launch vehicles use liquid propellants.
The primary specific impulse advantage of liquid propellants is due to the availability of high-performance oxidizers.  Several practical liquid oxidizers (liquid oxygen, dinitrogen tetroxide, and hydrogen peroxide) are available which have better specific impulse than the ammonium perchlorate used in most solid rockets when paired with suitable fuels.
Some gases, notably oxygen and nitrogen, may be able to be collected from the upper atmosphere, and transferred up to low Earth orbit for use in propellant depots at substantially reduced cost.
The main difficulties with liquid propellants are also with the oxidizers. Storable oxidizers, such as nitric acid and nitrogen tetroxide, tend to be extremely toxic and highly reactive, while cryogenic propellants by definition must be stored at low temperature and can also have reactivity/toxicity issues. Liquid oxygen (LOX) is the only flown cryogenic oxidizer. Others such as FLOX, a fluorine/LOX mix, have never been flown due to instability, toxicity, and explosivity. Several other unstable, energetic, and toxic oxidizers have been proposed: liquid ozone (O3), ClF3, and ClF5.
Liquid-fueled rockets require potentially troublesome valves, seals, and turbopumps, which increase the cost of the launch vehicle. Turbopumps are particularly troublesome due to high performance requirements.
The theoretical exhaust velocity of a given propellant chemistry is 
proportional to the energy released per unit of propellant mass (specific
energy).  In chemical rockets, unburned fuel or oxidizer represents the loss of chemical potential energy, which reduces the specific energy. However, most rockets run fuel-rich mixtures, which result in lower theoretical exhaust velocities.
However, fuel-rich mixtures also have lower molecular weight exhaust species. The nozzle of the rocket converts the thermal energy of the propellants into directed kinetic energy. This conversion happens in the time it takes for the propellants to flow from the combustion chamber through the engine throat and out the nozzle, usually on the order of one millisecond. Molecules store thermal energy in rotation, vibration, and translation, of which only the latter can easily be used to add energy to the rocket stage. Molecules with fewer atoms (like CO and H2) have fewer available vibrational and rotational modes than molecules with more atoms (like CO2 and H2O). Consequently, smaller molecules store less vibrational and rotational energy for a given amount of heat input, resulting in more translation energy being available to be converted to kinetic energy. The resulting improvement in nozzle efficiency is large enough that real rocket engines improve their actual exhaust velocity by running rich mixtures with somewhat lower theoretical exhaust velocities.
The effect of exhaust molecular weight on nozzle efficiency is most important for nozzles operating near sea level. High expansion rockets operating in a vacuum see a much smaller effect, and so are run less rich.
LOX/hydrocarbon rockets are run slightly rich (O/F mass ratio of 3 rather than stoichiometric of 3.4 to 4) because the energy release per unit mass drops off quickly as the mixture ratio deviates from stoichiometric.  LOX/LH2 rockets are run very rich (O/F mass ratio of 4 rather than stoichiometric 8) because hydrogen is so light that the energy release per unit mass of propellant drops very slowly with extra hydrogen.  In fact, LOX/LH2 rockets are generally limited in how rich they run by the performance penalty of the mass of the extra hydrogen tankage instead of the underlying chemistry.
Another reason for running rich is that off-stoichiometric mixtures burn cooler than stoichiometric mixtures, which makes engine cooling easier. Because fuel-rich combustion products are less chemically reactive (corrosive) than oxidizer-rich combustion products, a vast majority of rocket engines are designed to run fuel-rich. At least one exception exists: the Russian RD-180 preburner, which burns LOX and RP-1 at a ratio of 2.72.
Additionally, mixture ratios can be dynamic during launch. This can be exploited with designs that adjust the oxidizer to fuel ratio (along with overall thrust) throughout a flight to maximize overall system performance. For instance, during lift-off thrust is more valuable than specific impulse, and careful adjustment of the O/F ratio may allow higher thrust levels. Once the rocket is away from the launchpad, the engine O/F ratio can be tuned for higher efficiency.
Although liquid hydrogen gives a high Isp, its low density is a disadvantage: hydrogen occupies about 7 times more volume per kilogram than dense fuels such as kerosene. The fuel tankage, plumbing, and pump must be correspondingly larger. This increases the vehicle's dry mass, reducing performance. Liquid hydrogen is also relatively expensive to produce and store, and causes difficulties with design, manufacture, and operation of the vehicle. However, liquid hydrogen is extremely well suited to upper stage use where Isp is at a premium and thrust to weight ratios are less relevant.
Dense propellant launch vehicles have a higher takeoff mass due to lower Isp, but can more easily develop high takeoff thrusts due to the reduced volume of engine components. This means that vehicles with dense-fueled booster stages reach orbit earlier, minimizing losses due to gravity drag and reducing the effective delta-v requirement.
The proposed tripropellant rocket uses mainly dense fuel while at low altitude and switches across to hydrogen at higher altitude. Studies in the 1960s proposed single stage to orbit vehicles using this technique. The Space Shuttle approximated this by using dense solid rocket boosters for the majority of the thrust during the first 120 seconds. The main engines burned a fuel-rich hydrogen and oxygen mixture, operating continuously throughout the launch but providing the majority of thrust at higher altitudes after SRB burnout.
Hybrid propellants: a storable oxidizer used with a solid fuel, which retains most virtues of both liquids (high ISP) and solids (simplicity).
A hybrid-propellant rocket usually has a solid fuel and a liquid or NEMA oxidizer.  The fluid oxidizer can make it possible to throttle and restart the motor just like a liquid-fueled rocket.  Hybrid rockets can also be environmentally safer than solid rockets since some high-performance solid-phase oxidizers contain chlorine (specifically composites with ammonium perchlorate), versus the more benign liquid oxygen or nitrous oxide often used in hybrids. This is only true for specific hybrid systems.  There have been hybrids which have used chlorine or fluorine compounds as oxidizers and hazardous materials such as beryllium compounds mixed into the solid fuel grain. Because just one constituent is a fluid, hybrids can be simpler than liquid rockets depending motive force used to transport the fluid into the combustion chamber. Fewer fluids typically mean fewer and smaller piping systems, valves and pumps (if utilized).
Hybrid motors suffer two major drawbacks.  The first, shared with solid rocket motors, is that the casing around the fuel grain must be built to withstand full combustion pressure and often extreme temperatures as well.  However, modern composite structures handle this problem well, and when used with nitrous oxide and a solid rubber propellant (HTPB), relatively small percentage of fuel is needed anyway, so the combustion chamber is not especially large.
The primary remaining difficulty with hybrids is with mixing the propellants during the combustion process. In solid propellants, the oxidizer and fuel are mixed in a factory in carefully controlled conditions. Liquid propellants are generally mixed by the injector at the top of the combustion chamber, which directs many small swift-moving streams of fuel and oxidizer into one another.  Liquid-fueled rocket injector design has been studied at great length and still resists reliable performance prediction. In a hybrid motor, the mixing happens at the melting or evaporating surface of the fuel. The mixing is not a well-controlled process and generally, quite a lot of propellant is left unburned, which limits the efficiency of the motor. The combustion rate of the fuel is largely determined by the oxidizer flux and exposed fuel surface area.  This combustion rate is not usually sufficient for high power operations such as boost stages unless the surface area or oxidizer flux is high. Too high of oxidizer flux can lead to flooding and loss of flame holding that locally extinguishes the combustion. Surface area can be increased, typically by longer grains or multiple ports, but this can increase combustion chamber size, reduce grain strength and/or reduce volumetric loading. Additionally, as the burn continues, the hole down the center of the grain (the 'port') widens and the mixture ratio tends to become more oxidizer rich.
There has been much less development of hybrid motors than solid and liquid motors.  For military use, ease of handling and maintenance have driven the use of solid rockets.  For orbital work, liquid fuels are more efficient than hybrids and most development has concentrated there.  There has recently been an increase in hybrid motor development for nonmilitary suborbital work:
GOX (gaseous oxygen) was used as the oxidizer for the Buran program's orbital maneuvering system.
Some rocket designs impart energy to their propellants with external energy sources. For example, water rockets use a compressed gas, typically air, to force the water reaction mass out of the rocket.
Ion thrusters ionize a neutral gas and create thrust by accelerating the ions (or the plasma) by electric and/or magnetic fields.
Thermal rockets use inert propellants of low molecular weight that are chemically compatible with the heating mechanism at high temperatures. Solar thermal rockets and nuclear thermal rockets typically propose to use liquid hydrogen for a specific impulse of around 600–900 seconds, or in some cases water that is exhausted as steam for a specific impulse of about 190 seconds. Nuclear thermal rockets use the heat of nuclear fission to add energy to the propellant. Some designs separate the nuclear fuel and working fluid, minimizing the potential for radioactive contamination, but nuclear fuel loss was a persistent problem during real-world testing programs. Solar thermal rockets use concentrated sunlight to heat a propellant, rather than using a nuclear reactor.
For low performance applications, such as attitude control jets, compressed  gases such as nitrogen have been employed. Energy is stored in the pressure of the inert gas. However, due to the low density of all practical gases and high mass of the pressure vessel required to contain it, compressed gases see little current use.
In Project Orion and other nuclear pulse propulsion proposals, the propellant would be plasma debris from a series of nuclear explosions.
As described by the third of Newton's laws of motion of classical mechanics, all forces occur in pairs such that if one object exerts a force on another object, then the second object exerts an equal and opposite reaction force on the first.  The third law is also more generally stated as:  "To every action there is always opposed an equal reaction: or the mutual actions of two bodies upon each other are always equal, and directed to contrary parts." The attribution of which of the two forces is the action and which is the reaction is arbitrary. Either of the two can be considered the action, while the other is its associated reaction.
When something is exerting force on the ground, the ground will push back with equal force in the opposite direction. In certain fields of applied physics, such as biomechanics, this force by the ground is called 'ground reaction force'; the force by the object on the ground is viewed as the 'action'.
When someone wants to jump, he or she exerts additional downward force on the ground ('action'). Simultaneously, the ground exerts upward force on the person ('reaction'). If this upward force is greater than the person's weight, this will result in upward acceleration. When these forces are perpendicular to the ground, they are also called a normal force.
Likewise, the spinning wheels of a vehicle attempt to slide backward across the ground. If the ground is not too slippery, this results in a pair of friction forces: the 'action' by the wheel on the ground in backward direction, and the 'reaction' by the ground on the wheel in forward direction. This forward force propels the vehicle.
The Earth, among other planets, orbits the Sun because the Sun exerts a gravitational pull that acts as a centripetal force, holding the Earth to it, which would otherwise go shooting off into space. If the Sun's pull is considered an action, then Earth simultaneously exerts a reaction as a gravitational pull on the Sun. Earth's pull has the same amplitude as the Sun but in the opposite direction. Since the Sun's mass is so much larger than Earth's, the Sun does not generally appear to react to the pull of Earth, but in fact it does, as demonstrated in the animation (not to precise scale). A correct way of describing the combined motion of both objects (ignoring all other celestial bodies for the moment) is to say that they both orbit around the center of mass, referred to in astronomy as the barycenter, of the combined system.
Any mass on earth is pulled down by the gravitational force of the earth; this force is also called its weight. The corresponding 'reaction' is the gravitational force that mass exerts on the planet.
If the object is supported so that it remains at rest, for instance by a cable from which it is hanging, or by a surface underneath, or by a liquid on which it is floating, there is also a support force in upward direction (tension force, normal force, buoyant force, respectively). This support force is an 'equal and opposite' force; we know this not because of Newton's third law, but because the object remains at rest, so that the forces must be balanced.
To this support force there is also a 'reaction': the object pulls down on the supporting cable, or pushes down on the supporting surface or liquid. In this case, there are therefore four forces of equal magnitude:
Forces F1 and F2 are equal due of Newton's third law; the same is true for forces F3 and F4.
Forces F1 and F3 are equal if and only if the object is in equilibrium, and no other forces are applied. (This has nothing to do with Newton's third law.)
If a mass is hanging from a spring, the same considerations apply as before. However, if this system is then perturbed (e.g., the mass is given a slight kick upwards or downwards, say), the mass starts to oscillate up and down. Because of these accelerations (and subsequent decelerations), we conclude from Newton's second law that a net force is responsible for the observed change in velocity. The gravitational force pulling down on the mass is no longer equal to the upward elastic force of the spring. In the terminology of the previous section, F1 and F3 are no longer equal.
However, it is still true that F1 = F2 and F3 = F4, as this is required by Newton's third law.
The terms 'action' and 'reaction' have the misleading suggestion of causality, as if the 'action' is the cause and 'reaction' is the effect. It is therefore easy to think of the second force as being there because of the first, and even happening some time after the first. This is incorrect; the forces are perfectly simultaneous, and are there for the same reason.
When the forces are caused by a person's volition (e.g. a soccer player kicks a ball), this volitional cause often leads to an asymmetric interpretation, where the force by the player on the ball is considered the 'action' and the force by the ball on the player, the 'reaction'. But physically, the situation is symmetric. The forces on ball and player are both explained by their nearness, which results in a pair of contact forces (ultimately due to electric repulsion). That this nearness is caused by a decision of the player has no bearing on the physical analysis. As far as the physics is concerned, the labels 'action' and 'reaction' can be flipped.
One problem frequently observed by physics educators is that students tend to apply Newton's third law to pairs of 'equal and opposite' forces acting on the same object.
This is incorrect; the third law refers to forces on two different objects. In contrast, a book lying on a table is subject to a downward gravitational force (exerted by the earth) and to an upward normal force by the table, both forces acting on the same book. Since the book is not accelerating, these forces must be exactly balanced, according to Newton's second law. They are therefore 'equal and opposite', yet they are acting on the same object, hence they are not action-reaction forces in the sense of Newton's third law. The actual action-reaction forces in the sense of Newton's third law are the book pushing down on the table and the book's upward gravitational force on the earth.
Moreover, the forces acting on the book are not always equally strong; they will be different if the book is pushed down by a third force, or if the table is slanted, or if the table-and-book system is in an accelerating elevator. The case of any number of forces acting on the same object is covered by considering the sum of all forces.
A possible cause of this problem is that the third law is often stated in an abbreviated form: For every action there is an equal and opposite reaction, without the details, namely that these forces act on two different objects. Moreover, there is a causal connection between the weight of something and the normal force: if an object had no weight, it would not experience support force from the table, and the weight dictates how strong the support force will be. This causal relationship is not due to the third law but to other physical relations in the system.
Another common mistake is to state that "the centrifugal force that an object experiences is the reaction to the centripetal force on that object."
If an object were simultaneously subject to both a centripetal force and an equal and opposite centrifugal force, the resultant force would vanish and the object could not experience a circular motion. The centrifugal force is sometimes called a fictitious force or pseudo force, to underscore the fact that such a force only appears when calculations or measurements are conducted in non-inertial reference frames.
A vacuum is a space devoid of matter. The word is derived from the Latin adjective vacuus for "vacant" or "void".  An approximation to such vacuum is a region with a gaseous pressure much less than atmospheric pressure.  Physicists often discuss ideal test results that would occur in a perfect vacuum, which they sometimes simply call "vacuum" or free space, and use the term partial vacuum to refer to an actual imperfect vacuum as one might have in a laboratory or in space. In engineering and applied physics on the other hand, vacuum refers to any space in which the pressure is considerably lower than atmospheric pressure. The Latin term in vacuo is used to describe an object that is surrounded by a vacuum.
The quality of a partial vacuum refers to how closely it approaches a perfect vacuum. Other things equal, lower gas pressure means higher-quality vacuum. For example, a typical vacuum cleaner produces enough suction to reduce air pressure by around 20%. But higher-quality vacuums are possible. Ultra-high vacuum chambers, common in chemistry, physics, and engineering, operate below one trillionth (10−12) of atmospheric pressure (100 nPa), and can reach around 100 particles/cm3. Outer space is an even higher-quality vacuum, with the equivalent of just a few hydrogen atoms per cubic meter on average in intergalactic space.
Vacuum has been a frequent topic of philosophical debate since ancient Greek times, but was not studied empirically until the 17th century. Evangelista Torricelli produced the first laboratory vacuum in 1643, and other experimental techniques were developed as a result of his theories of atmospheric pressure. A Torricellian vacuum is created by filling a tall glass container closed at one end with mercury, and then inverting it in a bowl to contain the mercury (see below).
Vacuum became a valuable industrial tool in the 20th century with the introduction of incandescent light bulbs and vacuum tubes, and a wide array of vacuum technologies has since become available. The development of human spaceflight has raised interest in the impact of vacuum on human health, and on life forms in general.
The word vacuum comes from Latin 'an empty space, void', noun use of neuter of vacuus, meaning "empty", related to vacare, meaning "to be empty".
Vacuum is one of the few words in the English language that contains two consecutive letters u.
Historically, there has been much dispute over whether such a thing as a vacuum can exist. Ancient Greek philosophers debated the existence of a vacuum, or void, in the context of atomism, which posited void and atom as the fundamental explanatory elements of physics. Following Plato, even the abstract concept of a featureless void faced considerable skepticism: it could not be apprehended by the senses, it could not, itself, provide additional explanatory power beyond the physical volume with which it was commensurate and, by definition, it was quite literally nothing at all, which cannot rightly be said to exist. Aristotle believed that no void could occur naturally, because the denser surrounding material continuum would immediately fill any incipient rarity that might give rise to a void.
In his Physics, book IV, Aristotle offered numerous arguments against the void: for example, that motion through a medium which offered no impediment could continue ad infinitum, there being no reason that something would come to rest anywhere in particular. Lucretius argued for the existence of vacuum in the first century BC and Hero of Alexandria tried unsuccessfully to create an artificial vacuum in the first century AD.
In the medieval Muslim world, the physicist and Islamic scholar Al-Farabi wrote a treatise rejecting the existence of the vacuum in the 10th century. He concluded that air's volume can expand to fill available space, and therefore the concept of a perfect vacuum was incoherent. According to Nader El-Bizri, the physicist Ibn al-Haytham and the Mu'tazili theologians disagreed with Aristotle and Al-Farabi, and they supported the existence of a void. Using geometry, Ibn al-Haytham mathematically demonstrated that place (al-makan) is the imagined three-dimensional void between the inner surfaces of a containing body. According to Ahmad Dallal, Abū Rayhān al-Bīrūnī also states that "there is no observable evidence that rules out the possibility of vacuum". The suction pump was described by Arab engineer Al-Jazari in the 13th century, and later appeared in Europe from the 15th century.
European scholars such as Roger Bacon, Blasius of Parma and Walter Burley in the 13th and 14th century focused considerable attention on issues concerning the concept of a vacuum. Eventually following Stoic physics in this instance, scholars from the 14th century onward increasingly departed from the Aristotelian perspective in favor of a supernatural void beyond the confines of the cosmos itself, a conclusion widely acknowledged by the 17th century, which helped to segregate natural and theological concerns.
Almost two thousand years after Plato, René Descartes also proposed a geometrically based alternative theory of atomism, without the problematic nothing–everything dichotomy of void and atom. Although Descartes agreed with the contemporary position, that a vacuum does not occur in nature, the success of his namesake coordinate system and more implicitly, the spatial–corporeal component of his metaphysics would come to define the philosophically modern notion of empty space as a quantified extension of volume. By the ancient definition however, directional information and magnitude were conceptually distinct.
Medieval thought experiments into the idea of a vacuum considered whether a vacuum was present, if only for an instant, between two flat plates when they were rapidly separated. There was much discussion of whether the air moved in quickly enough as the plates were separated, or, as Walter Burley postulated, whether a 'celestial agent' prevented the vacuum arising.  The commonly held view that nature abhorred a vacuum was called horror vacui.  There was even speculation that even God could not create a vacuum if he wanted and the 1277 Paris condemnations of Bishop Etienne Tempier, which required there to be no restrictions on the powers of God, led to the conclusion that God could create a vacuum if he so wished.
Jean Buridan reported in the 14th century that teams of ten horses could not pull open bellows when the port was sealed.
The 17th century saw the first attempts to quantify measurements of partial vacuum. Evangelista Torricelli's mercury barometer of 1643 and Blaise Pascal's experiments both demonstrated a partial vacuum.
In 1654, Otto von Guericke invented the first vacuum pump and conducted his famous Magdeburg hemispheres experiment, showing that, owing to atmospheric pressure outside the hemispheres, teams of horses could not separate two hemispheres from which the air had been partially evacuated. Robert Boyle improved Guericke's design and with the help of Robert Hooke further developed vacuum pump technology. Thereafter, research into the partial vacuum lapsed until 1850 when August Toepler invented the Toepler Pump and in 1855 when Heinrich Geissler invented the mercury displacement pump, achieving a partial vacuum of about 10 Pa (0.1 Torr). A number of electrical properties become observable at this vacuum level, which renewed interest in further research.
While outer space provides the most rarefied example of a naturally occurring partial vacuum, the heavens were originally thought to be seamlessly filled by a rigid indestructible material called aether. Borrowing somewhat from the pneuma of Stoic physics, aether came to be regarded as the rarefied air from which it took its name, (see Aether (mythology)). Early theories of light posited a ubiquitous terrestrial and celestial medium through which light propagated. Additionally, the concept informed Isaac Newton's explanations of both refraction and of radiant heat.  19th century experiments into this luminiferous aether attempted to detect a minute drag on the Earth's orbit. While the Earth does, in fact, move through a relatively dense medium in comparison to that of interstellar space, the drag is so minuscule that it could not be detected. In 1912, astronomer Henry Pickering commented: "While the interstellar absorbing medium may be simply the ether,  is characteristic of a gas, and free gaseous molecules are certainly there".
Later, in 1930, Paul Dirac proposed a model of the vacuum as an infinite sea of particles possessing negative energy, called the Dirac sea. This theory helped refine the predictions of his earlier formulated Dirac equation, and successfully predicted the existence of the positron, confirmed two years later. Werner Heisenberg's uncertainty principle, formulated in 1927, predicted a fundamental limit within which instantaneous position and momentum, or energy and time can be measured. This has far reaching consequences on the "emptiness" of space between particles. In the late 20th century, so-called virtual particles that arise spontaneously from empty space were confirmed.
The strictest criterion to define a vacuum is a region of space and time where all the components of the stress–energy tensor are zero. This means that this region is devoid of energy and momentum, and by consequence, it must be empty of particles and other physical fields (such as electromagnetism) that contain energy and momentum.
In general relativity, a vanishing stress–energy tensor implies, through Einstein field equations, the vanishing of all the components of the Ricci tensor. Vacuum does not mean that the curvature of space-time is necessarily flat: the gravitational field can still produce curvature in a vacuum in the form of tidal forces and gravitational waves (technically, these phenomena are the components of the Weyl tensor). The black hole (with zero electric charge) is an elegant example of a region completely "filled" with vacuum, but still showing a strong curvature.
In classical electromagnetism, the vacuum of free space, or sometimes just free space or perfect vacuum, is a standard reference medium for electromagnetic effects. Some authors refer to this reference medium as classical vacuum, a terminology intended to separate this concept from QED vacuum or QCD vacuum, where vacuum fluctuations can produce transient virtual particle densities and a relative permittivity and relative permeability that are not identically unity.
In the theory of classical electromagnetism, free space has the following properties:
The vacuum of classical electromagnetism can be viewed as an idealized electromagnetic medium with the constitutive relations in SI units:
relating the electric displacement field D to the electric field E and the magnetic field or H-field H to the magnetic induction or B-field B. Here r is a spatial location and t is time.
In quantum mechanics and quantum field theory, the vacuum is defined as the state (that is, the solution to the equations of the theory) with the lowest possible energy (the ground state of the Hilbert space). In quantum electrodynamics this vacuum is referred to as 'QED vacuum' to distinguish it from the vacuum of quantum chromodynamics, denoted as QCD vacuum. QED vacuum is a state with no matter particles (hence the name), and no photons. As described above, this state is impossible to achieve experimentally. (Even if every matter particle could somehow be removed from a volume, it would be impossible to eliminate all the blackbody photons.) Nonetheless, it provides a good model for realizable vacuum, and agrees with a number of experimental observations as described next.
QED vacuum has interesting and complex properties. In QED vacuum, the electric and magnetic fields have zero average values, but their variances are not zero. As a result, QED vacuum contains vacuum fluctuations (virtual particles that hop into and out of existence), and a finite energy called vacuum energy. Vacuum fluctuations are an essential and ubiquitous part of quantum field theory. Some experimentally verified effects of vacuum fluctuations include spontaneous emission and the Lamb shift. Couloms law and the electric potential in vacuum near an electric charge are modified.
Theoretically, in QCD multiple vacuum states can coexist. The starting and ending of cosmological inflation is thought to have arisen from transitions between different vacuum states. For theories obtained by quantization of a classical theory, each stationary point of the energy in the configuration space gives rise to a single vacuum. String theory is believed to have a huge number of vacua – the so-called string theory landscape.
Outer space has very low density and pressure, and is the closest physical approximation of a perfect vacuum. But no vacuum is truly perfect, not even in interstellar space, where there are still a few hydrogen atoms per cubic meter.
Stars, planets, and moons keep their atmospheres by gravitational attraction, and as such, atmospheres have no clearly delineated boundary: the density of atmospheric gas simply decreases with distance from the object. The Earth's atmospheric pressure drops to about 32 millipascals (4.6×10−6 psi) at 100 kilometres (62 mi) of altitude, the Kármán line, which is a common definition of the boundary with outer space. Beyond this line, isotropic gas pressure rapidly becomes insignificant when compared to radiation pressure from the Sun and the dynamic pressure of the solar winds, so the definition of pressure becomes difficult to interpret. The thermosphere in this range has large gradients of pressure, temperature and composition, and varies greatly due to space weather. Astrophysicists prefer to use number density to describe these environments, in units of particles per cubic centimetre.
But although it meets the definition of outer space, the atmospheric density within the first few hundred kilometers above the Kármán line is still sufficient to produce significant drag on satellites. Most artificial satellites operate in this region called low Earth orbit and must fire their engines every couple of weeks or a few times a year (depending on solar activity). The drag here is low enough that it could theoretically be overcome by radiation pressure on solar sails, a proposed propulsion system for interplanetary travel. Planets are too massive for their trajectories to be significantly affected by these forces, although their atmospheres are eroded by the solar winds.
All of the observable universe is filled with large numbers of photons, the so-called cosmic background radiation, and quite likely a correspondingly large number of neutrinos. The current temperature of this radiation is about 3 K (−270.15 °C; −454.27 °F).
The quality of a vacuum is indicated by the amount of matter remaining in the system, so that a high quality vacuum is one with very little matter left in it. Vacuum is primarily measured by its absolute pressure, but a complete characterization requires further parameters, such as temperature and chemical composition. One of the most important parameters is the mean free path (MFP) of residual gases, which indicates the average distance that molecules will travel between collisions with each other. As the gas density decreases, the MFP increases, and when the MFP is longer than the chamber, pump, spacecraft, or other objects present, the continuum assumptions of fluid mechanics do not apply. This vacuum state is called high vacuum, and the study of fluid flows in this regime is called particle gas dynamics. The MFP of air at atmospheric pressure is very short, 70 nm, but at 100 mPa (~1×10−3 Torr) the MFP of room temperature air is roughly 100 mm, which is on the order of everyday objects such as vacuum tubes. The Crookes radiometer turns when the MFP is larger than the size of the vanes.
Vacuum quality is subdivided into ranges according to the technology required to achieve it or measure it. These ranges were defined in ISO 3529-1:2019 as shown in the following table (100 Pa corresponds to 0.75 Torr; Torr is non-SI unit):
Vacuum is measured in units of pressure, typically as a subtraction relative to ambient atmospheric pressure on Earth. But the amount of relative measurable vacuum varies with local conditions. On the surface of Venus, where ground-level atmospheric pressure is much higher than on Earth, much higher relative vacuum readings would be possible. On the surface of the moon with almost no atmosphere, it would be extremely difficult to create a measurable vacuum relative to the local environment.
Similarly, much higher than normal relative vacuum readings are possible deep in the Earth's ocean. A submarine maintaining an internal pressure of 1 atmosphere submerged to a depth of 10 atmospheres (98 metres; a 9.8-metre column of seawater has the equivalent weight of 1 atm) is effectively a vacuum chamber keeping out the crushing exterior water pressures, though the 1 atm inside the submarine would not normally be considered a vacuum.
Therefore, to properly understand the following discussions of vacuum measurement, it is important that the reader assumes the relative measurements are being done on Earth at sea level, at exactly 1 atmosphere of ambient atmospheric pressure.
The SI unit of pressure is the pascal (symbol Pa), but vacuum is often measured in torrs, named for an Italian physicist Torricelli (1608–1647). A torr is equal to the displacement of a millimeter of mercury (mmHg) in a manometer with 1 torr equaling 133.3223684 pascals above absolute zero pressure. Vacuum is often also measured on the barometric scale or as a percentage of atmospheric pressure in bars or atmospheres.  Low vacuum is often measured in millimeters of mercury (mmHg) or pascals (Pa) below standard atmospheric pressure. "Below atmospheric" means that the absolute pressure is equal to the current atmospheric pressure.
In other words, most low vacuum gauges that read, for example 50.79 Torr. Many inexpensive low vacuum gauges have a margin of error and may report a vacuum of 0 Torr but in practice this generally requires a two-stage rotary vane or other medium type of vacuum pump to go much beyond (lower than) 1 torr.
Many devices are used to measure the pressure in a vacuum, depending on what range of vacuum is needed.
Hydrostatic gauges (such as the mercury column manometer) consist of a vertical column of liquid in a tube whose ends are exposed to different pressures. The column will rise or fall until its weight is in equilibrium with the pressure differential between the two ends of the tube. The simplest design is a closed-end U-shaped tube, one side of which is connected to the region of interest. Any fluid can be used, but mercury is preferred for its high density and low vapour pressure. Simple hydrostatic gauges can measure pressures ranging from 1 torr (100 Pa) to above atmospheric. An important variation is the McLeod gauge which isolates a known volume of vacuum and compresses it to multiply the height variation of the liquid column. The McLeod gauge can measure vacuums as high as 10−6 torr (0.1 mPa), which is the lowest direct measurement of pressure that is possible with current technology. Other vacuum gauges can measure lower pressures, but only indirectly by measurement of other pressure-controlled properties. These indirect measurements must be calibrated via a direct measurement, most commonly a McLeod gauge.
The kenotometer is a particular type of hydrostatic gauge, typically used in power plants using steam turbines. The kenotometer measures the vacuum in the steam space of the condenser, that is, the exhaust of the last stage of the turbine.
Mechanical or elastic gauges depend on a Bourdon tube, diaphragm, or capsule, usually made of metal, which will change shape in response to the pressure of the region in question. A variation on this idea is the capacitance manometer, in which the diaphragm makes up a part of a capacitor. A change in pressure leads to the flexure of the diaphragm, which results in a change in capacitance. These gauges are effective from 103 torr to 10−4 torr, and beyond.
Thermal conductivity gauges rely on the fact that the ability of a gas to conduct heat decreases with pressure. In this type of gauge, a wire filament is heated by running current through it. A  thermocouple or Resistance Temperature Detector (RTD) can then be used to measure the temperature of the filament. This temperature is dependent on the rate at which the filament loses heat to the surrounding gas, and therefore on the thermal conductivity. A common variant is the Pirani gauge which uses a single platinum filament as both the heated element and RTD. These gauges are accurate from 10 torr to 10−3 torr, but they are sensitive to the chemical composition of the gases being measured.
Ionization gauges are used in ultrahigh vacuum. They come in two types: hot cathode and cold cathode. In the hot cathode version an electrically heated filament produces an electron beam. The electrons travel through the gauge and ionize gas molecules around them. The resulting ions are collected at a negative electrode. The current depends on the number of ions, which depends on the pressure in the gauge. Hot cathode gauges are accurate from 10−3 torr to 10−10 torr. The principle behind cold cathode version is the same, except that electrons are produced in a discharge created by a high voltage electrical discharge. Cold cathode gauges are accurate from 10−2 torr to 10−9 torr. Ionization gauge calibration is very sensitive to construction geometry, chemical composition of gases being measured, corrosion and surface deposits. Their calibration can be invalidated by activation at atmospheric pressure or low vacuum. The composition of gases at high vacuums will usually be unpredictable, so a mass spectrometer must be used in conjunction with the ionization gauge for accurate measurement.
Vacuum is useful in a variety of processes and devices. Its first widespread use was in the incandescent light bulb to protect the filament from chemical degradation. The chemical inertness produced by a vacuum is also useful for electron beam welding, cold welding, vacuum packing and vacuum frying. Ultra-high vacuum is used in the study of atomically clean substrates, as only a very good vacuum preserves atomic-scale clean surfaces for a reasonably long time (on the order of minutes to days). High to ultra-high vacuum removes the obstruction of air, allowing particle beams to deposit or remove materials without contamination. This is the principle behind chemical vapor deposition, physical vapor deposition, and dry etching which are essential to the fabrication of semiconductors and optical coatings, and to surface science. The reduction of convection provides the thermal insulation of thermos bottles. Deep vacuum lowers the boiling point of liquids and promotes low temperature outgassing which is used in freeze drying, adhesive preparation, distillation, metallurgy, and process purging. The electrical properties of vacuum make electron microscopes and vacuum tubes possible, including cathode ray tubes. Vacuum interrupters are used in electrical switchgear.  Vacuum arc processes are industrially important for production of certain grades of steel or high purity materials. The elimination of air friction is useful for flywheel energy storage and ultracentrifuges.
Vacuums are commonly used to produce suction, which has an even wider variety of applications. The Newcomen steam engine used vacuum instead of pressure to drive a piston. In the 19th century, vacuum was used for traction on Isambard Kingdom Brunel's experimental atmospheric railway.  Vacuum brakes were once widely used on trains in the UK but, except on heritage railways, they have been replaced by air brakes.
Manifold vacuum can be used to drive accessories on automobiles.  The best known application is the vacuum servo, used to provide power assistance for the brakes.  Obsolete applications include vacuum-driven windscreen wipers and Autovac fuel pumps.  Some aircraft instruments (Attitude Indicator (AI) and the Heading Indicator (HI)) are typically vacuum-powered, as protection against loss of all (electrically powered) instruments, since early aircraft often did not have electrical systems, and since there are two readily available sources of vacuum on a moving aircraft, the engine and an external venturi.
Vacuum induction melting uses electromagnetic induction within a vacuum.
Maintaining a vacuum in the condenser is an important aspect of the efficient operation of steam turbines. A steam jet ejector or liquid ring vacuum pump is used for this purpose.  The typical vacuum maintained in the condenser steam space at the exhaust of the turbine (also called condenser backpressure) is in the range 5 to 15 kPa (absolute), depending on the type of condenser and the ambient conditions.
Evaporation and sublimation into a vacuum is called outgassing. All materials, solid or liquid, have a small vapour pressure, and their outgassing becomes important when the vacuum pressure falls below this vapour pressure.  Outgassing has the same effect as a leak and will limit the achievable vacuum. Outgassing products may condense on nearby colder surfaces, which can be troublesome if they obscure optical instruments or react with other materials. This is of great concern to space missions, where an obscured telescope or solar cell can ruin an expensive mission.
The most prevalent outgassing product in vacuum systems is water absorbed by chamber materials. It can be reduced by desiccating or baking the chamber, and removing absorbent materials. Outgassed water can condense in the oil of rotary vane pumps and reduce their net speed drastically if gas ballasting is not used. High vacuum systems must be clean and free of organic matter to minimize outgassing.
Ultra-high vacuum systems are usually baked, preferably under vacuum, to temporarily raise the vapour pressure of all outgassing materials and boil them off. Once the bulk of the outgassing materials are boiled off and evacuated, the system may be cooled to lower vapour pressures and minimize residual outgassing during actual operation. Some systems are cooled well below room temperature by liquid nitrogen to shut down residual outgassing and simultaneously cryopump the system.
Fluids cannot generally be pulled, so a vacuum cannot be created by suction. Suction can spread and dilute a vacuum by letting a higher pressure push fluids into it, but the vacuum has to be created first before suction can occur. The easiest way to create an artificial vacuum is to expand the volume of a container. For example, the diaphragm muscle expands the chest cavity, which causes the volume of the lungs to increase. This expansion reduces the pressure and creates a partial vacuum, which is soon filled by air pushed in by atmospheric pressure.
To continue evacuating a chamber indefinitely without requiring infinite growth, a compartment of the vacuum can be repeatedly closed off, exhausted, and expanded again. This is the principle behind positive displacement pumps, like the manual water pump for example. Inside the pump, a mechanism expands a small sealed cavity to create a vacuum. Because of the pressure differential, some fluid from the chamber (or the well, in our example) is pushed into the pump's small cavity. The pump's cavity is then sealed from the chamber, opened to the atmosphere, and squeezed back to a minute size.
The above explanation is merely a simple introduction to vacuum pumping, and is not representative of the entire range of pumps in use. Many variations of the positive displacement pump have been developed, and many other pump designs rely on fundamentally different principles. Momentum transfer pumps, which bear some similarities to dynamic pumps used at higher pressures, can achieve much higher quality vacuums than positive displacement pumps. Entrapment pumps can capture gases in a solid or absorbed state, often with no moving parts, no seals and no vibration. None of these pumps are universal; each type has important performance limitations. They all share a difficulty in pumping low molecular weight gases, especially hydrogen, helium, and neon.
The lowest pressure that can be attained in a system is also dependent on many things other than the nature of the pumps. Multiple pumps may be connected in series, called stages, to achieve higher vacuums. The choice of seals, chamber geometry, materials, and pump-down procedures will all have an impact. Collectively, these are called vacuum technique. And sometimes, the final pressure is not the only relevant characteristic. Pumping systems differ in oil contamination, vibration, preferential pumping of certain gases, pump-down speeds, intermittent duty cycle, reliability, or tolerance to high leakage rates.
In ultra high vacuum systems, some very "odd" leakage paths and outgassing sources must be considered. The water absorption of aluminium and palladium becomes an unacceptable source of outgassing, and even the adsorptivity of hard metals such as stainless steel or titanium must be considered. Some oils and greases will boil off in extreme vacuums. The permeability of the metallic chamber walls may have to be considered, and the grain direction of the metallic flanges should be parallel to the flange face.
The lowest pressures currently achievable in laboratory are about 1×10−13 torrs (13 pPa).  However, pressures as low as 5×10−17 torrs (6.7 fPa) have been indirectly measured in a 4 K (−269.15 °C; −452.47 °F) cryogenic vacuum system. This corresponds to ≈100 particles/cm3.
Humans and animals exposed to vacuum will lose consciousness after a few seconds and die of hypoxia within minutes, but the symptoms are not nearly as graphic as commonly depicted in media and popular culture. The reduction in pressure lowers the temperature at which blood and other body fluids boil, but the elastic pressure of blood vessels ensures that this boiling point remains above the internal body temperature of 37 °C. Although the blood will not boil, the formation of gas bubbles in bodily fluids at reduced pressures, known as ebullism, is still a concern. The gas may bloat the body to twice its normal size and slow circulation, but tissues are elastic and porous enough to prevent rupture. Swelling and ebullism can be restrained by containment in a flight suit. Shuttle astronauts wore a fitted elastic garment called the Crew Altitude Protection Suit (CAPS) which prevents ebullism at pressures as low as 2 kPa (15 Torr). Rapid boiling will cool the skin and create frost, particularly in the mouth, but this is not a significant hazard.
Animal experiments show that rapid and complete recovery is normal for exposures shorter than 90 seconds, while longer full-body exposures are fatal and resuscitation has never been successful. A study by NASA on eight chimpanzees found all of them survived two and a half minute exposures to vacuum. There is only a limited amount of data available from human accidents, but it is consistent with animal data. Limbs may be exposed for much longer if breathing is not impaired. Robert Boyle was the first to show in 1660 that vacuum is lethal to small animals.
An experiment indicates that plants are able to survive in a low pressure environment (1.5 kPa) for about 30 minutes.
Cold or oxygen-rich atmospheres can sustain life at pressures much lower than atmospheric, as long as the density of oxygen is similar to that of standard sea-level atmosphere. The colder air temperatures found at altitudes of up to 3 km generally compensate for the lower pressures there. Above this altitude, oxygen enrichment is necessary to prevent altitude sickness in humans that did not undergo prior acclimatization, and spacesuits are necessary to prevent ebullism above 19 km.  Most spacesuits use only 20 kPa (150 Torr) of pure oxygen. This pressure is high enough to prevent ebullism, but decompression sickness and gas embolisms can still occur if decompression rates are not managed.
Rapid decompression can be much more dangerous than vacuum exposure itself. Even if the victim does not hold his or her breath, venting through the windpipe may be too slow to prevent the fatal rupture of the delicate alveoli of the lungs. Eardrums and sinuses may be ruptured by rapid decompression, soft tissues may bruise and seep blood, and the stress of shock will accelerate oxygen consumption leading to hypoxia. Injuries caused by rapid decompression are called barotrauma. A pressure drop of 13 kPa (100 Torr), which produces no symptoms if it is gradual, may be fatal if it occurs suddenly.
Some extremophile microorganisms, such as tardigrades, can survive vacuum conditions for periods of days or weeks.
A multistage rocket or step rocket is a launch vehicle that uses two or more rocket stages, each of which contains its own engines and propellant. A tandem or serial stage is mounted on top of another stage; a parallel stage is attached alongside another stage. The result is effectively two or more rockets stacked on top of or attached next to each other. Two-stage rockets are quite common, but rockets with as many as five separate stages have been successfully launched.
By jettisoning stages when they run out of propellant, the mass of the remaining rocket is decreased. Each successive stage can also be optimized for its specific operating conditions, such as decreased atmospheric pressure at higher altitudes. This staging allows the thrust of the remaining stages to more easily accelerate the rocket to its final speed and height.
In serial or tandem staging schemes, the first stage is at the bottom and is usually the largest, the second stage and subsequent upper stages are above it, usually decreasing in size. In parallel staging schemes solid or liquid rocket boosters are used to assist with launch. These are sometimes referred to as "stage 0". In the typical case, the first-stage and booster engines fire to propel the entire rocket upwards. When the boosters run out of fuel, they are detached from the rest of the rocket (usually with some kind of small explosive charge or explosive bolts) and fall away. The  first stage then burns to completion and falls off. This leaves a smaller rocket, with the second stage on the bottom, which then fires. Known in rocketry circles as staging, this process is repeated until the desired final velocity is achieved. In some cases with serial staging, the upper stage ignites before the separation—the interstage ring is designed with this in mind, and the thrust is used to help positively separate the two vehicles.
A multistage rocket is required to reach orbital speed. Single-stage-to-orbit designs are sought, but have not yet been demonstrated.
The reason multi-stage rockets are required is the limitation the laws of physics place on the maximum velocity achievable by a rocket of given fueled-to-dry mass ratio. This relation is given by the classical rocket equation:
where:
The delta v required to reach low Earth orbit (or the required velocity of a sufficiently heavy suborbital payload) requires a wet to dry mass ratio larger than can realistically be achieved in a single rocket stage. The multistage rocket overcomes this limit by splitting the delta-v into fractions. As each lower stage drops off and the succeeding stage fires, the rest of the rocket is still traveling near the burnout speed. Each lower stage's dry mass includes the propellant in the upper stages, and each succeeding upper stage has reduced its dry mass by discarding the useless dry mass of the spent lower stages.
A further advantage is that each stage can use a different type of rocket engine, each tuned for its particular operating conditions. Thus the lower-stage engines are designed for use at atmospheric pressure, while the upper stages can use engines suited to near vacuum conditions. Lower stages tend to require more structure than upper as they need to bear their own weight plus that of the stages above them. Optimizing the structure of each stage decreases the weight of the total vehicle and provides further advantage.
The advantage of staging comes at the cost of the lower stages lifting engines which are not yet being used, as well as making the entire rocket more complex and harder to build than a single stage. In addition, each staging event is a possible point of launch failure, due to separation failure, ignition failure, or stage collision. Nevertheless, the savings are so great that every rocket ever used to deliver a payload into orbit has had staging of some sort.
One of the most common measures of rocket efficiency is its specific impulse, which is defined as the thrust per flow rate (per second) of propellant consumption:
When rearranging the equation such that thrust is calculated as a result of the other factors, we have:
These equations show that a higher specific impulse means a more efficient rocket engine, capable of burning for longer periods of time.  In terms of staging, the initial rocket stages usually have a lower specific impulse rating, trading efficiency for superior thrust in order to quickly push the rocket into higher altitudes.  Later stages of the rocket usually have a higher specific impulse rating because the vehicle is further outside the atmosphere and the exhaust gas does not need to expand against as much atmospheric pressure.
When selecting the ideal rocket engine to use as an initial stage for a launch vehicle, a useful performance metric to examine is the thrust-to-weight ratio, and is calculated by the equation:
The common thrust-to-weight ratio of a launch vehicle is within the range of 1.3 to 2.0.
Another performance metric to keep in mind when designing each rocket stage in a mission is the burn time, which is the amount of time the rocket engine will last before it has exhausted all of its propellant.  For most non-final stages, thrust and specific impulse can be assumed constant, which allows the equation for burn time to be written as:
Where 




m


0





{\displaystyle m_{\mathrm {0} }}

 and 




m


f





{\displaystyle m_{\mathrm {f} }}

 are the initial and final masses of the rocket stage respectively.  In conjunction with the burnout time, the burnout height and velocity are obtained using the same values, and are found by these two equations:
When dealing with the problem of calculating the total burnout velocity or time for the entire rocket system, the general procedure for doing so is as follows:
It is important to note that the burnout time does not define the end of the rocket stage's motion, as the vehicle will still have a velocity that will allow it to coast upward for a brief amount of time until the acceleration of the planet's gravity gradually changes it to a downward direction.  The velocity and altitude of the rocket after burnout can be easily modeled using the basic physics equations of motion.
When comparing one rocket with another, it is impractical to directly compare the rocket's certain trait with the same trait of another because their individual attributes are often not independent of one another.  For this reason, dimensionless ratios have been designed to enable a more meaningful comparison between rockets.  The first is the initial to final mass ratio, which is the ratio between the rocket stage's full initial mass and the rocket stage's final mass once all of its fuel has been consumed.  The equation for this ratio is:
Where 




m


E





{\displaystyle m_{\mathrm {E} }}

 is the empty mass of the stage, 




m


p





{\displaystyle m_{\mathrm {p} }}

 is the mass of the propellant, and 




m


P
L





{\displaystyle m_{\mathrm {PL} }}

 is the mass of the payload.  
The second dimensionless performance quantity is the structural ratio, which is the ratio between the empty mass of the stage, and the combined empty mass and propellant mass as shown in this equation:
The last major dimensionless performance quantity is the payload ratio, which is the ratio between the payload mass and the combined mass of the empty rocket stage and the propellant:
After comparing the three equations for the dimensionless quantities,  it is easy to see that they are not independent of each other, and in fact, the initial to final mass ratio can be rewritten in terms of structural ratio and payload ratio:
These performance ratios can also be used as references for how efficient a rocket system will be when performing optimizations and comparing varying configurations for a mission.
For initial sizing, the rocket equations can be used to derive the amount of propellant needed for the rocket based on the specific impulse of the engine and the total impulse required in N·s.  The equation is:
where g is the gravity constant of Earth.  This also enables the volume of storage required for the fuel to be calculated if the density of the fuel is known, which is almost always the case when designing the rocket stage.  The volume is yielded when dividing the mass of the propellant by its density.  Asides from the fuel required, the mass of the rocket structure itself must also be determined, which requires taking into account the mass of the required thrusters, electronics, instruments, power equipment, etc. These are known quantities for typical off the shelf hardware that should be considered in the mid to late stages of the design, but for preliminary and conceptual design, a simpler approach can be taken.  Assuming one engine for a rocket stage provides all of the total impulse for that particular segment, a mass fraction can be used to determine the mass of the system.  The mass of the stage transfer hardware such as initiators and safe-and-arm devices are very small by comparison and can be considered negligible.
For modern day solid rocket motors, it is a safe and reasonable assumption to say that 91 to 94 percent of the total mass is fuel.  It is also important to note there is a small percentage of "residual" propellant that will be left stuck and unusable inside the tank, and should also be taken into consideration when determining amount of fuel for the rocket.  A common initial estimate for this residual propellant is five percent.  With this ratio and the mass of the propellant calculated, the mass of the empty rocket weight can be determined.  
Sizing rockets using a liquid bipropellant requires a slightly more involved approach because there are two separate tanks that are required:  one for the fuel, and one for the oxidizer.  The ratio of these two quantities is known as the mixture ratio, and is defined by the equation:
Where 




m


o
x





{\displaystyle m_{\mathrm {ox} }}

 is the mass of the oxidizer and 




m


f
u
e
l





{\displaystyle m_{\mathrm {fuel} }}

 is the mass of the fuel.  This mixture ratio not only governs the size of each tank, but also the specific impulse of the rocket.  Determining the ideal mixture ratio is a balance of compromises between various aspects of the rocket being designed, and can vary depending on the type of fuel and oxidizer combination being used.  For example, a mixture ratio of a bipropellant could be adjusted such that it may not have the optimal specific impulse, but will result in fuel tanks of equal size.  This would yield simpler and cheaper manufacturing, packing, configuring, and integrating of the fuel systems with the rest of the rocket, and can become a benefit that could outweigh the drawbacks of a less efficient specific impulse rating.  But suppose the defining constraint for the launch system is volume, and a low density fuel is required such as hydrogen.  This example would be solved by using an oxidizer-rich mixture ratio, reducing efficiency and specific impulse rating, but will meet a smaller tank volume requirement.
The ultimate goal of optimal staging is to maximize the payload ratio (see ratios under performance), meaning the largest amount of payload is carried up to the required burnout velocity using the least amount of non-payload mass, which comprises everything else. Here are a few quick rules and guidelines to follow in order to reach optimal staging:
The payload ratio can be calculated for each individual stage, and when multiplied together in sequence, will yield the overall payload ratio of the entire system.  It is important to note that when computing payload ratio for individual stages, the payload includes the mass of all the stages after the current one.  The overall payload ratio is:
Where n is the number of stages the rocket system comprises.  Similar stages yielding the same payload ratio simplify this equation, however that is seldom the ideal solution for maximizing payload ratio, and ΔV requirements may have to be partitioned unevenly as suggested in guideline tips 1 and 2 from above.  Two common methods of determining this perfect ΔV partition between stages are either a technical algorithm that generates an analytical solution that can be implemented by a program, or simple trial and error.  For the trial and error approach, it is best to begin with the final stage, calculating the initial mass which becomes the payload for the previous stage.  From there it is easy to progress all the way down to the initial stage in the same manner, sizing all the stages of the rocket system.
Restricted rocket staging is based on the simplified assumption that each of the stages of the rocket system have the same specific impulse, structural ratio, and payload ratio, the only difference being the total mass of each increasing stage is less than that of the previous stage.  Although this assumption may not be the ideal approach to yielding an efficient or optimal system, it greatly simplifies the equations for determining the burnout velocities, burnout times, burnout altitudes, and mass of each stage.  This would make for a better approach to a conceptual design in a situation where a basic understanding of the system behavior is preferential to a detailed, accurate design.  
One important concept to understand when undergoing restricted rocket staging, is how the burnout velocity is affected by the number of stages that split up the rocket system.  Increasing the number of stages for a rocket while keeping the specific impulse, payload ratios and structural ratios constant will always yield a higher burnout velocity than the same systems that use fewer stages.  However, the law of diminishing returns is evident in that each increment in number of stages gives less of an improvement in burnout velocity than the previous increment.  The burnout velocity gradually converges towards an asymptotic value as the number of stages increases towards a very high number.  In addition to diminishing returns in burnout velocity improvement, the main reason why real world rockets seldom use more than three stages is because of increase of weight and complexity in the system for each added stage, ultimately yielding a higher cost for deployment.
A rocket system that implements tandem staging means that each individual stage runs in order one after the other.  The rocket breaks free from the previous stage, then begins burning through the next stage in straight succession.  On the other hand, a rocket that implements parallel staging has two or more different stages that are active at the same time.  For example, the Space Shuttle has two Solid Rocket Boosters that burn simultaneously.  Upon launch, the boosters ignite, and at the end of the stage, the two boosters are discarded while the external fuel tank is kept for another stage.  
Most quantitative approaches to the design of the rocket system's performance are focused on tandem staging, but the approach can be easily modified to include parallel staging.  To begin with, the different stages of the rocket should be clearly defined.  Continuing with the previous example, the end of the first stage which is sometimes referred to as 'stage 0', can be defined as when the side boosters separate from the main rocket.  From there, the final mass of stage one can be considered the sum of the empty mass of stage one, the mass of stage two (the main rocket and the remaining unburned fuel) and the mass of the payload.
High-altitude and space-bound upper stages are designed to operate with little or no atmospheric pressure. This allows the use of lower pressure combustion chambers and engine nozzles with optimal vacuum expansion ratios. Some upper stages, especially those using hypergolic propellants like Delta-K or Ariane 5 ES second stage, are pressure fed, which eliminates the need for complex turbopumps. Other upper stages, such as the Centaur or DCSS, use liquid hydrogen expander cycle engines, or gas generator cycle engines like the Ariane 5 ECA's HM7B or the S-IVB's J-2. These stages are usually tasked with completing orbital injection and accelerating payloads into higher energy orbits such as GTO or to escape velocity. Upper stages, such as Fregat, used primarily to bring payloads from low Earth orbit to GTO or beyond are sometimes referred to as space tugs.
Each individual stage is generally assembled at its manufacturing site and shipped to the launch site; the term vehicle assembly refers to the mating of all rocket stage(s) and the spacecraft payload into a single assembly known as a space vehicle. Single-stage vehicles (suborbital), and multistage vehicles on the smaller end of the size range, can usually be assembled directly on the launch pad by lifting the stage(s) and spacecraft vertically in place by means of a crane.
This is generally not practical for larger space vehicles, which are assembled off the pad and moved into place on the launch site by various methods. NASA's Apollo/Saturn V manned Moon landing vehicle, and Space Shuttle, were assembled vertically onto mobile launcher platforms with attached launch umbilical towers, in a Vehicle Assembly Building, and then a special crawler-transporter moved the entire vehicle stack to the launch pad in an upright position. In contrast, vehicles such as the Russian Soyuz rocket and the SpaceX Falcon 9 are assembled horizontally in a processing hangar, transported horizontally, and then brought upright at the pad.
Spent upper stages of launch vehicles are a significant source of space debris remaining in orbit in a non-operational state for many years after use, and occasionally, large debris fields created from the breakup of a single upper stage while in orbit.
After the 1990s, spent upper stages are generally passivated after their use as a launch vehicle is complete in order to minimize risks while the stage remains derelict in orbit.  Passivation means removing any sources of stored energy remaining on the vehicle, as by dumping fuel or discharging batteries.
Many early upper stages, in both the Soviet and U.S. space programs, were not passivated after mission completion.  During the initial attempts to characterize the space debris problem, it became evident that a good proportion of all debris was due to the breaking up of rocket upper stages, particularly unpassivated upper-stage propulsion units.
An illustration and description in the 14th century Chinese Huolongjing by Jiao Yu and Liu Bowen shows the oldest known multistage rocket; this was the "fire-dragon issuing from the water" (火龙出水, huǒ lóng chū shuǐ), which was used mostly by the Chinese navy. It was a two-stage rocket that had booster rockets that would eventually burn out, yet before they did they automatically ignited a number of smaller rocket arrows that were shot out of the front end of the missile, which was shaped like a dragon's head with an open mouth. This multi-stage rocket may be considered the ancestor to the modern YingJi-62 ASCM. The British scientist and historian Joseph Needham points out that the written material and depicted illustration of this rocket come from the oldest stratum of the Huolongjing, which can be dated roughly 1300–1350 AD (from the book's part 1, chapter 3, page 23).
Another example of an early multistaged rocket is the Juhwa (走火) of Korean development. It was proposed by medieval Korean engineer, scientist and inventor Choe Museon and developed by the Firearms Bureau (火㷁道監) during the 14th century. The rocket had the length of 15 cm and 13 cm; the diameter was 2.2 cm. It was attached to an arrow 110 cm long; experimental records show that the first results were around 200m in range. There are records that show Korea kept developing this technology until it came to produce the Singijeon, or 'magical machine arrows' in the 16th century. 
The earliest experiments with multistage rockets in Europe were made in 1551 by Austrian Conrad Haas (1509–1576), the arsenal master of the town of Hermannstadt, Transylvania (now Sibiu/Hermannstadt, Romania). This concept was developed independently by at least five individuals:
The first high-speed multistage rockets were the RTV-G-4 Bumper rockets tested at the White Sands Proving Ground and later at Cape Canaveral from 1948 to 1950. These consisted of a V-2 rocket and a WAC Corporal sounding rocket. The greatest altitude ever reached was 393 km, attained on February 24, 1949, at White Sands.
In 1947, the Soviet rocket engineer and scientist Mikhail Tikhonravov developed a theory of parallel stages, which he called "packet rockets". In his scheme, three parallel stages were fired from liftoff, but all three engines were fueled from the outer two stages, until they are empty and could be ejected. This is more efficient than sequential staging, because the second-stage engine is never just dead weight. In 1951, Soviet engineer and scientist Dmitry Okhotsimsky carried out a pioneering engineering study of general sequential and parallel staging, with and without the pumping of fuel between stages. The design of the R-7 Semyorka emerged from that study.  The trio of rocket engines used in the first stage of the American Atlas I and Atlas II launch vehicles, arranged in a row, used parallel staging in a similar way: the outer pair of booster engines existed as a jettisonable pair which would, after they shut down, drop away with the lowermost outer skirt structure, leaving the central sustainer engine to complete the first stage's engine burn towards apogee or orbit.
Separation of each portion of a multistage rocket introduces additional risk into the success of the launch mission.  Reducing the number of separation events results in a reduction in complexity. 
Separation events occur when stages or strap-on boosters separate after use, when the payload fairing separates prior to orbital insertion, or when used, a launch escape system which separates after the early phase of a launch. Pyrotechnic fasteners, or in some cases pneumatic systems like on the Falcon 9 Full Thrust, are typically used to separate rocket stages.
The three-stage-to-orbit launch system is a commonly used rocket system to attain Earth orbit. The spacecraft uses three distinct stages to provide propulsion consecutively in order to achieve orbital velocity. It is intermediate between a four-stage-to-orbit launcher and a two-stage-to-orbit launcher.
Other designs (in fact, most modern medium- to heavy-lift designs) do not have all three stages inline on the main stack, instead having strap-on boosters for the "stage-0" with two core stages. In these designs, the boosters and first stage fire simultaneously instead of consecutively, providing extra initial thrust to lift the full launcher weight and overcome gravity losses and atmospheric drag. The boosters are jettisoned a few minutes into flight to reduce weight.
The four-stage-to-orbit launch system is a rocket system used to attain Earth orbit. The spacecraft uses four distinct stages to provide propulsion consecutively in order to achieve orbital velocity. It is intermediate between a five-stage-to-orbit launcher and a three-stage-to-orbit launcher.
Other designs do not have all four stages inline on the main stack, instead having strap-on boosters for the "stage-0" with three core stages. In these designs, the boosters and first stage fire simultaneously instead of consecutively, providing extra initial thrust to lift the full launcher weight and overcome gravity losses and atmospheric drag. The boosters are jettisoned a few minutes into flight to reduce weight.

In celestial mechanics, escape velocity or escape speed is the minimum speed needed for a free, non-propelled object to escape from the gravitational influence of a primary body, thus reaching an infinite distance from it.  It is typically stated as an ideal speed, ignoring atmospheric friction.  Although the term "escape velocity" is common, it is more accurately described as a speed than a velocity because it is independent of direction; the escape speed increases with the mass of the primary body and decreases with the distance from the primary body. The escape speed thus depends on how far the object has already traveled, and its calculation at a given distance takes into account that without new acceleration it will slow down as it travels—due to the massive body's gravity—but it will never quite slow to a stop. 
A rocket, continuously accelerated by its exhaust, can escape without ever reaching escape speed, since it continues to add kinetic energy from its engines. It can achieve escape at any speed, given sufficient propellant to provide new acceleration to the rocket to counter gravity's deceleration and thus maintain its speed. 
More generally, escape velocity is the speed at which the sum of an object's kinetic energy and its gravitational potential energy is equal to zero; an object which has achieved escape velocity is neither on the surface, nor in a closed orbit (of any radius). With escape velocity in a direction pointing away from the ground of a massive body, the object will move away from the body, slowing forever and approaching, but never reaching, zero speed.  Once escape velocity is achieved, no further impulse need be applied for it to continue in its escape. In other words, if given escape velocity, the object will move away from the other body, continually slowing, and will asymptotically approach zero speed as the object's distance approaches infinity, never to come back. Speeds higher than escape velocity retain a positive speed at infinite distance. Note that the minimum escape velocity assumes that there is no friction (e.g., atmospheric drag), which would increase the required instantaneous velocity to escape the gravitational influence, and that there will be no future acceleration or extraneous deceleration (for example from thrust or from gravity of other bodies), which would change the required instantaneous velocity.
Escape speed at a distance d from the center of a spherically symmetric primary body (such as a star or a planet) with mass M is given by the formula
where G is the universal gravitational constant (G ≈ 6.67×10−11 m3·kg−1·s−2). The escape speed is independent of the mass of the escaping object.  For example, the escape speed from Earth's surface is about 11.186 km/s (40,270 km/h; 25,020 mph; 36,700 ft/s).
When given an initial speed 



V


{\displaystyle V}

 greater than the escape speed 




v

e


,


{\displaystyle v_{e},}

 the object will asymptotically approach the hyperbolic excess speed 




v

∞


,


{\displaystyle v_{\infty },}

 satisfying the equation:
In these equations atmospheric friction (air drag) is not taken into account.
The existence of escape velocity is a consequence of conservation of energy and an energy field of finite depth. For an object with a given total energy, which is moving subject to conservative forces (such as a static gravity field) it is only possible for the object to reach combinations of locations and speeds which have that total energy; and places which have a higher potential energy than this cannot be reached at all. By adding speed (kinetic energy) to the object it expands the possible locations that can be reached, until, with enough energy, they become infinite.
For a given gravitational potential energy at a given position, the escape velocity is the minimum speed an object without propulsion needs to be able to "escape" from the gravity (i.e. so that gravity will never manage to pull it back). Escape velocity is actually a speed (not a velocity) because it does not specify a direction: no matter what the direction of travel is, the object can escape the gravitational field (provided its path does not intersect the planet).
An elegant way to derive the formula for escape velocity is to use the principle of conservation of energy (for another way, based on work, see below).  For the sake of simplicity, unless stated otherwise, we assume that an object will escape the gravitational field of a uniform spherical planet by moving away from it and that the only significant force acting on the moving object is the planet's gravity. Imagine that a spaceship of mass m is initially at a distance r from the center of mass of the planet, whose mass is M, and its initial speed is equal to its escape velocity, 




v

e




{\displaystyle v_{e}}

. At its final state, it will be an infinite distance away from the planet, and its speed will be negligibly small. Kinetic energy K and gravitational potential energy Ug are the only types of energy that we will deal with (we will ignore the drag of the atmosphere), so by the conservation of energy,
We can set Kfinal = 0 because final velocity is arbitrarily small, and Ugfinal = 0 because   final distance is infinity, so
where μ is the standard gravitational parameter.
The same result is obtained by a relativistic calculation, in which case the variable r represents the radial coordinate or reduced circumference of the Schwarzschild metric.
Defined a little more formally, "escape velocity" is the initial speed required to go from an initial point in a gravitational potential field to infinity and end at infinity with a residual speed of zero, without any additional acceleration. All speeds and velocities are measured with respect to the field. Additionally, the escape velocity at a point in space is equal to the speed that an object would have if it started at rest from an infinite distance and was pulled by gravity to that point.
In common usage, the initial point is on the surface of a planet or moon. On the surface of the Earth, the escape velocity is about 11.2 km/s, which is approximately 33 times the speed of sound (Mach 33) and several times the muzzle velocity of a rifle bullet (up to 1.7 km/s). However, at 9,000 km altitude in "space", it is slightly less than 7.1 km/s. Note that this escape velocity is relative to a non-rotating frame of reference, not relative to the moving surface of the planet or moon (see below).
The escape velocity is independent of the mass of the escaping object. It does not matter if the mass is 1 kg or 1,000 kg; what differs is the amount of energy required. For an object of mass 



m


{\displaystyle m}

 the energy required to escape the Earth's gravitational field is GMm / r, a function of the object's mass (where r is radius of the Earth, nominally 6,371 kilometres (3,959 mi), G is the gravitational constant, and M is the mass of the Earth, M = 5.9736 × 1024 kg). A related quantity is the specific orbital energy which is essentially the sum of the kinetic and potential energy divided by the mass. An object has reached escape velocity when the specific orbital energy is greater than or equal to zero.
An alternative expression for the escape velocity 




v

e




{\displaystyle v_{e}}

 particularly useful at the surface on the body is:
where r is the distance between the center of the body and the point at which escape velocity is being calculated and g is the gravitational acceleration at that distance (i.e., the surface gravity).
For a body with a spherically-symmetric distribution of mass, the escape velocity 




v

e




{\displaystyle v_{e}}

 from the surface is proportional to the radius assuming constant density, and proportional to the square root of the average density ρ.
where 



K
=




8
3


π
G


≈
2.364
×

10

−
5




 m


1.5




 kg


−
0.5




 s


−
1




{\textstyle K={\sqrt {{\frac {8}{3}}\pi G}}\approx 2.364\times 10^{-5}{\text{ m}}^{1.5}{\text{ kg}}^{-0.5}{\text{ s}}^{-1}}


Note that this escape velocity is relative to a non-rotating frame of reference, not relative to the moving surface of the planet or moon, as we now explain.
The escape velocity relative to the surface of a rotating body depends on direction in which the escaping body travels. For example, as the Earth's rotational velocity is 465 m/s at the equator, a rocket launched tangentially from the Earth's equator to the east requires an initial velocity of about 10.735 km/s relative to the moving surface at the point of launch to escape whereas a rocket launched tangentially from the Earth's equator to the west requires an initial velocity of about 11.665 km/s relative to that moving surface. The surface velocity decreases with the cosine of the geographic latitude, so space launch facilities are often located as close to the equator as feasible, e.g. the American Cape Canaveral (latitude 28°28′ N) and the French Guiana Space Centre (latitude 5°14′ N).
In most situations it is impractical to achieve escape velocity almost instantly, because of the acceleration implied, and also because if there is an atmosphere, the hypersonic speeds involved (on Earth a speed of 11.2 km/s, or 40,320 km/h) would cause most objects to burn up due to aerodynamic heating or be torn apart by atmospheric drag. For an actual escape orbit, a spacecraft will accelerate steadily out of the atmosphere until it reaches the escape velocity appropriate for its altitude (which will be less than on the surface). In many cases, the spacecraft may be first placed in a parking orbit (e.g. a low Earth orbit at 160–2,000 km) and then accelerated to the escape velocity at that altitude, which will be slightly lower (about 11.0 km/s at a low Earth orbit of 200 km). The required additional change in speed, however, is far less because the spacecraft already has a significant orbital speed (in low Earth orbit speed is approximately 7.8 km/s, or 28,080 km/h).
The escape velocity at a given height is 





2




{\displaystyle {\sqrt {2}}}

 times the speed in a circular orbit at the same height, (compare this with the velocity equation in circular orbit). This corresponds to the fact that the potential energy with respect to infinity of an object in such an orbit is minus two times its kinetic energy, while to escape the sum of potential and kinetic energy needs to be at least zero. The velocity corresponding to the circular orbit is sometimes called the first cosmic velocity, whereas in this context the escape velocity is referred to as the second cosmic velocity.
For a body in an elliptical orbit wishing to accelerate to an escape orbit the required speed will vary, and will be greatest at periapsis when the body is closest to the central body. However, the orbital speed of the body will also be at its highest at this point, and the change in velocity required will be at its lowest, as explained by the Oberth effect.
Technically escape velocity can either be measured as a relative to the other, central body or relative to center of mass or barycenter of the system of bodies. Thus for systems of two bodies, the term escape velocity can be ambiguous, but it is usually intended to mean the barycentric escape velocity of the less massive body. In gravitational fields, escape velocity refers to the escape velocity of zero mass test particles relative to the barycenter of the masses generating the field. In most situations involving spacecraft the difference is negligible. For a mass equal to a Saturn V rocket, the escape velocity relative to the launch pad is 253.5 am/s (8 nanometers per year) faster than the escape velocity relative to the mutual center of mass.
Ignoring all factors other than the gravitational force between the body and the object, an object projected vertically at speed 



v


{\displaystyle v}

 from the surface of a spherical body with escape velocity 




v

e




{\displaystyle v_{e}}

 and radius 



R


{\displaystyle R}

  will attain a maximum height 



h


{\displaystyle h}

 satisfying the equation
which, solving for h results in
where 



x
=
v

/


v

e




{\textstyle x=v/v_{e}}

 is the ratio of the original speed 



v


{\displaystyle v}

 to the escape velocity 




v

e


.


{\displaystyle v_{e}.}


Unlike escape velocity, the direction (vertically up) is important to achieve maximum height.
If an object attains exactly escape velocity, but is not directed straight away from the planet, then it will follow a curved path or trajectory. Although this trajectory does not form a closed shape, it can be referred to as an orbit. Assuming that gravity is the only significant force in the system, this object's speed at any point in the trajectory will be equal to the escape velocity at that point due to the conservation of energy, its total energy must always be 0, which implies that it always has escape velocity; see the derivation above. The shape of the trajectory will be a parabola whose focus is located at the center of mass of the planet. An actual escape requires a course with a trajectory that does not intersect with the planet, or its atmosphere, since this would cause the object to crash. When moving away from the source, this path is called an escape orbit. Escape orbits are known as C3 = 0 orbits. C3 is the characteristic energy, = −GM/2a, where a is the semi-major axis, which is infinite for parabolic trajectories.
If the body has a velocity greater than escape velocity then its path will form a hyperbolic trajectory and it will have an excess hyperbolic velocity, equivalent to the extra energy the body has. A relatively small extra delta-v above that needed to accelerate to the escape speed can result in a relatively large speed at infinity. Some orbital manoeuvres make use of this fact. For example, at a place where escape speed is 11.2 km/s, the addition of 0.4 km/s yields a hyperbolic excess speed of 3.02 km/s:
If a body in circular orbit (or at the periapsis of an elliptical orbit) accelerates along its direction of travel to escape velocity, the point of acceleration will form the periapsis of the escape trajectory. The eventual direction of travel will be at 90 degrees to the direction at the point of acceleration. If the body accelerates to beyond escape velocity the eventual direction of travel will be at a smaller angle, and indicated by one of the asymptotes of the hyperbolic trajectory it is now taking. This means the timing of the acceleration is critical if the intention is to escape in a particular direction.
If the speed at periapsis is v, then the eccentricity of the trajectory is given by:
This is valid for elliptical, parabolic, and hyperbolic trajectories. If the trajectory is hyperbolic or parabolic, it will asymptotically approach an angle 



θ


{\displaystyle \theta }

 from the direction at periapsis, with
The speed will asymptotically approach
In this table, the left-hand half gives the escape velocity from the visible surface (which may be gaseous as with Jupiter for example), relative to the centre of the planet or moon (that is, not relative to its moving surface). In the right-hand half, Ve refers to the speed relative to the central body (for example the sun), whereas Vte is the speed (at the visible surface of the smaller body) relative to the smaller body (planet or moon).
The last two columns will depend precisely where in orbit escape velocity is reached, as the orbits are not exactly circular (particularly Mercury and Pluto).
Let G be the gravitational constant and let M be the mass of the earth (or other gravitating body) and m be the mass of the escaping body or projectile. At a distance r from the centre of gravitation the body feels an attractive force
The work needed to move the body over a small distance dr against this force is therefore given by
The total work needed to move the body from the surface r0 of the gravitating body to infinity is then
In order to do this work to reach infinity, the body's minimal kinetic energy at departure must match this work, so the escape velocity v0 satisfies
which results in

An airbreathing jet engine (or ducted jet engine) is a jet engine that ejects a propelling (reaction) jet of hot exhaust gases after first taking in atmospheric air, followed by compression, heating and expansion back to atmospheric pressure through a nozzle. Alternatively the reaction jet may include a cold jet of ducted bypass air which has been compressed by a fan before returning to atmospheric pressure through an additional nozzle. These engines are gas turbine engines. Engines using only ram for the compression process, and no turbomachinery, are the ramjet and pulsejet.
All practical airbreathing jet engines heat the air by burning fuel. Alternatively a heat exchanger may be used (nuclear-powered jet engine). Most modern jet engines are turbofans. They replaced turbojets as they use less fuel.
The original air-breathing gas turbine jet engine was the turbojet. It was a concept brought to life by two engineers, Frank Whittle in England UK and Hans von Ohain in Germany. The turbojet compresses and heats air and then exhausts it as a high speed, high temperature jet to create thrust. While these engines are capable of giving high thrust levels, they are most efficient at very high speeds (over Mach 1), due to the low-mass-flow, high speed nature of the jet exhaust.
Modern turbofans are a development of the turbojet; they are basically a turbojet that includes a new section called the fan stage. Rather than using all of its exhaust gases to provide direct thrust like a turbojet, the turbofan engine extracts some of the power from the exhaust gases inside the engine and uses it to power the fan stage. The fan stage accelerates a large volume of air through a duct, bypassing the engine core (the actual gas turbine component of the engine), and expelling it at the rear as a jet, creating thrust. A proportion of the air that comes through the fan stage enters the engine core rather than being ducted to the rear, and is thus compressed and heated; some of the energy is extracted to power the compressors and fans, while the remainder is exhausted at the rear. This high-speed, hot-gas exhaust blends with the low speed, cool-air exhaust from the fan stage, and both contribute to the overall thrust of the engine. Depending on what proportion of cool air is bypassed around the engine core, a turbofan can be called low-bypass, high-bypass, or very-high-bypass engines.
Low bypass engines were the first turbofan engines produced, and provide the majority of their thrust from the hot core exhaust gases, while the fan stage only supplements this. These engines are still commonly seen on military fighter aircraft, because they have a smaller frontal area which creates less ram drag at supersonic speeds leaving more of the thrust produced by the engine to propel the aircraft. Their comparatively high noise levels and subsonic fuel consumption are deemed acceptable in such an application, whereas although the first generation of turbofan airliners used low-bypass engines, their high noise levels and fuel consumption mean they have fallen out of favor for large aircraft. High bypass engines have a much larger fan stage, and provide most of their thrust from the ducted air of the fan; the engine core provides power to the fan stage, and only a proportion of the overall thrust comes from the engine core exhaust stream.
Over the last several decades, there has been a move towards very high bypass engines, which use fans far larger than the engine core itself, which is typically a modern, high efficiency two or three-spool design. This high efficiency and power is what allows such large fans to be viable, and the increased thrust available (up to 75,000 lbs per engine in engines such as the Rolls-Royce Trent XWB or General Electric GENx), have allowed a move to large twin engine aircraft, such as the Airbus A350 or Boeing 777, as well as allowing twin engine aircraft to operate on long overwater routes, previously the domain of 3-engine or 4-engine aircraft.
Jet engines were designed to power aircraft, but have been used to power jet cars and jet boats for speed record attempts, and even for commercial uses such as by railroads for clearing snow and ice from switches in railyards (mounted in special rail cars), and by race tracks for drying off track surfaces after rain (mounted in special trucks with the jet exhaust blowing onto the track surface).
Airbreathing jet engines are nearly always internal combustion engines that obtain propulsion from the combustion of fuel inside the engine. Oxygen present in the atmosphere is used to oxidise a fuel source, typically a hydrocarbon-based jet fuel. The burning mixture expands greatly in volume, driving heated air through a propelling nozzle.
Gas turbine powered engines:
Ram powered jet engine:
Pulsed combustion jet engine:
Two engineers, Frank Whittle in the UK and Hans von Ohain in Germany, developed the turbojet concept independently into practical engines during the late 1930s.
Turbojets consist of an inlet, a compressor, a combustor, a turbine (that drives the compressor) and a propelling nozzle. The compressed air is heated in the combustor and passes through the turbine, then expands in the nozzle to produce a high speed propelling jet
Turbojets have a low propulsive efficiency below about Mach 2 and produce a lot of jet noise, both a result of the very high velocity of the exhaust. Modern jet propelled aircraft are powered by turbofans. These engines, with their lower exhaust velocities, produce less jet noise and use less fuel. Turbojets are still used to power medium range cruise missiles due to their high exhaust speed, low frontal area, which reduces drag, and relative simplicity, which reduces cost.
Most modern jet engines are turbofans. The low pressure compressor (LPC), usually known as a fan, compresses air into a bypass duct whilst its inner portion supercharges the core compressor. The fan is often an integral part of a multi-stage core LPC. The bypass airflow either passes to a separate 'cold nozzle' or mixes with low pressure turbine exhaust gases, before expanding through a 'mixed flow nozzle'.
In the 1960s there was little difference between civil and military jet engines, apart from the use of afterburning in some (supersonic) applications. Today, turbofans are used for airliners because they have an exhaust speed that is better matched to the subsonic flight speed of the airliner. At airliner flight speeds, the exhaust speed from a turbojet engine is excessively high and wastes energy. The lower exhaust speed from a turbofan gives better fuel consumption. The increased airflow from the fan gives higher thrust at low speeds. The lower exhaust speed also gives much lower jet noise.
The comparatively large frontal fan has several effects. Compared to a turbojet of identical thrust, a turbofan has a much larger air mass flow rate and the flow through the bypass duct generates a significant fraction of the thrust. The additional duct air has not been ignited which it gives it a slow speed, but no extra fuel is needed to provide this thrust. Instead, the energy is taken from the central core, which gives it also a reduced exhaust speed. The average velocity of the mixed exhaust air is thus reduced (low specific thrust) which is less wasteful of energy but reduces the top speed. Overall, a turbofan can be much more fuel efficient and quieter, and it turns out that the fan also allows greater net thrust to be available at slow speeds.
Thus civil turbofans today have a low exhaust speed (low specific thrust – net thrust divided by airflow) to keep jet noise to a minimum and to improve fuel efficiency. Consequently, the bypass ratio (bypass flow divided by core flow) is relatively high (ratios from 4:1 up to 8:1 are common), with the Rolls-Royce Trent XWB approaching 10:1. Only a single fan stage is required, because a low specific thrust implies a low fan pressure ratio.
Turbofans in civilian aircraft usually have a pronounced large front area to accommodate a very large fan, as their design involves a much larger mass of air bypassing the core so they can benefit from these effects, while in military aircraft, where noise and efficiency are less important compared to performance and drag, a smaller amount of air typically bypasses the core. Turbofans designed for subsonic civilian aircraft also usually have a just a single front fan, because their additional thrust is generated by a large additional mass of air which is only moderately compressed, rather than a smaller amount of air which is greatly compressed.
Military turbofans, however, have a relatively high specific thrust, to maximize the thrust for a given frontal area, jet noise being of less concern in military uses relative to civil uses. Multistage fans are normally needed to reach the relatively high fan pressure ratio needed for high specific thrust. Although high turbine inlet temperatures are often employed, the bypass ratio tends to be low, usually significantly less than 2.0.
Turboprop engines are jet engine derivatives, still gas turbines, that extract work from the hot-exhaust jet to turn a rotating shaft, which is then used to produce thrust by some other means.  While not strictly jet engines in that they rely on an auxiliary mechanism to produce thrust, turboprops are very similar to other turbine-based jet engines, and are often described as such.
In turboprop engines, a portion of the engine's thrust is produced by spinning a propeller, rather than relying solely on high-speed jet exhaust.  Producing thrust both ways, turboprops are occasionally referred to as a type of hybrid jet engine.  They differ from turbofans in that a traditional propeller, rather than a ducted fan, provides the majority of thrust.  Most turboprops use gear-reduction between the turbine and the propeller.  (Geared turbofans also feature gear reduction, but they are less common.)  The hot-jet exhaust is an important minority of thrust, and maximum thrust is obtained by matching the two thrust contributions.  Turboprops generally have better performance than turbojets or turbofans at low speeds where propeller efficiency is high, but become increasingly noisy and inefficient at high speeds.
Turboshaft engines are very similar to turboprops, differing in that nearly all energy in the exhaust is extracted to spin the rotating shaft, which is used to power machinery rather than a propeller, they therefore generate little to no jet thrust and are often used to power helicopters.
A propfan engine (also called "unducted fan", "open rotor", or "ultra-high bypass") is a jet engine that uses its gas generator to power an exposed fan, similar to turboprop engines. Like turboprop engines, propfans generate most of their thrust from the propeller and not the exhaust jet. The primary difference between turboprop and propfan design is that the propeller blades on a propfan are highly swept to allow them to operate at speeds around Mach 0.8, which is competitive with modern commercial turbofans. These engines have the fuel efficiency advantages of turboprops with the performance capability of commercial turbofans. While significant research and testing (including flight testing) has been conducted on propfans, none have entered production.
Major components of a turbojet including references to turbofans, turboprops and turboshafts:



The various components named above have constraints on how they are put together to generate the most efficiency or performance. The performance and efficiency of an engine can never be taken in isolation; for example fuel/distance efficiency of a supersonic jet engine maximises at about mach 2, whereas the drag for the vehicle carrying it is increasing as a square law and has much extra drag in the transonic region. The highest fuel efficiency for the overall vehicle is thus typically at Mach ~0.85.
For the engine optimisation for its intended use, important here is air intake design, overall size, number of compressor stages (sets of blades), fuel type, number of exhaust stages, metallurgy of components, amount of bypass air used, where the bypass air is introduced, and many other factors. An example is design of the air intake.
The thermodynamics of a typical air-breathing jet engine are modeled approximately by a Brayton Cycle which is a thermodynamic cycle that describes the workings of the gas turbine engine, which is the basis of the airbreathing jet engine and others. It is named after George Brayton (1830–1892), the American engineer who developed it, although it was originally proposed and patented by Englishman John Barber in 1791. It is also sometimes known as the Joule cycle.
The nominal net thrust quoted for a jet engine usually refers to the Sea Level Static (SLS) condition, either for the International Standard Atmosphere (ISA) or a hot day condition (e.g. ISA+10 °C). As an example, the GE90-76B has a take-off static thrust of 76,000 lbf (360 kN) at SLS, ISA+15 °C.
Naturally, net thrust will decrease with altitude, because of the lower air density. There is also, however, a flight speed effect.
Initially as the aircraft gains speed down the runway, there will be little increase in nozzle pressure and temperature, because the ram rise in the intake is very small. There will also be little change in mass flow. Consequently, nozzle gross thrust initially only increases marginally with flight speed. However, being an air breathing engine (unlike a conventional rocket) there is a penalty for taking on-board air from the atmosphere. This is known as ram drag. Although the penalty is zero at static conditions, it rapidly increases with flight speed causing the net thrust to be eroded.
As flight speed builds up after take-off, the ram rise in the intake starts to have a significant effect upon nozzle pressure/temperature and intake airflow, causing nozzle gross thrust to climb more rapidly. This term now starts to offset the still increasing ram drag, eventually causing net thrust to start to increase. In some engines, the net thrust at say Mach 1.0, sea level can even be slightly greater than the static thrust. Above Mach 1.0, with a subsonic inlet design, shock losses tend to decrease net thrust, however a suitably designed supersonic inlet can give a lower reduction in intake pressure recovery, allowing net thrust to continue to climb in the supersonic regime.
Jet engines are usually very reliable and have a very good safety record. However, failures do sometimes occur.
In some cases in jet engines the conditions in the engine due to airflow entering the engine or other variations can cause the compressor blades to stall. When this occurs the pressure in the engine blows out past the blades, and the stall is maintained until the pressure has decreased, and the engine has lost all thrust. The compressor blades will then usually come out of stall, and re-pressurize the engine. If conditions are not corrected, the cycle will usually repeat. This is called surge. Depending on the engine this can be highly damaging to the engine and creates worrying vibrations for the crew.
Fan, compressor or turbine blade failures have to be contained within the engine casing. To do this the engine has to be designed to pass blade containment tests as specified by certification authorities.
Bird ingestion is the term used when birds enter the intake of a jet engine. It is a common aircraft safety hazard and has caused fatal accidents. In 1988 an Ethiopian Airlines Boeing 737 ingested pigeons into both engines during take-off and then crashed in an attempt to return to the Bahir Dar airport; of the 104 people aboard, 35 died and 21 were injured. In another incident in 1995, a Dassault Falcon 20 crashed at a Paris airport during an emergency landing attempt after ingesting lapwings into an engine, which caused an engine failure and a fire in the airplane fuselage; all 10 people on board were killed.
Jet engines have to be designed to withstand the ingestion of birds of a specified weight and number, and to not lose more than a specified amount of thrust.  The weight and numbers of birds that can be ingested without hazarding the safe flight of the aircraft are related to the engine intake area. In 2009, an Airbus A320 aircraft, US Airways Flight 1549, ingested one Canada goose into each engine.  The plane ditched in the Hudson River after taking off from LaGuardia International Airport in New York City.  There were no fatalities.  The incident illustrated the hazards of ingesting birds beyond the "designed-for" limit.
The outcome of an ingestion event and whether it causes an accident, be it on a small fast plane, such as military jet fighters, or a large transport, depends on the number and weight of birds and where they strike the fan blade span or the nose cone. Core damage usually results with impacts near the blade root or on the nose cone.
Few birds fly high, so the greatest risk of a bird ingestion is during takeoff and landing and during low level flying.
If a jet plane is flying through air contaminated with volcanic ash, there is risk that ingested ash will cause erosion damage to the compressor blades, blockage of fuel nozzle air holes and blockage of the turbine cooling passages. Some of these effects may cause the engine to surge or flame-out during the flight. Re-lights are usually successful after flame-outs but with considerable loss of altitude. It was the case of British Airways Flight 9 which flew through volcanic dust at 37,000 ft. All 4 engines flamed out and re-light attempts were successful at about 13,000 ft.
One class of failure that has caused accidents is the uncontained failure, where rotating parts of the engine break off and exit through the case. These high energy parts can cut fuel and control lines, and can penetrate the cabin. Although fuel and control lines are usually duplicated for reliability, the crash of United Airlines Flight 232 was caused when hydraulic fluid lines for all three independent hydraulic systems were simultaneously severed by shrapnel from an uncontained engine failure. Prior to the United 232 crash, the probability of a simultaneous failure of all three hydraulic systems was considered as high as a billion-to-one. However, the statistical models used to come up with this figure did not account for the fact that the number-two engine was mounted at the tail close to all the hydraulic lines, nor the possibility that an engine failure would release many fragments in many directions. Since then, more modern aircraft engine designs have focused on keeping shrapnel from penetrating the cowling or ductwork, and have increasingly utilized high-strength composite materials to achieve the required penetration resistance while keeping the weight low.
In 2007 the cost of jet fuel, while highly variable from one airline to another, averaged 26.5% of total operating costs, making it the single largest operating expense for most airlines.
Jet engines are usually run on fossil fuels and are thus a source of carbon dioxide in the atmosphere. Jet engines can also run on biofuels or hydrogen, although hydrogen is usually produced from fossil fuels.
About 7.2% of the oil used in 2004 was consumed by jet engines.
Some scientists believe that jet engines are also a source of global dimming due to the water vapour in the exhaust causing cloud formations.
Nitrogen compounds are also formed during the combustion process from reactions with atmospheric nitrogen. At low altitudes this is not thought to be especially harmful, but for supersonic aircraft that fly in the stratosphere some destruction of ozone may occur.
Sulphates are also emitted if the fuel contains sulphur.
A ramjet is a form of airbreathing jet engine using the engine's forward motion to compress incoming air, without a rotary compressor. Ramjets cannot produce thrust at zero airspeed and thus cannot move an aircraft from a standstill. Ramjets require considerable forward speed to operate well, and as a class work most efficiently at speeds around Mach 3. This type of jet can operate up to speeds of Mach 6.
They consist of three sections; an inlet to compress incoming air, a combustor to inject and combust fuel, and a nozzle to expel the hot gases and produce thrust. Ramjets require a relatively high speed to efficiently compress the incoming air, so ramjets cannot operate at a standstill and they are most efficient at supersonic speeds. A key trait of ramjet engines is that combustion is done at subsonic speeds. The supersonic incoming air is dramatically slowed through the inlet, where it is then combusted at the much slower, subsonic, speeds. The faster the incoming air is, however, the less efficient it becomes to slow it to subsonic speeds. Therefore, ramjet engines are limited to approximately Mach 5.
Ramjets can be particularly useful in applications requiring a small and simple engine for high speed use, such as missiles, while weapon designers are looking to use ramjet technology in artillery shells to give added range: it is anticipated that a 120-mm mortar shell, if assisted by a ramjet, could attain a range of 22 mi (35 km). They have also been used successfully, though not efficiently, as tip jets on helicopter rotors.
Pulsejets are subsonic engines which also use ram compression, but with intermittent combustion unlike the continuous combustion used in a ramjet. They are a quite distinct type of jet engine.
Scramjets are an evolution of ramjets that are able to operate at much higher speeds than any other kind of airbreathing engine. They share a similar structure with ramjets, being a specially shaped tube that compresses air with no moving parts through ram-air compression.  They consist of an inlet, a combustor, and a nozzle. The primary difference between ramjets and scramjets is that scramjets do not slow the oncoming airflow to subsonic speeds for combustion. Thus, scramjets do not have the diffuser required by ramjets to slow the incoming airflow to subsonic speeds.  They use supersonic combustion instead and the name "scramjet" comes from "Supersonic Combusting Ramjet."
Scramjets start working at speeds of at least Mach 4, and have a maximum useful speed of approximately Mach 17. Due to aerodynamic heating at these high speeds, cooling poses a challenge to engineers.
Since scramjets use supersonic combustion they can operate at speeds above Mach 6 where traditional ramjets are too inefficient. Another difference between ramjets and scramjets comes from how each type of engine compresses the oncoming airflow: while the inlet provides most of the compression for ramjets, the high speeds at which scramjets operate allow them to take advantage of the compression generated by shock waves, primarily oblique shocks.
Very few scramjet engines have ever been built and flown. In May 2010 the Boeing X-51 set the endurance record for the longest scramjet burn at over 200 seconds.
Turbojet operation over the complete flight envelope from zero to Mach 3+ requires features to allow the compressor to function properly at the high inlet temperatures beyond Mach 2.5 as well as at low flight speeds. 
The J58 compressor solution was to bleed airflow from the 4th compressor stage at speeds above about Mach 2. The bleed flow, 20% at Mach 3, was returned to the engine via 6 external tubes to cool the afterburner liner and primary nozzle as well as to provide extra air for combustion. The J58 engine was the only operational turbojet engine, being designed to operate continuously even at maximum afterburning, for Mach 3.2 cruise.
An alternative solution is seen in a contemporary installation, which did not reach operational status, the Mach 3 GE YJ93/XB-70. It used a variable stator compressor. Yet another solution was specified in a proposal for a Mach 3 reconnaissance Phantom. This was pre-compressor cooling, albeit available for relatively short duration.
Jet engines can be run on almost any fuel. Hydrogen is a highly desirable fuel, as, although the energy per mole is not unusually high, the molecule is very much lighter than other molecules. The energy per kg of hydrogen is twice that of more common fuels and this gives twice the specific impulse. In addition, jet engines running on hydrogen are quite easy to build—the first ever turbojet was run on hydrogen. Also, although not duct engines, hydrogen-fueled rocket engines have seen extensive use.
However, in almost every other way, hydrogen is problematic. The downside of hydrogen is its density; in gaseous form the tanks are impractical for flight, but even in the form of liquid hydrogen it has a density one fourteenth that of water. It is also deeply cryogenic and requires very significant insulation that precludes it being stored in wings. The overall vehicle would end up being very large, and difficult for most airports to accommodate. Finally, pure hydrogen is not found in nature, and must be manufactured either via steam reforming or expensive electrolysis. A few experimental hydrogen-powered aircraft have flown with propellers, and jets have been proposed that may be feasible.
An idea originated by Robert P. Carmichael in 1955 is that hydrogen-fueled engines could theoretically have much higher performance than hydrocarbon-fueled engines if a heat exchanger were used to cool the incoming air. The low temperature allows lighter materials to be used, a higher mass-flow through the engines, and permits combustors to inject more fuel without overheating the engine.
This idea leads to plausible designs like Reaction Engines SABRE, that might permit single-stage-to-orbit launch vehicles, and ATREX, which could permit jet engines to be used up to hypersonic speeds and high altitudes for boosters for launch vehicles. The idea is also being researched by the EU for a concept to achieve non-stop antipodal supersonic passenger travel at Mach 5 (Reaction Engines A2).
The air turborocket is a form of combined-cycle jet engine.  The basic layout includes a gas generator, which produces high pressure gas, that drives a turbine/compressor assembly which compresses atmospheric air into a combustion chamber.  This mixture is then combusted before leaving the device through a nozzle and creating thrust.
There are many different types of air turborockets. The various types generally differ in how the gas generator section of the engine functions.
Air turborockets are often referred to as turboramjets, turboramjet rockets, turborocket expanders, and many others. As there is no consensus on which names apply to which specific concepts, various sources may use the same name for two different concepts.
To specify the RPM, or rotor speeds, of a jet engine, abbreviations are commonly used:
In many cases, instead of expressing rotor speeds (N1, N2) as RPM on cockpit displays, pilots are provided with the speeds expressed as a percentage of the design point speed. For example, at full power, the N1 might be 101.5% or 100%. This user interface decision has been made as a human factors consideration, since pilots are more likely to notice a problem with a two- or 3-digit percentage (where 100% implies a nominal value) than with a 5-digit RPM.

In mechanics, acceleration is the rate of change of the velocity of an object with respect to time. Accelerations are vector quantities (in that they have magnitude and direction). The orientation of an object's acceleration is given by the orientation of the net force acting on that object. The magnitude of an object's acceleration, as described by Newton's Second Law, is the combined effect of two causes:
The SI unit for acceleration is metre per second squared (m⋅s−2, 






m

s

2







{\displaystyle {\tfrac {\operatorname {m} }{\operatorname {s} ^{2}}}}

).
For example, when a vehicle starts from a standstill (zero velocity, in an inertial frame of reference) and travels in a straight line at increasing speeds, it is accelerating in the direction of travel. If the vehicle turns, an acceleration occurs toward the new direction and changes its motion vector.  The acceleration of the vehicle in its current direction of motion is called a linear (or tangential during circular motions) acceleration, the reaction to which the passengers on board experience as a force pushing them back into their seats. When changing direction, the effecting acceleration is called radial (or centripetal during circular motions) acceleration, the reaction to which the passengers experience as a centrifugal force. If the speed of the vehicle decreases, this is an acceleration in the opposite direction and mathematically a negative, sometimes called deceleration or retardation, and passengers experience the reaction to deceleration as an inertial force pushing them forward.  Such negative accelerations are often achieved by retrorocket burning in spacecraft. Both acceleration and deceleration are treated the same, as they are both changes in velocity. Each of these accelerations (tangential, radial, deceleration) is felt by passengers until their relative (differential) velocity are neutralized in reference to the acceleration due to change in speed.
An object's average acceleration over a period of time is its change in velocity, 



Δ

v



{\displaystyle \Delta \mathbf {v} }

, divided by the duration of the period, 



Δ
t


{\displaystyle \Delta t}

. Mathematically,








a

¯



=



Δ

v



Δ
t



.


{\displaystyle {\bar {\mathbf {a} }}={\frac {\Delta \mathbf {v} }{\Delta t}}.}


Instantaneous acceleration, meanwhile, is the limit of the average acceleration over an infinitesimal interval of time. In the terms of calculus, instantaneous acceleration is the derivative of the velocity vector with respect to time:





a

=

lim


Δ
t

→
0





Δ

v



Δ
t



=



d

v



d
t





{\displaystyle \mathbf {a} =\lim _{{\Delta t}\to 0}{\frac {\Delta \mathbf {v} }{\Delta t}}={\frac {d\mathbf {v} }{dt}}}


As acceleration is defined as the derivative of velocity, v, with respect to time t and velocity is defined as the derivative of position, x, with respect to time, acceleration can be thought of as the second derivative of x with respect to t:





a

=



d

v



d
t



=




d

2



x



d

t

2







{\displaystyle \mathbf {a} ={\frac {d\mathbf {v} }{dt}}={\frac {d^{2}\mathbf {x} }{dt^{2}}}}


(Here and elsewhere, if motion is in a straight line, vector quantities can be substituted by scalars in the equations.)
By the fundamental theorem of calculus, it can be seen that the integral of the acceleration function a(t) is the velocity function v(t); that is, the area under the curve of an acceleration vs. time (a vs. t) graph corresponds to the change of velocity.





Δ
v

=
∫

a


d
t


{\displaystyle \mathbf {\Delta v} =\int \mathbf {a} \,dt}


Likewise, the integral of the jerk function j(t), the derivative of the acceleration function, can be used to find  the change of acceleration at a certain time:





Δ
a

=
∫

j


d
t


{\displaystyle \mathbf {\Delta a} =\int \mathbf {j} \,dt}


Acceleration has the dimensions of velocity (L/T) divided by time, i.e. L T−2. The SI unit of acceleration is the metre per second squared (m s−2); or "metre per second per second", as the velocity in metres per second changes by the acceleration value, every second.
An object moving in a circular motion—such as a satellite orbiting the Earth—is accelerating due to the change of direction of motion, although its speed may be constant. In this case it is said to be undergoing centripetal (directed towards the center) acceleration.
Proper acceleration, the acceleration of a body relative to a free-fall condition, is measured by an instrument called an accelerometer.
In classical mechanics, for a body with constant mass, the (vector) acceleration of the body's center of mass is proportional to the net force vector (i.e. sum of all forces) acting on it (Newton’s second law):





F

=
m

a



⟹



a

=



F

m




{\displaystyle \mathbf {F} =m\mathbf {a} \quad \implies \quad \mathbf {a} ={\frac {\mathbf {F} }{m}}}


where F is the net force acting on the body, m is  the mass of the body, and a is the center-of-mass acceleration. As speeds approach the speed of light, relativistic effects become increasingly large.
The velocity of a particle moving on a curved path as a function of time can be written as:
with v(t) equal to the speed of travel along the path, and
a unit vector tangent to the path pointing in the direction of motion at the chosen moment in time. Taking into account both the changing speed v(t) and the changing direction of ut, the acceleration of a particle moving on a curved path can be written using the chain rule of differentiation for the product of two functions of time as:
where un is the  unit (inward) normal vector to the particle's trajectory (also called the principal normal), and r is its instantaneous radius of curvature based upon the osculating circle at time t. These components are called the tangential acceleration and the normal or radial acceleration (or centripetal acceleration in circular motion, see also circular motion and centripetal force).
Geometrical analysis of three-dimensional space curves, which explains tangent, (principal) normal and binormal, is described by the Frenet–Serret formulas.
Uniform or constant acceleration is a type of motion in which the velocity of an object changes by an equal amount in every equal time period.
A frequently cited example of uniform acceleration is that of an object in free fall in a uniform gravitational field. The acceleration of a falling body in the absence of resistances to motion is dependent only on the gravitational field strength g (also called acceleration due to gravity). By Newton's Second Law the force 





F

g





{\displaystyle \mathbf {F_{g}} }

 acting on a body is given by:
Because of the simple analytic properties of the case of constant acceleration, there are simple formulas relating the displacement, initial and time-dependent velocities, and acceleration to the time elapsed:
where
In particular, the motion can be resolved into two orthogonal parts, one of constant velocity and the other according to the above equations.  As Galileo showed, the net result is parabolic motion, which describes, e. g., the trajectory of a projectile in a vacuum near the surface of Earth.
In uniform circular motion, that is moving with constant speed along a circular path, a particle experiences an acceleration resulting from the change of the direction of the velocity vector, while its magnitude remains constant. The derivative of the location of a point on a curve with respect to time, i.e. its velocity, turns out to be always exactly tangential to the curve, respectively orthogonal to the radius in this point. Since in uniform motion the velocity in the tangential direction does not change, the acceleration must be in radial direction, pointing to the center of the circle. This acceleration constantly changes the direction of the velocity to be tangent in the neighboring point, thereby rotating the velocity vector along the circle.
Expressing centripetal acceleration vector in polar components, where 




r



{\displaystyle \mathbf {r} }

 is a vector from the centre of the circle to the particle with magnitude equal to this distance, and considering the orientation of the acceleration towards the center, yields
As usual in rotations, the speed 



v


{\displaystyle v}

 of a particle may be expressed as an angular speed with respect to a point at the distance 



r


{\displaystyle r}

 as
Thus 





a

c



=
−

ω

2



r


.


{\displaystyle \mathbf {a_{c}} =-\omega ^{2}\mathbf {r} \;.}


This acceleration and the mass of the particle determine the necessary centripetal force, directed toward the centre of the circle, as the net force acting on this particle to keep it in this uniform circular motion. The so-called 'centrifugal force', appearing to act outward on the body, is a so-called pseudo force experienced in the frame of reference of the body in circular motion, due to the body's linear momentum, a vector tangent to the circle of motion.
In a nonuniform circular motion, i.e., the speed along the curved path is changing, the acceleration has a non-zero component tangential to the curve, and is not confined to the principal normal, which directs to the center of the osculating circle, that determines the radius 



r


{\displaystyle r}

 for the centripetal acceleration. The tangential component is given by the angular acceleration 



α


{\displaystyle \alpha }

, i.e., the rate of change 



α
=



ω
˙





{\displaystyle \alpha ={\dot {\omega }}}

 of the angular speed 



ω


{\displaystyle \omega }

 times the radius 



r


{\displaystyle r}

. That is,
The sign of the tangential component of the acceleration is determined by the sign of the angular acceleration (



α


{\displaystyle \alpha }

), and the tangent is always directed at right angles to the radius vector.
The special theory of relativity describes the behavior of objects traveling relative to other objects at speeds approaching that of light in a vacuum. Newtonian mechanics is exactly revealed to be an approximation to reality, valid to great accuracy at lower speeds. As the relevant speeds increase toward the speed of light, acceleration no longer follows classical equations.
As speeds approach that of light, the acceleration produced by a given force decreases, becoming infinitesimally small as light speed is approached; an object with mass can approach this speed asymptotically, but never reach it.
Unless the state of motion of an object is known, it is impossible to distinguish whether an observed force is due to gravity or to acceleration—gravity and inertial acceleration have identical effects. Albert Einstein called this the equivalence principle, and said that only observers who feel no force at all—including the force of gravity—are justified in concluding that they are not accelerating.

In Newtonian mechanics, linear momentum, translational momentum, or simply momentum is the product of the mass and velocity of an object. It is a vector quantity, possessing a magnitude and a direction. If m is an object's mass and v is its velocity (also a vector quantity), then the object's momentum p is : 




p

=
m

v

.


{\displaystyle \mathbf {p} =m\mathbf {v} .}


In the International System of Units (SI), the unit of measurement of momentum is the kilogram metre per second (kg⋅m/s), which is equivalent to the newton-second.
Newton's second law of motion states that the rate of change of a body's momentum is equal to the net force acting on it. Momentum depends on the frame of reference, but in any inertial frame it is a conserved quantity, meaning that if a closed system is not affected by external forces, its total linear momentum does not change. Momentum is also conserved in special relativity (with a modified formula) and, in a modified form,  in electrodynamics, quantum mechanics, quantum field theory, and general relativity. It is an expression of one of the fundamental symmetries of space and time: translational symmetry.
Advanced formulations of classical mechanics, Lagrangian and Hamiltonian mechanics, allow one to choose coordinate systems that incorporate symmetries and constraints. In these systems the conserved quantity is  generalized momentum, and in general this is different from the kinetic momentum defined above. The concept of generalized momentum is carried over into quantum mechanics, where it becomes an operator on a wave function. The momentum and position operators are related by the Heisenberg uncertainty principle.
In continuous systems such as electromagnetic fields, fluid dynamics and deformable bodies, a momentum density can be defined, and a continuum version of the conservation of momentum leads to equations such as the Navier–Stokes equations for fluids or the Cauchy momentum equation for deformable solids or fluids.
Momentum is a vector quantity: it has both magnitude and direction. Since momentum has a direction, it can be used to predict the resulting direction and speed of motion of objects after they collide. Below, the basic properties of momentum are described in one dimension. The vector equations are almost identical to the scalar equations (see multiple dimensions).
The momentum of a particle is conventionally represented by the letter p. It is the product of two quantities, the particle's mass (represented by the letter m) and its velocity (v):
The unit of momentum is the product of the units of mass and velocity. In SI units, if the mass is in kilograms and the velocity is in meters per second then the momentum is in kilogram meters per second (kg⋅m/s). In cgs units, if the mass is in grams and the velocity in centimeters per second, then the momentum is in gram centimeters per second (g⋅cm/s).
Being a vector, momentum has magnitude and direction. For example, a 1 kg model airplane, traveling due north at 1 m/s in straight and level flight, has a momentum of 1 kg⋅m/s due north measured with reference to the ground.
The momentum of a system of particles is the vector sum of their momenta. If two particles have respective masses m1 and m2, and velocities v1 and v2, the total momentum is
The momenta of more than two particles can be added more generally with the following:
A system of particles has a center of mass, a point determined by the weighted sum of their positions:
If one or more of the particles is moving, the center of mass of the system will generally be moving as well (unless the system is in pure rotation around it). If the total mass of the particles is 



m


{\displaystyle m}

, and the center of mass is moving at velocity vcm, the momentum of the system is:
This is known as Euler's first law.
If the net force F applied to a particle is constant, and is applied for a time interval Δt, the momentum of the particle changes by an amount
In differential form, this is Newton's second law; the rate of change of the momentum of a particle is equal to the instantaneous force F acting on it,
If the net force experienced by a particle changes as a function of time, F(t), the change in momentum (or impulse J) between times t1 and t2 is
Impulse is measured in the derived units of the newton second (1 N⋅s = 1 kg⋅m/s) or dyne second (1 dyne⋅s = 1 g⋅cm/s)
Under the assumption of constant mass m, it is equivalent to write
hence the net force is equal to the mass of the particle times its acceleration.
Example: A model airplane of mass 1 kg accelerates from rest to a velocity of 6 m/s due north in 2 s. The net force required to produce this acceleration is 3 newtons due north. The change in momentum is 6 kg⋅m/s due north. The rate of change of momentum is 3 (kg⋅m/s)/s due north which is numerically equivalent to 3 newtons.
In a closed system (one that does not exchange any matter with its surroundings and is not acted on by external forces) the total momentum remains constant. This fact, known as the law of conservation of momentum, is implied by Newton's laws of motion. Suppose, for example, that two particles interact. As explained by the third law, the forces between them are equal in magnitude but opposite in direction. If the particles are numbered 1 and 2, the second law states that F1 = .mw-parser-output .sfrac{white-space:nowrap}.mw-parser-output .sfrac.tion,.mw-parser-output .sfrac .tion{display:inline-block;vertical-align:-0.5em;font-size:85%;text-align:center}.mw-parser-output .sfrac .num,.mw-parser-output .sfrac .den{display:block;line-height:1em;margin:0 0.1em}.mw-parser-output .sfrac .den{border-top:1px solid}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}dp1/dt and F2 = dp2/dt. Therefore,
with the negative sign indicating that the forces oppose. Equivalently,
If the velocities of the particles are u1 and u2 before the interaction, and afterwards they are v1 and v2, then
This law holds no matter how complicated the force is between particles. Similarly, if there are several particles, the momentum exchanged between each pair of particles adds to zero, so the total change in momentum is zero. This conservation law applies to all interactions, including collisions and separations caused by explosive forces. It can also be generalized to situations where Newton's laws do not hold, for example in the theory of relativity and in electrodynamics.
Momentum is a measurable quantity, and the measurement depends on the frame of reference. For example: if an aircraft of mass m kg is flying through the air at a speed of 50 m/s its momentum can be calculated to be 50m kg.m/s. If the aircraft is flying into a headwind of 5 m/s its speed relative to the surface of the Earth is only 45 m/s and its momentum can be calculated to be 45m kg.m/s. Both calculations are equally correct. In both frames of reference, any change in momentum will be found to be consistent with the relevant laws of physics.
Suppose a particle has position x in a stationary frame of reference. From the point of view of another frame of reference, moving at a uniform speed u, the position (represented by a primed coordinate) changes with time as
This is called a Galilean transformation. If the particle is moving at speed dx/dt = v in the first frame of reference, in the second, it is moving at speed
Since u does not change, the accelerations are the same:
Thus, momentum is conserved in both reference frames. Moreover, as long as the force has the same form, in both frames, Newton's second law is unchanged. Forces such as Newtonian gravity, which depend only on the scalar distance between objects, satisfy this criterion. This independence of reference frame is called Newtonian relativity or Galilean invariance.
A change of reference frame, can, often, simplify calculations of motion. For example, in a collision of two particles, a reference frame can be chosen, where, one particle begins at rest. Another, commonly used reference frame, is the center of mass frame – one that is moving with the center of mass. In this frame,
the total momentum is zero.
If two particles, each of known momentum, collide and coalesce, the law of conservation of momentum can be used to determine the momentum of the coalesced body. If the outcome of the collision is that the two particles separate, the law is not sufficient to determine the momentum of each particle. If the momentum of one particle after the collision is known, the law can be used to determine the momentum of the other particle. Alternatively if the combined kinetic energy after the collision is known, the law can be used to determine the momentum of each particle after the collision. Kinetic energy is usually not conserved. If it is conserved, the collision is called an elastic collision; if not, it is an inelastic collision.
An elastic collision is one in which no kinetic energy is transformed into heat or some other form of energy. Perfectly elastic collisions can occur when the objects do not touch each other, as for example in atomic or nuclear scattering where electric repulsion keeps the objects apart. A slingshot maneuver of a satellite around a planet can also be viewed as a perfectly elastic collision. A collision between two pool balls is a good example of an almost totally elastic collision, due to their high rigidity, but when bodies come in contact there is always some dissipation.
A head-on elastic collision between two bodies can be represented by velocities in one dimension, along a line passing through the bodies. If the velocities are u1 and u2 before the collision and v1 and v2 after, the equations expressing conservation of momentum and kinetic energy are:
A change of reference frame can simplify analysis of a collision. For example, suppose there are two bodies of equal mass m, one stationary and one approaching the other at a speed v (as in the figure). The center of mass is moving at speed v/2 and both bodies are moving towards it at speed v/2. Because of the symmetry, after the collision both must be moving away from the center of mass at the same speed. Adding the speed of the center of mass to both, we find that the body that was moving is now stopped and the other is moving away at speed v. The bodies have exchanged their velocities. Regardless of the velocities of the bodies, a switch to the center of mass frame leads us to the same conclusion. Therefore, the final velocities are given by
In general, when the initial velocities are known, the final velocities are given by
If one body has much greater mass than the other, its velocity will be little affected by a collision while the other body will experience a large change.
In an inelastic collision, some of the kinetic energy of the colliding bodies is converted into other forms of energy (such as heat or sound). Examples include traffic collisions, in which the effect of loss of kinetic energy can be seen in the damage to the vehicles; electrons losing some of their energy to atoms (as in the Franck–Hertz experiment); and particle accelerators in which the kinetic energy is converted into mass in the form of new particles.
In a perfectly inelastic collision (such as a bug hitting a windshield), both bodies have the same motion afterwards. A head-on inelastic collision between two bodies can be represented by velocities in one dimension, along a line passing through the bodies. If the velocities are u1 and u2 before the collision then in a perfectly inelastic collision both bodies will be travelling with velocity v after the collision. The equation expressing conservation of momentum is:
If one body is motionless to begin with (e.g. 




u

2


=
0


{\displaystyle u_{2}=0}

), the equation for conservation of momentum is
so
In a different situation, if the frame of reference is moving at the final velocity such that 



v
=
0


{\displaystyle v=0}

, the objects would be brought to rest by a perfectly inelastic collision and 100% of the kinetic energy is converted to other forms of energy. In this instance the initial velocities of the bodies would be non-zero, or the bodies would have to be massless.
One measure of the inelasticity of the collision is the coefficient of restitution CR, defined as the ratio of relative velocity of separation to relative velocity of approach. In applying this measure to a ball bouncing from a solid surface, this can be easily measured using the following formula:
The momentum and energy equations also apply to the motions of objects that begin together and then move apart. For example, an explosion is the result of a chain reaction that transforms potential energy stored in chemical, mechanical, or nuclear form into kinetic energy, acoustic energy, and electromagnetic radiation. Rockets also make use of conservation of momentum: propellant is thrust outward, gaining momentum, and an equal and opposite momentum is imparted to the rocket.
Real motion has both direction and velocity and must be represented by a vector. In a coordinate system with x, y, z axes, velocity has components vx in the x-direction, vy in the y-direction, vz in the z-direction. The vector is represented by a boldface symbol:
Similarly, the momentum is a vector quantity and is represented by a boldface symbol:
The equations in the previous sections, work in vector form if the scalars p and v are replaced by vectors p and v. Each vector equation represents three scalar equations. For example,
represents three equations:
The kinetic energy equations are exceptions to the above replacement rule. The equations are still one-dimensional, but each scalar represents the magnitude of the vector, for example,
Each vector equation represents three scalar equations. Often coordinates can be chosen so that only two components are needed, as in the figure. Each component can be obtained separately and the results combined to produce a vector result.
A simple construction involving the center of mass frame can be used to show that if a stationary elastic sphere is struck by a moving sphere, the two will head off at right angles after the collision (as in the figure).
The concept of momentum plays a fundamental role in explaining the behavior of variable-mass objects such as a rocket ejecting fuel or a star accreting gas. In analyzing such an object, one treats the object's mass as a function that varies with time: m(t). The momentum of the object at time t is therefore p(t) = m(t)v(t). One might then try to invoke Newton's second law of motion by saying that the external force F on the object is related to its momentum p(t) by F = dp/dt, but this is incorrect, as is the related expression found by applying the product rule to d(mv)/dt:
This equation does not correctly describe the motion of variable-mass objects. The correct equation is
where u is the velocity of the ejected/accreted mass as seen in the object's rest frame. This is distinct from v, which is the velocity of the object itself as seen in an inertial frame.
This equation is derived by keeping track of both the momentum of the object as well as the momentum of the ejected/accreted mass (dm). When considered together, the object and the mass (dm) constitute a closed system in which total momentum is conserved.
Newtonian physics assumes that absolute time and space exist outside of any observer; this gives rise to Galilean invariance. It also results in a prediction that the speed of light can vary from one reference frame to another. This is contrary to observation. In the special theory of relativity, Einstein keeps the postulate that the equations of motion do not depend on the reference frame, but assumes that the speed of light c is invariant. As a result, position and time in two reference frames are related by the Lorentz transformation instead of the Galilean transformation.
Consider, for example, one reference frame moving relative to another at velocity v in the x direction. The Galilean transformation gives the coordinates of the moving frame as
while the Lorentz transformation gives
where γ is the Lorentz factor:
Newton's second law, with mass fixed, is not invariant under a Lorentz transformation. However, it can be made invariant by making the inertial mass m of an object a function of velocity:
m0 is the object's invariant mass.
The modified momentum,
obeys Newton's second law:
Within the domain of classical mechanics, relativistic momentum closely approximates Newtonian momentum: at low velocity, γm0v is approximately equal to m0v, the Newtonian expression for momentum.
In the theory of special relativity, physical quantities are expressed in terms of four-vectors that include time as a fourth coordinate along with the three space coordinates. These vectors are generally represented by capital letters, for example R for position. The expression for the four-momentum depends on how the coordinates are expressed. Time may be given in its normal units or multiplied by the speed of light so that all the components of the four-vector have dimensions of length. If the latter scaling is used, an interval of proper time, τ, defined by
is invariant under Lorentz transformations (in this expression and in what follows the (+ − − −) metric signature has been used, different authors use different conventions). Mathematically this invariance can be ensured in one of two ways: by treating the four-vectors as Euclidean vectors and multiplying time by √−1; or by keeping time a real quantity and embedding the vectors in a Minkowski space. In a Minkowski space, the scalar product of two four-vectors U = (U0, U1, U2, U3) and V = (V0, V1, V2, V3) is defined as
In all the coordinate systems, the (contravariant) relativistic four-velocity is defined by
and the (contravariant) four-momentum is
where m0 is the invariant mass. If R = (ct, x, y, z) (in Minkowski space), then
Using Einstein's mass-energy equivalence, E = mc2, this can be rewritten as
Thus, conservation of four-momentum is Lorentz-invariant and implies conservation of both mass and energy.
The magnitude of the momentum four-vector is equal to m0c:
and is invariant across all reference frames.
The relativistic energy–momentum relationship holds even for massless particles such as photons; by setting m0 = 0 it follows that
In a game of relativistic "billiards", if a stationary particle is hit by a moving particle in an elastic collision, the paths formed by the two afterwards will form an acute angle. This is unlike the non-relativistic case where they travel at right angles.
The four-momentum of a planar wave can be related to a wave four-vector
For a particle, the relationship between temporal components,  E = ħ ω, is the Planck–Einstein relation, and the relation between spatial components, p = ħ k, describes a de Broglie matter wave.
Newton's laws can be difficult to apply to many kinds of motion because the motion is limited by constraints. For example, a bead on an abacus is constrained to move along its wire and a pendulum bob is constrained to swing at a fixed distance from the pivot. Many such constraints can be incorporated by changing the normal Cartesian coordinates to a set of generalized coordinates that may be fewer in number. Refined mathematical methods have been developed for solving mechanics problems in generalized coordinates. They introduce a generalized momentum, also known as the canonical or conjugate momentum, that extends the concepts of both linear momentum and angular momentum. To distinguish it from generalized momentum, the product of mass and velocity is also referred to as mechanical, kinetic or kinematic momentum. The two main methods are described below.
In Lagrangian mechanics, a Lagrangian is defined as the difference between the kinetic energy T and the potential energy V:
If the generalized coordinates are represented as a vector q = (q1, q2, ... , qN)  and time differentiation is represented by a dot over the variable, then the equations of motion (known as the Lagrange or Euler–Lagrange equations) are a set of N equations:
If a coordinate qi is not a Cartesian coordinate, the associated generalized momentum component pi does not necessarily have the dimensions of linear momentum. Even if qi is a Cartesian coordinate, pi will not be the same as the mechanical momentum if the potential depends on velocity.  Some sources represent the kinematic momentum by the symbol Π.
In this mathematical framework, a generalized momentum is associated with the generalized coordinates. Its components are defined as
Each component pj is said to be the conjugate momentum for the coordinate qj.
Now if a given coordinate qi does not appear in the Lagrangian (although its time derivative might appear), then
This is the generalization of the conservation of momentum.
Even if the generalized coordinates are just the ordinary spatial coordinates, the conjugate momenta are not necessarily the ordinary momentum coordinates. An example is found in the section on electromagnetism.
In Hamiltonian mechanics, the Lagrangian (a function of generalized coordinates and their derivatives) is replaced by a Hamiltonian that is a function of generalized coordinates and momentum. The Hamiltonian is defined as
where the momentum is obtained by differentiating the Lagrangian as above. The Hamiltonian equations of motion are
As in Lagrangian mechanics, if a generalized coordinate does not appear in the Hamiltonian, its conjugate momentum component is conserved.
Conservation of momentum is a mathematical consequence of the homogeneity (shift symmetry) of space (position in space is the canonical conjugate quantity to momentum). That is, conservation of momentum is a consequence of the fact that the laws of physics do not depend on position; this is a special case of Noether's theorem. For systems that do not have this symmetry, it may not be possible to define conservation of momentum. Examples where conservation of momentum does not apply include curved spacetimes in general relativity or time crystals in condensed matter physics.
In Maxwell's equations, the forces between particles are mediated by electric and magnetic fields. The electromagnetic force (Lorentz force) on a particle with charge q due to a combination of electric field E and magnetic field B is
(in SI units).: 2 
It has an electric potential φ(r, t) and magnetic vector potential A(r, t).
In the non-relativistic regime, its generalized momentum is
while in relativistic mechanics this becomes





P

=
γ
m


v


+
q

A

.


{\displaystyle \mathbf {P} =\gamma m\mathbf {\mathbf {v} } +q\mathbf {A} .}


The quantity 



V
=
q

A



{\displaystyle V=q\mathbf {A} }

 is sometimes called the potential momentum. It is the momentum due to the interaction of the particle with the electromagnetic fields. The name is an analogy with the potential energy 



U
=
q
φ


{\displaystyle U=q\varphi }

, which is the energy due to the interaction of the particle with the electromagnetic fields. These quantities form a four-vector, so the analogy is consistent; besides, the concept of potential momentum is important in explaining the so-called hidden-momentum of the electromagnetic fields
In Newtonian mechanics, the law of conservation of momentum can be derived from the law of action and reaction, which states that every force has a reciprocating equal and opposite force. Under some circumstances, moving charged particles can exert forces on each other in non-opposite directions. Nevertheless, the combined momentum of the particles and the electromagnetic field is conserved.
The Lorentz force imparts a momentum to the particle, so by Newton's second law the particle must impart a momentum to the electromagnetic fields.
In a vacuum, the momentum per unit volume is
where μ0 is the vacuum permeability and c is the speed of light. The momentum density is proportional to the Poynting vector S which gives the directional rate of energy transfer per unit area:
If momentum is to be conserved over the volume V over a region Q, changes in the momentum of matter through the Lorentz force must be balanced by changes in the momentum of the electromagnetic field and outflow of momentum. If Pmech is the momentum of all the particles in Q, and the particles are treated as a continuum, then Newton's second law gives
The electromagnetic momentum is
and the equation for conservation of each component i of the momentum is
The term on the right is an integral over the surface area Σ of the surface σ representing momentum flow into and out of the volume, and nj is a component of the surface normal of S. The quantity Tij is called the Maxwell stress tensor, defined as
The above results are for the microscopic Maxwell equations, applicable to electromagnetic forces in a vacuum (or on a very small scale in media). It is more difficult to define momentum density in media because the division into electromagnetic and mechanical is arbitrary. The definition of electromagnetic momentum density is modified to
where the H-field H is related to the B-field and the magnetization M by
The electromagnetic stress tensor depends on the properties of the media.
In quantum mechanics, momentum is defined as a self-adjoint operator on the wave function. The Heisenberg uncertainty principle defines limits on how accurately the momentum and position of a single observable system can be known at once. In quantum mechanics, position and momentum are conjugate variables.
For a single particle described in the position basis the momentum operator can be written as
where ∇ is the gradient operator, ħ is the reduced Planck constant, and i is the imaginary unit. This is a commonly encountered form of the momentum operator, though the momentum operator in other bases can take other forms. For example, in momentum space the momentum operator is represented as
where the operator p acting on a wave function ψ(p) yields that wave function multiplied by the value p, in an analogous fashion to the way that the position operator acting on a wave function ψ(x) yields that wave function multiplied by the value x.
For both massive and massless objects, relativistic momentum is related to the phase constant 



β


{\displaystyle \beta }

  by
Electromagnetic radiation (including visible light, ultraviolet light, and radio waves) is carried by photons. Even though photons (the particle aspect of light) have no mass, they still carry momentum. This leads to applications such as the solar sail. The calculation of the momentum of light within dielectric media is somewhat controversial (see Abraham–Minkowski controversy).
In fields such as fluid dynamics and solid mechanics, it is not feasible to follow the motion of individual atoms or molecules. Instead, the materials must be approximated by a continuum in which there is a particle or fluid parcel at each point that is assigned the average of the properties of atoms in a small region nearby. In particular, it has a density ρ and velocity v that depend on time t and position r. The momentum per unit volume is ρv.
Consider a column of water in hydrostatic equilibrium. All the forces on the water are in balance and the water is motionless. On any given drop of water, two forces are balanced. The first is gravity, which acts directly on each atom and molecule inside. The gravitational force per unit volume is ρg, where g is the gravitational acceleration. The second force is the sum of all the forces exerted on its surface by the surrounding water. The force from below is greater than the force from above by just the amount needed to balance gravity. The normal force per unit area is the pressure p. The average force per unit volume inside the droplet is the gradient of the pressure, so the force balance equation is
If the forces are not balanced, the droplet accelerates. This acceleration is not simply the partial derivative ∂v/∂t because the fluid in a given volume changes with time. Instead, the material derivative is needed:
Applied to any physical quantity, the material derivative includes the rate of change at a point and the changes due to advection as fluid is carried past the point. Per unit volume, the rate of change in momentum is equal to ρDv/Dt. This is equal to the net force on the droplet.
Forces that can change the momentum of a droplet include the gradient of the pressure and gravity, as above. In addition, surface forces can deform the droplet. In the simplest case, a shear stress τ, exerted by a force parallel to the surface of the droplet, is proportional to the rate of deformation or strain rate. Such a shear stress occurs if the fluid has a velocity gradient because the fluid is moving faster on one side than another. If the speed in the x direction varies with z, the  tangential force in direction x per unit area normal to the z direction is
where μ is the viscosity. This is also a flux, or flow per unit area, of x-momentum through the surface.
Including the effect of viscosity, the momentum balance equations for the incompressible flow of a Newtonian fluid are
These are known as the Navier–Stokes equations.
The momentum balance equations can be extended to more general materials, including solids. For each surface with normal in direction i and force in direction j, there is a stress component σij. The nine components make up the Cauchy stress tensor σ, which includes both pressure and shear. The local conservation of momentum is expressed by the Cauchy momentum equation:
where f is the body force.
The Cauchy momentum equation is broadly applicable to deformations of solids and liquids. The relationship between the stresses and the strain rate depends on the properties of the material (see Types of viscosity).
A disturbance in a medium gives rise to oscillations, or waves, that propagate away from their source. In a fluid, small changes in pressure p can often be described by the acoustic wave equation:
where c is the speed of sound. In a solid, similar equations can be obtained for propagation of pressure (P-waves) and shear (S-waves).
The flux, or transport per unit area, of a momentum component ρvj by a velocity vi is equal to ρ vjvj. In the linear approximation that leads to the above acoustic equation, the time average of this flux is zero. However, nonlinear effects can give rise to a nonzero average. It is possible for momentum flux to occur even though the wave itself does not have a mean momentum.
In about 530 AD, working in Alexandria, Byzantine philosopher John Philoponus developed a concept of momentum in his commentary to Aristotle's Physics. Aristotle claimed that everything that is moving must be kept moving by something. For example, a thrown ball must be kept moving by motions of the air.  Most writers continued to accept Aristotle's theory until the time of Galileo, but a few were skeptical. Philoponus pointed out the absurdity in Aristotle's claim that motion of an object is promoted by the same air that is resisting its passage. He proposed instead that an impetus was imparted to the object in the act of throwing it. Ibn Sīnā (also known by his Latinized name Avicenna) read Philoponus and published his own theory of motion in The Book of Healing in 1020. He agreed that an impetus is imparted to a projectile by the thrower; but unlike Philoponus, who believed that it was a temporary virtue that would decline even in a vacuum, he viewed it as a persistent, requiring external forces such as air resistance to dissipate it.
The work of Philoponus, and possibly that of Ibn Sīnā, was read and refined by the European philosophers Peter Olivi and Jean Buridan. Buridan, who in about 1350 was made rector of the University of Paris, referred to impetus being proportional to the weight times the speed. Moreover, Buridan's theory was different from his predecessor's in that he did not consider impetus to be self-dissipating, asserting that a body would be arrested by the forces of air resistance and gravity which might be opposing its impetus.
René Descartes believed that the total "quantity of motion" (Latin: quantitas motus) in the universe is conserved, where the quantity of motion is understood as the product of size and speed. This should not be read as a statement of the modern law of momentum, since he had no concept of mass as distinct from weight and size, and more important, he believed that it is speed rather than velocity that is conserved. So for Descartes if a moving object were to bounce off a surface, changing its direction but not its speed, there would be no change in its quantity of motion. Galileo, in his Two New Sciences, used the Italian word impeto to similarly describe Descartes' quantity of motion.
Leibniz, in his "Discourse on Metaphysics", gave an argument against Descartes' construction of the conservation of the "quantity of motion" using an example of dropping blocks of different sizes different distances.  He points out that force is conserved but quantity of motion, construed as the product of size and speed of an object, is not conserved.
Christiaan Huygens concluded quite early that Descartes's laws for the elastic collision of two bodies must be wrong, and he formulated the correct laws. An important step was his recognition of the Galilean invariance of the problems. His views then took many years to be circulated. He passed them on in person to William Brouncker and Christopher Wren in London, in 1661. What Spinoza wrote to Henry Oldenburg about them, in 1666 which was during the Second Anglo-Dutch War, was guarded. Huygens had actually worked them out in a manuscript De motu corporum ex percussione in the period 1652–6. The war ended in 1667, and Huygens announced his results to the Royal Society in 1668. He published them in the Journal des sçavans in 1669.
The first correct statement of the law of conservation of momentum was by English mathematician John Wallis in his 1670 work, Mechanica sive De Motu, Tractatus Geometricus: "the initial state of the body, either of rest or of motion, will persist" and "If the force is greater than the resistance, motion will result". Wallis used momentum for quantity of motion, and vis for force. Newton's Philosophiæ Naturalis Principia Mathematica, when it was first published in 1687, showed a similar casting around for words to use for the mathematical momentum. His Definition II defines quantitas motus, "quantity of motion", as "arising from the velocity and quantity of matter conjointly", which identifies it as momentum. Thus when in Law II he refers to mutatio motus, "change of motion", being proportional to the force impressed, he is generally taken to mean momentum and not motion. It remained only to assign a standard term to the quantity of motion. The first use of "momentum" in its proper mathematical sense is not clear but by the time of Jennings's Miscellanea in 1721, five years before the final edition of Newton's Principia Mathematica, momentum M or "quantity of motion" was being defined for students as "a rectangle", the product of Q and V, where Q is "quantity of material" and V is "velocity", s/t.
In 1728, the Cyclopedia states:
"The Momentum, Impetus, or Quantity of Motion of any Body, is the Factum  of its Velocity, (or the Space it moves in a given Time, see .mw-parser-output span.smallcaps{font-variant:small-caps}.mw-parser-output span.smallcaps-smaller{font-size:85%}Motion) multiplied into its Mass."An airfoil (American English) or aerofoil (British English) is the cross-sectional shape of an object whose motion through a gas is capable of generating significant lift, such as a wing, a sail, or the blades of propeller, rotor, or turbine.
A solid body moving through a fluid produces an aerodynamic force.  The component of this force perpendicular to the relative freestream velocity is called lift.  The component parallel to the relative freestream velocity  is called drag.  An airfoil is a streamlined shape that is capable of generating significantly more lift than drag. Airfoils can be designed for use at different speeds by modifying their geometry: those for subsonic flight generally have a rounded leading edge, while those designed for supersonic flight tend to be slimmer with a sharp leading edge. All have a sharp trailing edge. Foils of similar function designed with water as the working fluid are called hydrofoils.
The lift on an airfoil is primarily the result of its angle of attack. When oriented at a suitable angle, the airfoil deflects the oncoming air (for fixed-wing aircraft, a downward force), resulting in a force on the airfoil in the direction opposite to the deflection. This force is known as aerodynamic force and can be resolved into two components: lift and drag. Most foil shapes require a positive angle of attack to generate lift, but cambered airfoils can generate lift at zero angle of attack. This "turning" of the air in the vicinity of the airfoil creates curved streamlines, resulting in lower pressure on one side and higher pressure on the other. This pressure difference is accompanied by a velocity difference, via Bernoulli's principle, so the resulting flowfield about the airfoil has a higher average velocity on the upper surface than on the lower surface. In some situations (e.g. inviscid potential flow) the lift force can be related directly to the average top/bottom velocity difference without computing the pressure by using the concept of circulation and the Kutta–Joukowski theorem.
The wings and stabilizers of fixed-wing aircraft, as well as helicopter rotor blades, are built with airfoil-shaped cross sections. Airfoils are also found in propellers, fans, compressors and turbines. Sails are also airfoils, and the underwater surfaces of sailboats, such as the centerboard, rudder, and keel, are similar in cross-section and operate on the same principles as airfoils. Swimming and flying creatures and even many plants and sessile organisms employ airfoils/hydrofoils: common examples being bird wings, the bodies of fish, and the shape of sand dollars. An airfoil-shaped wing can create downforce on an automobile or other motor vehicle, improving traction.
When the wind is obstructed by an object such as a flat plate, a building, or the deck of a bridge, the object will experience drag and also an aerodynamic force perpendicular to the wind. This does not mean the object qualifies as an airfoil. Airfoils are highly-efficient lifting shapes, able to generate more lift than similarly sized flat plates of the same area, and able to generate lift with significantly less drag. Airfoils are used in the design of aircraft, propellers, rotor blades, wind turbines and other applications of aeronautical engineering.
A lift and drag curve obtained in wind tunnel testing is shown on the right. The curve represents an airfoil with a positive camber so some lift is produced  at zero angle of attack. With increased angle of attack, lift increases in a roughly linear relation, called the slope of the lift curve. At about 18 degrees this airfoil stalls, and lift falls off quickly beyond that. The drop in lift can be explained by the action of the upper-surface boundary layer, which separates and greatly thickens over the upper surface at and past the stall angle. The thickened boundary layer's displacement thickness changes the airfoil's effective shape, in particular it reduces its effective camber, which modifies the overall flow field so as to reduce the circulation and the lift. The thicker boundary layer also causes a large increase in pressure drag, so that the overall drag increases sharply near and past the stall point.
Airfoil design is a major facet of aerodynamics. Various airfoils serve different flight regimes. Asymmetric airfoils can generate lift at zero angle of attack, while a symmetric airfoil may better suit frequent inverted flight as in an aerobatic airplane. In the region of the ailerons and near a wingtip a symmetric airfoil can be used to increase the range of angles of attack to avoid spin–stall. Thus a large range of angles can be used without boundary layer separation. Subsonic airfoils have a round leading edge, which is naturally insensitive to the angle of attack. The cross section is not strictly circular, however: the radius of curvature is increased before the wing achieves maximum thickness to minimize the chance of boundary layer separation. This elongates the wing and moves the point of maximum thickness back from the leading edge.
Supersonic airfoils are much more angular in shape and can have a very sharp leading edge, which is very sensitive to angle of attack. A supercritical airfoil has its maximum thickness close to the leading edge to have a lot of length to slowly shock the supersonic flow back to subsonic speeds. Generally such transonic airfoils and also the supersonic airfoils have a low camber to reduce drag divergence. Modern aircraft wings may have different airfoil sections along the wing span, each one optimized for the conditions in each section of the wing.
Movable high-lift devices, flaps and sometimes slats, are fitted to airfoils on almost every aircraft. A trailing edge flap acts similarly to an aileron; however, it, as opposed to an aileron, can be retracted partially into the wing if not used.
A laminar flow wing has a maximum thickness in the middle camber line. Analyzing the Navier–Stokes equations in the linear regime shows that a negative pressure gradient along the flow has the same effect as reducing the speed. So with the maximum camber in the middle, maintaining a laminar flow over a larger percentage of the wing at a higher cruising speed is possible. However, some surface contamination will disrupt the laminar flow, making it turbulent.  For example, with rain on the wing, the flow will be turbulent. Under certain conditions, insect debris on the wing will cause the loss of small regions of laminar flow as well. Before NASA's research in the 1970s and 1980s the aircraft design community understood from application attempts in the WW II era that laminar flow wing designs were not practical using common manufacturing tolerances and surface imperfections. That belief changed after new manufacturing methods were developed with composite materials (e.g. laminar-flow airfoils developed by Professor Franz Wortmann for use with wings made of fibre-reinforced plastic). Machined metal methods were also introduced. NASA's research in the 1980s revealed the practicality and usefulness of laminar flow wing designs and opened the way for laminar-flow applications on modern practical aircraft surfaces, from subsonic general aviation aircraft to transonic large transport aircraft, to supersonic designs.
Schemes have been devised to define airfoils – an example is the NACA system.  Various airfoil generation systems are also used. An example of a general purpose airfoil that finds wide application, and pre–dates the NACA system, is the Clark-Y. Today, airfoils can be designed for specific functions by the use of computer programs.
The various terms related to airfoils are defined below:
The geometry of the airfoil is described with a variety of terms :
The shape of the airfoil is defined using the following geometrical parameters:
Some important parameters to describe an airfoil's shape are its camber and its thickness. For example, an airfoil of the NACA 4-digit series such as the NACA 2415 (to be read as 2 – 4 – 15) describes an airfoil with a camber of 0.02 chord located at 0.40 chord, with 0.15 chord of maximum thickness.
Finally, important concepts used to describe the airfoil's behaviour when moving through a fluid are:
Thin airfoil theory is a simple theory of airfoils that relates angle of attack to lift for incompressible, inviscid flows. It was devised by German mathematician Max Munk and further refined by British aerodynamicist Hermann Glauert and others in the 1920s. The theory idealizes the flow around an airfoil as two-dimensional flow around a thin airfoil.  It can be imagined as addressing an airfoil of zero thickness and infinite wingspan.
Thin airfoil theory was particularly notable in its day because it provided a sound theoretical basis for the following important properties of airfoils in two-dimensional inviscid flow:
As a consequence of (3), the section lift coefficient of a symmetric airfoil of infinite wingspan is:
(The above expression is also applicable to a cambered airfoil where 



α



{\displaystyle \alpha \!}

 is the angle of attack measured relative to the zero-lift line instead of the chord line.)
Also as a consequence of (3), the section lift coefficient of a cambered airfoil of infinite wingspan is:
Thin airfoil theory does not account for the stall of the airfoil, which usually occurs at an angle of attack between 10° and 15° for typical airfoils. In the mid-late 2000s, however, a theory predicting the onset of leading-edge stall was proposed by Wallace J. Morris II in his doctoral thesis.  Morris's subsequent refinements contain the details on the current state of theoretical knowledge on the leading-edge stall phenomenon. Morris's theory predicts the critical angle of attack for leading-edge stall onset as the condition at which a global separation zone is predicted in the solution for the inner flow. Morris's theory demonstrates that a subsonic flow about a thin airfoil can be described in terms of an outer region, around most of the airfoil chord, and an inner region, around the nose, that asymptotically match each other. As the flow in the outer region is dominated by classical thin airfoil theory, Morris's equations exhibit many components of thin airfoil theory.
The airfoil is modeled as a thin lifting mean-line (camber line).  The mean-line, y(x), is considered to produce a distribution of vorticity 



γ
(
s
)


{\displaystyle \gamma (s)}

 along the line, s.  By the Kutta condition, the vorticity is zero at the trailing edge.  Since the airfoil is thin, x (chord position) can be used instead of s, and all angles can be approximated as small.
From the Biot–Savart law, this vorticity produces a flow field 



w
(
x
)


{\displaystyle w(x)}

 where




x


{\displaystyle x}

 is the location where induced velocity is produced, 




x
′



{\displaystyle x'}

 is the location of the vortex element producing the velocity and 



c


{\displaystyle c}

 is the chord length of the airfoil.
Since there is no flow normal to the curved surface of the airfoil, 



w
(
x
)


{\displaystyle w(x)}

 balances that from the component of main flow 



V


{\displaystyle V}

, which is locally normal to the plate – the main flow is locally inclined to the plate by an angle 



α
−
d
y

/

d
x


{\displaystyle \alpha -dy/dx}

.  That is:
This integral equation can be solved for 



γ
(
x
)


{\displaystyle \gamma (x)}

, after replacing x by
as a Fourier series in 




A

n


sin
⁡
(
n
θ
)


{\displaystyle A_{n}\sin(n\theta )}

 with a modified lead term 




A

0


(
1
+
cos
⁡
(
θ
)
)

/

sin
⁡
(
θ
)


{\displaystyle A_{0}(1+\cos(\theta ))/\sin(\theta )}

.
That is
(These terms are known as the Glauert integral).
The coefficients are given by
and
By the Kutta–Joukowski theorem, the total lift force F is proportional to
and its moment M about the leading edge to
The calculated Lift coefficient depends only on the first two terms of the Fourier series, as
The moment M about the leading edge depends only on 




A

0


,

A

1




{\displaystyle A_{0},A_{1}}

 and 




A

2




{\displaystyle A_{2}}

, as
The moment about the 1/4 chord point will thus be
From this it follows that the center of pressure is aft of the 'quarter-chord' point 0.25 c, by
The aerodynamic center, AC, is at the quarter-chord point. The AC is where the pitching moment M′ does not vary with a change in lift coefficient, i.e.,
A reaction control system (RCS) is a spacecraft system that uses thrusters and reaction control wheels to provide attitude control, and sometimes propulsion. Use of diverted engine thrust to provide stable attitude control of a short-or-vertical takeoff and landing aircraft below conventional winged flight speeds, such as with the Harrier "jump jet", may also be referred to as a reaction control system.
An RCS is capable of providing small amounts of thrust in any desired direction or combination of directions. An RCS is also capable of providing torque to allow control of rotation (roll, pitch, and yaw).
Reaction control systems often use combinations of large and small (vernier) thrusters, to allow different levels of response.
Spacecraft reaction control systems are used for:
Because spacecraft only contain a finite amount of fuel and there is little chance to refill them, alternative reaction control systems have been developed so that fuel can be conserved. For stationkeeping, some spacecraft (particularly those in geosynchronous orbit) use high-specific impulse engines such as arcjets, ion thrusters, or Hall effect thrusters. To control orientation, a few spacecraft, including the ISS, use momentum wheels which spin to control rotational rates on the vehicle.
The Mercury space capsule and Gemini reentry module both used groupings of nozzles to provide attitude control. The thrusters were located off their center of mass, thus providing a torque to rotate the capsule. The Gemini capsule was also capable of adjusting its reentry course by rolling, which directed its off-center lifting force. The Mercury thrusters used a hydrogen peroxide monopropellant which turned to steam when forced through a tungsten screen, and the Gemini thrusters used hypergolic mono-methyl hydrazine fuel oxidized with nitrogen tetroxide.
The Gemini spacecraft was also equipped with a hypergolic Orbit Attitude and Maneuvering System, which made it the first crewed spacecraft with translation as well as rotation capability. In-orbit attitude control was achieved by firing pairs of eight 25-pound-force (110 N) thrusters located around the circumference of its adapter module at the extreme aft end. Lateral translation control was provided by four 100-pound-force (440 N) thrusters around the circumference at the forward end of the adaptor module (close to the spacecraft's center of mass). Two forward-pointing 85-pound-force (380 N) thrusters at the same location, provided aft translation, and two 100-pound-force (440 N) thrusters located in the aft end of the adapter module provided forward thrust, which could be used to change the craft's orbit. The Gemini reentry module also had a separate Reentry Control System of sixteen thrusters located at the base of its nose, to provide rotational control during reentry.
The Apollo Command Module had a set of twelve hypergolic thrusters for attitude control, and directional reentry control similar to Gemini.
The Apollo Service Module and Lunar Module each had a set of sixteen R-4D hypergolic thrusters, grouped into external clusters of four, to provide both translation and attitude control. The clusters were located near the craft's average centers of mass, and were fired in pairs in opposite directions for attitude control.
A pair of translation thrusters are located at the rear of the Soyuz spacecraft; the counter-acting thrusters are similarly paired in the middle of the spacecraft (near the center of mass) pointing outwards and forward. These act in pairs to prevent the spacecraft from rotating. The thrusters for the lateral directions are mounted close to the center of mass of the spacecraft, in pairs as well.
The suborbital X-15 and a companion training aero-spacecraft, the NF-104 AST, both intended to travel to an altitude that rendered their aerodynamic control surfaces unusable, established a convention for locations for thrusters on winged vehicles not intended to dock in space; that is, those that only have attitude control thrusters. Those for pitch and yaw are located in the nose, forward of the cockpit, and replace a standard radar system. Those for roll are located at the wingtips. The X-20, which would have gone into orbit, continued this pattern.
Unlike these, the Space Shuttle Orbiter had many more thrusters, which were required to control vehicle attitude in both orbital flight and during the early part of atmospheric entry, as well as carry out rendezvous and docking maneuvers in orbit. Shuttle thrusters were grouped in the nose of the vehicle and on each of the two aft Orbital Maneuvering System pods. No nozzles interrupted the heat shield on the underside of the craft; instead, the nose RCS nozzles which control positive pitch were mounted on the side of the vehicle, and were canted downward. The downward-facing negative pitch thrusters were located in the OMS pods mounted in the tail/afterbody.
The International Space Station uses electrically powered control moment gyroscopes (CMG) for primary attitude control, with RCS thruster systems as backup and augmentation systems.
Gimbaled thrust is the system of thrust vectoring used in most rockets, including the Space Shuttle, the Saturn V lunar rockets, and the Falcon 9.
In a gimbaled thrust system, the engine or just the exhaust nozzle of the rocket can be swiveled on two axes (pitch and yaw ) from side to side. As the nozzle is moved, the direction of the thrust is changed relative to the center of gravity of the rocket.
The diagram illustrates three cases. The middle rocket shows the straight-line flight configuration in which the direction of thrust is along the center line of the rocket and through the center of gravity of the rocket. On the rocket at the left, the nozzle has been deflected to the left and the thrust line is now inclined to the rocket center line at an angle called the gimbal angle. Since the thrust no longer passes through the center of gravity, a torque is generated about the center of gravity and the nose of the rocket turns to the left. If the nozzle is gimbaled back along the center line, the rocket will move to the left. On the rocket at the right, the nozzle has been deflected to the right and the nose is moved to the right.
A reaction wheel (RW) is used primarily by spacecraft for three-axis attitude control, and does not require rockets or external applicators of torque. They provide a high pointing accuracy,: 362  and are particularly useful when the spacecraft must be rotated by very small amounts, such as keeping a telescope pointed at a star.
A reaction wheel is sometimes operated as (and referred to as) a momentum wheel, by operating it at a constant (or near-constant) rotation speed, to provide a satellite with a large amount of stored angular momentum. Doing so alters the spacecraft's rotational dynamics so that disturbance torques perpendicular to one axis of the satellite (the axis parallel to the wheel's spin axis) do not result directly in spacecraft angular motion about the same axis as the disturbance torque; instead, they result in (generally smaller) angular motion (precession) of that spacecraft axis about a perpendicular axis. This has the effect of tending to stabilize that spacecraft axis to point in a nearly-fixed direction,: 362  allowing for a less-complicated attitude control system. Satellites using this "momentum-bias" stabilization approach include SCISAT-1; by orienting the momentum wheel's axis to be parallel to the orbit-normal vector, this satellite is in a "pitch momentum bias" configuration.
A control moment gyroscope (CMG) is a related but different type of attitude actuator, generally consisting of a momentum wheel mounted in a one-axis or two-axis gimbal.: 362  When mounted to a rigid spacecraft, applying a constant torque to the wheel using one of the gimbal motors causes the spacecraft to develop a constant angular velocity about a perpendicular axis, thus allowing control of the spacecraft's pointing direction. CMGs are generally able to produce larger sustained torques than RWs with less motor heating, and are preferentially used in larger or more-agile (or both) spacecraft, including Skylab, Mir, and the International Space Station.
Reaction wheels are used to control the attitude of a satellite without the use of thrusters, which reduces the mass fraction needed for fuel.
They work by equipping the spacecraft with an electric motor attached to a flywheel, which, when its rotation speed is changed, causes the spacecraft to begin to counter-rotate proportionately through conservation of angular momentum. Reaction wheels can rotate a spacecraft only around its center of mass (see torque); they are not capable of moving the spacecraft from one place to another (see translational force).
For three-axis control, reaction wheels must be mounted along at least three directions, with extra wheels providing redundancy to the attitude control system. A redundant mounting configuration could consist of four wheels along tetrahedral axes, or a spare wheel carried in addition to a three axis configuration.: 369  Changes in speed (in either direction) are controlled electronically by computer. The strength of the materials used in a reaction wheel determine the speed at which the wheel would come apart, and therefore how much angular momentum it can store.
Since the reaction wheel is a small fraction of the spacecraft's total mass, easily controlled, temporary changes in its speed result in small changes in angle. The wheels therefore permit very precise changes in a spacecraft's attitude. For this reason, reaction wheels are often used to aim spacecraft carrying cameras or telescopes.
Over time, reaction wheels may build up enough stored momentum to exceed the maximum speed of the wheel, called saturation, which will need to be canceled. Designers therefore supplement reaction wheel systems with other attitude control mechanisms. In the presence of a magnetic field (as in low Earth orbit), a spacecraft can employ magnetorquers (better known as torque rods) to transfer angular momentum to Earth through its planetary magnetic field.: 368  In the absence of a magnetic field, the most efficient practice is to use either high-efficiency attitude jets such as ion thrusters, or small, lightweight solar sails placed in locations away from the spacecraft's center of mass, such as on solar cell arrays or projecting masts.
Beresheet was launched on a Falcon 9 rocket on 22 February 2019 1:45 UTC, with the goal of landing on the moon. Beresheet uses the low-energy transfer technique to save fuel. Since its fourth maneuver in its elliptical orbit, to prevent shakes when the amount of liquid fuel ran low, there was a need to use a reaction wheel.
The James Webb Space Telescope has six reaction wheels built by Rockwell Collins Deutschland.
LightSail 2 was launched on 25 June 2019, focused around the concept of a solar sail. LightSail 2 uses a reaction wheel system to change orientation by very small amounts, allowing it to receive different amounts of momentum from the light across the sail, resulting in a higher altitude. 

The failure of one or more reaction wheels can cause a spacecraft to lose its ability to maintain attitude (orientation) and thus potentially cause a mission failure. Recent studies conclude that these failures can be correlated with space weather effects. These events probably caused failures by inducing electrostatic discharge in the steel ball bearings of Ithaco wheels, compromising the smoothness of the mechanism.
Two servicing missions to the Hubble Space Telescope have replaced a reaction wheel. In February 1997, the Second Servicing Mission (STS-82) replaced one after 'electrical anomalies', rather than any mechanical problem. Study of the returned mechanism provided a rare opportunity to study equipment that had undergone long-term service (seven years) in space, particularly for the effects of vacuum on lubricants. The lubricating compound was found to be in 'excellent condition'. In 2002, during Servicing Mission 3B (STS-109), astronauts from the shuttle Columbia replaced another reaction wheel. Neither of these wheels had failed and Hubble was designed with four redundant wheels, and maintained pointing ability so long as three were functional.
In 2004, during the mission of the Hayabusa spacecraft, an X-axis reaction wheel failed. The Y-axis wheel failed in 2005, causing the craft to rely on chemical thrusters to maintain attitude control.
From July 2012 to May 11, 2013, two out of the four reaction wheels in the Kepler telescope failed. This loss severely affected Kepler's ability to maintain a sufficiently precise orientation to continue its original mission. On August 15, 2013, engineers concluded that Kepler's reaction wheels cannot be recovered and that planet-searching using the transit method (measuring changes in star brightness caused by orbiting planets) could not continue. Although the failed reaction wheels still function, they are experiencing friction exceeding acceptable levels, and consequently hindering the ability of the telescope to properly orient itself. The Kepler telescope was returned to its "point rest state", a stable configuration that uses small amounts of thruster fuel to compensate for the failed reaction wheels, while the Kepler team considered alternative uses for Kepler that do not require the extreme accuracy in its orientation needed by the original mission. On May 16, 2014, NASA extended the Kepler mission to a new mission named K2, which uses Kepler differently, but allows it to continue searching for exoplanets. On October 30, 2018, NASA announced the end of the Kepler mission after it was determined that the fuel supply had been exhausted.
The NASA space probe Dawn had excess friction in one reaction wheel in June 2010. It was originally scheduled to depart Vesta and begin its two-and-a-half-year journey to Ceres on August 26, 2012; however, a problem with another of the spacecraft's reaction wheels forced Dawn to briefly delay its departure from Vesta's gravity until September 5, 2012, and it planned to use thruster jets instead of the reaction wheels during the three-year journey  to Ceres. The loss of the reaction wheels limited the camera observations on the approach to Ceres.
On the evening of Tuesday, January 18, 2022, a possible failure of one of Swift Observatory's reaction wheels caused the mission control team to power off the suspected wheel, putting the observatory in safe mode as a precaution. This is the first time a reaction wheel has experienced a failure in Swift's 17 years of operations.

Thrust vectoring, also known as thrust vector control (TVC), is the ability of an aircraft, rocket, or other vehicle to manipulate the direction of the thrust from its engine(s) or motor(s) to control the attitude or angular velocity of the vehicle.
In rocketry and ballistic missiles that fly outside the atmosphere, aerodynamic control surfaces are ineffective, so thrust vectoring is the primary means of attitude control. Exhaust vanes and gimbaled engines were used in the 1930s by Robert
Goddard.
For aircraft, the method was originally envisaged to provide upward vertical thrust as a means to give aircraft vertical (VTOL) or short (STOL) takeoff and landing ability. Subsequently, it was realized that using vectored thrust in combat situations enabled aircraft to perform various maneuvers not available to conventional-engined planes. To perform turns, aircraft that use no thrust vectoring must rely on aerodynamic control surfaces only, such as ailerons or elevator; aircraft with vectoring must still use control surfaces, but to a lesser extent.
In missile literature originating from Russian sources, thrust vectoring is often referred to as gas-dynamic steering or gas-dynamic control.
Nominally, the line of action of the thrust vector of a rocket nozzle passes through the vehicle's centre of mass, generating zero net moment about the mass centre. It is possible to generate pitch and yaw moments by deflecting the main rocket thrust vector so that it does not pass through the mass centre. Because the line of action is generally oriented nearly parallel to the roll axis, roll control usually requires the use of two or more separately hinged nozzles or a separate system altogether, such as fins, or vanes in the exhaust plume of the rocket engine, deflecting the main thrust. Thrust vector control (TVC) is only possible when the propulsion system is creating thrust; separate mechanisms are required for attitude and flight path control during other stages of flight.
Thrust vectoring can be achieved by four basic means:
Thrust vectoring for many liquid rockets is achieved by gimbaling the whole engine.  This involves moving the entire combustion chamber and outer engine bell as on the Titan II's twin first-stage motors, or even the entire engine assembly including the related fuel and oxidizer pumps.  The Saturn V and the Space Shuttle used gimbaled engines.
A later method developed for solid propellant ballistic missiles achieves thrust vectoring by deflecting only the nozzle of the rocket using electric actuators or hydraulic cylinders. The nozzle is attached to the missile via a ball joint with a hole in the centre, or a flexible seal made of a thermally resistant material, the latter generally requiring more torque and a higher power actuation system.  The Trident C4 and D5 systems are controlled via hydraulically actuated nozzle. The STS SRBs used gimbaled nozzles.
Another method of thrust vectoring used on solid propellant ballistic missiles is liquid injection, in which the rocket nozzle is fixed, but a fluid is introduced into the exhaust flow from injectors mounted around the aft end of the missile.  If the liquid is injected on only one side of the missile, it modifies that side of the exhaust plume, resulting in different thrust on that side and an asymmetric net force on the missile.  This was the control system used on the Minuteman II and the early SLBMs of the United States Navy.
An effect similar to thrust vectoring can be produced with multiple vernier thrusters, small auxiliary combustion chambers which lack their own turbopumps and can gimbal on one axis. These were used on the Atlas and R-7 missiles and are still used on the Soyuz rocket, which is descended from the R-7, but are seldom used on new designs due to their complexity and weight. These are distinct from reaction control system thrusters, which are fixed and independent rocket engines used for maneuvering in space.
One of the earliest methods of thrust vectoring in rocket engines was to place vanes in the engine's exhaust stream. These exhaust vanes or jet vanes allow the thrust to be deflected without moving any parts of the engine, but reduce the rocket's efficiency. They have the benefit of allowing roll control with only a single engine, which nozzle gimbaling does not. The V-2 used graphite exhaust vanes and aerodynamic vanes, as did the Redstone, derived from the V-2. The Sapphire and Nexo rockets of the amateur group Copenhagen Suborbitals provide a modern example of jet vanes. Jet vanes must be made of a refractory material or actively cooled to prevent them from melting. Sapphire used solid copper vanes for copper's high heat capacity and thermal conductivity, and Nexo used graphite for its high melting point, but unless actively cooled, jet vanes will undergo significant erosion. This, combined with jet vanes' inefficiency, mostly precludes their use in new rockets.
Some smaller sized atmospheric tactical missiles, such as the AIM-9X Sidewinder, eschew flight control surfaces and instead use mechanical vanes to deflect rocket motor exhaust to one side.
By using mechanical vanes to deflect the exhaust of the missile's rocket motor, a missile can steer itself even shortly after being launched (when the missile is moving slowly, before it has reached a high speed). This is because even though the missile is moving at a low speed, the rocket motor's exhaust has a high enough speed to provide sufficient forces on the mechanical vanes. Thus, thrust vectoring can reduce a missile's minimum range. For example, anti-tank missiles such as the Eryx and the PARS 3 LR use thrust vectoring for this reason.
Some other projectiles that use thrust-vectoring:
Most currently operational vectored thrust aircraft use turbofans with rotating nozzles or vanes to deflect the exhaust stream. This method can successfully deflect thrust through as much as 90 degrees, relative to the aircraft centreline. However, the engine must be sized for vertical lift, rather than normal flight, which results in a weight penalty. Afterburning (or Plenum Chamber Burning, PCB, in the bypass stream) is difficult to incorporate and is impractical for take-off and landing thrust vectoring, because the very hot exhaust can damage runway surfaces. Without afterburning it is hard to reach supersonic flight speeds. A PCB engine, the Bristol Siddeley BS100, was cancelled in 1965.
Tiltrotor aircraft vector thrust via rotating turboprop engine nacelles. The mechanical complexities of this design are quite troublesome, including twisting flexible internal components and driveshaft power transfer between engines. Most current tiltrotor designs feature two rotors in a side-by-side configuration. If such a craft is flown in a way where it enters a vortex ring state, one of the rotors will always enter slightly before the other, causing the aircraft to perform a drastic and unplanned roll.
Thrust vectoring is also used as a control mechanism for airships. An early application was the British Army airship Delta, which first flew in 1912. It was later used on HMA (His Majesty's Airship) No. 9r, a British rigid airship that first flew in 1916 and the twin 1930s-era U.S. Navy rigid airships USS Akron and USS Macon that were used as airborne aircraft carriers, and a similar form of thrust vectoring is also particularly valuable today for the control of modern non-rigid airships. In this use, most of the load is usually supported by buoyancy and vectored thrust is used to control the motion of the aircraft.  The first airship that used a control system based on pressurized air was Enrico Forlanini's Omnia Dir in 1930s.
A design for a jet incorporating thrust vectoring was submitted in 1949 to the British Air Ministry by Percy Walwyn; Walwyn's drawings are preserved at the National Aerospace Library at Farnborough. Official interest was curtailed when it was realised that the designer was a patient in a mental hospital.
Now being researched, Fluidic Thrust Vectoring (FTV) diverts thrust via secondary fluidic injections. Tests show that air forced into a jet engine exhaust stream can deflect thrust up to 15 degrees. Such nozzles are desirable for their lower mass and cost (up to 50% less), inertia (for faster, stronger control response), complexity (mechanically simpler, fewer or no moving parts or surfaces, less maintenance), and radar cross section for stealth. This will likely be used in many unmanned aerial vehicle (UAVs), and 6th generation fighter aircraft.
Thrust-vectoring flight control (TVFC) is obtained through deflection of the aircraft jets in some or all of the pitch, yaw and roll directions. In the extreme, deflection of the jets in yaw, pitch and roll creates desired forces and moments enabling complete directional control of the aircraft flight path without the implementation of the conventional aerodynamic flight controls (CAFC). TVFC can also be used to hold stationary flight in areas of the flight envelope where the main aerodynamic surfaces are stalled. TVFC includes control of STOVL aircraft during the hover and during the transition between hover and forward speeds below 50 knots where aerodynamic surfaces are ineffective.
When vectored thrust control uses a single propelling jet, as with a single-engined aircraft, the ability to produce rolling moments may not be possible. An example is an afterburning supersonic nozzle where nozzle functions are throat area, exit area, pitch vectoring and yaw vectoring. These functions are controlled by four separate actuators. A simpler variant using only three actuators would not have independent exit area control.
When TVFC is implemented to complement CAFC, agility and safety of the aircraft are maximized. Increased safety may occur in the event of malfunctioning CAFC as a result of battle damage.
To implement TVFC a variety of nozzles both mechanical and fluidic may be applied.  This includes convergent and convergent-divergent nozzles that may be fixed or geometrically variable. It also includes variable mechanisms within a fixed nozzle, such as rotating cascades and rotating exit vanes. Within these aircraft nozzles, the geometry itself may vary from two-dimensional (2-D) to axisymmetric or elliptic. The number of nozzles on a given aircraft to achieve TVFC can vary from one on a CTOL aircraft to a minimum of four in the case of STOVL aircraft.
An example of 2D thrust vectoring is the Rolls-Royce Pegasus engine used in the Hawker Siddeley Harrier, as well as in the AV-8B Harrier II variant.
Widespread use of thrust vectoring for enhanced maneuverability in Western production-model fighter aircraft didn't occur until the deployment of the Lockheed Martin F-22 Raptor fifth-generation jet fighter in 2005, with its afterburning, 2D thrust-vectoring Pratt &amp; Whitney F119 turbofan.
While the Lockheed Martin F-35 Lightning II uses a conventional afterburning turbofan (Pratt &amp; Whitney F135) to facilitate supersonic operation, its F-35B variant, developed for joint usage by the US Marine Corps, Royal Air Force, Royal Navy, and Italian Navy, also incorporates a vertically mounted, low-pressure shaft-driven remote fan, which is driven through a clutch during landing from the engine. Both the exhaust from this fan and the main engine's fan are deflected by thrust vectoring nozzles, to provide the appropriate combination of lift and propulsive thrust. It is not conceived for enhanced maneuverability in combat, only for VTOL operation, and the F-35A and F-35C do not use thrust vectoring at all.
The Sukhoi Su-30MKI, produced by India under licence at Hindustan Aeronautics Limited, is in active service with the Indian Air Force. The TVC makes the aircraft highly maneuverable, capable of near-zero airspeed at high angles of attack without stalling, and dynamic aerobatics at low speeds. The Su-30MKI is powered by two Al-31FP afterburning turbofans. The TVC nozzles of the MKI are mounted 32 degrees outward to longitudinal engine axis (i.e. in the horizontal plane) and can be deflected ±15 degrees in the vertical plane. This produces a corkscrew effect, greatly enhancing the turning capability of the aircraft.
A few computerized studies add thrust vectoring to extant passenger airliners, like the Boeing 727 and 747, to prevent catastrophic failures, while the experimental X-48C may be jet-steered in the future.
Examples of rockets and missiles which use thrust vectoring include both large systems such as the Space Shuttle Solid Rocket Booster (SRB), S-300P (SA-10) surface-to-air missile, UGM-27 Polaris nuclear ballistic missile and RT-23 (SS-24) ballistic missile and smaller battlefield weapons such as Swingfire.
The principles of air thrust vectoring have been recently adapted to military sea applications in the form of fast water-jet steering that provide super-agility. Examples are the fast patrol boat Dvora Mk-III, the Hamina class missile boat and the US Navy's Littoral combat ships.
Thrust vectoring can convey two main benefits: VTOL/STOL, and higher maneuverability. Aircraft are usually optimized to maximally exploit one benefit, though will gain in the other.
8. Wilson, Erich A., "An Introduction to Thrust-Vectored Aircraft Nozzles", ISBN 978-3-659-41265-3
Spin-stabilisation is the method of stabilizing a satellite or launch vehicle by means of spin. For most satellite applications this approach has been superseded by three-axis stabilisation.  It is also used in non-satellite applications such as rifle and artillery.
Despinning can be achieved by various techniques, including yo-yo de-spin.
On rockets with a solid motor upper stage, spin stabilization is used to keep the motor from drifting off course as they don't have their own thrusters. Usually small rockets are used to spin up the spacecraft and rocket then fire the rocket and send the craft off.
Some rockets, like the Jupiter-C, Delta II, Minotaur V and the satellite Aryabhata are spin-stabilised.
The Pioneer 4 spacecraft, the second object sent on a lunar flyby in 1959, maintained its attitude using spin-stabilization.
The Schiaparelli EDM lander was spun up to 2.5 RPM before being ejected from the ExoMars Trace Gas Orbiter prior to its attempted landing on Mars in October 2016.
Another spin-stabilized spacecraft is Juno, which arrived at Jupiter orbit in 2016.
In operation as a third stage, the Star 48 rocket booster sits on top of spin table, and before it is separated it is spun up to stabilize it during the separation from the previous stage.


This rocketry article is a stub. You can help Wikipedia by expanding it.

In physics, gravity (from Latin  gravitas 'weight') is a fundamental interaction which causes mutual attraction between all things with mass or energy. Gravity is by far the weakest of the four fundamental interactions, approximately 1038 times weaker than the strong interaction, 1036 times weaker than the electromagnetic force and 1029 times weaker than the weak interaction. As a result, it has no significant influence at the level of subatomic particles. However, gravity is the most significant interaction between objects at the macroscopic scale, and it determines the motion of planets, stars, galaxies, and even light.
On Earth, gravity gives weight to physical objects, and the Moon's gravity causes tides in the oceans. Gravity also has many important biological functions, helping to guide the growth of plants through the process of gravitropism and influencing the circulation of fluids in multicellular organisms. Investigation into the effects of weightlessness has shown that gravity may play a role in immune system function and cell differentiation within the human body.
The gravitational attraction between the original gaseous matter in the Universe allowed it to coalesce and form stars which eventually condensed into galaxies, so gravity is responsible for many of the large-scale structures in the Universe. Gravity has an infinite range, although its effects become weaker as objects get farther away.
Gravity is most accurately described by the general theory of relativity (proposed by Albert Einstein in 1915), which describes gravity not as a force, but as the curvature of spacetime, caused by the uneven distribution of mass, and causing masses to move along geodesic lines. The most extreme example of this curvature of spacetime is a black hole, from which nothing—not even light—can escape once past the black hole's event horizon. However, for most applications, gravity is well approximated by Newton's law of universal gravitation, which describes gravity as a force causing any two bodies to be attracted toward each other, with magnitude proportional to the product of their masses and inversely proportional to the square of the distance between them.
Current models of particle physics imply that the earliest instance of gravity in the Universe, possibly in the form of quantum gravity, supergravity or a gravitational singularity, along with ordinary space and time, developed during the Planck epoch (up to 10−43 seconds after the birth of the Universe), possibly from a primeval state, such as a false vacuum, quantum vacuum or virtual particle, in a currently unknown manner. Scientists are currently working to develop a theory of gravity consistent with quantum mechanics, a quantum gravity theory, which would allow gravity to be united in a common mathematical framework (a theory of everything) with the other three fundamental interactions of physics.
The nature and mechanism of gravity was explored by a wide range of ancient scholars. In Greece, Aristotle believed that objects fell towards the Earth because the Earth was the center of the Universe and attracted all of the mass in the Universe towards it. He also thought that the speed of a falling object should increase with its weight, a conclusion which was later shown to be false. While Aristotle's view was widely accepted throughout Ancient Greece, there were other thinkers such as Plutarch who correctly predicted that the attraction of gravity was not unique to the Earth.
Although he didn't understand gravity as a force, the ancient Greek philosopher Archimedes discovered the center of gravity of a triangle. He also postulated that if two equal weights did not have the same center of gravity, the center of gravity of the two weights together would be in the middle of the line that joins their centers of gravity.
In India, the mathematician-astronomer Aryabhata first identified gravity to explain why objects are not driven away from the Earth by the centrifugal force of the planet's rotation. Later, in the seventh century CE, Brahmagupta proposed the idea that gravity is an attractive force which draws objects to the Earth and used the term gurutvākarṣaṇ to describe it.
In the ancient Middle East, gravity was a topic of fierce debate. The Persian intellectual Al-Biruni believed that the force of gravity was not unique to the Earth, and he correctly assumed that other heavenly bodies should exert a gravitational attraction as well. In contrast, Al-Khazini held the same position as Aristotle that all matter in the Universe is attracted to the center of the Earth.
In the mid-16th century, various European scientists experimentally disproved the Aristotelian notion that heavier objects fall at a faster rate. In particular, the Spanish Dominican priest Domingo de Soto wrote in 1551 that bodies in free fall uniformly accelerate. De Soto may have been influenced by earlier experiments conducted by other Dominican priests in Italy, including those by Benedetto Varchi, Francesco Beato, Luca Ghini, and Giovan Bellaso which contradicted Aristotle's teachings on the fall of bodies.  The mid-16th century Italian physicist Giambattista Benedetti published papers claiming that, due to specific gravity, objects made of the same material but with different masses would fall at the same speed. With the 1586 Delft tower experiment, the Flemish physicist Simon Stevin observed that two cannonballs of differing sizes and weights fell at the same rate when dropped from a tower. Finally, in the late 16th century, Galileo Galilei performed his famous Leaning Tower of Pisa experiment in order to show once again that balls of different weights would fall at the same speed. Combining this knowledge with careful measurements of balls rolling down inclines, Galileo firmly established that gravitational acceleration is the same for all objects. Galileo postulated that air resistance is the reason that objects with a low density and high surface area fall more slowly in an atmosphere. 
In 1604, Galileo correctly hypothesized that the distance of a falling object is proportional to the square of the time elapsed. This was later confirmed by Italian scientists Jesuits Grimaldi and Riccioli between 1640 and 1650. They also calculated the magnitude of the Earth's gravity by measuring the oscillations of a pendulum.
In 1684, Newton sent a manuscript to Edmond Halley titled De motu corporum in gyrum ('On the motion of bodies in an orbit'), which provided a physical justification for Kepler's laws of planetary motion. Halley was impressed by the manuscript and urged Newton to expand on it, and a few years later Newton published a groundbreaking book called Philosophiæ Naturalis Principia Mathematica (Mathematical Principles of Natural Philosophy). In this book, Newton described gravitation as a universal force, and claimed  that "the forces which keep the planets in their orbs must  reciprocally as the squares of their distances from the centers about which they revolve." This statement was later condensed into the following inverse-square law:




F
=
G




m

1



m

2




r

2




,


{\displaystyle F=G{\frac {m_{1}m_{2}}{r^{2}}},}

where F is the force, m1 and m2 are the masses of the objects interacting, r is the distance between the centers of the masses and G is the gravitational constant.
Newton's Principia was well-received by the scientific community, and his law of gravitation quickly spread across the European world. More than a century later, in 1821, his theory of gravitation rose to even greater prominence when it was used to predict the existence of Neptune. In that year, the French astronomer Alexis Bouvard used this theory to create a table modeling the orbit of Uranus, which was shown to differ significantly from the planet's actual trajectory. In order to explain this discrepancy, many astronomers speculated that there might be a large object beyond the orbit of Uranus which was disrupting its orbit. In 1846, the astronomers John Couch Adams and Urbain Le Verrier independently used Newton's law to predict Neptune's location in the night sky, and the planet was discovered there within a day.
Eventually, astronomers noticed an eccentricity in the orbit of the planet Mercury which could not be explained by Newton's theory: the perihelion of the orbit was increasing by about 42.98 arcseconds per century. The most obvious explanation for this discrepancy was an as-yet-undiscovered celestial body (such as a planet orbiting the Sun even closer than Mercury), but all efforts to find such a body turned out to be fruitless. Finally, in 1915, Albert Einstein developed a theory of general relativity which was able to accurately model Mercury's orbit.
In general relativity, the effects of gravitation are ascribed to spacetime curvature instead of a force. Einstein began to toy with this idea in the form of the equivalence principle, a discovery which he later described as "the happiest thought of my life." In this theory, free fall is considered to be equivalent to inertial motion, meaning that free-falling inertial objects are accelerated relative to non-inertial observers on the ground. In contrast to Newtonian physics, Einstein believed that it was possible for this acceleration to occur without any force being applied to the object.
Einstein proposed that spacetime is curved by matter, and that free-falling objects are moving along locally straight paths in curved spacetime. These straight paths are called geodesics. As in Newton's first law of motion, Einstein believed that a force applied to an object would cause it to deviate from a geodesic. For instance, people standing on the surface of the Earth are prevented from following a geodesic path because the mechanical resistance of the Earth exerts an upward force on them. This explains why moving along the geodesics in spacetime is considered inertial.
Einstein's description of gravity was quickly accepted by the majority of physicists, as it was able to explain a wide variety of previously baffling experimental results. In the coming years, a wide range of experiments provided additional support for the idea of general relativity. Today, Einstein's theory of relativity is used for all gravitational calculations where absolute precision is desired, although Newton's inverse-square law continues to be a useful and fairly accurate approximation.
In modern physics, general relativity remains the framework for the understanding of gravity. Physicists continue to work to find solutions to the Einstein field equations that form the basis of general relativity, while some scientists have speculated that general relativity may not be applicable at all in certain scenarios.
The Einstein field equations are a system of 10 partial differential equations which describe how matter effects the curvature of spacetime. The system is often expressed in the following simplified form:





G

μ
ν


+
Λ

g

μ
ν


=



8
π
G


c

4





T

μ
ν


,


{\displaystyle G_{\mu u }+\Lambda g_{\mu u }={\frac {8\pi G}{c^{4}}}T_{\mu u },}


where Gμν is the Einstein tensor, gμν is the metric tensor, Tμν is the stress–energy tensor, Λ is the cosmological constant, 



G


{\displaystyle G}

 is Newton's gravitational constant and 



c


{\displaystyle c}

 is the speed of light. The term 






8
π
G


c

4






{\displaystyle {\frac {8\pi G}{c^{4}}}}

 is sometimes referred to as the Einstein gravitational constant 



κ


{\displaystyle \kappa }

.
A major area of research is the discovery of exact solutions to the Einstein field equations. Solving these equations amounts to calculating a precise value for the metric tensor (which defines the curvature and geometry of spacetime) under certain physical conditions. There is no formal definition for what constitutes such solutions, but most scientists agree that they should be expressable using elementary functions or linear differential equations. Some of the most notable solutions of the equations include: 
Today, there remain many important situations in which the Einstein field equations have not been solved. Chief among these is the two-body problem, which concerns the geometry of spacetime around two mutually interacting massive objects (such as the Sun and the Earth, or the two stars in a binary star system). The situation gets even more complicated when considering the interactions of three or more massive bodies (the "n-body problem"), and some scientists suspect that the Einstein field equations will never be solved in this context. However, it is still possible to construct an approximate solution to the field equations in the n-body problem by using the technique of post-Newtonian expansion. In general, the extreme nonlinearity of Einstein's field equations makes it difficult to solve them in all but the most specific cases.
Despite its success in predicting the effects of gravity at large scales, general relativity is ultimately a classical theory which is incompatible with quantum mechanics. In most cases, this doesn't matter, because gravity is usually only relevant at large scales while quantum physics is only relevant at very small ones. However, in order to understand environments such as the singularity of a black hole (where an extremely large amount of mass is compressed into a small space), many physicists have begun to search for a theory that could unite both gravity and quantum mechanics under a more general framework.
One path is to describe gravity in the framework of quantum field theory, which has been successful to accurately describe the other fundamental interactions. The electromagnetic force arises from an exchange of virtual photons, where the QFT description of gravity is that there is an exchange of virtual gravitons. This description reproduces general relativity in the classical limit. However, this approach fails at short distances of the order of the Planck length, where a more complete theory of quantum gravity (or a new approach to quantum mechanics) is required.
Testing the predictions of general relativity has historically been difficult, because they are almost identical to the predictions of Newtonian gravity for small energies and masses. Still, since its development, an ongoing series of experimental results have provided support for the theory:
Every planetary body (including the Earth) is surrounded by its own gravitational field, which can be conceptualized with Newtonian physics as exerting an attractive force on all objects. Assuming a spherically symmetrical planet, the strength of this field at any given point above the surface is proportional to the planetary body's mass and inversely proportional to the square of the distance from the center of the body.
The strength of the gravitational field is numerically equal to the acceleration of objects under its influence. The rate of acceleration of falling objects near the Earth's surface varies very slightly depending on latitude, surface features such as mountains and ridges, and perhaps unusually high or low sub-surface densities. For purposes of weights and measures, a standard gravity value is defined by the International Bureau of Weights and Measures, under the International System of Units (SI).
The force of gravity on Earth is the resultant (vector sum) of two forces: (a) The gravitational attraction in accordance with Newton's universal law of gravitation, and (b) the centrifugal force, which results from the choice of an earthbound, rotating frame of reference. The force of gravity is weakest at the equator because of the centrifugal force caused by the Earth's rotation and because points on the equator are furthest from the center of the Earth. The force of gravity varies with latitude and increases from about 9.780 m/s2 at the Equator to about 9.832 m/s2 at the poles. Canada's Hudson Bay has less gravity than any place on Earth.
The earliest gravity (possibly in the form of quantum gravity, supergravity or a gravitational singularity), along with ordinary space and time, developed during the Planck epoch (up to 10−43 seconds after the birth of the Universe), possibly from a primeval state (such as a false vacuum, quantum vacuum or virtual particle), in a currently unknown manner.
General relativity predicts that energy can be transported out of a system through gravitational radiation. The first indirect evidence for gravitational radiation was through measurements of the Hulse–Taylor binary in 1973. This system consists of a pulsar and neutron star in orbit around one another. Its orbital period has decreased since its initial discovery due to a loss of energy, which is consistent for the amount of energy loss due to gravitational radiation. This research was awarded the Nobel Prize in Physics in 1993.
The first direct evidence for gravitational radiation was measured on 14 September 2015 by the LIGO detectors. The gravitational waves emitted during the collision of two black holes 1.3 billion-light years from Earth were measured. This observation confirms the theoretical predictions of Einstein and others that such waves exist. It also opens the way for practical observation and understanding of the nature of gravity and events in the Universe including the Big Bang. Neutron star and black hole formation also create detectable amounts of gravitational radiation. This research was awarded the Nobel Prize in physics in 2017.
In December 2012, a research team in China announced that it had produced measurements of the phase lag of Earth tides during full and new moons which seem to prove that the speed of gravity is equal to the speed of light. This means that if the Sun suddenly disappeared, the Earth would keep orbiting the vacant point normally for 8 minutes, which is the time light takes to travel that distance. The team's findings were released in the Chinese Science Bulletin in February 2013.
In October 2017, the LIGO and Virgo detectors received gravitational wave signals within 2 seconds of gamma ray satellites and optical telescopes seeing signals from the same direction. This confirmed that the speed of gravitational waves was the same as the speed of light.
There are some observations that are not adequately accounted for, which may point to the need for better theories of gravity or perhaps be explained in other ways.

China (Chinese: 中国; pinyin: Zhōngguó), officially the People's Republic of China (PRC; Chinese: 中华人民共和国; pinyin: Zhōnghuá Rénmín Gònghéguó), is a country in East Asia. It is the world's most populous country, with a population of more than 1.4 billion people. China spans five geographical time zones and borders 14 countries, the second most of any country in the world after Russia. Covering an area of approximately 9.6 million square kilometers (3,700,000 sq mi), it is the world's third or fourth largest country. The country consists of 23 provinces, five autonomous regions, four municipalities, and two Special Administrative Regions (Hong Kong and Macau). The national capital is Beijing, and the most populous city and financial center is Shanghai.
Modern Chinese trace their origins back to a cradle of civilization in the fertile basin of the Yellow River in the North China Plain. The semi-legendary Xia dynasty in the 21st century BCE and the well-attested Shang and Zhou dynasties developed a bureaucratic political system to serve hereditary monarchies, or dynasties, and the Hundred Schools of Thought debated the relation of state, family, and individual. In the third century BCE, Qin's wars of unification finally created the first Chinese empire, the short-lived Qin dynasty. The more stable Han dynasty (206 BCE – 220 CE) established a model for nearly two millennia in which the Chinese empire was one of the world's foremost economic powers. The empire expanded, fractured and re-unified, was conquered, absorbed foreign religions and ideas, and made world-leading scientific advances, such as Four Great Inventions, gunpowder, paper, the compass, and printing. After centuries of disunion following the fall of the Han, the Tang dynasty (618–907) achieved what the Roman Empire could not: reunification of the empire. The multi-ethnic Tang welcomed foreign trade and culture that came over the Silk Road and adapted Buddhism to Chinese needs. The early modern Song dynasty (960–1279) became increasingly urban and commercial. The civilian scholar-official or literati used the examination system and  the doctrines of Neo-Confucianism to replace the military aristocrats of earlier dynasties. The Mongol invasion established the Yuan dynasty in 1279, but the Ming dynasty (1368–1644) re-established Han Chinese control. The Manchu-led Qing dynasty nearly doubled the empire's territory and established a multi-ethnic state that was the basis of the modern Chinese nation, but suffered heavy losses to foreign imperialism in the 19th century.
The Chinese monarchy collapsed in 1912 with the Xinhai Revolution, when the Republic of China (ROC) replaced the Qing dynasty. Japan invaded China in 1937, starting the Second Sino-Japanese War and temporarily halting the civil war between the Chinese Communist Party (CCP) and the Kuomintang (KMT). The surrender and expulsion of Japanese forces from China in 1945 left a power vacuum in the country, which led to renewed fighting between the CCP and the Kuomintang. The civil war ended in 1949 with the division of Chinese territory; the CCP established the People's Republic of China on the mainland while the Kuomintang-led ROC government retreated to the island of Taiwan. Both claim to be the sole legitimate government of China, although the United Nations has recognized the PRC as the sole representation since 1971. From 1959 to 1961, the PRC implemented an economic and social campaign called the Great Leap Forward that resulted in an estimated 15 to 55 million deaths, mostly through starvation. China conducted a series of economic reforms since 1978, and entered into the World Trade Organization in 2001.
China is currently governed as a unitary one-party socialist republic by the CCP. China is a permanent member of the United Nations Security Council and a founding member of several multilateral and regional cooperation organizations such as the Asian Infrastructure Investment Bank, the Silk Road Fund, the New Development Bank, the Shanghai Cooperation Organisation, and the RCEP, and is a member of the BRICS, the G8+5, the G20, the APEC, and the East Asia Summit. It ranks among the lowest in international measurements of civil liberties, government transparency, freedom of the press, freedom of religion and ethnic minorities. The Chinese authorities have been criticized by political dissidents and human rights activists for widespread human rights abuses, including political repression, mass censorship, mass surveillance of their citizens and violent suppression of protests. Chinese emigration is a significant phenomenon.
China is the world's largest economy by GDP at purchasing power parity, the second-largest economy by nominal GDP, and the second-wealthiest country. The country is one of the fastest growing major economies and is the world's largest manufacturer and exporter. China is a recognized nuclear-weapon state with the world's largest standing army by military personnel and second-largest defense budget. China is considered to be a potential superpower due to its large markets, growing military strength, economic potential, and influence in international affairs. China is a permanent member of the United Nations Security Council.

The word "China" has been used in English since the 16th century; however, it was not a word used by the Chinese themselves during this period. Its origin has been traced through Portuguese, Malay, and Persian back to the Sanskrit word Chīna, used in ancient India. "China" appears in Richard Eden's 1555 translation of the 1516 journal of the Portuguese explorer Duarte Barbosa. Barbosa's usage was derived from Persian Chīn (چین), which was in turn derived from Sanskrit Cīna (चीन). Cīna was first used in early Hindu scripture, including the Mahābhārata (5th century BCE) and the Laws of Manu (2nd century BCE). In 1655, Martino Martini suggested that the word China is derived ultimately from the name of the Qin dynasty (221–206 BCE). Although usage in Indian sources precedes this dynasty, this derivation is still given in various sources. The origin of the Sanskrit word is a matter of debate, according to the Oxford English Dictionary. Alternative suggestions include the names for Yelang and the Jing or Chu state.
The official name of the modern state is the "People's Republic of China" (simplified Chinese: 中华人民共和国; traditional Chinese: 中華人民共和國; pinyin: Zhōnghuá Rénmín Gònghéguó). The shorter form is "China" Zhōngguó (中国; 中國) from zhōng ("central") and guó ("state"), a term which developed under the Western Zhou dynasty in reference to its royal demesne. It was then applied to the area around Luoyi (present-day Luoyang) during the Eastern Zhou and then to China's Central Plain before being used as an occasional synonym for the state under the Qing. It was often used as a cultural concept to distinguish the Huaxia people from perceived "barbarians". The name Zhongguo is also translated as "Middle Kingdom" in English. China (PRC) is sometimes referred to as the Mainland when distinguishing the ROC from the PRC.
Archaeological evidence suggests that early hominids inhabited China 2.25 million years ago. The hominid fossils of Peking Man, a Homo erectus who used fire, were discovered in a cave at Zhoukoudian near Beijing; they have been dated to between 680,000 and 780,000 years ago. The fossilized teeth of Homo sapiens (dated to 125,000–80,000 years ago) have been discovered in Fuyan Cave in Dao County, Hunan. Chinese proto-writing existed in Jiahu around 7000 BCE, at Damaidi around 6000 BCE, Dadiwan from 5800 to 5400 BCE, and Banpo dating from the 5th millennium BCE. Some scholars have suggested that the Jiahu symbols (7th millennium BCE) constituted the earliest Chinese writing system.
According to Chinese tradition, the first dynasty was the Xia, which emerged around 2100 BCE. The Xia dynasty marked the beginning of China's political system based on hereditary monarchies, or dynasties, which lasted for a millennium. The Xia dynasty was considered mythical by historians until scientific excavations found early Bronze Age sites at Erlitou, Henan in 1959. It remains unclear whether these sites are the remains of the Xia dynasty or of another culture from the same period. The succeeding Shang dynasty is the earliest to be confirmed by contemporary records. The Shang ruled the plain of the Yellow River in eastern China from the 17th to the 11th century BCE. Their oracle bone script (from c. 1500 BCE) represents the oldest form of Chinese writing yet found and is a direct ancestor of modern Chinese characters.
The Shang was conquered by the Zhou, who ruled between the 11th and 5th centuries BCE, though centralized authority was slowly eroded by feudal warlords. Some principalities eventually emerged from the weakened Zhou, no longer fully obeyed the Zhou king, and continually waged war with each other during the 300-year Spring and Autumn period. By the time of the Warring States period of the 5th–3rd centuries BCE, there were only seven powerful states left.
The Warring States period ended in 221 BCE after the state of Qin conquered the other six kingdoms, reunited China and established the dominant order of autocracy. King Zheng of Qin proclaimed himself the First Emperor of the Qin dynasty. He enacted Qin's legalist reforms throughout China, notably the forced standardization of Chinese characters, measurements, road widths (i.e., cart axles' length), and currency. His dynasty also conquered the Yue tribes in Guangxi, Guangdong, and Vietnam. The Qin dynasty lasted only fifteen years, falling soon after the First Emperor's death, as his harsh authoritarian policies led to widespread rebellion.
Following a widespread civil war during which the imperial library at Xianyang was burned, the Han dynasty emerged to rule China between 206 BCE and CE 220, creating a cultural identity among its populace still remembered in the ethnonym of the Han Chinese. The Han expanded the empire's territory considerably, with military campaigns reaching Central Asia, Mongolia, South Korea, and Yunnan, and the recovery of Guangdong and northern Vietnam from Nanyue. Han involvement in Central Asia and Sogdia helped establish the land route of the Silk Road, replacing the earlier path over the Himalayas to India. Han China gradually became the largest economy of the ancient world. Despite the Han's initial decentralization and the official abandonment of the Qin philosophy of Legalism in favor of Confucianism, Qin's legalist institutions and policies continued to be employed by the Han government and its successors.
After the end of the Han dynasty, a period of strife known as Three Kingdoms followed, whose central figures were later immortalized in one of the Four Classics of Chinese literature. At its end, Wei was swiftly overthrown by the Jin dynasty. The Jin fell to civil war upon the ascension of a developmentally disabled emperor; the Five Barbarians then invaded and ruled northern China as the Sixteen States. The Xianbei unified them as the Northern Wei, whose Emperor Xiaowen reversed his predecessors' apartheid policies and enforced a drastic sinification on his subjects, largely integrating them into Chinese culture. In the south, the general Liu Yu secured the abdication of the Jin in favor of the Liu Song. The various successors of these states became known as the Northern and Southern dynasties, with the two areas finally reunited by the Sui in 581. The Sui restored the Han to power through China, reformed its agriculture, economy and imperial examination system, constructed the Grand Canal, and patronized Buddhism. However, they fell quickly when their conscription for public works and a failed war in northern Korea provoked widespread unrest.
Under the succeeding Tang and Song dynasties, Chinese economy, technology, and culture entered a golden age. The Tang dynasty retained control of the Western Regions and the Silk Road, which brought traders to as far as Mesopotamia and the Horn of Africa, and made the capital Chang'an a cosmopolitan urban center. However, it was devastated and weakened by the An Lushan Rebellion in the 8th century. In 907, the Tang disintegrated completely when the local military governors became ungovernable. The Song dynasty ended the separatist situation in 960, leading to a balance of power between the Song and Khitan Liao. The Song was the first government in world history to issue paper money and the first Chinese polity to establish a permanent standing navy which was supported by the developed shipbuilding industry along with the sea trade.
Between the 10th and 11th centuries, the population of China doubled in size to around 100 million people, mostly because of the expansion of rice cultivation in central and southern China, and the production of abundant food surpluses. The Song dynasty also saw a revival of Confucianism, in response to the growth of Buddhism during the Tang, and a flourishing of philosophy and the arts, as landscape art and porcelain were brought to new levels of maturity and complexity. However, the military weakness of the Song army was observed by the Jurchen Jin dynasty. In 1127, Emperor Huizong of Song and the capital Bianjing were captured during the Jin–Song Wars. The remnants of the Song retreated to southern China.
The Mongol conquest of China began in 1205 with the gradual conquest of Western Xia by Genghis Khan, who also invaded Jin territories. In 1271, the Mongol leader Kublai Khan established the Yuan dynasty, which conquered the last remnant of the Song dynasty in 1279. Before the Mongol invasion, the population of Song China was 120 million citizens; this was reduced to 60 million by the time of the census in 1300. A peasant named Zhu Yuanzhang led a rebellion that overthrew the Yuan in 1368 and founded the Ming dynasty as the Hongwu Emperor. Under the Ming dynasty, China enjoyed another golden age, developing one of the strongest navies in the world and a rich and prosperous economy amid a flourishing of art and culture. It was during this period that admiral Zheng He led the Ming treasure voyages throughout the Indian Ocean, reaching as far as East Africa.
In the early years of the Ming dynasty, China's capital was moved from Nanjing to Beijing. With the budding of capitalism, philosophers such as Wang Yangming further critiqued and expanded Neo-Confucianism with concepts of individualism and equality of four occupations. The scholar-official stratum became a supporting force of industry and commerce in the tax boycott movements, which, together with the famines and defense against Japanese invasions of Korea (1592–1598) and Manchu invasions led to an exhausted treasury. In 1644, Beijing was captured by a coalition of peasant rebel forces led by Li Zicheng. The Chongzhen Emperor committed suicide when the city fell. The Manchu Qing dynasty, then allied with Ming dynasty general Wu Sangui, overthrew Li's short-lived Shun dynasty and subsequently seized control of Beijing, which became the new capital of the Qing dynasty.
The Qing dynasty, which lasted from 1644 until 1912, was the last imperial dynasty of China. Its conquest of the Ming (1618–1683) cost 25 million lives and the economy of China shrank drastically. After the Southern Ming ended, the further conquest of the Dzungar Khanate added Mongolia, Tibet and Xinjiang to the empire. The centralized autocracy was strengthened to suppress anti-Qing sentiment with the policy of valuing agriculture and restraining commerce, the Haijin ("sea ban"), and ideological control as represented by the literary inquisition, causing social and technological stagnation.
In the mid-19th century, the Qing dynasty experienced Western imperialism in the Opium Wars with Britain and France. China was forced to pay compensation, open treaty ports, allow extraterritoriality for foreign nationals, and cede Hong Kong to the British under the 1842 Treaty of Nanking, the first of the Unequal Treaties. The First Sino-Japanese War (1894–1895) resulted in Qing China's loss of influence in the Korean Peninsula, as well as the cession of Taiwan to Japan.
The Qing dynasty also began experiencing internal unrest in which tens of millions of people died, especially in the White Lotus Rebellion, the failed Taiping Rebellion that ravaged southern China in the 1850s and 1860s and the Dungan Revolt (1862–1877) in the northwest. The initial success of the Self-Strengthening Movement of the 1860s was frustrated by a series of military defeats in the 1880s and 1890s.
In the 19th century, the great Chinese diaspora began. Losses due to emigration were added to by conflicts and catastrophes such as the Northern Chinese Famine of 1876–1879, in which between 9 and 13 million people died. The Guangxu Emperor drafted a reform plan in 1898 to establish a modern constitutional monarchy, but these plans were thwarted by the Empress Dowager Cixi. The ill-fated anti-foreign Boxer Rebellion of 1899–1901 further weakened the dynasty. Although Cixi sponsored a program of reforms, the Xinhai Revolution of 1911–1912 brought an end to the Qing dynasty and established the Republic of China. Puyi, the last Emperor of China, abdicated in 1912.
On 1 January 1912, the Republic of China was established, and Sun Yat-sen of the Kuomintang (the KMT or Nationalist Party) was proclaimed provisional president. On 12 February 1912, regent Empress Dowager Longyu sealed the imperial abdication decree on behalf of 4 year old Puyi, the last emperor of China, ending 5,000 years of monarchy in China. In March 1912, the presidency was given to Yuan Shikai, a former Qing general who in 1915 proclaimed himself Emperor of China. In the face of popular condemnation and opposition from his own Beiyang Army, he was forced to abdicate and re-establish the republic in 1916.
After Yuan Shikai's death in 1916, China was politically fragmented. Its Beijing-based government was internationally recognized but virtually powerless; regional warlords controlled most of its territory. In the late 1920s, the Kuomintang under Chiang Kai-shek, the then Principal of the Republic of China Military Academy, was able to reunify the country under its own control with a series of deft military and political maneuverings, known collectively as the Northern Expedition. The Kuomintang moved the nation's capital to Nanjing and implemented "political tutelage", an intermediate stage of political development outlined in Sun Yat-sen's San-min program for transforming China into a modern democratic state. The political division in China made it difficult for Chiang to battle the communist-led People's Liberation Army (PLA), against whom the Kuomintang had been warring since 1927 in the Chinese Civil War. This war continued successfully for the Kuomintang, especially after the PLA retreated in the Long March, until Japanese aggression and the 1936 Xi'an Incident forced Chiang to confront Imperial Japan.
The Second Sino-Japanese War (1937–1945), a theater of World War II, forced an uneasy alliance between the Kuomintang and the Communists. Japanese forces committed numerous war atrocities against the civilian population; in all, as many as 20 million Chinese civilians died. An estimated 40,000 to 300,000 Chinese were massacred in the city of Nanjing alone during the Japanese occupation. During the war, China, along with the UK, the United States, and the Soviet Union, were referred to as "trusteeship of the powerful" and were recognized as the Allied "Big Four" in the Declaration by United Nations. Along with the other three great powers, China was one of the four major Allies of World War II, and was later considered one of the primary victors in the war. After the surrender of Japan in 1945, Taiwan, including the Pescadores, was handed over to Chinese control. However, the validity of this handover is controversial, in that whether Taiwan's sovereignty was legally transferred and whether China is a legitimate recipient, due to complex issues that arose from the handling of Japan's surrender, resulting in the unresolved political status of Taiwan, which is a flashpoint of potential war between China and Taiwan. China emerged victorious but war-ravaged and financially drained. The continued distrust between the Kuomintang and the Communists led to the resumption of civil war. Constitutional rule was established in 1947, but because of the ongoing unrest, many provisions of the ROC constitution were never implemented in mainland China.
Before the existence of the People's Republic, the CCP had declared several areas of the country as the Chinese Soviet Republic (Jiangxi Soviet), a predecessor state to the PRC, in November 1931 in Ruijin, Jiangxi. The Jiangxi Soviet was wiped out by the KMT armies in 1934 and was relocated to Yan'an in Shaanxi where the Long March concluded in 1935. It would be the base of the communists before major combat in the Chinese Civil War ended in 1949. Afterwards, the CCP gain control of most of mainland China, and the Kuomintang retreating offshore to Taiwan, reducing its territory to only Taiwan, Hainan, and their surrounding islands.
On 1 October 1949, CCP Chairman Mao Zedong formally proclaimed the establishment of the People's Republic of China at the new nation's founding ceremony and inaugural military parade in Tiananmen Square, Beijing. In 1950, the People's Liberation Army captured Hainan from the ROC and incorporated Tibet. However, remaining Kuomintang forces continued to wage an insurgency in western China throughout the 1950s.
The government consolidated its popularity among the peasants through land reform, which included the execution of between 1 and 2 million landlords. China developed an independent industrial system and its own nuclear weapons. The Chinese population increased from 550 million in 1950 to 900 million in 1974. However, the Great Leap Forward, an idealistic massive reform project, resulted in an estimated 15 to 55 million deaths between 1959 and 1961, mostly from starvation. In 1966, Mao and his allies launched the Cultural Revolution, sparking a decade of political recrimination and social upheaval that lasted until Mao's death in 1976. In October 1971, the PRC replaced the Republic of China in the United Nations, and took its seat as a permanent member of the Security Council. This UN action also created the problem of the political status of Taiwan and the Two Chinas issue. See Cross-Strait relations and "Taiwan, China".
After Mao's death, the Gang of Four was quickly arrested by Hua Guofeng and held responsible for the excesses of the Cultural Revolution. Elder Deng Xiaoping took power in 1978, and instituted significant economic reforms. The CCP loosened governmental control over citizens' personal lives, and the communes were gradually disbanded in favor of working contracted to households. This marked China's transition from a planned economy to a mixed economy with an increasingly open-market environment. China adopted its current constitution on 4 December 1982. In 1989, the suppression of student protests in Tiananmen Square brought condemnations and sanctions against the Chinese government from various foreign countries.
Jiang Zemin, Li Peng and Zhu Rongji led the nation in the 1990s. Under their administration, China's economic performance pulled an estimated 150 million peasants out of poverty and sustained an average annual gross domestic product growth rate of 11.2%. British Hong Kong and Portuguese Macau returned to China in 1997 and 1999, respectively, as the Hong Kong and Macau special administrative regions under the principle of One country, two systems. The country joined the World Trade Organization in 2001, and maintained its high rate of economic growth under Hu Jintao and Wen Jiabao's leadership in the 2000s. However, the growth also severely impacted the country's resources and environment, and caused major social displacement.
Chinese Communist Party general secretary Xi Jinping has ruled since 2012 and has pursued large-scale efforts to reform China's economy (which has suffered from structural instabilities and slowing growth), and has also reformed the one-child policy and penal system, as well as instituting a vast anti corruption crackdown. In 2013, China initiated the Belt and Road Initiative, a global infrastructure investment project.
The global COVID-19 pandemic originated in Wuhan and was first identified from an outbreak in December 2019. The Chinese government response has included a zero-COVID strategy, making it one of few countries to pursue this approach. The country's economy continued to broaden recovery from the recession during the pandemic, with stable job creation and record international trade growth, although retail consumption was still slower than predicted.
On 1 July 2021, the People's Republic of China celebrated the 100th anniversary of the establishment of the CCP (first of the Two Centenaries) with a huge gathering in Tiananmen Square and cultural artistic performance in Beijing National Stadium in Beijing.
China's landscape is vast and diverse, ranging from the Gobi and Taklamakan Deserts in the arid north to the subtropical forests in the wetter south. The Himalaya, Karakoram, Pamir and Tian Shan mountain ranges separate China from much of South and Central Asia. The Yangtze and Yellow Rivers, the third- and sixth-longest in the world, respectively, run from the Tibetan Plateau to the densely populated eastern seaboard. China's coastline along the Pacific Ocean is 14,500 km (9,000 mi) long and is bounded by the Bohai, Yellow, East China and South China seas. China connects through the Kazakh border to the Eurasian Steppe which has been an artery of communication between East and West since the Neolithic through the Steppe Route – the ancestor of the terrestrial Silk Road(s).
The territory of China lies between latitudes 18° and 54° N, and longitudes 73° and 135° E. The geographical center of China is marked by the Center of the Country Monument at 35°50′40.9″N 103°27′7.5″E﻿ / ﻿35.844694°N 103.452083°E﻿ / 35.844694; 103.452083﻿ (Geographical center of China). China's landscapes vary significantly across its vast territory. In the east, along the shores of the Yellow Sea and the East China Sea, there are extensive and densely populated alluvial plains, while on the edges of the Inner Mongolian plateau in the north, broad grasslands predominate. Southern China is dominated by hills and low mountain ranges, while the central-east hosts the deltas of China's two major rivers, the Yellow River and the Yangtze River. Other major rivers include the Xi, Mekong, Brahmaputra and Amur. To the west sit major mountain ranges, most notably the Himalayas. High plateaus feature among the more arid landscapes of the north, such as the Taklamakan and the Gobi Desert. The world's highest point, Mount Everest (8,848 m), lies on the Sino-Nepalese border. The country's lowest point, and the world's third-lowest, is the dried lake bed of Ayding Lake (−154 m) in the Turpan Depression.
China's climate is mainly dominated by dry seasons and wet monsoons, which lead to pronounced temperature differences between winter and summer. In the winter, northern winds coming from high-latitude areas are cold and dry; in summer, southern winds from coastal areas at lower latitudes are warm and moist.
A major environmental issue in China is the continued expansion of its deserts, particularly the Gobi Desert. Although barrier tree lines planted since the 1970s have reduced the frequency of sandstorms, prolonged drought and poor agricultural practices have resulted in dust storms plaguing northern China each spring, which then spread to other parts of East Asia, including Japan and Korea. China's environmental watchdog, SEPA, stated in 2007 that China is losing 4,000 km2 (1,500 sq mi) per year to desertification. Water quality, erosion, and pollution control have become important issues in China's relations with other countries. Melting glaciers in the Himalayas could potentially lead to water shortages for hundreds of millions of people. According to academics, in order to limit climate change in China to 1.5 °C (2.7 °F) electricity generation from coal in China without carbon capture must be phased out by 2045. Official government statistics about Chinese agricultural productivity are considered unreliable, due to exaggeration of production at subsidiary government levels. Much of China has a climate very suitable for agriculture and the country has been the world's largest producer of rice, wheat, tomatoes, eggplant, grapes, watermelon, spinach, and many other crops.
China is one of 17 megadiverse countries, lying in two of the world's major biogeographic realms: the Palearctic and the Indomalayan. By one measure, China has over 34,687 species of animals and vascular plants, making it the third-most biodiverse country in the world, after Brazil and Colombia. The country signed the Rio de Janeiro Convention on Biological Diversity on 11 June 1992, and became a party to the convention on 5 January 1993. It later produced a National Biodiversity Strategy and Action Plan, with one revision that was received by the convention on 21 September 2010.
China is home to at least 551 species of mammals (the third-highest such number in the world), 1,221 species of birds (eighth), 424 species of reptiles (seventh) and 333 species of amphibians (seventh). Wildlife in China shares habitat with, and bears acute pressure from, the world's largest population of humans. At least 840 animal species are threatened, vulnerable or in danger of local extinction in China, due mainly to human activity such as habitat destruction, pollution and poaching for food, fur and ingredients for traditional Chinese medicine. Endangered wildlife is protected by law, and as of 2005, the country has over 2,349 nature reserves, covering a total area of 149.95 million hectares, 15 percent of China's total land area. Most wild animals have been eliminated from the core agricultural regions of east and central China, but they have fared better in the mountainous south and west. The Baiji was confirmed extinct on 12 December 2006.
China has over 32,000 species of vascular plants, and is home to a variety of forest types. Cold coniferous forests predominate in the north of the country, supporting animal species such as moose and Asian black bear, along with over 120 bird species. The understory of moist conifer forests may contain thickets of bamboo. In higher montane stands of juniper and yew, the bamboo is replaced by rhododendrons. Subtropical forests, which are predominate in central and southern China, support a high density of plant species including numerous rare endemics. Tropical and seasonal rainforests, though confined to Yunnan and Hainan Island, contain a quarter of all the animal and plant species found in China. China has over 10,000 recorded species of fungi, and of them, nearly 6,000 are higher fungi.
In the early 2000s, China has suffered from environmental deterioration and pollution due to its rapid pace of industrialization. While regulations such as the 1979 Environmental Protection Law are fairly stringent, they are poorly enforced, as they are frequently disregarded by local communities and government officials in favor of rapid economic development. China is the country with the second highest death toll because of air pollution, after India. There are approximately 1 million deaths caused by exposure to ambient air pollution. Although China ranks as the highest CO2 emitting country in the world, it only emits 8 tons of CO2 per capita, significantly lower than developed countries such as the United States (16.1), Australia (16.8) and South Korea (13.6).
In recent years, China has clamped down on pollution. In March 2014, CCP General Secretary Xi Jinping "declared war" on pollution during the opening of the National People's Congress. After extensive debate lasting nearly two years, the parliament approved a new environmental law in April. The new law empowers environmental enforcement agencies with great punitive power and large fines for offenders, defines areas which require extra protection, and gives independent environmental groups more ability to operate in the country. In 2020, Chinese Communist Party general secretary Xi Jinping announced that China aims to peak emissions before 2030 and go carbon-neutral by 2060 in accordance with the Paris climate accord. According to Climate Action Tracker, if accomplished it would lower the expected rise in global temperature by 0.2 – 0.3 degrees – "the biggest single reduction ever estimated by the Climate Action Tracker". In September 2021 Xi Jinping announced that China will not build "coal-fired power projects abroad". The decision can be "pivotal" in reducing emissions. The Belt and Road Initiative did not include financing such projects already in the first half of 2021.
The country also had significant water pollution problems: 8.2% of China's rivers had been polluted by industrial and agricultural waste in 2019. China had a 2018 Forest Landscape Integrity Index mean score of 7.14/10, ranking it 53rd globally out of 172 countries. In 2020, a sweeping law was passed by the Chinese government to protect the ecology of the Yangtze River. The new laws include strengthening ecological protection rules for hydropower projects along the river, banning chemical plants within 1 kilometer of the river, relocating polluting industries, severely restricting sand mining as well as a complete fishing ban on all the natural waterways of the river, including all its major tributaries and lakes.
China is also the world's leading investor in renewable energy and its commercialization, with $52 billion invested in 2011 alone; it is a major manufacturer of renewable energy technologies and invests heavily in local-scale renewable energy projects. By 2015, over 24% of China's energy was derived from renewable sources, while most notably from hydroelectric power: a total installed capacity of 197 GW makes China the largest hydroelectric power producer in the world. China also has the largest power capacity of installed solar photovoltaics system and wind power system in the world. Greenhouse gas emissions by China are the world's largest, as is renewable energy in China. Despite its emphasis on renewables, China remains deeply connected to global oil markets. Russia's single largest buyer, China takes in 20% of Russian oil exports, averaging 1.6 million barrels of crude oil per day in 2021.
The People's Republic of China is the second-largest country in the world by land area after Russia. China's total area is generally stated as being approximately 9,600,000 km2 (3,700,000 sq mi). Specific area figures range from 9,572,900 km2 (3,696,100 sq mi) according to the Encyclopædia Britannica, to 9,596,961 km2 (3,705,407 sq mi) according to the UN Demographic Yearbook, and the CIA World Factbook.
China has the longest combined land border in the world, measuring 22,117 km (13,743 mi) and its coastline covers approximately 14,500 km (9,000 mi) from the mouth of the Yalu River (Amnok River) to the Gulf of Tonkin. China borders 14 nations and extends across much of East Asia, bordering Vietnam, Laos, and Myanmar (Burma) in Southeast Asia; India, Bhutan, Nepal, Afghanistan, and Pakistan in South Asia; Tajikistan, Kyrgyzstan and Kazakhstan in Central Asia; and Russia, Mongolia, and North Korea in Inner Asia and Northeast Asia. Additionally, China shares maritime boundaries with South Korea, Japan, Vietnam, and the Philippines.
The Chinese constitution states that the People's Republic of China "is a socialist state governed by a people's democratic dictatorship that is led by the working class and based on an alliance of workers and peasants," and that the state institutions "shall practice the principle of democratic centralism." The PRC is one of the world's only socialist states governed by a communist party. The Chinese government has been variously described as communist and socialist, but also as authoritarian and corporatist, with heavy restrictions in many areas, most notably against free access to the Internet, freedom of the press, freedom of assembly, the right to have children, free formation of social organizations and freedom of religion.
Although the Chinese Communist Party describes China as a "socialist consultative democracy", the country is commonly described as an authoritarian surveillance state and a dictatorship. Its current political, ideological and economic system has been termed by its leaders as a "consultative democracy" "people's democratic dictatorship", "socialism with Chinese characteristics" (which is Marxism adapted to Chinese circumstances) and the "socialist market economy" respectively.
Since 2018, the main body of the Chinese constitution declares that "the defining feature of socialism with Chinese characteristics is the leadership of the Chinese Communist Party (CCP)." The 2018 amendments constitutionalized the de facto one-party state status of China, wherein the CCP General Secretary (party leader) holds ultimate power and authority over state and government and serves as the informal Paramount leader. The current General Secretary is Xi Jinping, who took office on 15 November 2012, and was re-elected on 25 October 2017. The electoral system is pyramidal. Local People's Congresses are directly elected, and higher levels of People's Congresses up to the National People's Congress (NPC) are indirectly elected by the People's Congress of the level immediately below. Another eight political parties, have representatives in the NPC and the Chinese People's Political Consultative Conference (CPPCC). China supports the Leninist principle of "democratic centralism", but critics describe the elected National People's Congress as a "rubber stamp" body.
Since both the CCP and the People's Liberation Army (PLA) promote according to seniority, it is possible to discern distinct generations of Chinese leadership. In official discourse, each group of leadership is identified with a distinct extension of the ideology of the party. Historians have studied various periods in the development of the government of the People's Republic of China by reference to these "generations".
China is a one-party state led by the Chinese Communist Party (CCP). The National People's Congress in 2018 altered the country's constitution to remove the two-term limit on holding the Presidency of China, permitting the current leader, Xi Jinping, to remain president of China (and General Secretary of the Chinese Communist Party) for an unlimited time, earning criticism for creating dictatorial governance. The President is the titular head of state, elected by the National People's Congress. The Premier is the head of government, presiding over the State Council composed of four vice premiers and the heads of ministries and commissions. The incumbent president is Xi Jinping, who is also the General Secretary of the Chinese Communist Party and the Chairman of the Central Military Commission, making him China's paramount leader. The incumbent premier is Li Keqiang, who is also a senior member of the CCP Politburo Standing Committee, China's de facto top decision-making body.
In 2017, Xi called on the communist party to further tighten its grip on the country, to uphold the unity of the party leadership, and achieve the "Chinese Dream of national rejuvenation". Political concerns in China include the growing gap between rich and poor and government corruption. Nonetheless, the level of public support for the government and its management of the nation is high, with 80–95% of Chinese citizens expressing satisfaction with the central government, according to a 2011 survey. A 2020 survey from the Canadian Institutes of Health Research also found that 75% of Chinese were satisfied with the government on information dissemination amidst the COVID-19 pandemic, while 67% were satisfied with its delivery of daily necessities.
The People's Republic of China is officially divided into 23 provinces, five autonomous regions (each with a designated minority group), and four municipalities—collectively referred to as "mainland China"—as well as the special administrative regions (SARs) of Hong Kong and Macau. Geographically, all 31 provincial divisions of mainland China can be grouped into six regions: North China, Northeast China, East China, South Central China, Southwest China, and Northwest China.
China considers Taiwan to be its 23rd province, although Taiwan is governed by the Republic of China (ROC), which rejects the PRC's claim. Conversely, the ROC constitution claims sovereignty over all divisions governed by the PRC.
The PRC has diplomatic relations with 175 countries and maintains embassies in 162. In 2019, China had the largest diplomatic network in the world. Its legitimacy is disputed by the Republic of China and a few other countries; it is thus the largest and most populous state with limited recognition, with a population of more than 1.4 billion. In 1971, the PRC replaced the Republic of China as the sole representative of China in the United Nations and as one of the five permanent members of the United Nations Security Council. China was also a former member and leader of the Non-Aligned Movement, and still considers itself an advocate for developing countries. Along with Brazil, Russia, India and South Africa, China is a member of the BRICS group of emerging major economies and hosted the group's third official summit at Sanya, Hainan in April 2011.
Under its interpretation of the One-China policy, Beijing has made it a precondition to establishing diplomatic relations that the other country acknowledges its claim to Taiwan and severs official ties with the government of the Republic of China. Chinese officials have protested on numerous occasions when foreign countries have made diplomatic overtures to Taiwan, especially in the matter of armament sales.
Much of current Chinese foreign policy is reportedly based on Premier Zhou Enlai's Five Principles of Peaceful Coexistence, and is also driven by the concept of "harmony without uniformity", which encourages diplomatic relations between states despite ideological differences. This policy may have led China to support states that are regarded as dangerous or repressive by Western nations, such as Zimbabwe, North Korea and Iran. China has a close economic and military relationship with Russia, and the two states often vote in unison in the UN Security Council.
China became the world's largest trading nation in 2013, as measured by the sum of imports and exports, as well as the world's biggest commodity importer. comprising roughly 45% of maritime's dry-bulk market.
By 2016, China was the largest trading partner of 124 other countries. China is the largest trading partner for the ASEAN nations, with a total trade value of $345.8 billion in 2015 accounting for 15.2% of ASEAN's total trade. ASEAN is also China's largest trading partner. In 2020, China became the largest trading partner of the European Union for goods, with the total value of goods trade reaching nearly $700 billion. China, along with ASEAN, Japan, South Korea, Australia and New Zealand, is a member of the Regional Comprehensive Economic Partnership, the world's largest free-trade area covering 30% of the world's population and economic output. China became a member of the World Trade Organization (WTO) in 2001. In 2004, it proposed an entirely new East Asia Summit (EAS) framework as a forum for regional security issues. The EAS, which includes ASEAN Plus Three, India, Australia and New Zealand, held its inaugural summit in 2005.
China has had a long and complex trade relationship with the United States. In 2000, the United States Congress approved "permanent normal trade relations" (PNTR) with China, allowing Chinese exports in at the same low tariffs as goods from most other countries. China has a significant trade surplus with the United States, its most important export market. In the early 2010s, US politicians argued that the Chinese yuan was significantly undervalued, giving China an unfair trade advantage.
Since the turn of the century, China has followed a policy of engaging with African nations for trade and bilateral co-operation; in 2019, Sino-African trade totalled $208 billion, having grown 20 times over two decades. According to Madison Condon "China finances more infrastructure projects in Africa than the World Bank and provides billions of dollars in low-interest loans to the continent's emerging economies." China maintains extensive and highly diversified trade links with the European Union. China has furthermore strengthened its trade ties with major South American economies, and is the largest trading partner of Brazil, Chile, Peru, Uruguay, Argentina, and several others.
China's Belt and Road Initiative has expanded significantly over the last six years and, as of April 2020, includes 138 countries and 30 international organizations. In addition to intensifying foreign policy relations, the focus here is particularly on building efficient transport routes. The focus is particularly on the maritime Silk Road with its connections to East Africa and Europe and there are Chinese investments or related declarations of intent at numerous ports such as Gwadar, Kuantan, Hambantota, Piraeus and Trieste. However many of these loans made under the Belt and Road program are unsustainable and China has faced a number of calls for debt relief from debtor nations.
Ever since its establishment after the Chinese Civil War, the PRC has claimed the territories governed by the Republic of China (ROC), a separate political entity today commonly known as Taiwan, as a part of its territory. It regards the island of Taiwan as its Taiwan Province, Kinmen and Matsu as a part of Fujian Province and islands the ROC controls in the South China Sea as a part of Hainan Province and Guangdong Province. These claims are controversial because of the complicated Cross-Strait relations, with the PRC treating the One-China Principle as one of its most important diplomatic principles.
China has resolved its land borders with 12 out of 14 neighboring countries, having pursued substantial compromises in most of them. As of 2022, China currently has a disputed land border with India (Sino-Indian border dispute) and Bhutan.
China is additionally involved in maritime disputes with multiple countries over the ownership of several small islands in the East and South China Seas, such as Socotra Rock, the Senkaku Islands and the entirety of South China Sea Islands, along with the EEZ disputes over East China Sea.
China uses a massive espionage network of cameras, facial recognition software, sensors, surveillance of personal technology, and a social credit system as a means of social control of persons living in the country. The Chinese democracy movement, social activists, and some members of the Chinese Communist Party believe in the need for social and political reform. While economic and social controls have been significantly relaxed in China since the 1970s, political freedom is still tightly restricted. The Constitution of the People's Republic of China states that the "fundamental rights" of citizens include freedom of speech, freedom of the press, the right to a fair trial, freedom of religion, universal suffrage, and property rights. However, in practice, these provisions do not afford significant protection against criminal prosecution by the state. Although some criticisms of government policies and the ruling Communist Party are tolerated, censorship of political speech and information, most notably on the Internet, are routinely used to prevent collective action. By 2020, China plans to give all its citizens a personal "social credit" score based on how they behave. The social credit system, first piloted in 2014, is considered a form of mass surveillance which uses big data analysis technology.
A number of foreign governments, foreign press agencies, and NGOs have criticized China's human rights record, alleging widespread civil rights violations such as detention without trial, forced abortions, forced confessions, torture, restrictions of fundamental rights, and excessive use of the death penalty. The government suppresses popular protests and demonstrations that it considers a potential threat to "social stability", as was the case with the Tiananmen Square protests of 1989.
The Chinese state is regularly accused of large-scale repression and human rights abuses in Tibet and Xinjiang, including violent police crackdowns and religious suppression throughout the Chinese nation. At least one million members of China's Muslim Uyghur minority have been detained in mass detention camps, termed "Vocational Education and Training Centers", aimed at changing the political thinking of detainees, their identities, and their religious beliefs. According to the U.S. Department of State, actions including political indoctrination, torture, physical and psychological abuse, forced sterilization, sexual abuse, and forced labor are common in these facilities. The state has also sought to control offshore reporting of tensions in Xinjiang, intimidating foreign-based reporters by detaining their family members. According to a 2020 report, China's treatment of Uyghurs meets the UN definition of genocide, and several groups called for a UN investigation. On 19 January 2021, the United States Secretary of State, Mike Pompeo, announced that the United States Department of State had determined that "genocide and crimes against humanity" had been perpetrated by China against the Uyghurs.
Global studies from Pew Research Center in 2014 and 2017 ranked the Chinese government's restrictions on religion as among the highest in the world, despite low to moderate rankings for religious-related social hostilities in the country. The Global Slavery Index estimated that in 2016 more than 3.8 million people were living in "conditions of modern slavery", or 0.25% of the population, including victims of human trafficking, forced labor, forced marriage, child labor, and state-imposed forced labor. The state-imposed forced system was formally abolished in 2013, but it is not clear to which extent its various practices have stopped. The Chinese penal system includes labor prison factories, detention centers, and re-education camps, collectively known as laogai ("reform through labor"). The Laogai Research Foundation in the United States estimated that there were over a thousand slave labor prisons and camps in China.
In 2019, a study called for the mass retraction of more than 400 scientific papers on organ transplantation, because of fears the organs were obtained unethically from Chinese prisoners. While the government says 10,000 transplants occur each year, a report by the Falun Gong-linked IETAC alleged that between 60,000 and 100,000 organs are transplanted each year and claimed that this gap was being made up by executed prisoners of conscience.
With nearly 2.2 million active troops, the People's Liberation Army (PLA) is the largest standing military force in the world, commanded by the Central Military Commission (CMC). China has the second-biggest military reserve force, only behind North Korea. The PLA consists of the Ground Force (PLAGF), the Navy (PLAN), the Air Force (PLAAF), the Rocket Force (PLARF) and the Strategic Support Force (PLASSF). According to the Chinese government, military budget for 2017 totalled US$151.5 billion, constituting the world's second-largest military budget, although the military expenditures-GDP ratio with 1.3% of GDP is below world average. However, many authorities – including SIPRI and the U.S. Office of the Secretary of Defense claim that China hides its real level of military spending, which is allegedly much higher than the official budget.
China boasts the world's third-most powerful military, with the world's third-largest stockpile of nuclear weapons.
Since 2010, China has had the world's second-largest economy in terms of nominal GDP, totaling approximately US$15.66 trillion (101.6 trillion Yuan) as of 2020. In terms of purchasing power parity (PPP GDP), China's economy has been the largest in the world since 2014, according to the World Bank. China is also the world's fastest-growing major economy. According to the World Bank, China's GDP grew from $150 billion in 1978 to $14.28 trillion by 2019. China's economic growth has been consistently above 6 percent since the introduction of economic reforms in 1978. China is also the world's largest exporter and second-largest importer of goods. Between 2010 and 2019, China's contribution to global GDP growth has been 25% to 39%. It is the largest engine of global growth for the world economy, accounting for 25–30% global total expansion since the financial crisis of 2008–2009. It now accounts for 15% of total world output.
China had one of the largest economies in the world for most of the past two thousand years, during which it has seen cycles of prosperity and decline. Since economic reforms began in 1978, China has developed into a highly diversified economy and one of the most consequential players in international trade. Major sectors of competitive strength include manufacturing, retail, mining, steel, textiles, automobiles, energy generation, green energy, banking, electronics, telecommunications, real estate, e-commerce, and tourism. China has three out of the ten largest stock exchanges in the world—Shanghai, Hong Kong and Shenzhen—that together have a market capitalization of over $15.9 trillion, as of October 2020. China has four (Shanghai, Hong Kong, Beijing, and Shenzhen) out of the world's top ten most competitive financial centers, which is more than any country in the 2020 Global Financial Centres Index. By 2035, China's four cities (Shanghai, Beijing, Guangzhou and Shenzhen) are projected to be among the global top ten largest cities by nominal GDP according to a report by Oxford Economics.
China has been the world's No. 1 manufacturer since 2010, after overtaking the US, which had been No. 1 for the previous hundred years. China has also been No. 2 in high-tech manufacturing since 2012, according to US National Science Foundation. China is the second largest retail market in the world, next to the United States. China leads the world in e-commerce, accounting for 40% of the global market share in 2016 and more than 50% of the global market share in 2019. China is the world's leader in electric vehicles, manufacturing and buying half of all the plug-in electric cars (BEV and PHEV) in the world in 2018. China is also the leading producer of batteries for electric vehicles as well as several key raw materials for batteries. China had 174 GW of installed solar capacity by the end of 2018, which amounts to more than 40% of the global solar capacity.
Foreign and some Chinese sources have claimed that official Chinese government statistics overstate China's economic growth. However, several Western academics and institutions have stated that China's economic growth is higher than indicated by official figures.
China has a large informal economy, which arose as a result of the country's economic opening. The informal economy is a source of employment and income for workers, but it is unrecognized and suffers from lower productivity.
As of 2020, China was second in the world, after the US, in total number of billionaires and total number of millionaires, with 698 Chinese billionaires and 4.4 million millionaires. In 2019, China overtook the US as the home to the highest number of people who have a net personal wealth of at least $110,000, according to the global wealth report by Credit Suisse. According to the Hurun Global Rich List 2020, China is home to five of the world's top ten cities (Beijing, Shanghai, Hong Kong, Shenzhen, and Guangzhou in the 1st, 3rd, 4th, 5th, and 10th spots, respectively) by the highest number of billionaires, which is more than any other country. China had 85 female billionaires as of January 2021, two-thirds of the global total, and minted 24 new female billionaires in 2020.
However, it ranks behind over 60 countries (out of around 180) in per capita economic output, making it an upper-middle income country. According to the IMF, on a per capita income basis among countries with a large population of over 100 million as of 2021, China ranked 3rd by GDP per capita (nominal) and 5th by GDP per capita (PPP). Additionally, its development is highly uneven. Its major cities and coastal areas are far more prosperous compared to rural and interior regions. China brought more people out of extreme poverty than any other country in history—between 1978 and 2018, China reduced extreme poverty by 800 million. China reduced the extreme poverty rate—per international standard, it refers to an income of less than $1.90/day—from 88% in 1981 to 1.85% by 2013. According to the World Bank, the number of Chinese in extreme poverty fell from 756 million to 25 million between 1990 and 2013. The portion of people in China living below the international poverty line of $1.90 per day (2011 PPP) fell to 0.3% in 2018 from 66.3% in 1990. Using the lower-middle income poverty line of $3.20 per day, the portion fell to 2.9% in 2018 from 90.0% in 1990. Using the upper-middle income poverty line of $5.50 per day, the portion fell to 17.0% from 98.3% in 1990.
From its founding in 1949 until late 1978, the People's Republic of China was a Soviet-style centrally planned economy. Following Mao's death in 1976 and the consequent end of the Cultural Revolution, Deng Xiaoping and the new Chinese leadership began to reform the economy and move towards a more market-oriented mixed economy under one-party rule. Agricultural collectivization was dismantled and farmlands privatized, while foreign trade became a major new focus, leading to the creation of Special Economic Zones (SEZs). Inefficient state-owned enterprises (SOEs) were restructured and unprofitable ones were closed outright, resulting in massive job losses. Modern-day China is mainly characterized as having a market economy based on private property ownership, and is one of the leading examples of state capitalism. The state still dominates in strategic "pillar" sectors such as energy production and heavy industries, but private enterprise has expanded enormously, with around 30 million private businesses recorded in 2008. In 2018, private enterprises in China accounted for 60% of GDP, 80% of urban employment and 90% of new jobs.
In the early 2010s, China's economic growth rate began to slow amid domestic credit troubles, weakening international demand for Chinese exports and fragility in the global economy. China's GDP was slightly larger than Germany's in 2007; however, by 2017, China's $12.2 trillion-economy became larger than those of Germany, UK, France and Italy combined. In 2018, the IMF reiterated its forecast that China will overtake the US in terms of nominal GDP by 2030. Economists also expect China's middle class to expand to 600 million people by 2025.
In 2020, China was the only major economy in the world to grow, recording a 2.3% growth due to its success in taming the coronavirus within its borders.
China is a member of the WTO and is the world's largest trading power, with a total international trade value of US$4.62 trillion in 2018. Its foreign exchange reserves reached US$3.1 trillion as of 2019, making its reserves by far the world's largest. In 2012, China was the world's largest recipient of inward foreign direct investment (FDI), attracting $253 billion. In 2014, China's foreign exchange remittances were $US64 billion making it the second largest recipient of remittances in the world. China also invests abroad, with a total outward FDI of $62.4 billion in 2012, and a number of major takeovers of foreign firms by Chinese companies. China is a major owner of US public debt, holding trillions of dollars worth of U.S. Treasury bonds. China's undervalued exchange rate has caused friction with other major economies, and it has also been widely criticized for manufacturing large quantities of counterfeit goods.
Following the 2007–08 financial crisis, Chinese authorities sought to actively wean off of its dependence on the U.S. dollar as a result of perceived weaknesses of the international monetary system. To achieve those ends, China took a series of actions to further the internationalization of the Renminbi. In 2008, China established dim sum bond market and expanded the Cross-Border Trade RMB Settlement Pilot Project, which helps establish pools of offshore RMB liquidity. This was followed with bilateral agreements to settle trades directly in renminbi with Russia, Japan, Australia, Singapore, the United Kingdom, and Canada. As a result of the rapid internationalization of the renminbi, it became the eighth-most-traded currency in the world, an emerging international reserve currency, and a component of the IMF's special drawing rights; however, partly due to capital controls that make the renminbi fall short of being a fully convertible currency, it remains far behind the Euro, Dollar and Japanese Yen in international trade volumes.
China has had the world's largest middle class population since 2015, and the middle class grew to a size of 400 million by 2018. In 2020, a study by the Brookings Institution forecast that China's middle-class will reach 1.2 billion by 2027 (almost 4 times the entire U.S. population today), making up one fourth of the world total.
From 1978 to 2018, the average standard of living multiplied by a factor of twenty-six. Wages in China have grown a lot in the last 40 years—real (inflation-adjusted) wages grew seven-fold from 1978 to 2007. Per capita incomes have risen significantly – when the PRC was founded in 1949, per capita income in China was one fifth of the world average; per capita incomes now equal the world average itself. By 2018, median wages in Chinese cities such as Shanghai were about the same as or higher than the wages in Eastern European countries. China has the world's highest number of billionaires, with nearly 878 as of October 2020, increasing at the rate of roughly five per week. China has a high level of economic inequality, which has increased in the past few decades. In 2018 China's Gini coefficient was 0.467, according to the World Bank.
China was a world leader in science and technology until the Ming dynasty. Ancient Chinese discoveries and inventions, such as papermaking, printing, the compass, and gunpowder (the Four Great Inventions), became widespread across East Asia, the Middle East and later Europe. Chinese mathematicians were the first to use negative numbers. By the 17th century, the Western hemisphere surpassed China in scientific and technological advancement. The causes of this early modern Great Divergence continue to be debated by scholars.
After repeated military defeats by the European colonial powers and Japan in the 19th century, Chinese reformers began promoting modern science and technology as part of the Self-Strengthening Movement. After the Communists came to power in 1949, efforts were made to organize science and technology based on the model of the Soviet Union, in which scientific research was part of central planning. After Mao's death in 1976, science and technology was promoted as one of the Four Modernizations, and the Soviet-inspired academic system was gradually reformed.
Since the end of the Cultural Revolution, China has made significant investments in scientific research and is quickly catching up with the US in R&amp;D spending. In 2017, China spent $279 billion on scientific research and development. According to the OECD, China spent 2.11% of its GDP on research and development (R&amp;D) in 2016. Science and technology are seen as vital for achieving China's economic and political goals, and are held as a source of national pride to a degree sometimes described as "techno-nationalism". According to the World Intellectual Property Indicators, China received 1.54 million patent applications in 2018, representing nearly half of patent applications worldwide, more than double the US. In 2019, China was No. 1 in international patents application. China was ranked 12th, 3rd in Asia &amp; Oceania region and 2nd for countries with a population of over 100 million in the Global Innovation Index in 2021, it has increased its ranking considerably since 2013, where it was ranked 35th. China ranks first globally in the important indicators, including patents, utility models, trademarks, industrial designs, and creative goods exports and it also has 2 (Shenzhen-Hong Kong-Guangzhou and Beijing in the 2nd and 3rd spots respectively) of the global top 5 science and technology clusters, which is more than any other country. Chinese tech companies Huawei and ZTE were the top 2 filers of international patents in 2017. Chinese-born academicians have won the Nobel Prize in Physics four times, the Nobel Prize in Chemistry, Nobel Prize in Physiology or Medicine and Fields Medal once respectively, though most of them conducted their prize-winning research in western nations.
China is developing its education system with an emphasis on science, technology, engineering and mathematics (STEM); in 2009, China graduated over 10,000 PhD engineers, and as many as 500,000 BSc graduates, more than any other country. China also became the world's largest publisher of scientific papers since 2016. Chinese technology companies such as Huawei, Xiaomi and Lenovo have become world leaders in telecommunications, consumer electronics and personal computing, and Chinese supercomputers are consistently ranked among the world's most powerful. China has been the world's largest market for industrial robots since 2013 and will account for 45% of newly installed robots from 2019 to 2021.
The Chinese space program is one of the world's most active. In 1970, China launched its first satellite, Dong Fang Hong I, becoming the fifth country to do so independently. In 2003, China became the third country to independently send humans into space, with Yang Liwei's spaceflight aboard Shenzhou 5; as of 2022, fourteen Chinese nationals have journeyed into space, including two women. In 2011, China launched its first space station testbed, Tiangong-1. In 2013, China successfully landed the Chang'e 3 lander and Yutu rover onto the lunar surface. In 2016, the first quantum science satellite was launched in partnership with Austria dedicated to testing the fundamentals of quantum communication in space. In 2019, China became the first country to land a probe—Chang'e 4—on the Far side of the Moon. In 2020, the first experimental 6G test satellite was launched and Chang'e 5 successfully returned moon samples to the Earth, making China the third country to do so independently after the United States and the Soviet Union. In 2021, China became the second nation in history to independently land a rover (Zhurong) on Mars, joining the United States.
Currently, China is building its own space station, Tiangong, in low Earth orbit. The construction of the space station is targeted to be completed by the end of 2022 with the launch of two new modules.
After a decades-long infrastructural boom, China has produced numerous world-leading infrastructural projects: China has the world's largest bullet train network, the most supertall skyscrapers in the world, the world's largest power plant (the Three Gorges Dam), the largest energy generation capacity in the world, a global satellite navigation system with the largest number of satellites in the world, and has initiated the Belt and Road Initiative, a large global infrastructure building initiative with funding on the order of $50–100 billion per year. The Belt and Road Initiative could be one of the largest development plans in modern history.
China is the largest telecom market in the world and currently has the largest number of active cellphones of any country in the world, with over 1.5 billion subscribers, as of 2018. It also has the world's largest number of internet and broadband users, with over 800 million Internet users as of 2018—equivalent to around 60% of its population—and almost all of them being mobile as well. By 2018, China had more than 1 billion 4G users, accounting for 40% of world's total. China is making rapid advances in 5G—by late 2018, China had started large-scale and commercial 5G trials.
China Mobile, China Unicom and China Telecom, are the three large providers of mobile and internet in China. China Telecom alone served more than 145 million broadband subscribers and 300 million mobile users; China Unicom had about 300 million subscribers; and China Mobile, the biggest of them all, had 925 million users, as of 2018. Combined, the three operators had over 3.4 million 4G base-stations in China. Several Chinese telecommunications companies, most notably Huawei and ZTE, have been accused of spying for the Chinese military.
China has developed its own satellite navigation system, dubbed Beidou, which began offering commercial navigation services across Asia in 2012 as well as global services by the end of 2018. Upon the completion of the 35th Beidou satellite, which was launched into orbit on 23 June 2020, Beidou followed GPS and GLONASS as the third completed global navigation satellite in the world.
Since the late 1990s, China's national road network has been significantly expanded through the creation of a network of national highways and expressways. In 2018, China's highways had reached a total length of 142,500 km (88,500 mi), making it the longest highway system in the world. China has the world's largest market for automobiles, having surpassed the United States in both auto sales and production. A side-effect of the rapid growth of China's road network has been a significant rise in traffic accidents, though the number of fatalities in traffic accidents fell by 20% from 2007 to 2017. In urban areas, bicycles remain a common mode of transport, despite the increasing prevalence of automobiles – as of 2012, there are approximately 470 million bicycles in China.
China's railways, which are state-owned, are among the busiest in the world, handling a quarter of the world's rail traffic volume on only 6 percent of the world's tracks in 2006. As of 2017, the country had 127,000 km (78,914 mi) of railways, the second longest network in the world. The railways strain to meet enormous demand particularly during the Chinese New Year holiday, when the world's largest annual human migration takes place.
China's high-speed rail (HSR) system started construction in the early 2000s. By the end of 2020, high speed rail in China had reached 37,900 kilometers (23,550 miles) of dedicated lines alone, making it the longest HSR network in the world. Services on the Beijing–Shanghai, Beijing–Tianjin, and Chengdu–Chongqing Lines reach up to 350 km/h (217 mph), making them the fastest conventional high speed railway services in the world. With an annual ridership of over 2.29 billion passengers in 2019 it is the world's busiest. The network includes the Beijing–Guangzhou–Shenzhen High-Speed Railway, the single longest HSR line in the world, and the Beijing–Shanghai High-Speed Railway, which has three of longest railroad bridges in the world. The Shanghai Maglev Train, which reaches 431 km/h (268 mph), is the fastest commercial train service in the world.
Since 2000, the growth of rapid transit systems in Chinese cities has accelerated. As of January 2021, 44 Chinese cities have urban mass transit systems in operation and 39 more have metro systems approved. As of 2020, China boasts the five longest metro systems in the world with the networks in Shanghai, Beijing, Guangzhou, Chengdu and Shenzhen being the largest.
There were approximately 229 airports in 2017, with around 240 planned by 2020. China has over 2,000 river and seaports, about 130 of which are open to foreign shipping. In 2017, the Ports of Shanghai, Hong Kong, Shenzhen, Ningbo-Zhoushan, Guangzhou, Qingdao and Tianjin ranked in the Top 10 in the world in container traffic and cargo tonnage.
On 13 April 2022, a new freight railway route for freight trains from the city of Xi'an was launched, it will pass through Kazakhstan, Azerbaijan, Romania, Hungary, Slovakia, the Czech Republic and reach the German city of Mannheim. The length of the route is 11.3 thousand km.
Water supply and sanitation infrastructure in China is facing challenges such as rapid urbanization, as well as water scarcity, contamination, and pollution. According to data presented by the Joint Monitoring Program for Water Supply and Sanitation of WHO and UNICEF in 2015, about 36% of the rural population in China still did not have access to improved sanitation. The ongoing South–North Water Transfer Project intends to abate water shortage in the north.
The national census of 2020 recorded the population of the People's Republic of China as approximately 1,411,778,724. About 17.95% of the population were 14 years old or younger, 63.35% were between 15 and 59 years old, and 18.7% were over 60 years old. The population growth rate for 2013 is estimated to be 0.46%. China used to make up much of the world's poor; now it makes up much of the world's middle class. Although a middle-income country by Western standards, China's rapid growth has pulled hundreds of millions—800 million, to be more precise—of its people out of poverty since 1978. By 2013, less than 2% of the Chinese population lived below the international poverty line of US$1.9 per day, down from 88% in 1981. From 2009 to 2018, the unemployment rate in China has averaged about 4%.
Given concerns about population growth, China implemented a two-child limit during the 1970s, and, in 1979, began to advocate for an even stricter limit of one child per family. Beginning in the mid-1980s, however, given the unpopularity of the strict limits, China began to allow some major exemptions, particularly in rural areas, resulting in what was actually a "1.5"-child policy from the mid-1980s to 2015 (ethnic minorities were also exempt from one child limits). The next major loosening of the policy was enacted in December 2013, allowing families to have two children if one parent is an only child. In 2016, the one-child policy was replaced in favor of a two-child policy. According to data from the 2020 census, China's total fertility rate is 1.3, but some experts believe that after adjusting for the transient effects of the relaxation of restrictions, the country's actual total fertility rate is as low as 1.1.
According to one group of scholars, one-child limits had little effect on population growth or the size of the total population. However, these scholars have been challenged. Their own counterfactual model of fertility decline without such restrictions implies that China averted more than 500 million births between 1970 and 2015, a number which may reach one billion by 2060 given all the lost descendants of births averted during the era of fertility restrictions, with one-child restrictions accounting for the great bulk of that reduction.
The policy, along with traditional preference for boys, may have contributed to an imbalance in the sex ratio at birth. According to the 2010 census, the sex ratio at birth was 118.06 boys for every 100 girls, which is beyond the normal range of around 105 boys for every 100 girls. The 2010 census found that males accounted for 51.27 percent of the total population. However, China's sex ratio is more balanced than it was in 1953, when males accounted for 51.82 percent of the total population.
China legally recognizes 56 distinct ethnic groups, who altogether comprise the Zhonghua Minzu. The largest of these nationalities are the ethnic Chinese or "Han", who constitute more than 90% of the total
population. The Han Chinese – the world's largest single ethnic group – outnumber other ethnic groups in every provincial-level division except Tibet and Xinjiang. Ethnic minorities account for less than 10% of the population of China, according to the 2010 census. Compared with the 2000 population census, the Han population increased by 66,537,177 persons, or 5.74%, while the population of the 55 national minorities combined increased by 7,362,627 persons, or 6.92%. The 2010 census recorded a total of 593,832 foreign nationals living in China. The largest such groups were from South Korea (120,750), the
United States (71,493) and Japan (66,159).
There are as many as 292 living languages in China. The languages most commonly spoken belong to the Sinitic branch of the Sino-Tibetan language family, which contains Mandarin (spoken by 70% of the population), and other varieties of Chinese language: Yue (including Cantonese and Taishanese), Wu (including Shanghainese and Suzhounese), Min (including Fuzhounese, Hokkien and Teochew), Xiang, Gan and Hakka. Languages of the Tibeto-Burman branch, including Tibetan, Qiang, Naxi and Yi, are spoken across the Tibetan and Yunnan–Guizhou Plateau. Other ethnic minority languages in southwest China include Zhuang, Thai, Dong and Sui of the Tai-Kadai family, Miao and Yao of the Hmong–Mien family, and Wa of the Austroasiatic family. Across northeastern and northwestern China, local ethnic groups speak Altaic languages including Manchu, Mongolian and several Turkic languages: Uyghur, Kazakh, Kyrgyz, Salar and Western Yugur. Korean is spoken natively along the border with North Korea. Sarikoli, the language of Tajiks in western Xinjiang, is an Indo-European language. Taiwanese aborigines, including a small population on the mainland, speak Austronesian languages.
Standard Mandarin, a variety of Mandarin based on the Beijing dialect, is the official national language of China and is used as a lingua franca in the country between people of different linguistic backgrounds. Mongolian, Uyghur, Tibetan, Zhuang and various other languages are also regionally recognized throughout the country.
Chinese characters have been used as the written script for the Sinitic languages for thousands of years. They allow speakers of mutually unintelligible Chinese varieties to communicate with each other through writing. In 1956, the government introduced simplified characters, which have supplanted the older traditional characters in mainland China. Chinese characters are romanized using the Pinyin system. Tibetan uses an alphabet based on an Indic script. Uyghur is most commonly written in Persian alphabet-based Uyghur Arabic alphabet. The Mongolian script used in China and the Manchu script are both derived from the Old Uyghur alphabet. Zhuang uses both an official Latin alphabet script and a traditional Chinese character script.
China has urbanized significantly in recent decades. The percent of the country's population living in urban areas increased from 20% in 1980 to over 60% in 2019. It is estimated that China's urban population will reach one billion by 2030, potentially equivalent to one-eighth of the world population.
China has over 160 cities with a population of over one million, including the 17 megacities as of 2021 (cities with a population of over 10 million) of Chongqing, Shanghai, Beijing, Chengdu, Guangzhou, Shenzhen, Tianjin, Xi'an, Suzhou, Zhengzhou, Wuhan, Hangzhou, Linyi, Shijiazhuang, Dongguan, Qingdao and Changsha. Among them, the total permanent population of Chongqing, Shanghai, Beijing and Chengdu is above 20 million. Shanghai is China's most populous urban area while Chongqing is its largest city proper, the only city in China with the largest permanent population of over 30 million. By 2025, it is estimated that the country will be home to 221 cities with over a million inhabitants. The figures in the table below are from the 2017 census, and are only estimates of the urban populations within administrative city limits; a different ranking exists when considering the total municipal populations (which includes suburban and rural populations). The large "floating populations" of migrant workers make conducting censuses in urban areas difficult; the figures below include only long-term residents.
Since 1986, compulsory education in China comprises primary and junior secondary school, which together last for nine years. In 2019, about 89.5 percent of students continued their education at a three-year senior secondary school. The Gaokao, China's national university entrance exam, is a prerequisite for entrance into most higher education institutions. In 2010, 27 percent of secondary school graduates are enrolled in higher education. This number increased significantly over the last years, reaching a tertiary school enrolment of 58.42 percent in 2020. Vocational education is available to students at the secondary and tertiary level. More than 10 million Chinese students graduated from vocational colleges nationwide every year.
China has the largest education system in the world, with about 282 million students and 17.32 million full-time teachers in over 530,000 schools. In February 2006, the government pledged to provide completely free nine-year education, including textbooks and fees. Annual education investment went from less than US$50 billion in 2003 to more than US$817 billion in 2020. However, there remains an inequality in education spending. In 2010, the annual education expenditure per secondary school student in Beijing totalled ¥20,023, while in Guizhou, one of the poorest provinces in China, only totalled ¥3,204. Free compulsory education in China consists of primary school and junior secondary school between the ages of 6 and 15. In 2020, the graduation enrollment ratio at compulsory education level reached 95.2 percent, exceeding average levels recorded in high-income countries, and around 91.2% of Chinese have received secondary education.
China's literacy rate has grown dramatically, from only 20% in 1949 and 65.5% in 1979. to 96% of the population over age 15 in 2018. In the same year, China (Beijing, Shanghai, Jiangsu, and Zhejiang) was ranked the highest in the world in the Programme for International Student Assessment ranking for all three categories of Mathematics, Science and Reading. China ranks first in the all-time medal count at the International Mathematical Olympiad with 168 goal medals since its first participation in 1985. China also ranks first in the all-time medal count at the International Physics Olympiad, the International Chemistry Olympiad, and the International Olympiad in Informatics.
China had over 3,000 universities, with over 40 million students enrolled in mainland China. As of 2021, China had the world's second-highest number of top universities (the highest in Asia &amp; Oceania region). Currently, China trails only the United States in terms of representation on lists of top 200 universities according to the Academic Ranking of World Universities (ARWU). China is home to the two best universities (Tsinghua University and Peking University) in the whole Asia-Oceania region and emerging countries according to the Times Higher Education World University Rankings. As of 2022, two universities in Mainland China rank in the world's top 15, with Peking University (12th) and Tsinghua University (14th) and three other universities ranking in the world's top 50, namely Fudan, Zhejiang, and Shanghai Jiao Tong according to the QS World University Ranking. These universities are members of the C9 League, an alliance of elite Chinese universities offering comprehensive and leading education.
The National Health and Family Planning Commission, together with its counterparts in the local commissions, oversees the health needs of the Chinese population. An emphasis on public health and preventive medicine has characterized Chinese health policy since the early 1950s. At that time, the Communist Party started the Patriotic Health Campaign, which was aimed at improving sanitation and hygiene, as well as treating and preventing several diseases. Diseases such as cholera, typhoid and scarlet fever, which were previously rife in China, were nearly eradicated by the campaign.
After Deng Xiaoping began instituting economic reforms in 1978, the health of the Chinese public improved rapidly because of better nutrition, although many of the free public health services provided in the countryside disappeared along with the People's Communes. Healthcare in China became mostly privatized, and experienced a significant rise in quality. In 2009, the government began a 3-year large-scale healthcare provision initiative worth US$124 billion. By 2011, the campaign resulted in 95% of China's population having basic health insurance coverage. In 2011, China was estimated to be the world's third-largest supplier of pharmaceuticals, but its population has suffered from the development and distribution of counterfeit medications.
As of 2017, the average life expectancy at birth in China is 76 years, and the infant mortality rate is 7 per thousand. Both have improved significantly since the 1950s. Rates of stunting, a condition caused by malnutrition, have declined from 33.1% in 1990 to 9.9% in 2010. Despite significant improvements in health and the construction of advanced medical facilities, China has several emerging public health problems, such as respiratory illnesses caused by widespread air pollution, hundreds of millions of cigarette smokers, and an increase in obesity among urban youths. China's large population and densely populated cities have led to serious disease outbreaks in recent years, such as the 2003 outbreak of SARS, although this has since been largely contained. In 2010, air pollution caused 1.2 million premature deaths in China.
The COVID-19 pandemic was first identified in Wuhan in December 2019. Further studies are being carried out around the world on a possible origin for the virus. The Chinese government has been criticized for its handling of the epidemic and accused of concealing the extent of the outbreak before it became an international pandemic.
The government of the People's Republic of China officially espouses state atheism, and has conducted antireligious campaigns to this end. Religious affairs and issues in the country are overseen by the State Administration for Religious Affairs. Freedom of religion is guaranteed by China's constitution, although religious organizations that lack official approval can be subject to state persecution.
Over the millennia, Chinese civilization has been influenced by various religious movements. The "three teachings", including Confucianism, Taoism, and Buddhism (Chinese Buddhism), historically have a significant role in shaping Chinese culture, enriching a theological and spiritual framework which harks back to the early Shang and Zhou dynasty. Chinese popular or folk religion, which is framed by the three teachings and other traditions, consists in allegiance to the shen (神), a character that signifies the "energies of generation", who can be deities of the environment or ancestral principles of human groups, concepts of civility, culture heroes, many of whom feature in Chinese mythology and history. Among the most popular cults are those of Mazu (goddess of the seas), Huangdi (one of the two divine patriarchs of the Chinese race), Guandi (god of war and business), Caishen (god of prosperity and richness), Pangu and many others. China is home to many of the world's tallest religious statues, including the tallest of all, the Spring Temple Buddha in Henan.
Clear data on religious affiliation in China is difficult to gather due to varying definitions of "religion" and the unorganized, diffusive nature of Chinese religious traditions. Scholars note that in China there is no clear boundary between three teachings religions and local folk religious practice. A 2015 poll conducted by Gallup International found that 61% of Chinese people self-identified as "convinced atheist", though it is worthwhile to note that Chinese religions or some of their strands are definable as non-theistic and humanistic religions, since they do not believe that divine creativity is completely transcendent, but it is inherent in the world and in particular in the human being. According to a 2014 study, approximately 74% are either non-religious or practice Chinese folk belief, 16% are Buddhists, 2% are Christians, 1% are Muslims, and 8% adhere to other religions including Taoists and folk salvationism. In addition to Han people's local religious practices, there are also various ethnic minority groups in China who maintain their traditional autochthone religions. The various folk religions today comprise 2–3% of the population, while Confucianism as a religious self-identification is common within the intellectual class. Significant faiths specifically connected to certain ethnic groups include Tibetan Buddhism and the Islamic religion of the Hui, Uyghur, Kazakh, Kyrgyz and other peoples in Northwest China. The 2010 population census reported the total number of Muslims in the country as 23.14 million.
A 2021 poll from Ipsos and the Policy Institute at King's College London found that 35% of Chinese people said there was tension between different religious groups, which was the second lowest percentage of the 28 countries surveyed.
Since ancient times, Chinese culture has been heavily influenced by Confucianism. For much of the country's dynastic era, opportunities for social advancement could be provided by high performance in the prestigious imperial examinations, which have their origins in the Han dynasty. The literary emphasis of the exams affected the general perception of cultural refinement in China, such as the belief that calligraphy, poetry and painting were higher forms of art than dancing or drama. Chinese culture has long emphasized a sense of deep history and a largely inward-looking national perspective. Examinations and a culture of merit remain greatly valued in China today.
The first leaders of the People's Republic of China were born into the traditional imperial order but were influenced by the May Fourth Movement and reformist ideals. They sought to change some traditional aspects of Chinese culture, such as rural land tenure, sexism, and the Confucian system of education, while preserving others, such as the family structure and culture of obedience to the state. Some observers see the period following the establishment of the PRC in 1949 as a continuation of traditional Chinese dynastic history, while others claim that the Communist Party's rule has damaged the foundations of Chinese culture, especially through political movements such as the Cultural Revolution of the 1960s, where many aspects of traditional culture were destroyed, having been denounced as "regressive and harmful" or "vestiges of feudalism". Many important aspects of traditional Chinese morals and culture, such as Confucianism, art, literature, and performing arts like Peking opera, were altered to conform to government policies and propaganda at the time. Access to foreign media remains heavily restricted.
Today, the Chinese government has accepted numerous elements of traditional Chinese culture as being integral to Chinese society. With the rise of Chinese nationalism and the end of the Cultural Revolution, various forms of traditional Chinese art, literature, music, film, fashion and architecture have seen a vigorous revival, and folk and variety art in particular have sparked interest nationally and even worldwide. A poll in October 2020 of respondents in Spain, Slovakia, Latvia, Serbia, and Russia found that majorities in those countries considered China to be "culturally attractive".
China received 55.7 million inbound international visitors in 2010, and in 2012 was the third-most-visited country in the world. It also experiences an enormous volume of domestic tourism; an estimated 740 million Chinese holidaymakers traveled within the country in October 2012. China hosts the world's second-largest number of World Heritage Sites (56) after Italy, and is one of the most popular tourist destinations in the world (first in the Asia-Pacific). It is forecast by Euromonitor International that China will become the world's most popular destination for tourists by 2030.
Chinese literature is based on the literature of the Zhou dynasty. Concepts covered within the Chinese classic texts present a wide range of thoughts and subjects including calendar, military, astrology, herbology, geography and many others. Some of the most important early texts include the I Ching and the Shujing within the Four Books and Five Classics which served as the Confucian authoritative books for the state-sponsored curriculum in dynastic era. Inherited from the Classic of Poetry, classical Chinese poetry developed to its floruit during the Tang dynasty. Li Bai and Du Fu opened the forking ways for the poetic circles through romanticism and realism respectively. Chinese historiography began with the Shiji, the overall scope of the historiographical tradition in China is termed the Twenty-Four Histories, which set a vast stage for Chinese fictions along with Chinese mythology and folklore. Pushed by a burgeoning citizen class in the Ming dynasty, Chinese classical fiction rose to a boom of the historical, town and gods and demons fictions as represented by the Four Great Classical Novels which include Water Margin, Romance of the Three Kingdoms, Journey to the West and Dream of the Red Chamber. Along with the wuxia fictions of Jin Yong and Liang Yusheng, it remains an enduring source of popular culture in the East Asian cultural sphere.
In the wake of the New Culture Movement after the end of the Qing dynasty, Chinese literature embarked on a new era with written vernacular Chinese for ordinary citizens. Hu Shih and Lu Xun were pioneers in modern literature. Various literary genres, such as misty poetry, scar literature, young adult fiction and the xungen literature, which is influenced by magic realism, emerged following the Cultural Revolution. Mo Yan, a xungen literature author, was awarded the Nobel Prize in Literature in 2012.
Chinese cuisine is highly diverse, drawing on several millennia of culinary history and geographical variety, in which the most influential are known as the "Eight Major Cuisines", including Sichuan, Cantonese, Jiangsu, Shandong, Fujian, Hunan, Anhui, and Zhejiang cuisines. All of them are featured by the precise skills of shaping, heating, and flavoring. Chinese cuisine is also known for its width of cooking methods and ingredients, as well as food therapy that is emphasized by traditional Chinese medicine. Generally, China's staple food is rice in the south, wheat-based breads and noodles in the north. The diet of the common people in pre-modern times was largely grain and simple vegetables, with meat reserved for special occasions. The bean products, such as tofu and soy milk, remain as a popular source of protein. Pork is now the most popular meat in China, accounting for about three-fourths of the country's total meat consumption. While pork dominates the meat market, there is also the vegetarian Buddhist cuisine and the pork-free Chinese Islamic cuisine. Southern cuisine, due to the area's proximity to the ocean and milder climate, has a wide variety of seafood and vegetables; it differs in many respects from the wheat-based diets across dry northern China. Numerous offshoots of Chinese food, such as Hong Kong cuisine and American Chinese food, have emerged in the nations that play host to the Chinese diaspora.
Chinese music covers a highly diverse range of music from traditional music to modern music. Chinese music dates back before the pre-imperial times. Traditional Chinese musical instruments were traditionally grouped into eight categories known as bayin (八音). Traditional Chinese opera is a form of musical theatre in China originating thousands of years and has regional style forms such as Beijing opera and Cantonese opera. Chinese pop (C-Pop) includes mandopop and cantopop. Chinese rap, Chinese hip hop and Hong Kong hip hop have become popular in contemporary times.
Cinema was first introduced to China in 1896 and the first Chinese film, Dingjun Mountain, was released in 1905. China has the largest number of movie screens in the world since 2016, China became the largest cinema market in the world in 2020. The top 3 highest-grossing films in China currently are Wolf Warrior 2 (2017), Ne Zha (2019), and The Wandering Earth (2019).
Hanfu is the historical clothing of the Han people in China. The qipao or cheongsam is a popular Chinese female dress. The hanfu movement has been popular in contemporary times and seeks to revitalize Hanfu clothing.
China has one of the oldest sporting cultures in the world. There is evidence that archery (shèjiàn) was practiced during the Western Zhou dynasty. Swordplay (jiànshù) and cuju, a sport loosely related to association football date back to China's early dynasties as well.
Physical fitness is widely emphasized in Chinese culture, with morning exercises such as qigong and t'ai chi ch'uan widely practiced, and commercial gyms and private fitness clubs are gaining popularity across the country. Basketball is currently the most popular spectator sport in China. The Chinese Basketball Association and the American National Basketball Association have a huge following among the people, with native or ethnic Chinese players such as Yao Ming and Yi Jianlian held in high esteem. China's professional football league, now known as Chinese Super League, was established in 1994, it is the largest football market in Asia. Other popular sports in the country include martial arts, table tennis, badminton, swimming and snooker. Board games such as go (known as wéiqí in Chinese), xiangqi, mahjong, and more recently chess, are also played at a professional level. In addition, China is home to a huge number of cyclists, with an estimated 470 million bicycles as of 2012. Many more traditional sports, such as dragon boat racing, Mongolian-style wrestling and horse racing are also popular.
China has participated in the Olympic Games since 1932, although it has only participated as the PRC since 1952. China hosted the 2008 Summer Olympics in Beijing, where its athletes received 48 gold medals – the highest number of gold medals of any participating nation that year. China also won the most medals of any nation at the 2012 Summer Paralympics, with 231 overall, including 95 gold medals. In 2011, Shenzhen in Guangdong, China hosted the 2011 Summer Universiade. China hosted the 2013 East Asian Games in Tianjin and the 2014 Summer Youth Olympics in Nanjing; the first country to host both regular and Youth Olympics. Beijing and its nearby city Zhangjiakou of Hebei province collaboratively hosted the 2022 Olympic Winter Games, making Beijing the first dual olympic city in the world by holding both the Summer Olympics and the Winter Olympics.
Notably, the Encyclopædia Britannica specifies the United States' area (excluding coastal and territorial waters) as 9,525,067 km2, which is less than either source's figure given for China's area. Therefore, while it can be determined that China has a larger area excluding coastal and territorial waters, it is unclear which country has a larger area including coastal and territorial waters.
The United Nations Statistics Division's figure for the United States is 9,833,517 km2 (3,796,742 sq mi) and China is 9,596,961 km2 (3,705,407 sq mi). These closely match the CIA World Factbook figures and similarly include coastal and territorial waters for the United States, but exclude coastal and territorial waters for China.
Further explanation of disputed ranking: The dispute about which is the world's third-largest country arose from the inclusion of coastal and territorial waters for the United States. This discrepancy was deduced from comparing the CIA World Factbook and its previous iterations against the information for United States in Encyclopædia Britannica, particularly its footnote section. In sum, according to older versions of the CIA World Factbook (from 1982 to 1996), the U.S. was listed as the world's fourth-largest country (after Russia, Canada, and China) with a total area of 9,372,610 km2 (3,618,780 sq mi). However, in the 1997 edition, the U.S. added coastal waters to its total area (increasing it to 9,629,091 km2 (3,717,813 sq mi)). And then again in 2007, U.S. added territorial water to its total area (increasing it to 9,833,517 km2 (3,796,742 sq mi)). During this time, China's total area remained unchanged. In other words, no coastal or territorial water area was added to China's total area figure. The United States has a coastal water area of 109,362 km2 (42,225 sq mi), and a territorial water area of 195,213 km2 (75,372 sq mi), for a total of 304,575 km2 (117,597 sq mi) of additional water space. This is larger than entire countries like Italy, New Zealand and the United Kingdom. Adding this figure to the U.S. will boost it over China in ranking since China's coastal and territorial water figures are currently unknown (no official publication) and thus cannot be added into China's total area figure.
 World portal
Coordinates: 35°N 103°E﻿ / ﻿35°N 103°E﻿ / 35; 103
The Space Age is a period encompassing the activities related to the Space Race, space exploration, space technology, and the cultural developments influenced by these events, beginning with the launch of Sputnik 1 during 1957, and continuing to the present.
The Space Age began with the development of several technologies that converged with the October 4, 1957 launch of Sputnik 1 by the Soviet Union. This was the world's first artificial satellite, orbiting the Earth in 96.17 minutes and weighing 83 kg (183 lb). The launch of Sputnik 1 ushered in a new era of political, scientific and technological achievements that became known as the Space Age, by the rapid development of new technology and a race for achievement, mostly between the United States and the Soviet Union. Rapid advances were made in rocketry, materials science, and other areas. Much of the technology originally developed for space applications has been spun off and found additional uses. One such example is memory foam.
Prior to human spaceflights being attempted, various animals were flown to space to identify potential detrimental effects of microgravity and radiation exposure at high altitudes.
The Space Age reached its peak with the Apollo program that captured the imagination of much of the world's population. The landing of Apollo 11 was watched by over 500 million people around the world and is widely recognized as one of the defining moments of the 20th century. Since then, public attention has largely moved to other areas.
In the United States, the Space Shuttle Challenger disaster in 1986 marked a significant decline in crewed Shuttle launches. Following the disaster, NASA grounded all Shuttles for safety concerns until 1988. During the 1990s funding for space-related programs fell sharply as the remaining structures of the now-dissolved Soviet Union disintegrated and NASA no longer had any direct competition.
Since then, participation in space launches has increasingly widened to include more governments and commercial interests. Since the 1990s, the public perception of space exploration and space-related technologies has been that such endeavors are increasingly commonplace.
NASA permanently grounded all U.S. Space Shuttles in 2011. NASA has since relied on Russia and SpaceX to take American astronauts to and from the International Space Station.
In the early 21st century, the Ansari X Prize competition was set up to help jump-start private spaceflight. The winner, Space Ship One in 2004, became the first spaceship not funded by a government agency.
Several countries now have space programs; from related technology ventures to full-fledged space programs with launch facilities. There are many scientific and commercial satellites in use today, with thousands of satellites in orbit, and several countries have plans to send humans into space. Some of the countries joining this new race are France, India, China, Israel and the United Kingdom, all of which have employed surveillance satellites. There are several other countries with less extensive space programs, including Brazil, Germany, Ukraine, and Spain.
As for the United States space program, NASA is currently constructing a deep-space crew capsule named the Orion. NASA's goal with this new space capsule is to carry humans to Mars. The Orion spacecraft is due to be completed in the early 2020s. NASA is hoping that this mission will “usher in a new era of space exploration.”
Another major factor affecting the current Space Age is the privatization of space flight. A significant private spaceflight company is SpaceX which became the proprietor of one of world's most capable operational launch vehicle when they launched their current largest rocket, the Falcon Heavy in 2018. Elon Musk, the founder and CEO of SpaceX, has put forward the goal of establishing a colony of one million people on Mars and the company is developing its Starship launch vehicle to facilitate this. Since the Demo-2 mission for NASA in 2020 in which SpaceX launched astronauts for the first time to the International Space Station, the company has maintained an orbital human spaceflight capability. Blue Origin, a private company founded by Amazon.com founder Jeff Bezos, is developing rockets for use in space tourism, commercial satellite launches, and eventual missions to the Moon and beyond. Richard Branson's company Virgin Galactic is concentrating on launch vehicles for space tourism. A spinoff company, Virgin Orbit, air-launches small satellites with their LauncherOne rocket. Another small-satellite launcher, Rocket Lab, has developed the Electron rocket and the Photon satellite bus for sending spacecraft further into the Solar System.
The Space Age might also be considered to have begun much earlier than October 4, 1957, because in June 1944, a German V-2 rocket became the first manmade object to enter space, albeit only briefly. Some even consider March 1926 as the beginning of the Space Age, when American rocket pioneer Robert H. Goddard launched the world's first liquid fuel rocket, though his rocket did not reach outer space. Also already in the 1920s the world's first large-scale experimental rocket program, Opel-RAK, was initiated under the leadership of Fritz von Opel and Max Valier. Speed records for ground and rail vehicles were achieved in 1928 and von Opel piloted the world's first public flight of a rocket plane, Opel RAK.1. The Great Depression put an end to the Opel-RAK program but it nevertheless had a strong and long-lasting impact on later spaceflight pioneers, in particular on Wernher von Braun who would eventually head the Nazi era V2 program.
Since the V-2 rocket flight was undertaken in secrecy, it was not public knowledge for many years afterward. Further, the German launches, as well as the subsequent sounding rocket tests performed in both the United States and the Soviet Union during the late 1940s and early 1950s, were not considered significant enough to start a new age because they did not reach orbit. Having a rocket powerful enough to reach orbit meant that a nation could place a payload anywhere on the planet, or to use another term, possessed an intercontinental ballistic missile. The fact that after such a development nowhere on Earth was safe from a nuclear warhead is why the orbital standard is commonly used to define when the space age began.
The Space Age is considered to have influenced:
The Space Age also inspired musical genres: 

Apollo 11 (July 16–24, 1969) was the American spaceflight that first landed humans on the Moon. Commander Neil Armstrong and lunar module pilot Buzz Aldrin landed the Apollo Lunar Module Eagle on July 20, 1969, at 20:17 UTC, and Armstrong became the first person to step onto the Moon's surface six hours and 39 minutes later, on July 21 at 02:56 UTC. Aldrin joined him 19 minutes later, and they spent about two and a quarter hours together exploring the site they had named Tranquility Base upon landing. Armstrong and Aldrin collected 47.5 pounds (21.5 kg) of lunar material to bring back to Earth as pilot Michael Collins flew the Command Module Columbia in lunar orbit, and were on the Moon's surface for 21 hours, 36 minutes before lifting off to rejoin Columbia.
Apollo 11 was launched by a Saturn V rocket from Kennedy Space Center on Merritt Island, Florida, on July 16 at 13:32 UTC, and it was the fifth crewed mission of NASA's Apollo program. The Apollo spacecraft had three parts: a command module (CM) with a cabin for the three astronauts, the only part that returned to Earth; a service module (SM), which supported the command module with propulsion, electrical power, oxygen, and water; and a lunar module (LM) that had two stages—a descent stage for landing on the Moon and an ascent stage to place the astronauts back into lunar orbit.
After being sent to the Moon by the Saturn V's third stage, the astronauts separated the spacecraft from it and traveled for three days until they entered lunar orbit. Armstrong and Aldrin then moved into Eagle and landed in the Sea of Tranquility on July 20. The astronauts used Eagle's ascent stage to lift off from the lunar surface and rejoin Collins in the command module. They jettisoned Eagle before they performed the maneuvers that propelled Columbia out of the last of its 30 lunar orbits onto a trajectory back to Earth. They returned to Earth and splashed down in the Pacific Ocean on July 24 after more than eight days in space.
Armstrong's first step onto the lunar surface was broadcast on live TV to a worldwide audience. He described the event as "one small step for  man, one giant leap for mankind." Apollo 11 effectively proved US victory in the Space Race to demonstrate spaceflight superiority, by fulfilling a national goal proposed in 1961 by President John F. Kennedy, "before this decade is out, of landing a man on the Moon and returning him safely to the Earth."
In the late 1950s and early 1960s, the United States was engaged in the Cold War, a geopolitical rivalry with the Soviet Union. On October 4, 1957, the Soviet Union launched Sputnik 1, the first artificial satellite. This surprise success fired fears and imaginations around the world. It demonstrated that the Soviet Union had the capability to deliver nuclear weapons over intercontinental distances, and challenged American claims of military, economic and technological superiority. This precipitated the Sputnik crisis, and triggered the Space Race to prove which superpower would achieve superior spaceflight capability. President Dwight D. Eisenhower responded to the Sputnik challenge by creating the National Aeronautics and Space Administration (NASA), and initiating Project Mercury, which aimed to launch a man into Earth orbit. But on April 12, 1961, Soviet cosmonaut Yuri Gagarin became the first person in space, and the first to orbit the Earth. Nearly a month later, on May 5, 1961, Alan Shepard became the first American in space, completing a 15-minute suborbital journey. After being recovered from the Atlantic Ocean, he received a congratulatory telephone call from Eisenhower's successor, John F. Kennedy.
Since the Soviet Union had higher lift capacity launch vehicles, Kennedy chose, from among options presented by NASA, a challenge beyond the capacity of the existing generation of rocketry, so that the US and Soviet Union would be starting from a position of equality. A crewed mission to the Moon would serve this purpose.

On May 25, 1961, Kennedy addressed the United States Congress on "Urgent National Needs" and declared:.mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 40px}.mw-parser-output .templatequote .templatequotecite{line-height:1.5em;text-align:left;padding-left:1.6em;margin-top:0}I believe that this nation should commit itself to achieving the goal, before this decade  is out, of landing a man on the Moon and returning him safely to the Earth. No single space project in this period will be more impressive to mankind, or more important for the long-range exploration of space; and none will be so difficult or expensive to accomplish. We propose to accelerate the development of the appropriate lunar space craft. We propose to develop alternate liquid and solid fuel boosters, much larger than any now being developed, until certain which is superior. We propose additional funds for other engine development and for unmanned explorations—explorations which are particularly important for one purpose which this nation will never overlook: the survival of the man who first makes this daring flight. But in a very real sense, it will not be one man going to the Moon—if we make this judgment affirmatively, it will be an entire nation. For all of us must work to put him there. On September 12, 1962, Kennedy delivered another speech before a crowd of about 40,000 people in the Rice University football stadium in Houston, Texas. A widely quoted refrain from the middle portion of the speech reads as follows:
There is no strife, no prejudice, no national conflict in outer space as yet. Its hazards are hostile to us all. Its conquest deserves the best of all mankind, and its opportunity for peaceful cooperation may never come again. But why, some say, the Moon? Why choose this as our goal? And they may well ask, why climb the highest mountain? Why, 35 years ago, fly the Atlantic? Why does Rice play Texas?
We choose to go to the Moon! We choose to go to the Moon ... We choose to go to the Moon in this decade and do the other things, not because they are easy, but because they are hard; because that goal will serve to organize and measure the best of our energies and skills, because that challenge is one that we are willing to accept, one we are unwilling to postpone, and one we intend to win, and the others, too.In spite of that, the proposed program faced the opposition of many Americans and was dubbed a "moondoggle" by Norbert Wiener, a mathematician at the Massachusetts Institute of Technology. The effort to land a man on the Moon already had a name: Project Apollo. When Kennedy met with Nikita Khrushchev, the Premier of the Soviet Union in June 1961, he proposed making the Moon landing a joint project, but Khrushchev did not take up the offer. Kennedy again proposed a joint expedition to the Moon in a speech to the United Nations General Assembly on September 20, 1963. The idea of a joint Moon mission was abandoned after Kennedy's death.
An early and crucial decision was choosing lunar orbit rendezvous over both direct ascent and Earth orbit rendezvous. A space rendezvous is an orbital maneuver in which two spacecraft navigate through space and meet up. In July 1962 NASA head James Webb announced that lunar orbit rendezvous would be used and that the Apollo spacecraft would have three major parts: a command module (CM) with a cabin for the three astronauts, and the only part that returned to Earth; a service module (SM), which supported the command module with propulsion, electrical power, oxygen, and water; and a lunar module (LM) that had two stages—a descent stage for landing on the Moon, and an ascent stage to place the astronauts back into lunar orbit. This design meant the spacecraft could be launched by a single Saturn V rocket that was then under development.
Technologies and techniques required for Apollo were developed by Project Gemini. The Apollo project was enabled by NASA's adoption of new advances in semiconductor electronic technology, including metal-oxide-semiconductor field-effect transistors (MOSFETs) in the Interplanetary Monitoring Platform (IMP) and silicon integrated circuit (IC) chips in the Apollo Guidance Computer (AGC).
Project Apollo was abruptly halted by the Apollo 1 fire on January 27, 1967, in which astronauts Gus Grissom, Ed White, and Roger B. Chaffee died, and the subsequent investigation. In October 1968, Apollo 7 evaluated the command module in Earth orbit, and in December Apollo 8 tested it in lunar orbit. In March 1969, Apollo 9 put the lunar module through its paces in Earth orbit, and in May Apollo 10 conducted a "dress rehearsal" in lunar orbit. By July 1969, all was in readiness for Apollo 11 to take the final step onto the Moon.
The Soviet Union appeared  to be winning the Space Race by beating the US to firsts, but its early lead was overtaken by the US Gemini program and Soviet failure to develop the N1 launcher, which would have been comparable to the Saturn V. The Soviets tried to beat the US to return lunar material to the Earth by means of uncrewed probes. On July 13, three days before Apollo 11's launch, the Soviet Union launched Luna 15, which reached lunar orbit before Apollo 11. During descent, a malfunction caused Luna 15 to crash in Mare Crisium about two hours before Armstrong and Aldrin took off from the Moon's surface to begin their voyage home. The Nuffield Radio Astronomy Laboratories radio telescope in England recorded transmissions from Luna 15 during its descent, and these were released in July 2009 for the 40th anniversary of Apollo 11.
The initial crew assignment of Commander Neil Armstrong, Command Module Pilot (CMP) Jim Lovell, and Lunar Module Pilot (LMP) Buzz Aldrin on the backup crew for Apollo 9 was officially announced on November 20, 1967. Lovell and Aldrin had previously flown together as the crew of Gemini 12. Due to design and manufacturing delays in the LM, Apollo 8 and Apollo 9 swapped prime and backup crews, and Armstrong's crew became the backup for Apollo 8. Based on the normal crew rotation scheme, Armstrong was then expected to command Apollo 11.
There would be one change. Michael Collins, the CMP on the Apollo 8 crew, began experiencing trouble with his legs. Doctors diagnosed the problem as a bony growth between his fifth and sixth vertebrae, requiring surgery. Lovell took his place on the Apollo 8 crew, and when Collins recovered he joined Armstrong's crew as CMP. In the meantime, Fred Haise filled in as backup LMP, and Aldrin as backup CMP for Apollo 8. Apollo 11 was the second American mission where all the crew members had prior spaceflight experience, the first being Apollo 10. The next was STS-26 in 1988.
Deke Slayton gave Armstrong the option to replace Aldrin with Lovell, since some thought Aldrin was difficult to work with. Armstrong had no issues working with Aldrin but thought it over for a day before declining. He thought Lovell deserved to command his own mission (eventually Apollo 13).
The Apollo 11 prime crew had none of the close cheerful camaraderie characterized by that of Apollo 12. Instead, they forged an amiable working relationship. Armstrong in particular was notoriously aloof, but Collins, who considered himself a loner, confessed to rebuffing Aldrin's attempts to create a more personal relationship. Aldrin and Collins described the crew as "amiable strangers". Armstrong did not agree with the assessment, and said "... all the crews I was on worked very well together."
The backup crew consisted of Lovell as Commander, William Anders as CMP, and Haise as LMP. Anders had flown with Lovell on Apollo 8. In early 1969, he accepted a job with the National Aeronautics and Space Council effective August 1969, and announced he would retire as an astronaut at that time. Ken Mattingly was moved from the support crew into parallel training with Anders as backup CMP in case Apollo 11 was delayed past its intended July launch date, at which point Anders would be unavailable.
By the normal crew rotation in place during Apollo, Lovell, Mattingly, and Haise were scheduled to fly on Apollo 14 after backing up for Apollo 11. Later, Lovell's crew was forced to switch places with Alan Shepard's tentative Apollo 13 crew to give Shepard more training time.
During Projects Mercury and Gemini, each mission had a prime and a backup crew. For Apollo, a third crew of astronauts was added, known as the support crew. The support crew maintained the flight plan, checklists and mission ground rules, and ensured the prime and backup crews were apprised of changes. They developed procedures, especially those for emergency situations, so these were ready for when the prime and backup crews came to train in the simulators, allowing them to concentrate on practicing and mastering them. For Apollo 11, the support crew consisted of Ken Mattingly, Ronald Evans and Bill Pogue.
The capsule communicator (CAPCOM) was an astronaut at the Mission Control Center in Houston, Texas, who was the only person who communicated directly with the flight crew. For Apollo 11, the CAPCOMs were: Charles Duke, Ronald Evans, Bruce McCandless II, James Lovell, William Anders, Ken Mattingly, Fred Haise, Don L. Lind, Owen K. Garriott and Harrison Schmitt.
The flight directors for this mission were:
Other key personnel who played important roles in the Apollo 11 mission include the following.
The Apollo 11 mission emblem was designed by Collins, who wanted a symbol for "peaceful lunar landing by the United States". At Lovell's suggestion, he chose the bald eagle, the national bird of the United States, as the symbol. Tom Wilson, a simulator instructor, suggested an olive branch in its beak to represent their peaceful mission. Collins added a lunar background with the Earth in the distance. The sunlight in the image was coming from the wrong direction; the shadow should have been in the lower part of the Earth instead of the left. Aldrin, Armstrong and Collins decided the Eagle and the Moon would be in their natural colors, and decided on a blue and gold border. Armstrong was concerned that "eleven" would not be understood by non-English speakers, so they went with "Apollo 11", and they decided not to put their names on the patch, so it would "be representative of everyone who had worked toward a lunar landing".
An illustrator at the Manned Spacecraft Center (MSC) did the artwork, which was then sent off to NASA officials for approval. The design was rejected. Bob Gilruth, the director of the MSC felt the talons of the eagle looked "too warlike". After some discussion, the olive branch was moved to the talons. When the Eisenhower dollar coin was released in 1971, the patch design provided the eagle for its reverse side. The design was also used for the smaller Susan B. Anthony dollar unveiled in 1979.
After the crew of Apollo 10 named their spacecraft Charlie Brown and Snoopy, assistant manager for public affairs Julian Scheer wrote to George Low, the Manager of the Apollo Spacecraft Program Office at the MSC, to suggest the Apollo 11 crew be less flippant in naming their craft. The name Snowcone was used for the CM and Haystack was used for the LM in both internal and external communications during early mission planning.
The LM was named Eagle after the motif which was featured prominently on the mission insignia. At Scheer's suggestion, the CM was named Columbia after Columbiad, the giant cannon that launched a spacecraft (also from Florida) in Jules Verne's 1865 novel From the Earth to the Moon. It also referred to Columbia, a historical name of the United States.  In Collins' 1976 book, he said Columbia was in reference to Christopher Columbus.
The astronauts had personal preference kits (PPKs), small bags containing personal items of significance they wanted to take with them on the mission. Five 0.5-pound (0.23 kg) PPKs were carried on Apollo 11: three (one for each astronaut) were stowed on Columbia before launch, and two on Eagle.
Neil Armstrong's LM PPK contained a piece of wood from the Wright brothers' 1903 Wright Flyer's left propeller and a piece of fabric from its wing, along with a diamond-studded astronaut pin originally given to Slayton by the widows of the Apollo 1 crew. This pin had been intended to be flown on that mission and given to Slayton afterwards, but following the disastrous launch pad fire and subsequent funerals, the widows gave the pin to Slayton. Armstrong took it with him on Apollo 11.
NASA's Apollo Site Selection Board announced five potential landing sites on February 8, 1968. These were the result of two years' worth of studies based on high-resolution photography of the lunar surface by the five uncrewed probes of the Lunar Orbiter program and information about surface conditions provided by the Surveyor program. The best Earth-bound telescopes could not resolve features with the resolution Project Apollo required. The landing site had to be close to the lunar equator to minimize the amount of propellant required, clear of obstacles to minimize maneuvering, and flat to simplify the task of the landing radar. Scientific value was not a consideration.
Areas that appeared promising on photographs taken on Earth were often found to be totally unacceptable. The original requirement that the site be free of craters had to be relaxed, as no such site was found. Five sites were considered: Sites 1 and 2 were in the Sea of Tranquility (Mare Tranquillitatis); Site 3 was in the Central Bay (Sinus Medii); and Sites 4 and 5 were in the Ocean of Storms (Oceanus Procellarum).
The final site selection was based on seven criteria:
The requirement for the Sun angle was particularly restrictive, limiting the launch date to one day per month. A landing just after dawn was chosen to limit the temperature extremes the astronauts would experience. The Apollo Site Selection Board selected Site 2, with Sites 3 and 5 as backups in the event of the launch being delayed. In May 1969, Apollo 10's lunar module flew to within 15 kilometers (9.3 mi) of Site 2, and reported it was acceptable.
During the first press conference after the Apollo 11 crew was announced, the first question was, "Which one of you gentlemen will be the first man to step onto the lunar surface?" Slayton told the reporter it had not been decided, and Armstrong added that it was "not based on individual desire".
One of the first versions of the egress checklist had the lunar module pilot exit the spacecraft before the commander, which matched what had been done on Gemini missions, where the commander had never performed the spacewalk. Reporters wrote in early 1969 that Aldrin would be the first man to walk on the Moon, and Associate Administrator George Mueller told reporters he would be first as well. Aldrin heard that Armstrong would be the first because Armstrong was a civilian, which made Aldrin livid. Aldrin attempted to persuade other lunar module pilots he should be first, but they responded cynically about what they perceived as a lobbying campaign. Attempting to stem interdepartmental conflict, Slayton told Aldrin that Armstrong would be first since he was the commander. The decision was announced in a press conference on April 14, 1969.
For decades, Aldrin believed the final decision was largely driven by the lunar module's hatch location. Because the astronauts had their spacesuits on and the spacecraft was so small, maneuvering to exit the spacecraft was difficult. The crew tried a simulation in which Aldrin left the spacecraft first, but he damaged the simulator while attempting to egress. While this was enough for mission planners to make their decision, Aldrin and Armstrong were left in the dark on the decision until late spring. Slayton told Armstrong the plan was to have him leave the spacecraft first, if he agreed. Armstrong said, "Yes, that's the way to do it."
The media accused Armstrong of exercising his commander's prerogative to exit the spacecraft first. Chris Kraft revealed in his 2001 autobiography that a meeting occurred between Gilruth, Slayton, Low, and himself to make sure Aldrin would not be the first to walk on the Moon. They argued that the first person to walk on the Moon should be like Charles Lindbergh, a calm and quiet person. They made the decision to change the flight plan so the commander was the first to egress from the spacecraft.
The ascent stage of LM-5 Eagle arrived at the Kennedy Space Center on January 8, 1969, followed by the descent stage four days later, and CSM-107 Columbia on January 23. There were several differences between Eagle and Apollo 10's LM-4 Snoopy; Eagle had a VHF radio antenna to facilitate communication with the astronauts during their EVA on the lunar surface; a lighter ascent engine; more thermal protection on the landing gear; and a package of scientific experiments known as the Early Apollo Scientific Experiments Package (EASEP). The only change in the configuration of the command module was the removal of some insulation from the forward hatch. The CSM was mated on January 29, and moved from the Operations and Checkout Building to the Vehicle Assembly Building on April 14.
The S-IVB third stage of Saturn V AS-506 had arrived on January 18, followed by the S-II second stage on February 6, S-IC first stage on February 20, and the Saturn V Instrument Unit on February 27. At 12:30 on May 20, the 5,443-tonne (5,357-long-ton; 6,000-short-ton) assembly departed the Vehicle Assembly Building atop the crawler-transporter, bound for Launch Pad 39A, part of Launch Complex 39, while Apollo 10 was still on its way to the Moon. A countdown test commenced on June 26, and concluded on July 2. The launch complex was floodlit on the night of July 15, when the crawler-transporter carried the mobile service structure back to its parking area. In the early hours of the morning, the fuel tanks of the S-II and S-IVB stages were filled with liquid hydrogen. Fueling was completed by three hours before launch. Launch operations were partly automated, with 43 programs written in the ATOLL programming language.
Slayton roused the crew shortly after 04:00, and they showered, shaved, and had the traditional pre-flight breakfast of steak and eggs with Slayton and the backup crew. They then donned their space suits and began breathing pure oxygen. At 06:30, they headed out to Launch Complex 39. Haise entered Columbia about three hours and ten minutes before launch time. Along with a technician, he helped Armstrong into the left-hand couch at 06:54. Five minutes later, Collins joined him, taking up his position on the right-hand couch. Finally, Aldrin entered, taking the center couch. Haise left around two hours and ten minutes before launch. The closeout crew sealed the hatch, and the cabin was purged and pressurized. The closeout crew then left the launch complex about an hour before launch time. The countdown became automated at three minutes and twenty seconds before launch time. Over 450 personnel were at the consoles in the firing room.
An estimated one million spectators watched the launch of Apollo 11 from the highways and beaches in the vicinity of the launch site. Dignitaries included the Chief of Staff of the United States Army, General William Westmoreland, four cabinet members, 19 state governors, 40 mayors, 60 ambassadors and 200 congressmen. Vice President Spiro Agnew viewed the launch with former president Lyndon B. Johnson and his wife Lady Bird Johnson. Around 3,500 media representatives were present. About two-thirds were from the United States; the rest came from 55 other countries. The launch was televised live in 33 countries, with an estimated 25 million viewers in the United States alone. Millions more around the world listened to radio broadcasts. President Richard Nixon viewed the launch from his office in the White House with his NASA liaison officer, Apollo astronaut Frank Borman.
Saturn V AS-506 launched Apollo 11 on July 16, 1969, at 13:32:00 UTC (9:32:00 EDT). At 13.2 seconds into the flight, the launch vehicle began to roll into its flight azimuth of 72.058°. Full shutdown of the first-stage engines occurred about 2 minutes and 42 seconds into the mission, followed by separation of the S-IC and ignition of the S-II engines. The second stage engines then cut off and separated at about 9 minutes and 8 seconds, allowing the first ignition of the S-IVB engine a few seconds later.
Apollo 11 entered a near-circular Earth orbit at an altitude of 100.4 nautical miles (185.9 km) by 98.9 nautical miles (183.2 km), twelve minutes into its flight. After one and a half orbits, a second ignition of the S-IVB engine pushed the spacecraft onto its trajectory toward the Moon with the trans-lunar injection (TLI) burn at 16:22:13 UTC. About 30 minutes later, with Collins in the left seat and at the controls, the transposition, docking, and extraction maneuver was performed. This involved separating Columbia from the spent S-IVB stage, turning around, and docking with Eagle still attached to the stage. After the LM was extracted, the combined spacecraft headed for the Moon, while the rocket stage flew on a trajectory past the Moon. This was done to avoid the third stage colliding with the spacecraft, the Earth, or the Moon. A slingshot effect from passing around the Moon threw it into an orbit around the Sun.
On July 19 at 17:21:50 UTC, Apollo 11 passed behind the Moon and fired its service propulsion engine to enter lunar orbit. In the thirty orbits that followed, the crew saw passing views of their landing site in the southern Sea of Tranquility about 12 miles (19 km) southwest of the crater Sabine D. The site was selected in part because it had been characterized as relatively flat and smooth by the automated Ranger 8 and Surveyor 5 landers and the Lunar Orbiter mapping spacecraft, and because it was unlikely to present major landing or EVA challenges. It lay about 25 kilometers (16 mi) southeast of the Surveyor 5 landing site, and 68 kilometers (42 mi) southwest of Ranger 8's crash site.
At 12:52:00 UTC on July 20, Aldrin and Armstrong entered Eagle, and began the final preparations for lunar descent. At 17:44:00 Eagle separated from Columbia. Collins, alone aboard Columbia, inspected Eagle as it pirouetted before him to ensure the craft was not damaged, and that the landing gear was correctly deployed. Armstrong exclaimed: "The Eagle has wings!"
As the descent began, Armstrong and Aldrin found themselves passing landmarks on the surface two or three seconds early, and reported that they were "long"; they would land miles west of their target point. Eagle was traveling too fast. The problem could have been mascons—concen­tra­tions of high mass in a region or regions of the Moon's crust that contains a gravitational anomaly, potentially altering Eagle's trajectory. Flight Director Gene Kranz speculated that it could have resulted from extra air pressure in the docking tunnel. Or it could have been the result of Eagle's pirouette maneuver.
Five minutes into the descent burn, and 6,000 feet (1,800 m) above the surface of the Moon, the LM guidance computer (LGC) distracted the crew with the first of several unexpected 1201 and 1202 program alarms. Inside Mission Control Center, computer engineer Jack Garman told Guidance Officer Steve Bales it was safe to continue the descent, and this was relayed to the crew. The program alarms indicated "executive overflows", meaning the guidance computer could not complete all its tasks in real-time and had to postpone some of them. Margaret Hamilton, the Director of Apollo Flight Computer Programming at the MIT Charles Stark Draper Laboratory later recalled:
To blame the computer for the Apollo 11 problems is like blaming the person who spots a fire and calls the fire department. Actually, the computer was programmed to do more than recognize error conditions. A complete set of recovery programs was incorporated into the software. The software's action, in this case, was to eliminate lower priority tasks and re-establish the more important ones. The computer, rather than almost forcing an abort, prevented an abort. If the computer hadn't recognized this problem and taken recovery action, I doubt if Apollo 11 would have been the successful Moon landing it was.During the mission, the cause was diagnosed as the rendezvous radar switch being in the wrong position, causing the computer to process data from both the rendezvous and landing radars at the same time. Software engineer Don Eyles concluded in a 2005 Guidance and Control Conference paper that the problem was due to a hardware design bug previously seen during testing of the first uncrewed LM in Apollo 5. Having the rendezvous radar on (so it was warmed up in case of an emergency landing abort) should have been irrelevant to the computer, but an electrical phasing mismatch between two parts of the rendezvous radar system could cause the stationary antenna to appear to the computer as dithering back and forth between two positions, depending upon how the hardware randomly powered up. The extra spurious cycle stealing, as the rendezvous radar updated an involuntary counter, caused the computer alarms.
When Armstrong again looked outside, he saw that the computer's landing target was in a boulder-strewn area just north and east of a 300-foot-diameter (91 m) crater (later determined to be West crater), so he took semi-automatic control. Armstrong considered landing short of the boulder field so they could collect geological samples from it, but could not since their horizontal velocity was too high. Throughout the descent, Aldrin called out navigation data to Armstrong, who was busy piloting Eagle. Now 107 feet (33 m) above the surface, Armstrong knew their propellant supply was dwindling and was determined to land at the first possible landing site.
Armstrong found a clear patch of ground and maneuvered the spacecraft towards it. As he got closer, now 250 feet (76 m) above the surface, he discovered his new landing site had a crater in it. He cleared the crater and found another patch of level ground. They were now 100 feet (30 m) from the surface, with only 90 seconds of propellant remaining. Lunar dust kicked up by the LM's engine began to impair his ability to determine the spacecraft's motion. Some large rocks jutted out of the dust cloud, and Armstrong focused on them during his descent so he could determine the spacecraft's speed.
A light informed Aldrin that at least one of the 67-inch (170 cm) probes hanging from Eagle's footpads had touched the surface a few moments before the landing and he said: "Contact light!" Armstrong was supposed to immediately shut the engine down, as the engineers suspected the pressure caused by the engine's own exhaust reflecting off the lunar surface could make it explode, but he forgot. Three seconds later, Eagle landed and Armstrong shut the engine down. Aldrin immediately said "Okay, engine stop. ACA—out of detent." Armstrong acknowledged: "Out of detent. Auto." Aldrin continued: "Mode control—both auto. Descent engine command override off. Engine arm—off. 413 is in."
ACA was the Attitude Control Assembly—the LM's control stick. Output went to the LGC to command the reaction control system (RCS) jets to fire. "Out of Detent" meant the stick had moved away from its centered position; it was spring-centered like the turn indicator in a car. LGC address 413 contained the variable that indicated the LM had landed.
Eagle landed at 20:17:40 UTC on Sunday July 20 with 216 pounds (98 kg) of usable fuel remaining. Information available to the crew and mission controllers during the landing showed the LM had enough fuel for another 25 seconds of powered flight before an abort without touchdown would have become unsafe, but post-mission analysis showed that the real figure was probably closer to 50 seconds. Apollo 11 landed with less fuel than most subsequent missions, and the astronauts encountered a premature low fuel warning. This was later found to be the result of the propellant sloshing more than expected, uncovering a fuel sensor. On subsequent missions, extra anti-slosh baffles were added to the tanks to prevent this.
Armstrong acknowledged Aldrin's completion of the post-landing checklist with "Engine arm is off", before responding to the CAPCOM, Charles Duke, with the words, "Houston, Tranquility Base here. The Eagle has landed." Armstrong's unrehearsed change of call sign from "Eagle" to "Tranquility Base" emphasized to listeners that landing was complete and successful. Duke mispronounced his reply as he expressed the relief at Mission Control: "Roger, Twan—Tranquility, we copy you on the ground. You got a bunch of guys about to turn blue. We're breathing again. Thanks a lot."
Two and a half hours after landing, before preparations began for the EVA, Aldrin radioed to Earth:
This is the LM pilot. I'd like to take this opportunity to ask every person listening in, whoever and wherever they may be, to pause for a moment and contemplate the events of the past few hours and to give thanks in his or her own way.He then took communion privately. At this time NASA was still fighting a lawsuit brought by atheist Madalyn Murray O'Hair (who had objected to the Apollo 8 crew reading from the Book of Genesis) demanding that their astronauts refrain from broadcasting religious activities while in space. For this reason, Aldrin chose to refrain from directly mentioning taking communion on the Moon. Aldrin was an elder at the Webster Presbyterian Church, and his communion kit was prepared by the pastor of the church, Dean Woodruff. Webster Presbyterian possesses the chalice used on the Moon and commemorates the event each year on the Sunday closest to July 20. The schedule for the mission called for the astronauts to follow the landing with a five-hour sleep period, but they chose to begin preparations for the EVA early, thinking they would be unable to sleep.
Preparations for Neil Armstrong and Buzz Aldrin to walk on the Moon began at 23:43. These took longer than expected; three and a half hours instead of two. During training on Earth, everything required had been neatly laid out in advance, but on the Moon the cabin contained a large number of other items as well, such as checklists, food packets, and tools. Six hours and thirty-nine minutes after landing Armstrong and Aldrin were ready to go outside, and Eagle was depressurized.
Eagle's hatch was opened at 02:39:33. Armstrong initially had some difficulties squeezing through the hatch with his portable life support system (PLSS). Some of the highest heart rates recorded from Apollo astronauts occurred during LM egress and ingress. At 02:51 Armstrong began his descent to the lunar surface. The remote control unit on his chest kept him from seeing his feet. Climbing down the nine-rung ladder, Armstrong pulled a D-ring to deploy the modular equipment stowage assembly (MESA) folded against Eagle's side and activate the TV camera.
Apollo 11 used slow-scan television (TV) incompatible with broadcast TV, so it was displayed on a special monitor and a conventional TV camera viewed this monitor (thus, a broadcast of a broadcast), significantly reducing the quality of the picture. The signal was received at Goldstone in the United States, but with better fidelity by Honeysuckle Creek Tracking Station near Canberra in Australia. Minutes later the feed was switched to the more sensitive Parkes radio telescope in Australia. Despite some technical and weather difficulties, ghostly black and white images of the first lunar EVA were received and broadcast to at least 600 million people on Earth. Copies of this video in broadcast format were saved and are widely available, but recordings of the original slow scan source transmission from the lunar surface were likely destroyed during routine magnetic tape re-use at NASA.
After describing the surface dust as "very fine-grained" and "almost like a powder", at 02:56:15, six and a half hours after landing, Armstrong stepped off Eagle's footpad and declared: "That's one small step for  man, one giant leap for mankind."
Armstrong intended to say "That's one small step for a man", but the word "a" is not audible in the transmission, and thus was not initially reported by most observers of the live broadcast. When later asked about his quote, Armstrong said he believed he said "for a man", and subsequent printed versions of the quote included the "a" in square brackets. One explanation for the absence may be that his accent caused him to slur the words "for a" together; another is the intermittent nature of the audio and video links to Earth, partly because of storms near Parkes Observatory. A more recent digital analysis of the tape claims to reveal the "a" may have been spoken but obscured by static. Other analysis points to the claims of static and slurring as "face-saving fabrication", and that Armstrong himself later admitted to misspeaking the line.
About seven minutes after stepping onto the Moon's surface, Armstrong collected a contingency soil sample using a sample bag on a stick. He then folded the bag and tucked it into a pocket on his right thigh. This was to guarantee there would be some lunar soil brought back in case an emergency required the astronauts to abandon the EVA and return to the LM. Twelve minutes after the sample was collected, he removed the TV camera from the MESA and made a panoramic sweep, then mounted it on a tripod. The TV camera cable remained partly coiled and presented a tripping hazard throughout the EVA. Still photography was accomplished with a Hasselblad camera that could be operated hand held or mounted on Armstrong's Apollo space suit. Aldrin joined Armstrong on the surface. He described the view with the simple phrase: "Magnificent desolation."
Armstrong said moving in the lunar gravity, one-sixth of Earth's, was "even perhaps easier than the simulations ... It's absolutely no trouble to walk around." Aldrin joined him on the surface and tested methods for moving around, including two-footed kangaroo hops. The PLSS backpack created a tendency to tip backward, but neither astronaut had serious problems maintaining balance. Loping became the preferred method of movement. The astronauts reported that they needed to plan their movements six or seven steps ahead. The fine soil was quite slippery. Aldrin remarked that moving from sunlight into Eagle's shadow produced no temperature change inside the suit, but the helmet was warmer in sunlight, so he felt cooler in shadow. The MESA failed to provide a stable work platform and was in shadow, slowing work somewhat. As they worked, the moonwalkers kicked up gray dust, which soiled the outer part of their suits.
The astronauts planted the Lunar Flag Assembly containing a flag of the United States on the lunar surface, in clear view of the TV camera. Aldrin remembered, "Of all the jobs I had to do on the Moon the one I wanted to go the smoothest was the flag raising." But the astronauts struggled with the telescoping rod and could only jam the pole about 2 inches (5 cm) into the hard lunar surface. Aldrin was afraid it might topple in front of TV viewers. But he gave "a crisp West Point salute". Before Aldrin could take a photo of Armstrong with the flag, President Richard Nixon spoke to them through a telephone-radio transmission, which Nixon called "the most historic phone call ever made from the White House." Nixon originally had a long speech prepared to read during the phone call, but Frank Borman, who was at the White House as a NASA liaison during Apollo 11, convinced Nixon to keep his words brief.
Nixon: Hello, Neil and Buzz. I'm talking to you by telephone from the Oval Room at the White House. And this certainly has to be the most historic telephone call ever made from the White House. I just can't tell you how proud we all are of what you have done. For every American, this has to be the proudest day of our lives. And for people all over the world, I am sure that they too join with Americans in recognizing what an immense feat this is. Because of what you have done, the heavens have become a part of man's world. And as you talk to us from the Sea of Tranquility, it inspires us to redouble our efforts to bring peace and tranquility to Earth. For one priceless moment in the whole history of man, all the people on this Earth are truly one: one in their pride in what you have done, and one in our prayers that you will return safely to Earth.Armstrong: Thank you, Mr. President. It's a great honor and privilege for us to be here, representing not only the United States, but men of peace of all nations, and with interest and a curiosity, and men with a vision for the future. It's an honor for us to be able to participate here today.They deployed the EASEP, which included a passive seismic experiment package used to measure moonquakes and a retroreflector array used for the lunar laser ranging experiment. Then Armstrong walked 196 feet (60 m) from the LM to snap photos at the rim of Little West Crater while Aldrin collected two core samples. He used the geologist's hammer to pound in the tubes—the only time the hammer was used on Apollo 11—but was unable to penetrate more than 6 inches (15 cm) deep. The astronauts then collected rock samples using scoops and tongs on extension handles. Many of the surface activities took longer than expected, so they had to stop documenting sample collection halfway through the allotted 34 minutes. Aldrin shoveled 6 kilograms (13 lb) of soil into the box of rocks in order to pack them in tightly. Two types of rocks were found in the geological samples: basalt and breccia. Three new minerals were discovered in the rock samples collected by the astronauts: armalcolite, tranquillityite, and pyroxferroite. Armalcolite was named after Armstrong, Aldrin, and Collins. All have subsequently been found on Earth.
While on the surface, Armstrong uncovered a plaque mounted on the LM ladder, bearing two drawings of Earth (of the Western and Eastern Hemispheres), an inscription, and signatures of the astronauts and President Nixon. The inscription read:
Here men from the planet Earth first set foot upon the Moon July 1969, A. D. We came in peace for all mankind.At the behest of the Nixon administration to add a reference to God, NASA included the vague date as a reason to include A.D., which stands for Anno Domini, "in the year of our Lord" (although it should have been placed before the year, not after).
Mission Control used a coded phrase to warn Armstrong his metabolic rates were high, and that he should slow down. He was moving rapidly from task to task as time ran out. As metabolic rates remained generally lower than expected for both astronauts throughout the walk, Mission Control granted the astronauts a 15-minute extension. In a 2010 interview, Armstrong explained that NASA limited the first moonwalk's time and distance because there was no empirical proof of how much cooling water the astronauts' PLSS backpacks would consume to handle their body heat generation while working on the Moon.
Aldrin entered Eagle first. With some difficulty the astronauts lifted film and two sample boxes containing 21.55 kilograms (47.5 lb) of lunar surface material to the LM hatch using a flat cable pulley device called the Lunar Equipment Conveyor (LEC). This proved to be an inefficient tool, and later missions preferred to carry equipment and samples up to the LM by hand. Armstrong reminded Aldrin of a bag of memorial items in his sleeve pocket, and Aldrin tossed the bag down. Armstrong then jumped onto the ladder's third rung, and climbed into the LM. After transferring to LM life support, the explorers lightened the ascent stage for the return to lunar orbit by tossing out their PLSS backpacks, lunar overshoes, an empty Hasselblad camera, and other equipment. The hatch was closed again at 05:11:13. They then pressurized the LM and settled down to sleep.
Presidential speech writer William Safire had prepared an In Event of Moon Disaster announcement for Nixon to read in the event the Apollo 11 astronauts were stranded on the Moon. The remarks were in a memo from Safire to Nixon's White House Chief of Staff H. R. Haldeman, in which Safire suggested a protocol the administration might follow in reaction to such a disaster. According to the plan, Mission Control would "close down communications" with the LM, and a clergyman would "commend their souls to the deepest of the deep" in a public ritual likened to burial at sea. The last line of the prepared text contained an allusion to Rupert Brooke's First World War poem, "The Soldier".
While moving inside the cabin, Aldrin accidentally damaged the circuit breaker that would arm the main engine for liftoff from the Moon. There was a concern this would prevent firing the engine, stranding them on the Moon. A felt-tip pen was sufficient to activate the switch.
After more than .mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}21+1⁄2 hours on the lunar surface, in addition to the scientific instruments, the astronauts left behind: an Apollo 1 mission patch in memory of astronauts Roger Chaffee, Gus Grissom, and Edward White, who died when their command module caught fire during a test in January 1967; two memorial medals of Soviet cosmonauts Vladimir Komarov and Yuri Gagarin, who died in 1967 and 1968 respectively; a memorial bag containing a gold replica of an olive branch as a traditional symbol of peace; and a silicon message disk carrying the goodwill statements by Presidents Eisenhower, Kennedy, Johnson, and Nixon along with messages from leaders of 73 countries around the world. The disk also carries a listing of the leadership of the US Congress, a listing of members of the four committees of the House and Senate responsible for the NASA legislation, and the names of NASA's past and then-current top management.
After about seven hours of rest, the crew was awakened by Houston to prepare for the return flight. Two and a half hours later, at 17:54:00 UTC, they lifted off in Eagle's ascent stage to rejoin Collins aboard Columbia in lunar orbit. Film taken from the LM ascent stage upon liftoff from the Moon reveals the American flag, planted some 25 feet (8 m) from the descent stage, whipping violently in the exhaust of the ascent stage engine. Aldrin looked up in time to witness the flag topple: "The ascent stage of the LM separated ... I was concentrating on the computers, and Neil was studying the attitude indicator, but I looked up long enough to see the flag fall over." Subsequent Apollo missions planted their flags farther from the LM.
During his day flying solo around the Moon, Collins never felt lonely. Although it has been said "not since Adam has any human known such solitude", Collins felt very much a part of the mission. In his autobiography he wrote: "this venture has been structured for three men, and I consider my third to be as necessary as either of the other two". In the 48 minutes of each orbit when he was out of radio contact with the Earth while Columbia passed round the far side of the Moon, the feeling he reported was not fear or loneliness, but rather "awareness, anticipation, satisfaction, confidence, almost exultation".
One of Collins' first tasks was to identify the lunar module on the ground. To give Collins an idea where to look, Mission Control radioed that they believed the lunar module landed about 4 miles (6.4 km) off target. Each time he passed over the suspected lunar landing site, he tried in vain to find the module. On his first orbits on the back side of the Moon, Collins performed maintenance activities such as dumping excess water produced by the fuel cells and preparing the cabin for Armstrong and Aldrin to return.
Just before he reached the dark side on the third orbit, Mission Control informed Collins there was a problem with the temperature of the coolant. If it became too cold, parts of Columbia might freeze. Mission Control advised him to assume manual control and implement Environmental Control System Malfunction Procedure 17. Instead, Collins flicked the switch on the system from automatic to manual and back to automatic again, and carried on with normal housekeeping chores, while keeping an eye on the temperature. When Columbia came back around to the near side of the Moon again, he was able to report that the problem had been resolved. For the next couple of orbits, he described his time on the back side of the Moon as "relaxing". After Aldrin and Armstrong completed their EVA, Collins slept so he could be rested for the rendezvous. While the flight plan called for Eagle to meet up with Columbia, Collins was prepared for a contingency in which he would fly Columbia down to meet Eagle.
Eagle rendezvoused with Columbia at 21:24 UTC on July 21, and the two docked at 21:35. Eagle's ascent stage was jettisoned into lunar orbit at 23:41. Just before the Apollo 12 flight, it was noted that Eagle was still likely to be orbiting the Moon. Later NASA reports mentioned that Eagle's orbit had decayed, resulting in it impacting in an "uncertain location" on the lunar surface. In 2021, however, some calculations show that the lander may still be in orbit.

On July 23, the last night before splashdown, the three astronauts made a television broadcast in which Collins commented:  ... The Saturn V rocket which put us in orbit is an incredibly complicated piece of machinery, every piece of which worked flawlessly ... We have always had confidence that this equipment will work properly. All this is possible only through the blood, sweat, and tears of a number of people ... All you see is the three of us, but beneath the surface are thousands and thousands of others, and to all of those, I would like to say, "Thank you very much."
Aldrin added: This has been far more than three men on a mission to the Moon; more, still, than the efforts of a government and industry team; more, even, than the efforts of one nation. We feel that this stands as a symbol of the insatiable curiosity of all mankind to explore the unknown ... Personally, in reflecting on the events of the past several days, a verse from Psalms comes to mind. "When I consider the heavens, the work of Thy fingers, the Moon and the stars, which Thou hast ordained; What is man that Thou art mindful of him?"
Armstrong concluded: The responsibility for this flight lies first with history and with the giants of science who have preceded this effort; next with the American people, who have, through their will, indicated their desire; next with four administrations and their Congresses, for implementing that will; and then, with the agency and industry teams that built our spacecraft, the Saturn, the Columbia, the Eagle, and the little EMU, the spacesuit and backpack that was our small spacecraft out on the lunar surface. We would like to give special thanks to all those Americans who built the spacecraft; who did the construction, design, the tests, and put their hearts and all their abilities into those craft. To those people tonight, we give a special thank you, and to all the other people that are listening and watching tonight, God bless you. Good night from Apollo 11. On the return to Earth, a bearing at the Guam tracking station failed, potentially preventing communication on the last segment of the Earth return. A regular repair was not possible in the available time but the station director, Charles Force, had his ten-year-old son Greg use his small hands to reach into the housing and pack it with grease. Greg was later thanked by Armstrong.
The aircraft carrier USS Hornet, under the command of Captain Carl J. Seiberlich, was selected as the primary recovery ship (PRS) for Apollo 11 on June 5, replacing its sister ship, the LPH USS Princeton, which had recovered Apollo 10 on May 26. Hornet was then at her home port of Long Beach, California. On reaching Pearl Harbor on July 5, Hornet embarked the Sikorsky SH-3 Sea King helicopters of HS-4, a unit which specialized in recovery of Apollo spacecraft, specialized divers of UDT Detachment Apollo, a 35-man NASA recovery team, and about 120 media representatives. To make room, most of Hornet's air wing was left behind in Long Beach. Special recovery equipment was also loaded, including a boilerplate command module used for training.
On July 12, with Apollo 11 still on the launch pad, Hornet departed Pearl Harbor for the recovery area in the central Pacific, in the vicinity of 10°36′N 172°24′E﻿ / ﻿10.600°N 172.400°E﻿ / 10.600; 172.400. A presidential party consisting of Nixon, Borman, Secretary of State William P. Rogers and National Security Advisor Henry Kissinger flew to Johnston Atoll on Air Force One, then to the command ship USS Arlington in Marine One. After a night on board, they would fly to Hornet in Marine One for a few hours of ceremonies. On arrival aboard Hornet, the party was greeted by the Commander-in-Chief, Pacific Command (CINCPAC), Admiral John S. McCain Jr., and NASA Administrator Thomas O. Paine, who flew to Hornet from Pago Pago in one of Hornet's carrier onboard delivery aircraft.
Weather satellites were not yet common, but US Air Force Captain Hank Brandli had access to top-secret spy satellite images. He realized that a storm front was headed for the Apollo recovery area. Poor visibility which could make locating the capsule difficult, and strong upper-level winds which "would have ripped their parachutes to shreds" according to Brandli, posed a serious threat to the safety of the mission. Brandli alerted Navy Captain Willard S. Houston Jr., the commander of the Fleet Weather Center at Pearl Harbor, who had the required security clearance. On their recommendation, Rear Admiral Donald C. Davis, commander of Manned Spaceflight Recovery Forces, Pacific, advised NASA to change the recovery area, each man risking his career. A new location was selected 215 nautical miles (398 km) northeast.
This altered the flight plan. A different sequence of computer programs was used, one never before attempted. In a conventional entry, trajectory event P64 was followed by P67. For a skip-out re-entry, P65 and P66 were employed to handle the exit and entry parts of the skip. In this case, because they were extending the re-entry but not actually skipping out, P66 was not invoked and instead, P65 led directly to P67. The crew were also warned they would not be in a full-lift (heads-down) attitude when they entered P67. The first program's acceleration subjected the astronauts to 6.5 standard gravities (64 m/s2); the second, to 6.0 standard gravities (59 m/s2).
Before dawn on July 24, Hornet launched four Sea King helicopters and three Grumman E-1 Tracers. Two of the E-1s were designated as "air boss" while the third acted as a communications relay aircraft. Two of the Sea Kings carried divers and recovery equipment. The third carried photographic equipment, and the fourth carried the decontamination swimmer and the flight surgeon. At 16:44 UTC (05:44 local time) Columbia's drogue parachutes were deployed. This was observed by the helicopters. Seven minutes later Columbia struck the water forcefully 2,660 km (1,440 nmi) east of Wake Island, 380 km (210 nmi) south of Johnston Atoll, and 24 km (13 nmi) from Hornet, at 13°19′N 169°9′W﻿ / ﻿13.317°N 169.150°W﻿ / 13.317; -169.150. 82 °F (28 °C) with 6 feet (1.8 m) seas and winds at 17 knots (31 km/h; 20 mph) from the east were reported under broken clouds at 1,500 feet (460 m) with visibility of 10 nautical miles (19 km; 12 mi) at the recovery site. Reconnaissance aircraft flying to the original splashdown location reported the conditions Brandli and Houston had predicted.
During splashdown, Columbia landed upside down but was righted within ten minutes by flotation bags activated by the astronauts. A diver from the Navy helicopter hovering above attached a sea anchor to prevent it from drifting. More divers attached flotation collars to stabilize the module and positioned rafts for astronaut extraction.
The divers then passed biological isolation garments (BIGs) to the astronauts, and assisted them into the life raft. The possibility of bringing back pathogens from the lunar surface was considered remote, but NASA took precautions at the recovery site. The astronauts were rubbed down with a sodium hypochlorite solution and Columbia wiped with Povidone-iodine to remove any lunar dust that might be present. The astronauts were winched on board the recovery helicopter. BIGs were worn until they reached isolation facilities on board Hornet. The raft containing decontamination materials was intentionally sunk.
After touchdown on Hornet at 17:53 UTC, the helicopter was lowered by the elevator into the hangar bay, where the astronauts walked the 30 feet (9.1 m) to the Mobile quarantine facility (MQF), where they would begin the Earth-based portion of their 21 days of quarantine. This practice would continue for two more Apollo missions, Apollo 12 and Apollo 14, before the Moon was proven to be barren of life, and the quarantine process dropped. Nixon welcomed the astronauts back to Earth. He told them: "s a result of what you've done, the world has never been closer together before."
After Nixon departed, Hornet was brought alongside the 5-short-ton (4.5 t) Columbia, which was lifted aboard by the ship's crane, placed on a dolly and moved next to the MQF. It was then attached to the MQF with a flexible tunnel, allowing the lunar samples, film, data tapes and other items to be removed. Hornet returned to Pearl Harbor, where the MQF was loaded onto a Lockheed C-141 Starlifter and airlifted to the Manned Spacecraft Center. The astronauts arrived at the Lunar Receiving Laboratory at 10:00 UTC on July 28. Columbia was taken to Ford Island for deactivation, and its pyrotechnics made safe. It was then taken to Hickham Air Force Base, from whence it was flown to Houston in a Douglas C-133 Cargomaster, reaching the Lunar Receiving Laboratory on July 30.
In accordance with the Extra-Terrestrial Exposure Law, a set of regulations promulgated by NASA on July 16 to codify its quarantine protocol, the astronauts continued in quarantine. After three weeks in confinement (first in the Apollo spacecraft, then in their trailer on Hornet, and finally in the Lunar Receiving Laboratory), the astronauts were given a clean bill of health. On August 10, 1969, the Interagency Committee on Back Contamination met in Atlanta and lifted the quarantine on the astronauts, on those who had joined them in quarantine (NASA physician William Carpentier and MQF project engineer John Hirasaki), and on Columbia itself. Loose equipment from the spacecraft remained in isolation until the lunar samples were released for study.
On August 13, the three astronauts rode in ticker-tape parades in their honor in New York and Chicago, with an estimated six million attendees. On the same evening in Los Angeles there was an official state dinner to celebrate the flight, attended by members of Congress, 44 governors, Chief Justice of the United States Warren E. Burger and his predecessor, Earl Warren, and ambassadors from 83 nations at the Century Plaza Hotel. Nixon and Agnew honored each astronaut with a presentation of the Presidential Medal of Freedom.
The three astronauts spoke before a joint session of Congress on September 16, 1969. They presented two US flags, one to the House of Representatives and the other to the Senate, that they had carried with them to the surface of the Moon. The flag of American Samoa on Apollo 11 is on display at the Jean P. Haydon Museum in Pago Pago, the capital of American Samoa.
This celebration began a 38-day world tour that brought the astronauts to 22 foreign countries and included visits with the leaders of many countries. The crew toured from September 29 to November 5. Many nations honored the first human Moon landing with special features in magazines or by issuing Apollo 11 commemorative postage stamps or coins.
Humans walking on the Moon and returning safely to Earth accomplished Kennedy's goal set eight years earlier. In Mission Control during the Apollo 11 landing, Kennedy's speech flashed on the screen, followed by the words "TASK ACCOMPLISHED, July 1969". The success of Apollo 11 demonstrated the United States' technological superiority; and with the success of Apollo 11, America had won the Space Race.
New phrases permeated into the English language. "If they can send a man to the Moon, why can't they ...?" became a common saying following Apollo 11. Armstrong's words on the lunar surface also spun off various parodies.
While most people celebrated the accomplishment, disenfranchised Americans saw it as a symbol of the divide in America, evidenced by protesters led by Ralph Abernathy outside of Kennedy Space Center the day before Apollo 11 launched. NASA Administrator Thomas Paine met with Abernathy at the occasion, both hoping that the space program can spur progress also in other regards, such as poverty in the US. Paine was then asked, and agreed, to host protesters as spectators at the launch, and Abernathy, awestruck by the spectacle, prayed for the astronauts. Racial and financial inequalities frustrated citizens who wondered why money spent on the Apollo program was not spent taking care of humans on Earth. A poem by Gil Scott-Heron called "Whitey on the Moon" (1970) illustrated the racial inequality in the United States that was highlighted by the Space Race. The poem starts with:
A rat done bit my sister Nell.
(with Whitey on the moon)
Her face and arms began to swell.
(and Whitey's on the moon)
I can't pay no doctor bill.
(but Whitey's on the moon)
Ten years from now I'll be paying still.
(while Whitey's on the moon)

Twenty percent of the world's population watched humans walk on the Moon for the first time. While Apollo 11 sparked the interest of the world, the follow-on Apollo missions did not hold the interest of the nation. One possible explanation was the shift in complexity. Landing someone on the Moon was an easy goal to understand; lunar geology was too abstract for the average person. Another is that Kennedy's goal of landing humans on the Moon had already been accomplished. A well-defined objective helped Project Apollo accomplish its goal, but after it was completed it was hard to justify continuing the lunar missions.
While most Americans were proud of their nation's achievements in space exploration, only once during the late 1960s did the Gallup Poll indicate that a majority of Americans favored "doing more" in space as opposed to "doing less". By 1973, 59 percent of those polled favored cutting spending on space exploration. The Space Race had been won, and Cold War tensions were easing as the US and Soviet Union entered the era of détente. This was also a time when inflation was rising, which put pressure on the government to reduce spending. What saved the space program was that it was one of the few government programs that had achieved something great. Drastic cuts, warned Caspar Weinberger, the deputy director of the Office of Management and Budget, might send a signal that "our best years are behind us".
After the Apollo 11 mission, officials from the Soviet Union said landing humans on the Moon was dangerous and unnecessary. At the time the Soviet Union was attempting to retrieve lunar samples robotically. The Soviets publicly denied there was a race to the Moon, and indicated they were not making an attempt. Mstislav Keldysh said in July 1969, "We are concentrating wholly on the creation of large satellite systems." It was revealed in 1989 that the Soviets had tried to send people to the Moon, but were unable due to technological difficulties. The public's reaction in the Soviet Union was mixed. The Soviet government limited the release of information about the lunar landing, which affected the reaction. A portion of the populace did not give it any attention, and another portion was angered by it.
The Apollo 11 landing is referenced in the songs "Armstrong, Aldrin and Collins" by The Byrds on the 1969 album Ballad of Easy Rider and "Coon on the Moon" by Howlin' Wolf on the 1973 album The Back Door Wolf.
The command module Columbia went on a tour of the United States, visiting 49 state capitals, the District of Columbia, and Anchorage, Alaska. In 1971, it was transferred to the Smithsonian Institution, and was displayed at the National Air and Space Museum (NASM) in Washington, DC. It was in the central Milestones of Flight exhibition hall in front of the Jefferson Drive entrance, sharing the main hall with other pioneering flight vehicles such as the Wright Flyer, Spirit of St. Louis, Bell X-1, North American X-15 and Friendship 7.
Columbia was moved in 2017 to the NASM Mary Baker Engen Restoration Hangar at the Steven F. Udvar-Hazy Center in Chantilly, Virginia, to be readied for a four-city tour titled Destination Moon: The Apollo 11 Mission. This included Space Center Houston from October 14, 2017, to March 18, 2018, the Saint Louis Science Center from April 14 to September 3, 2018, the Senator John Heinz History Center in Pittsburgh from September 29, 2018, to February 18, 2019, and its last location at Museum of Flight in Seattle from March 16 to September 2, 2019. Continued renovations at the Smithsonian allowed time for an additional stop for the capsule, and it was moved to the Cincinnati Museum Center. The ribbon cutting ceremony was on September 29, 2019.
For 40 years Armstrong's and Aldrin's space suits were displayed in the museum's Apollo to the Moon exhibit, until it permanently closed on December 3, 2018, to be replaced by a new gallery which was scheduled to open in 2022. A special display of Armstrong's suit was unveiled for the 50th anniversary of Apollo 11 in July 2019. The quarantine trailer, the flotation collar and the flotation bags are in the Smithsonian's Steven F. Udvar-Hazy Center annex near Washington Dulles International Airport in Chantilly, Virginia, where they are on display along with a test lunar module.
The descent stage of the LM Eagle remains on the Moon. In 2009, the Lunar Reconnaissance Orbiter (LRO) imaged the various Apollo landing sites on the surface of the Moon, for the first time with sufficient resolution to see the descent stages of the lunar modules, scientific instruments, and foot trails made by the astronauts. The remains of the ascent stage lie at an unknown location on the lunar surface, after being abandoned and impacting the Moon. The location is uncertain because Eagle ascent stage was not tracked after it was jettisoned, and the lunar gravity field is sufficiently non-uniform to make the orbit of the spacecraft unpredictable after a short time.
In March 2012 a team of specialists financed by Amazon founder Jeff Bezos located the F-1 engines from the S-IC stage that launched Apollo 11 into space. They were found on the Atlantic seabed using advanced sonar scanning. His team brought parts of two of the five engines to the surface. In July 2013, a conservator discovered a serial number under the rust on one of the engines raised from the Atlantic, which NASA confirmed was from Apollo 11. The S-IVB third stage which performed Apollo 11's trans-lunar injection remains in a solar orbit near to that of Earth.
The main repository for the Apollo Moon rocks is the Lunar Sample Laboratory Facility at the Lyndon B. Johnson Space Center in Houston, Texas. For safekeeping, there is also a smaller collection stored at White Sands Test Facility near Las Cruces, New Mexico. Most of the rocks are stored in nitrogen to keep them free of moisture. They are handled only indirectly, using special tools. Over 100 research laboratories around the world conduct studies of the samples, and approximately 500 samples are prepared and sent to investigators every year.
In November 1969, Nixon asked NASA to make up about 250 presentation Apollo 11 lunar sample displays for 135 nations, the fifty states of the United States and its possessions, and the United Nations. Each display included Moon dust from Apollo 11. The rice-sized particles were four small pieces of Moon soil weighing about 50 mg and were enveloped in a clear acrylic button about as big as a United States half dollar coin. This acrylic button magnified the grains of lunar dust. The Apollo 11 lunar sample displays were given out as goodwill gifts by Nixon in 1970.
The Passive Seismic Experiment ran until the command uplink failed on August 25, 1969. The downlink failed on December 14, 1969. As of 2018, the Lunar Laser Ranging experiment remains operational.
Armstrong's Hasselblad camera was thought to be lost or left on the Moon surface.
In 2015, after Armstrong died in 2012, his widow contacted the National Air and Space Museum to inform them she had found a white cloth bag in one of Armstrong's closets. The bag contained various items, which should have been left behind in the lunar module, including the 16mm Data Acquisition Camera that had been used to capture images of the first Moon landing. The camera is currently on display at the National Air and Space Museum.
On July 15, 2009, Life.com released a photo gallery of previously unpublished photos of the astronauts taken by Life photographer Ralph Morse prior to the Apollo 11 launch. From July 16 to 24, 2009, NASA streamed the original mission audio on its website in real time 40 years to the minute after the events occurred. It is in the process of restoring the video footage and has released a preview of key moments. In July 2010, air-to-ground voice recordings and film footage shot in Mission Control during the Apollo 11 powered descent and landing was re-synchronized and released for the first time. The John F. Kennedy Presidential Library and Museum set up an Adobe Flash website that rebroadcasts the transmissions of Apollo 11 from launch to landing on the Moon.
On July 20, 2009, Armstrong, Aldrin, and Collins met with US President Barack Obama at the White House. "We expect that there is, as we speak, another generation of kids out there who are looking up at the sky and are going to be the next Armstrong, Collins, and Aldrin", Obama said. "We want to make sure that NASA is going to be there for them when they want to take their journey." On August 7, 2009, an act of Congress awarded the three astronauts a Congressional Gold Medal, the highest civilian award in the United States. The bill was sponsored by Florida Senator Bill Nelson and Florida Representative Alan Grayson.
A group of British scientists interviewed as part of the anniversary events reflected on the significance of the Moon landing:
It was carried out in a technically brilliant way with risks taken ... that would be inconceivable in the risk-averse world of today ... The Apollo programme is arguably the greatest technical achievement of mankind to date ... nothing since Apollo has come close  the excitement that was generated by those astronauts—Armstrong, Aldrin and the 10 others who followed them.On June 10, 2015, Congressman Bill Posey introduced resolution H.R. 2726 to the 114th session of the United States House of Representatives directing the United States Mint to design and sell commemorative coins in gold, silver and clad for the 50th anniversary of the Apollo 11 mission. On January 24, 2019, the Mint released the Apollo 11 Fiftieth Anniversary commemorative coins to the public on its website.
A documentary film, Apollo 11, with restored footage of the 1969 event, premiered in IMAX on March 1, 2019, and broadly in theaters on March 8.
The Smithsonian Institute's National Air and Space Museum and NASA sponsored the "Apollo 50 Festival" on the National Mall in Washington DC. The three day (July 18 to 20, 2019) outdoor festival featured hands-on exhibits and activities, live performances, and speakers such as Adam Savage and NASA scientists.
As part of the festival, a projection of the 363-foot (111 m) tall Saturn V rocket was displayed on the east face of the 555-foot (169 m) tall Washington Monument from July 16 through the 20th from 9:30 pm until 11:30 pm (EDT). The program also included a 17-minute show that combined full-motion video projected on the Washington Monument to recreate the assembly and launch of the Saturn V rocket. The projection was joined by a 40-foot (12 m) wide recreation of the Kennedy Space Center countdown clock and two large video screens showing archival footage to recreate the time leading up to the moon landing. There were three shows per night on July 19–20, with the last show on Saturday, delayed slightly so the portion where Armstrong first set foot on the Moon would happen exactly 50 years to the second after the actual event.
On July 19, 2019, the Google Doodle paid tribute to the Apollo 11 Moon Landing, complete with a link to an animated YouTube video with voiceover by astronaut Michael Collins.
Aldrin, Collins, and Armstrong's sons were hosted by President Donald Trump in the Oval Office.
In some of the following sources, times are shown in the format hours:minutes:seconds (e.g. 109:24:15), referring to the mission's Ground Elapsed Time (GET), based on the official launch time of July 16, 1969, 13:32:00 UTC (000:00:00 GET).

Fireworks are a class of low explosive pyrotechnic devices used for aesthetic and entertainment purposes. The most common use of a firework is as part of a fireworks display (also called a fireworks show or pyrotechnics), a display of the effects produced by firework devices.
Fireworks take many forms to produce the four primary effects: noise, light, smoke, as well as floating materials (confetti most notably). They may be designed to burn with colored flames and sparks including red, orange, yellow, green, blue, purple and silver. Displays are common throughout the world and are the focal point of many cultural and religious celebrations.
Fireworks are generally classified as to where they perform, either as a ground or aerial firework. In the latter case they may provide their own propulsion (skyrocket) or be shot into the air by a mortar (aerial shell).
The most common feature of fireworks is a paper or pasteboard tube or casing filled with the combustible material, often pyrotechnic stars. A number of these tubes or cases are often combined so as to make when kindled, a great variety of sparkling shapes, often variously colored. A skyrocket is a common form of firework, although the first skyrockets were used in warfare. The aerial shell, however, is the backbone of today's commercial aerial display, and a smaller version for consumer use is known as the festival ball in the United States.
Fireworks were originally invented in China. Cultural events and festivities such as the Chinese New Year and the Mid-Autumn Moon Festival were and still are times when fireworks are guaranteed sights. China is the largest manufacturer and exporter of fireworks in the world.
Silent fireworks are becoming popular for providing all the beauty without the added explosive sounds imitating artillery and warfare that traumatize pets, wildlife, and many humans. The Italian town of Collecchio switched to silent fireworks in 2015, mandating the switch.
The earliest fireworks came from China during the Song dynasty (960–1279). Fireworks were used to accompany many festivities. The art and science of firework making has developed into an independent profession. In China, pyrotechnicians were respected for their knowledge of complex techniques in mounting firework displays.
During the Han dynasty (202 BC – 220 AD), people threw bamboo stems into a fire to produce an explosion with a loud sound. In later times, gunpowder packed into small containers was used to mimic the sounds of burning bamboo. Exploding bamboo stems and gunpowder firecrackers were interchangeably known as baozhu (爆竹) or baogan (爆竿). It was during the Song dynasty that people manufactured the first firecrackers comprising tubes made from rolled sheets of paper containing gunpowder and a fuse. They also strung these firecrackers together into large clusters, known as bian (lit. "whip") or bianpao (lit. "whip cannon"), so the firecrackers could be set off one by one in close sequence. By the 12th and possibly the 11th century, the term baozhang (爆仗) was used to specifically refer to gunpowder firecrackers.
During the Song dynasty, many of the common people could purchase various kinds of fireworks from market vendors. Grand displays of fireworks were also known to be held. In 1110, a large fireworks display in a martial demonstration was held to entertain Emperor Huizong of Song (r. 1100–1125) and his court. A record from 1264 states that a rocket-propelled firework went off near the Empress Dowager Gong Sheng and startled her during a feast held in her honor by her son Emperor Lizong of Song (r. 1224–1264). Rocket propulsion was common in warfare, as evidenced by the Huolongjing compiled by Liu Bowen (1311–1375) and Jiao Yu (fl. c. 1350–1412). In 1240 the Arabs acquired knowledge of gunpowder and its uses from China. A Syrian named Hasan al-Rammah wrote of rockets, fireworks, and other incendiaries, using terms that suggested he derived his knowledge from Chinese sources, such as his references to fireworks as "Chinese flowers".
In regards to colored fireworks, this was derived and developed from earlier (possibly Han dynasty or soon thereafter) Chinese application of chemical substances to create colored smoke and fire. Such application appears in the Huolongjing (14th century) and Wubeizhi (preface of 1621, printed 1628), which describes recipes, several of which used low-nitrate gunpowder, to create military signal smokes with various colors. In the Wubei Huolongjing (武備火龍經; Ming, completed after 1628), two formulas appears for firework-like signals, the sanzhangju (三丈菊) and baizhanglian (百丈蓮), that produces silver sparkles in the smoke. In the Huoxilüe (火戲略; 1753) by Zhao Xuemin (趙學敏), there are several recipes with low-nitrate gunpowder and other chemical substances to tint flames and smoke. These included, for instance, arsenical sulphide for yellow, copper acetate (verdigris) for green, lead carbonate for lilac-white, and mercurous chloride (calomel) for white. The Chinese pyrotechnics were described by the French author Antoine Caillot (1818): "It is certain that the variety of colours which the Chinese have the secret of giving to flame is the greatest mystery of their fireworks." Similarly, the English geographer Sir John Barrow (ca. 1797) wrote "The diversity of colours indeed with which the Chinese have the secret of cloathing fire seems to be the chief merit of their pyrotechny."
Fireworks were produced in Europe by the 14th century, becoming popular by the 17th century. Lev Izmailov, ambassador of Peter the Great, once reported from China: "They make such fireworks that no one in Europe has ever seen." In 1758, the Jesuit missionary Pierre Nicolas le Chéron d'Incarville, living in Beijing, wrote about the methods and composition on how to make many types of Chinese fireworks to the Paris Academy of Sciences, which revealed and published the account five years later. Amédée-François Frézier published his revised work Traité des feux d'artice pour le spectacle (Treatise on Fireworks) in 1747 (originally 1706), covering the recreational and ceremonial uses of fireworks, rather than their military uses. Music for the Royal Fireworks was composed by George Frideric Handel in 1749 to celebrate the Peace treaty of Aix-la-Chapelle, which had been declared the previous year.
"Prior to the nineteenth century and the advent of modern chemistry they  must have been relatively dull and unexciting." Bertholet in 1786 discovered that oxidations with potassium chlorate resulted in a violet emission. Subsequent developments revealed that oxidations with the chlorates of barium, strontium, copper, and sodium result in intense emission of bright colors. The isolation of metallic magnesium and aluminium marked another breakthrough as these metals burn with an intense silvery light.
Improper use of fireworks may be dangerous, both to the person operating them (risks of burns and wounds) and to bystanders; in addition, they may start fires after landing on flammable material. For this reason, the use of fireworks is generally legally restricted. Display fireworks are restricted by law for use by professionals; consumer items, available to the public, are smaller versions containing limited amounts of explosive material to reduce potential danger.
Fireworks are also a problem for animals, both domestic and wild, which can be frightened by their noise, leading to them running away, often into danger, or hurting themselves on fences or in other ways in an attempt to escape. Frightened birds also may abandon nests and not return to complete rearing their young.
Pyrotechnical competitions involving fireworks are held in many countries. The most prestigious fireworks competition is the Montreal Fireworks Festival, an annual competition held in Montreal, Quebec, Canada. Another magnificent competition is Le Festival d'Art Pyrotechnique held in the summer annually at the Bay of Cannes in Côte d'Azur, France. The World Pyro Olympics is an annual competition amongst the top fireworks companies in the world. It is held in Manila, Philippines. The event is one of the largest and most intense international fireworks competitions.
Enthusiasts in the United States have formed clubs which unite hobbyists and professionals. The groups provide safety instruction and organize meetings and private "shoots" at remote premises where members shoot commercial fireworks as well as fire pieces of their own manufacture. Clubs secure permission to fire items otherwise banned by state or local ordinances. Competition among members and between clubs, demonstrating everything from single shells to elaborate displays choreographed to music, are held.
One of the oldest clubs is Crackerjacks, Inc., organized in 1976 in the Eastern Seaboard region of the U.S.
The Pyrotechnics Guild International, Inc. or PGI, founded in 1969, is an independent worldwide nonprofit organization of amateur and professional fireworks enthusiasts. It is notable for its large number of members, around 3,500 in total. The PGI exists solely to further the safe usage and enjoyment of both professional grade and consumer grade fireworks while both advancing the art and craft of pyrotechnics and preserving its historical aspects. Each August the PGI conducts its annual week-long convention, where some the world's biggest and best fireworks displays occur. Vendors, competitors, and club members come from around the US and from various parts of the globe to enjoy the show and to help out at this all-volunteer event. Aside from the nightly firework shows, the competition is a highlight of the convention. This is a completely unique event where individual classes of hand-built fireworks are competitively judged, ranging from simple fireworks rockets to extremely large and complex aerial shells. Some of the biggest, best, most intricate fireworks displays in the United States take place during the convention week.
Amateur and professional members can come to the convention to purchase fireworks, paper goods, novelty items, non-explosive chemical components and much more at the PGI trade show. Before the nightly fireworks displays and competitions, club members have a chance to enjoy open shooting of any and all legal consumer or professional grade fireworks, as well as testing and display of hand-built fireworks. The week ends with the Grand Public Display on Friday night, which gives the chosen display company a chance to strut their stuff in front of some of the world's biggest fireworks aficionados. The stakes are high and much planning is put into the show. In 1994 a shell of 36 inches (914 mm) in diameter was fired during the convention, more than twice as large as the largest shell usually seen in the US, and shells as large as 24 inches (610 mm) are frequently fired.
Both fireworks and firecrackers are a popular tradition during Halloween in Vancouver, although apparently this is not the custom elsewhere in Canada.
In the Republic of Ireland and Northern Ireland there are many fireworks displays, during the Halloween season. The largest are in the cities of Belfast, Derry, and Dublin. The 2010 Derry Halloween fireworks attracted an audience of more than 20,000 people. The sale of fireworks is strongly restricted in the Republic of Ireland, although many illegal fireworks are sold throughout October or smuggled from Northern Ireland. In the Republic the maximum punishment for possessing fireworks without a licence, or lighting fireworks in a public place, is a €10,000 fine  and a five-year prison sentence.
Two firework displays on All Hallows' Eve in the United States are the annual "Happy Hallowishes" show at Walt Disney World's Magic Kingdom "Mickey's Not-So-Scary Halloween Party" event, which began in 2005, and the "Halloween Screams" at Disneyland Park, which began in 2009.
In Australia, fireworks displays are used in the public celebration of major events such as New Year's Eve and Australia Day. Notable annual fireworks events include the Sydney New Year's Eve Midnight Fireworks show and the City of Perth Skyworks. In the Northern Territory, "Cracker Night" is celebrated every 1 July on Territory Day, where residents are allowed to buy and use fireworks without a permit.
In France, fireworks are traditionally displayed on the eve of Bastille day (14 July) to commemorate the French revolution and the storming of the Bastille on that same day in 1789. Every city in France lights up the sky for the occasion with a special mention to Paris that offers a spectacle around the Eiffel Tower.
In Hungary fireworks are used on 20 August, which is a national celebration day 
Indians throughout the world celebrate with fireworks as part of their popular "festival of lights" (Diwali) in Oct-Nov every year.
During the summer in Japan, fireworks festivals (花火大会, hanabi taikai) are held nearly every day someplace in the country, in total numbering more than 200 during August. The festivals consist of large fireworks shows, the largest of which use between 100,000 and 120,000 rounds (Tondabayashi, Osaka), and can attract more than 800,000 spectators. Street vendors set up stalls to sell various drinks and staple Japanese food (such as yakisoba, okonomiyaki, takoyaki, kakigōri (shaved ice), and traditionally held festival games, such as kingyo-sukui, or goldfish scooping.
Even today, men and women attend these events wearing the traditional yukata, summer kimono, or jinbei, and gather in large social circles of family or friends to sit picnic-like, eating and drinking, while watching the show.
The first fireworks festival in Japan was held in 1733.
Sumidagawa Fireworks Festival is one of the many being celebrated annually throughout Japan in summer.
Fireworks have been used in Malta for hundreds of years. When the islands were ruled by the Order of St John, fireworks were used on special occasions such as the election of a new Grand Master, the appointment of a new Pope or the birth of a prince.
Nowadays, fireworks are used in village feasts throughout the summer. The Malta International Fireworks Festival is also held annually.
Pyrotechnics experts from around the world have competed in Monte Carlo, Monaco, since 1966. The festival runs from July to August every year, and the winner returns in 18 November for the fireworks display on the night before the National Day of Monaco. The event is held in Port Hercule, beginning at around 9:30pm every night, depending on the sunset.
The Singapore Fireworks Celebrations (previously the Singapore Fireworks Festival) is an annual event held in Singapore as part of its National Day celebrations. The festival features local and foreign teams which launch displays on different nights. While currently non-competitive in nature, the organizer has plans to introduce a competitive element in the future.
The annual festival has grown in magnitude, from 4,000 rounds used in 2004, to 6,000 in 2005, to more than 9,100 in 2006.
Busan International Fireworks Festival is one of the most significant fireworks festivals in Asia.
In Switzerland fireworks are often used on 1 August, which is a national celebration day.
One of the biggest occasions for fireworks in the UK is Guy Fawkes Night held each year on 5 November, to celebrate the foiling of the Catholic Gunpowder Plot on 5 November 1605, an attempt to kill King James I. The Guardian newspaper said in 2008 that Britain's biggest Guy Fawkes night events were:
The main firework celebrations in the UK are by the public who buy from many suppliers.
America's earliest settlers brought their enthusiasm for fireworks to the United States. Fireworks and black ash were used to celebrate important events long before the American Revolutionary War. The very first celebration of Independence Day was in 1777, six years before Americans knew whether or not the new nation would survive the war; fireworks were a part of all festivities. In 1789, George Washington's inauguration was accompanied by a fireworks display.. George Marshall was an American naval hero during the War of 1812 and other campaigns. He was a Master Gunner and pyrotechnics specialist who wrote Marshall's Practical Marine Gunnery  in 1822.  The book outlines chemical formulas for the composition of fireworks.  This early fascination with fireworks' noise and color continues today with fireworks displays commonly included in Independence Day celebrations.
In 2004, Disneyland, in Anaheim, California, pioneered the commercial use of aerial fireworks launched with compressed air rather than gunpowder. The display shell explodes in the air using an electronic timer. The advantages of compressed air launch are a reduction in fumes, and much greater accuracy in height and timing. The Walt Disney Company is now the largest consumer of fireworks in the world.
In addition to large public displays, people often buy small quantities of fireworks for their own celebrations. Fireworks on general sale are usually less powerful than professional fireworks. Types include firecrackers, rockets, cakes (multishot aerial fireworks), and smoke balls.
Fireworks can also be used in an agricultural capacity as to frighten away birds.
Colors in fireworks are usually generated by pyrotechnic stars—usually just called stars—which produce intense light when ignited. Stars contain four basic types of ingredients.
Some of the more common color-producing compounds are tabulated here. The color of a compound in a firework will be the same as its color in a flame test (shown at right). Not all compounds that produce a colored flame are appropriate for coloring fireworks, however. Ideal colorants will produce a pure, intense color when present in moderate concentration.
The color of sparks is limited to red/orange, yellow/gold and white/silver. This is explained by light emission from an incandescent solid particle in contrast to the element-specific emission from the vapor phase of a flame. Light emitted from a solid particle is defined by black-body radiation. Low boiling metals can form sparks with an intensively colored glowing shell surrounding the basic particle. This is caused by vapor phase combustion of the metal.
Lithium (medium red)
Li2CO3 (lithium carbonate) LiCl (lithium chloride)
Rubidium (violet-red)
RbNO3 (rubidium nitrate)
The brightest stars, often called Mag Stars, are fueled by aluminium. Magnesium is rarely used in the fireworks industry due to its lack of ability to form a protective oxide layer. Often an alloy of both metals called magnalium is used.
Many of the chemicals used in the manufacture of fireworks are non-toxic, while many more have some degree of toxicity, can cause skin sensitivity, or exist in dust form and are thereby inhalation hazards. Still others are poisons if directly ingested or inhaled.
The following table lists the principal elements used in modern pyrotechnics. Some elements are used in their elemental form such as particles of titanium, aluminium, iron, zirconium, and magnesium. These elements burn in the presence of air (O2) or oxidants (perchlorate, chlorate). Most elements in pyrotechnics are in the form of salts.
A cake is a cluster of individual tubes linked by fuse that fires a series of aerial effects. Tube diameters can range in size from .mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}1⁄4–4 inches (6.4–101.6 mm), and a single cake can have more than 1,000 shots. The variety of effects within individual cakes is often such that they defy descriptive titles and are instead given cryptic names such as "Bermuda Triangle", "Pyro Glyphics", "Waco Wakeup", and "Poisonous Spider", to name a few. Others are simply quantities of 2.5–4 in (64–102 mm) shells fused together in single-shot tubes.
A shell containing several large stars that travel a short distance before breaking apart into smaller stars, creating a crisscrossing grid-like effect. Strictly speaking, a crossette star should split into 4 pieces which fly off symmetrically, making a cross. Once limited to silver or gold effects, colored crossettes such as red, green, or white are now very common.
A spherical break of colored stars, similar to a peony, but with stars that leave a visible trail of sparks.
Essentially the same as a peony shell, but with fewer and larger stars. These stars travel a longer-than-usual distance from the shell break before burning out. For instance, if a 3 in (76 mm) peony shell is made with a star size designed for a 6 in (152 mm)  shell, it is then considered a dahlia. Some dahlia shells are cylindrical rather than spherical to allow for larger stars.
A type of Chrysanthemum or Peony, with a center cluster of non-moving stars, normally of a contrasting color or effect.
Inserts that propel themselves rapidly away from the shell burst, often resembling fish swimming away.
Named for the shape of its break, this shell features heavy long-burning tailed stars that only travel a short distance from the shell burst before free-falling to the ground. Also known as a waterfall shell. Sometimes there is a glittering through the "waterfall".
Kamuro is a Japanese word meaning "boys haircut", which is what this shell resembles when fully exploded in the air. It is a dense burst of glittering silver or gold stars which leave a heavy glitter trail and shine bright in the night's sky.
A mine (a.k.a. pot à feu) is a ground firework that expels stars and/or other garnitures into the sky. Shot from a mortar like a shell, a mine consists of a canister with the lift charge on the bottom with the effects placed on top. Mines can project small reports, serpents, small shells, as well as just stars. Although mines up to 12 inches (305 mm) diameter appear on occasion, they are usually between 3–5 inches (76–127 mm) in diameter.
A large shell containing several smaller shells of various sizes and types. The initial burst scatters the shells across the sky before they explode. Also called a bouquet shell. When a shell contains smaller shells of the same size and type, the effect is usually referred to as "Thousands". Very large bouquet shells (up to 48 inches ) are frequently used in Japan.
A shell containing a relatively few large comet stars arranged in such a way as to burst with large arms or tendrils, producing a palm tree-like effect. Proper palm shells feature a thick rising tail that displays as the shell ascends, thereby simulating the tree trunk to further enhance the "palm tree" effect. One might also see a burst of color inside the palm burst (given by a small insert shell) to simulate coconuts.
A spherical break of colored stars that burn without a tail effect. The peony is the most commonly seen shell type.
A shell with stars specially arranged so as to create a ring. Variations include smiley faces, hearts, and clovers.
A Roman candle is a long tube containing several large stars which fire at a regular interval. These are commonly arranged in fan shapes or crisscrossing shapes, at a closer proximity to the audience. Some larger Roman candles contain small shells (bombettes) rather than stars.
A shell intended to produce a loud report rather than a visual effect. Salute shells usually contain flash powder, producing a quick flash followed by a very loud report resembling military artillery. Titanium may be added to the flash powder mix to produce a cloud of bright sparks around the flash. Salutes are commonly used in large quantities during finales to create intense noise and brightness. They are often cylindrical in shape to allow for a larger payload of flash powder, but ball shapes are common and cheaper as well. Salutes are also called Maroons.
A shell containing a fast burning tailed or charcoal star that is burst very hard so that the stars travel in a straight and flat trajectory before slightly falling and burning out. This appears in the sky as a series of radial lines much like the legs of a spider.
An effect created by large, slow-burning stars within a shell that leave a trail of large glittering sparks behind and make a sizzling noise. The "time" refers to the fact that these stars burn away gradually, as opposed to the standard brocade "rain" effect where a large amount of glitter material is released at once.
Similar to a chrysanthemum, but with long-burning silver or gold stars that produce a soft, dome-shaped weeping willow-like effect.
Farfalle is an effect in Italian fireworks with spinning silver sprays in the air.
Similar to a Farfalle but has spinning stars
Fireworks pose risks of injury to people, and of damage, largely as a fire hazard. The explosions added to fireworks may frighten and traumatize animals and people. Wildlife may die while fleeing in a panic and in affected areas birds may abandon nests containing their young.
Fireworks produce smoke and dust that may contain residues of heavy metals, sulfur-coal compounds and some low concentration toxic chemicals. These by-products of fireworks combustion will vary depending on the mix of ingredients of a particular firework. (The color green, for instance, may be produced by adding the various compounds and salts of barium, some of which are toxic, and some of which are not.) Some fishers have noticed and reported to environmental authorities that firework residues can hurt fish and other water-life because some may contain toxic compounds (such as antimony sulfide or arsenic). This is a subject of much debate due to the fact that large-scale pollution from other sources makes it difficult to measure the amount of pollution that comes specifically from fireworks. The possible toxicity of any fallout may also be affected by the amount of black powder used, type of oxidizer, colors produced and launch method.
Perchlorate salts, when in solid form, dissolve and move rapidly in groundwater and surface water. Even in low concentrations in drinking water supplies, perchlorate ions are known to inhibit the uptake of iodine by the thyroid gland. As of 2010, there are no federal drinking water standards for perchlorates in the United States, but the
US Environmental Protection Agency has studied the impacts of perchlorates on the environment as well as drinking water.
Several U.S. states have enacted drinking water standard for perchlorates, including Massachusetts in 2006. California's legislature enacted AB 826, the Perchlorate Contamination Prevention Act of 2003, requiring California's Department of Toxic Substance Control (DTSC) to adopt regulations specifying best management practices for perchlorate-containing substances. The Perchlorate Best Management Practices were adopted on 31 December 2005 and became operative on 1 July 2006. California issued drinking water standards in 2007. Several other states, including Arizona, Maryland, Nevada, New Mexico, New York, and Texas have established non-enforceable, advisory levels for perchlorates.
The courts have also taken action with regard to perchlorate contamination. For example, in 2003, a federal district court in California found that Comprehensive Environmental Response, Compensation and Liability Act (CERCLA) applied because perchlorate is ignitable and therefore a "characteristic" hazardous waste.
Pollutants from fireworks raise concerns because of potential health risks associated with hazardous by-products. For most people the effects of exposure to low levels of toxins from many sources over long periods are unknown. For persons with asthma or multiple chemical sensitivity the smoke from fireworks may aggravate existing health problems.
Pollution is also a concern because fireworks often contain heavy metals as source of color. However, gunpowder smoke and the solid residues are basic, and as such the net effect of fireworks on acid rain is debatable. What is not disputed is that most consumer fireworks leave behind a considerable amount of solid debris, including both readily biodegradable components as well as nondegradable plastic items. Concerns over pollution, consumer safety, and debris have restricted the sale and use of consumer fireworks in many countries. Professional displays, on the other hand, remain popular around the world.
Others argue that alleged concern over pollution from fireworks constitutes a red herring, since the amount of contamination from fireworks is minuscule in comparison to emissions from sources such as the burning of fossil fuels. In the US, some states and local governments restrict the use of fireworks in accordance with the Clean Air Act which allows laws relating to the prevention and control of outdoor air pollution to be enacted. Few governmental entities, by contrast, effectively limit pollution from burning fossil fuels such as diesel fuel or coal. Coal-fueled electricity generation alone is a much greater source of heavy metal contamination in the environment than fireworks.
Some companies within the U.S. fireworks industry claim they are working with Chinese manufacturers to reduce and ultimately hope to eliminate of the pollutant perchlorate.
Fireworks are illegal in most Australian states and territories, unless part of a display by a licensed pyrotechnician and with a permit. However Tasmania, ACT and Northern Territory allow consumer use with a permit (dependent on calendar date and circumstances). On 1 July for Territory Day you can freely use fireworks without a permit in the Northern Territory.
Small novelties such as party poppers and sparklers are legal for consumers across Australia.
On 24 August 2009, the ACT Government announced a complete ban on backyard fireworks.
The use, storage and sale of commercial-grade fireworks in Canada is licensed by Natural Resources Canada's Explosive Regulatory Division (ERD). Unlike their consumer counterpart, commercial-grade fireworks function differently, and come in a wide range of sizes from 50 mm (2 inches) up to 300 mm (11+13⁄16 inches) or more in diameter. Commercial grade fireworks require a Fireworks Operator Certificate (FOC), obtained from the ERD by completing a one-day safety course. There are two categories of FOC: one for pyrotechnics (those used on stage and in movies) and another for display fireworks (those used in dedicated fireworks shows). Each requires completion of its own course, although there are special categories of FOC which allow visiting operators to run their shows with the assistance of a Canadian supervisor.
The display fireworks FOC has two levels: assistant (which allows you to work under a qualified supervisor until you are familiar with the basics), and fully licensed. A fully licensed display fireworks operator can also be further endorsed for marine launch, flying saucers, and other more technically demanding fireworks displays.
The pyrotechnician FOC has three levels: pyrotechnician (which allows work under a supervisor), supervising pyrotechnician, and special effects pyrotechnician (which allows the fabrication of certain types of pyrotechnic devices). Additionally, a special effects pyrotechnician can be endorsed for the use of detonating cord.
Since commercial-grade fireworks are shells which are loaded into separate mortars by hand, there is danger in every stage of the setup. Setup of these fireworks involves the placement and securing of mortars on wooden or wire racks; loading of the shells; and if electronically firing, wiring and testing. The mortars are generally made of FRE (fiber-reinforced epoxy) or HDPE (high-density polyethylene). Older mortars made of sheet steel have been banned by most countries due to the problem of shrapnel produced during a misfire.
Setup of mortars in Canada for an oblong firing site require that a mortar be configured at an angle of 10 to 15 degrees down-range with a safety distance of at least 200 meters (660 ft) down-range and 100 meters (330 ft) surrounding the mortars, plus distance adjustments for wind speed and direction. In June 2007, the ERD approved circular firing sites for use with vertically fired mortars with a safety distance of at least 175-meter (574 ft) radius, plus distance adjustments for wind speed and direction.
Loading of shells is a delicate process, and must be done with caution, and a loader must ensure not only the mortar is clean, but also make sure that no part of their body is directly over the mortar in case of a premature fire. Wiring the shells is a painstaking process; whether the shells are being fired manually or electronically, any "chain fusing" or wiring of electrical ignitors, care must be taken to prevent the fuse (an electrical match, often incorrectly called a squib) from igniting. If the setup is wired electrically, the electrical matches are usually plugged into a "firing rail" or "breakout box" that runs back to the main firing board; from there, the Firing Board is simply hooked up to a car battery, and can proceed with firing the show when ready.
Since commercial-grade fireworks are so much larger and more powerful, setup, and firing crews are always under great pressure to ensure they safely set up, fire, and clean up after a show.
In Chile, the manufacture, importation, possession and use of fireworks is prohibited to unauthorized individuals; only certified firework companies can legally use fireworks. As they are considered a type of explosive, offenders can in principle be tried before military courts, although this is unusual in practice.
The European Union's policy is aimed at harmonising and standardising the EU member states' policies on the regulation of production, transportation, sale, consumption and overall safety of fireworks across Europe.
In Belgium, each municipality can decide how to regulate fireworks. During New Year's Eve, lighting fireworks without a licence is allowed in 35% of the 308 Flemish municipalities, in around 50% a permit from the burgemeester (mayor) is required, and around 14% of municipalities have banned consumer fireworks altogether.
In Finland those under 18 years old haven't been allowed to buy any fireworks since 2009. Safety goggles are required. The use of fireworks is generally allowed on the evening and night of New Year's Eve, 31 December. In some municipalities of Western Finland it is allowed to use fireworks without a fire station's permission on the last weekend of August. With the fire station's permission, fireworks can be used year-round.
In Germany, amateurs over 18 years old are allowed to buy and ignite fireworks of Category F2 for several hours on 31 December and 1 January; each German municipality is authorised to limit the number of hours this may last locally. The sale of Category F3 and F4 fireworks to consumers is prohibited. Lighting fireworks is forbidden near churches, hospitals, retirement homes and wooden or thatch-roofed buildings. All major German cities organise professional fireworks shows.
In addition to the previously existing regulations, there was a nationwide ban on the sale of category F2 fireworks to consumers on New Year's Eve 2020/2021 during the COVID-19 pandemic, with the aim to relieve the burden on hospitals by reducing the number of emergencies due to injuries caused by fireworks on New Year's Eve.
In 2015, the Italian town of Collecchio mandated silent fireworks, being among the first to make the switch without losing the beauty of the visual displays.
In the Netherlands, fireworks cannot be sold to anyone under the age of 16. It may only be sold during a period of three days before a new year. If one of these days is a Sunday, that day is excluded from sale and sale may commence one day earlier.
In the Republic of Ireland, fireworks are illegal and possession is punishable by huge fines and/or prison. However, around Halloween a large amount of fireworks are set off, due to the ease of being able to purchase from Northern Ireland.
In Sweden, fireworks can only be purchased and used by people 18 or older. Fire crackers used to be banned, but are now allowed under European Union fireworks policy.
In Iceland, the Icelandic law states that anyone may purchase and use fireworks during a certain period around New Year's Eve. Most places that sell fireworks in Iceland make their own rules about age of buyers, usually it is around 16. The people of Reykjavík spend enormous sums of money on fireworks, most of which are fired as midnight approaches on 31 December. As a result, every New Year's Eve the city is lit up with fireworks displays.
Fireworks in New Zealand are available from 2 to 5 November, around Guy Fawkes Day, and may be purchased only by those 18 years of age and older (up from 14 years pre-2007). Despite the restriction on when fireworks may be sold, there is no restriction regarding when fireworks may be used. The types of fireworks available to the public are multi-shot "cakes", Roman candles, single shot shooters, ground and wall spinners, fountains, cones, sparklers, and various novelties, such as smoke bombs and Pharaoh's serpents. Consumer fireworks are also not allowed to be louder than 90 decibels.
In Norway, fireworks can only be purchased and used by people 18 or older. Sale is restricted to a few days before New Year's Eve. Rockets are not allowed.
Fireworks in the UK have become more strictly regulated since 1997. Since 2005, the law has been harmonised gradually, in accordance with other EU member state laws.
Fireworks are mostly used in England, Scotland and Wales around Diwali, in late October or early November, and Guy Fawkes Night, 5 November. In the UK, responsibility for the safety of firework displays is shared between the Health and Safety Executive, fire brigades and local authorities. Currently, there is no national system of licensing for fireworks operators, but in order to purchase display fireworks, operators must have licensed explosives storage and public liability insurance.
Fireworks cannot be sold to people under the age of 18 and are not permitted to be set off between 11pm and 7am with exceptions only for:
The maximum legal NEC (net explosive content) of a UK firework available to the public is two kilograms. Jumping jacks, strings of firecrackers, shell firing tubes, bangers and mini-rockets were all banned during the late 1990s. In 2004, single-shot air bombs and bottle rockets were banned, and rocket sizes were limited. From March 2008 any firework with more than 5% flashpowder per tube has been classified 1.3G. The aim of these measures was to eliminate "pocket money" fireworks, and to limit the disruptive effects of loud bangs.
In the United States, the laws governing fireworks vary widely from state to state, or from county to county. Federal, state, and local authorities govern the use of display fireworks in the United States. At the federal level, the Consumer Product Safety Commission (CPSC) regulates consumer fireworks through the Federal Hazardous Substances Act (FHSA). The National Fire Protection Association (NFPA) sets forth a set of codes that give the minimum standards of display fireworks use and safety in the U.S. Both state and local jurisdictions can further add restrictions on the use and safety requirements of display fireworks. There are currently 46 states in the United States in which fireworks are legal for consumer use.
In military terminology, a missile is a guided airborne ranged weapon capable of self-propelled flight usually by a jet engine or rocket motor. Missiles are thus also called guided missiles or guided rockets (when in rocket form). Missiles have five system components: targeting, guidance system, flight system, engine and warhead.  Missiles come in types adapted for different purposes: surface-to-surface and air-to-surface missiles (ballistic, cruise, anti-ship, anti-tank, etc.), surface-to-air missiles (and anti-ballistic), air-to-air missiles, and anti-satellite weapons.
Airborne explosive devices without propulsion are referred to as shells if fired by an artillery piece and bombs if dropped by an aircraft. Unguided jet- or rocket-propelled weapons are usually described as rocket artillery.
Historically, the word missile referred to any projectile that is thrown, shot or propelled towards a target; this usage is still recognized today.
The first missiles to be used operationally were a series of missiles developed by Nazi Germany in World War II. Most famous of these are the V-1 flying bomb and V-2 rocket, both of which used a mechanical autopilot to keep the missile flying along a pre-chosen route. Less well known were a series of Anti-Ship and Anti-aircraft missiles, typically based on a simple radio control (command guidance) system directed by the operator. However, these early systems in World War II were only built in small numbers.
Guided missiles have a number of different system components:
The most common method of guidance is to use some form of radiation, such as infrared, lasers, or radio waves, to guide the missile onto its target. This radiation may emanate from the target (such as the heat of an engine or the radio waves from an enemy radar), it may be provided by the missile itself (such as radar), or it may be provided by a friendly third party (such as the radar of the launch vehicle/platform, or a laser designator operated by friendly infantry). The first two are often known as fire-and-forget as they need no further support or control from the launch vehicle/platform in order to function. Another method is to use TV guidance, with visible light or infrared pictures produced in order to see the target. The pictures may be used either by a human operator who steers the missile onto its target or by a computer doing much the same job. One of the more bizarre guidance methods instead used a pigeon to steer a missile to its target. Some missiles also have a home-on-jam capability to guide itself to a radar-emitting source. Many missiles use a combination of two or more methods to improve accuracy and the chances of successful engagement.
Another method is to target the missile by knowing the location of the target and using a guidance system such as INS, TERCOM, or satellite guidance. This guidance system guides the missile by knowing the missile's current position and the position of the target and then calculating a course between them. This job can also be performed somewhat crudely by a human operator who can see the target and the missile and guide it using either cable- or radio-based remote control, or by an automatic system that can simultaneously track the target and the missile.
Furthermore, some missiles use initial targeting, sending them to a target area, where they will switch to primary targeting, using either radar or IR targeting to acquire the target.
Whether a guided missile uses a targeting system, a guidance system or both, it needs a flight system. The flight system uses the data from the targeting or guidance system to maneuver the missile in flight, allowing it to counter inaccuracies in the missile or to follow a moving target. There are two main systems: vectored thrust (for missiles that are powered throughout the guidance phase of their flight) and aerodynamic maneuvering (wings, fins, canard (aeronautics), etc.).
Missiles are powered by an engine, generally either a type of rocket engine or jet engine. Rockets are generally of the solid-propellant type for ease of maintenance and fast deployment, although some larger ballistic missiles use liquid-propellant rockets. Jet engines are generally used in cruise missiles, most commonly of the turbojet type, due to its relative simplicity and low frontal area. Turbofans and ramjets are the only other common forms of jet engine propulsion, although any type of engine could theoretically be used. Long-range missiles may have multiple engine stages, particularly in those launched from the surface. These stages may all be of similar types or may include a mix of engine types − for example, surface-launched cruise missiles often have a rocket booster for launching and a jet engine for sustained flight.
Some missiles may have additional propulsion from another source at launch; for example, the V1 was launched by a catapult, and the MGM-51 Shillelagh was fired out of a tank gun (using a smaller charge than would be used for a shell).
Missiles generally have one or more explosive warheads, although other weapon types may also be used. The warheads of a missile provide its primary destructive power (many missiles have extensive secondary destructive power due to the high kinetic energy of the weapon and unburnt fuel that may be on board). Warheads are most commonly of the high explosive type, often employing shaped charges to exploit the accuracy of a guided weapon to destroy hardened targets. Other warhead types include submunitions, incendiaries, nuclear weapons, chemical, biological or radiological weapons or kinetic energy penetrators. Warheadless missiles are often used for testing and training purposes.
Missiles are generally categorized by their launch platform and intended target. In broadest terms, these will either be surface (ground or water) or air, and then sub-categorized by range and the exact target type (such as anti-tank or anti-ship). Many weapons are designed to be launched from both surface or the air, and a few are designed to attack either surface or air targets (such as the ADATS missile). Most weapons require some modification in order to be launched from the air or surface, such as adding boosters to the surface-launched version.
After the boost stage, ballistic missiles follow a trajectory mainly determined by ballistics. The guidance is for relatively small deviations from that.
Ballistic missiles are largely used for land attack missions. Although normally associated with nuclear weapons, some conventionally armed ballistic missiles are in service, such as MGM-140 ATACMS. The V2 had demonstrated that a ballistic missile could deliver a warhead to a target city with no possibility of interception, and the introduction of nuclear weapons meant it could efficiently do damage when it arrived. The accuracy of these systems was fairly poor, but post-war development by most military forces improved the basic Inertial navigation system concept to the point where it could be used as the guidance system on Intercontinental ballistic missiles flying thousands of kilometers. Today, the ballistic missile represents the only strategic deterrent in most military forces; however, some ballistic missiles are being adapted for conventional roles, such as the Russian Iskander or the Chinese DF-21D anti-ship ballistic missile. Ballistic missiles are primarily surface-launched from mobile launchers, silos, ships or submarines, with air launch being theoretically possible with a weapon such as the cancelled Skybolt missile.
The Russian Topol M (SS-27 Sickle B) is the fastest (7,320 m/s) missile currently in service.
The V1 had been successfully intercepted during World War II, but this did not make the cruise missile concept entirely useless. After the war, the US deployed a small number of nuclear-armed cruise missiles in Germany, but these were considered to be of limited usefulness. Continued research into much longer-ranged and faster versions led to the US's SM-64 Navaho and its Soviet counterparts, the Burya and Buran cruise missile. However, these were rendered largely obsolete by the ICBM, and none were used operationally. Shorter-range developments have become widely used as highly accurate attack systems, such as the US Tomahawk missile and Russian Kh-55. Cruise missiles are generally further divided into subsonic or supersonic weapons - supersonic weapons such as BrahMos (India, Russia) are difficult to shoot down, whereas subsonic weapons tend to be much lighter and cheaper, allowing more to be fired.
Cruise missiles are generally associated with land-attack operations, but also have an important role as anti-shipping weapons. They are primarily launched from air, sea or submarine platforms in both roles, although land-based launchers also exist.
Another major German missile development project was the anti-shipping class (such as the Fritz X and Henschel Hs 293), intended to stop any attempt at a cross-channel invasion. However, the British were able to render their systems useless by jamming their radios, and missiles with wire guidance were not ready by D-Day. After the war, the anti-shipping class slowly developed and became a major class in the 1960s with the introduction of the low-flying jet- or rocket-powered cruise missiles known as "sea-skimmers". These became famous during the Falklands War, when an Argentine Exocet missile disabled a Royal Navy destroyer.
A number of anti-submarine missiles also exist; these generally use the missile in order to deliver another weapon system such as a torpedo or depth charge to the location of the submarine, at which point the other weapon will conduct the underwater phase of the mission.
By the end of WWII, all forces had widely introduced unguided rockets using high-explosive anti-tank warheads as their major anti-tank weapon (see Panzerfaust, Bazooka). However, these had a limited useful range of 100 m or so, and the Germans were looking to extend this with the use of a missile using wire guidance, the X-7. After the war, this became a major design class in the later 1950s and, by the 1960s, had developed into practically the only non-tank anti-tank system in general use. During the 1973 Yom Kippur War between Israel and Egypt, the 9M14 Malyutka (aka Sagger) man-portable anti-tank missile proved potent against Israeli tanks. While other guidance systems have been tried, the basic reliability of wire guidance means this will remain the primary means to control anti-tank missiles in the near future. Anti-tank missiles may be launched from aircraft, vehicles or by ground troops in the case of smaller weapons.
By 1944, US and British air forces were sending huge air fleets over occupied Europe, increasing the pressure on the Luftwaffe day and night fighter forces. The Germans were keen to get some sort of useful ground-based anti-aircraft system into operation. Several systems were under development, but none had reached operational status before the war's end. The US Navy also started missile research to deal with the Kamikaze threat. By 1950, systems based on this early research started to reach operational service, including the US Army's MIM-3 Nike Ajax and the Navy's "3T's" (Talos, Terrier, Tartar), soon followed by the Soviet S-25 Berkut and S-75 Dvina and French and British systems. Anti-aircraft weapons exist for virtually every possible launch platform, with surface-launched systems ranging from huge, self-propelled or ship-mounted launchers to man-portable systems. Subsurface-to-air missiles are usually launched from below water (usually from submarines).
Like most missiles, the S-300, S-400, Advanced Air Defence and MIM-104 Patriot are for defense against short-range missiles and carry explosive warheads.
In the case of a large closing speed, a projectile without explosives is used; just a collision is sufficient to destroy the target. See Missile Defense Agency for the following systems being developed:
Used for the first time by Soviet pilots in the summer of 1939 during the Battle of Khalkhin Gol. On August 20, 1939, the Japanese Nakajima Ki-27 fighter was attacked by the Soviet Polikarpov I-16 fighter of Captain N. Zvonarev. He fired a rocket salvo from a distance of about a kilometer, after which the Ki-27 crashed to the ground. A group of Polikarpov I-16 fighters under command of Captain N. Zvonarev were using RS-82 rockets against Japanese aircraft, shooting down 16 fighters and 3 bombers in total.
German experience in World War II demonstrated that destroying a large aircraft was quite difficult, and they had invested considerable effort into air-to-air missile systems to do this. Their Messerschmitt Me 262's jets often carried R4M rockets, and other types of "bomber destroyer" aircraft had unguided rockets as well. In the post-war period, the R4M served as the pattern for a number of similar systems, used by almost all interceptor aircraft during the 1940s and 1950s. Most rockets (except for the AIR-2 Genie, due to its nuclear warhead with a large blast radius) had to be carefully aimed at relatively close range to hit the target successfully. The United States Navy and U.S. Air Force began deploying guided missiles in the early 1950s, most famous being the US Navy's AIM-9 Sidewinder and the USAF's AIM-4 Falcon. These systems have continued to advance, and modern air warfare consists almost entirely of missile firing. In the Falklands War, less powerful British Harriers were able to defeat faster Argentinian opponents using American AIM-9L missiles. The latest heat-seeking designs can lock onto a target from various angles, not just from behind, where the heat signature from the engines is strongest. Other types rely on radar guidance (either on board or "painted" by the launching aircraft). Air-to-air missiles also have a wide range of sizes, ranging from helicopter-launched self-defense weapons with a range of a few kilometers, to long-range weapons designed for interceptor aircraft such as the R-37 (missile).
In the 1950s and 1960s, Soviet designers started work on an anti-satellite weapon as part of the Istrebitel Sputnikov program ("istrebitel sputnikov" literally means "destroyer of satellites"). After a lengthy development process of roughly twenty years, it was finally decided that the testing of these weapons be canceled. This was when the United States started testing their own systems. The Brilliant Pebbles defense system proposed during the 1980s would have used kinetic energy collisions without explosives. Anti-satellite weapons may be launched either by an aircraft or a surface platform, depending on the design. To date, only a few known tests have occurred. As of 2019, only 4 countries - China, India, United States, and Russia have operational anti-satellite weapons.
A weapon, arm or armament is any implement or device that can be used to inflict physical damage, harm or kill. Weapons are used to increase the efficacy and efficiency of activities such as hunting, crime, law enforcement, self-defense, and warfare, as well as murder or suicide. In broader context, weapons may be construed to include anything used to gain a tactical, strategic, material or mental advantage over an adversary or enemy target.
While ordinary objects – sticks, rocks, bottles, chairs, vehicles – can be used as weapons, many are expressly designed for the purpose; these range from simple implements such as clubs, axes and swords, to complicated modern firearms, tanks, intercontinental ballistic missiles, biological weapons, and cyberweapons. Something that has been re-purposed, converted, or enhanced to become a weapon of war is termed weaponized, such as a weaponized virus or weaponized laser.
The use of weapons is a major driver of cultural evolution and human history up to today, since weapons are a type of tool which is used to dominate and subdue autonomous agents such as animals and by that allow for an expansion of the cultural niche, while simultaneously other weapon users (i.e., agents such as humans, groups, cultures) are able to adapt to weapons of enemies by learning, triggering a continuous process of competitive technological, skill and cognitive improvement (arms race).
The use of objects as weapons has been observed among chimpanzees, leading to speculation that early hominids used weapons as early as five million years ago. However, this can not be confirmed using physical evidence because wooden clubs, spears, and unshaped stones would have left an ambiguous record. The earliest unambiguous weapons to be found are the Schöningen spears, eight wooden throwing spears dating back more than 300,000 years. At the site of Nataruk in Turkana, Kenya, numerous human skeletons dating to 10,000 years ago may present evidence of traumatic injuries to the head, neck, ribs, knees and hands, including obsidian projectiles embedded in the bones that might have been caused from arrows and clubs during conflict between two hunter-gatherer groups. But the evidence interpretation of warfare at Nataruk has been challenged.
The earliest ancient weapons were evolutionary improvements of late neolithic implements, but significant improvements in materials and crafting techniques led to a series of revolutions in military technology.
The development of metal tools began with copper during the Copper Age (about 3,300 BC) and was followed by the Bronze Age, leading to the creation of the Bronze Age sword and similar weapons.
During the Bronze Age, the first defensive structures and fortifications appeared as well, indicating an increased need for security. Weapons designed to breach fortifications followed soon after, such as the battering ram, which was in use by 2500 BC.
The development of iron-working around 1300 BC in Greece had an important impact on the development of ancient weapons. It was not the introduction of early Iron Age swords, however, as they were not superior to their bronze predecessors,
but rather the domestication of the horse and widespread use of spoked wheels by c. 2000 BC. This led to the creation of the light, horse-drawn chariot, whose improved mobility proved important during this era. Spoke-wheeled chariot usage peaked around 1300 BC and then declined, ceasing to be militarily relevant by the 4th century BC.
Cavalry developed once horses were bred to support the weight of a human. The horse extended the range and increased the speed of attacks.
In addition to land based weaponry, warships, such as the trireme, were in use by the 7th century BC.
European warfare during the Post-classical history was dominated by elite groups of knights supported by massed infantry (both in combat and ranged roles). They were involved in mobile combat and sieges which involved various siege weapons and tactics. Knights on horseback developed tactics for charging with lances providing an impact on the enemy formations and then drawing more practical weapons (such as swords) once they entered into the melee. By contrast, infantry, in the age before structured formations, relied on cheap, sturdy weapons such as spears and billhooks in close combat and bows from a distance. As armies became more professional, their equipment was standardized and infantry transitioned to pikes. Pikes are normally seven to eight feet in length, and used in conjunction with smaller side-arms (short sword).
In Eastern and Middle Eastern warfare, similar tactics were developed independent of European influences.
The introduction of gunpowder from the Asia at the end of this period revolutionized warfare. Formations of musketeers, protected by pikemen came to dominate open battles, and the cannon replaced the trebuchet as the dominant siege weapon.
The European Renaissance marked the beginning of the implementation of firearms in western warfare. Guns and rockets were introduced to the battlefield.
Firearms are qualitatively different from earlier weapons because they release energy from combustible propellants such as gunpowder, rather than from a counter-weight or spring. This energy is released very rapidly and can be replicated without much effort by the user. Therefore even early firearms such as the arquebus were much more powerful than human-powered weapons. Firearms became increasingly important and effective during the 16th century to 19th century, with progressive improvements in ignition mechanisms followed by revolutionary changes in ammunition handling and propellant. During the U.S. Civil War new applications of firearms including the machine gun and ironclad warship emerged that would still be recognizable and useful military weapons today, particularly in limited conflicts. In the 19th century warship propulsion changed from sail power to fossil fuel-powered steam engines.
Since the mid-18th century North American French-Indian war through the beginning of the 20th century, human-powered weapons were reduced from the primary weaponry of the battlefield yielding to gunpowder-based weaponry. Sometimes referred to as the "Age of Rifles", this period was characterized by the development of firearms for infantry and cannons for support, as well as the beginnings of mechanized weapons such as the machine gun.
Of particular note, Howitzers were able to destroy masonry fortresses and other fortifications, and this single invention caused a Revolution in Military Affairs (RMA), establishing tactics and doctrine that are still in use today. See Technology during World War I for a detailed discussion.
An important feature of industrial age warfare was technological escalation – innovations were rapidly matched through replication or countered by another innovation. The technological escalation during World War I was profound, including the wide introduction of aircraft into warfare, and naval warfare with the introduction of aircraft carriers.
World War I marked the entry of fully industrialized warfare as well as weapons of mass destruction (e.g., chemical and biological weapons), and new weapons were developed quickly to meet wartime needs. Above all, it promised to the military commanders the independence from the horse and the resurgence in maneuver warfare through extensive use of motor vehicles. The changes that these military technologies underwent before and during the Second World War were evolutionary, but defined the development for the rest of the century.
This period of innovation in weapon design continued in the inter-war period (between WW I and WW II) with continuous evolution of weapon systems by all major industrial powers.  The major armament firms were the Schneider-Creusot (based in France), the Škoda Works (Czechoslovakia), and Vickers (Great Britain). The 1920s were committed to disarmament and outlawing of war and poison gas, but rearmament picked up rapidly in the 1930s. The munitions makers responded nimbly to the rapidly shifting strategic and economic landscape. The main purchasers of munitions from the big three companies were Romania, Yugoslavia, Greece, and Turkey—and, to a lesser extent, in Poland, Finland, the Baltic States, and the Soviet Union.
Realistic critics understood that war could not really be outlawed, but its worst excesses might be banned. Poison gas became the focus of a worldwide crusade in the 1920s.  Poison gas did not win battles, and the generals did not want it. The soldiers hated it far more intensely than bullets or explosive shells. By 1918, chemical shells made up 35 per cent of French ammunition supplies, 25 per cent of British, and 20 per cent of the American stock. The “Protocol for the Prohibition of the Use in War of Asphyxiating, Poisonous, or Other Gases and of Bacteriological Methods of Warfare”  was issued in 1925, and was accepted as policy by all major countries. In 1937 poison gas was manufactured in large quantities, but not used except against nations that lacked modern weapons or gas masks.
Many modern military weapons, particularly ground-based ones, are relatively minor improvements of weapon systems developed during World War II. See military technology during World War II for a detailed discussion.
World War II however, perhaps marked the most frantic period of weapons development in the history of humanity. Massive numbers of new designs and concepts were fielded, and all existing technologies were improved between 1939 and 1945. The most powerful weapon invented during this period was the atomic bomb, however many other weapons influenced the world, such as jet planes and radar, but were overshadowed by the visibility of nuclear weapons and long-range rockets.
Since the realization of mutually assured destruction (MAD), the nuclear option of all-out war is no longer considered a survivable scenario.  During the Cold War in the years following World War II, both the United States and the Soviet Union engaged in a nuclear arms race. Each country and their allies continually attempted to out-develop each other in the field of nuclear armaments. Once the joint technological capabilities reached the point of being able to ensure the destruction of the Earth x100 fold, then a new tactic had to be developed.  With this realization, armaments development funding shifted back to primarily sponsoring the development of conventional arms technologies for support of limited wars rather than total war.
The arms industry is a global industry that involves the sales and manufacture of weaponry. It consists of a commercial industry involved in the research and development, engineering, production, and servicing of military material, equipment, and facilities. Many industrialized countries have a domestic arms-industry to supply their own military forces - and some also have a substantial trade in weapons for use by its citizens, for self-defence, hunting or sporting purposes.
Contracts to supply a given country's military are awarded by governments, making arms contracts of substantial political importance. The link between politics and the arms trade can result in the development a "military–industrial complex", where the armed forces, commerce, and politics become closely linked.
According to research institute SIPRI, the volume of international transfers of major weapons in 2010–14 was 16 percent higher than in 2005–2009, and the arms sales of the world’s 100 largest private arms-producing and military services companies totalled  $420 billion in 2018.
The production, possession, trade and use of many weapons are controlled. This may be at a local or central government level, or international treaty. Examples of such controls include:
All countries have laws and policies regulating aspects such as the manufacture, sale, transfer, possession, modification and use of small arms by civilians.
Countries which regulate access to firearms will typically restrict access to certain categories of firearms and then restrict the categories of persons who may be granted a license for access to such firearms. There may be separate licenses for hunting, sport shooting (a.k.a. target shooting), self-defense, collecting, and concealed carry, with different sets of requirements, permissions, and responsibilities.
International treaties and agreements place restrictions upon the development, production, stockpiling, proliferation and usage of weapons from small arms and heavy weapons to weapons of mass destruction. Arms control is typically exercised through the use of diplomacy which seeks to impose such limitations upon consenting participants, although it may also comprise efforts by a nation or group of nations to enforce limitations upon a non-consenting country.
Arms trafficking is the trafficking of contraband weapons and ammunition. What constitutes legal trade in firearms varies widely, depending on local and national laws.
There are a number of issue around the potential ongoing risks from deployed weapons, the safe storage of weapons, and their eventual disposal when no longer effective or safe.
Strange and exotic weapons are a recurring feature or theme in science fiction. In some cases, weapons first introduced in science fiction have now been made a reality. Other science fiction weapons remain purely fictional, and are often beyond the realms of known physical possibility.
At its most prosaic, science fiction features an endless variety of sidearms, mostly variations on real weapons such as guns and swords. Among the best-known of these are the phaser used in the Star Trek television series, films and novels and the lightsaber and blaster featured in the Star Wars movies, comics, novels and TV series.
In addition to adding action and entertainment value, weaponry in science fiction sometimes become themes when they touch on deeper concerns, often motivated by contemporary issues. One example is science fiction that deals with weapons of mass destruction.
→
In aircraft, an ejection seat or ejector seat is a system designed to rescue the pilot or other crew of an aircraft (usually military) in an emergency. In most designs, the seat is propelled out of the aircraft by an explosive charge or rocket motor, carrying the pilot with it. The concept of an ejectable escape crew capsule has also been tried. Once clear of the aircraft, the ejection seat deploys a parachute. Ejection seats are common on certain types of military aircraft.
A bungee-assisted escape from an aircraft took place in 1910. In 1916, Everard Calthrop, an early inventor of parachutes, patented an ejector seat using compressed air.
The modern layout for an ejection seat was first introduced  by Romanian inventor Anastase Dragomir in the late 1920s. The design featured a parachuted cell (a dischargeable chair from an aircraft or other vehicle).  It was successfully tested on 25 August 1929 at the Paris-Orly Airport near Paris and in October 1929 at Băneasa, near Bucharest. Dragomir patented his "catapult-able cockpit" at the French Patent Office.
The design was perfected during World War II. Prior to this, the only means of escape from an incapacitated aircraft was to jump clear ("bail out"), and in many cases this was difficult due to injury, the difficulty of egress from a confined space, g forces, the airflow past the aircraft, and other factors.
The first ejection seats were developed independently during World War II by Heinkel and SAAB. Early models were powered by compressed air and the first aircraft to be fitted with such a system was the Heinkel He 280 prototype jet-engined fighter in 1940. One of the He 280 test pilots, Helmut Schenk, became the first person to escape from a stricken aircraft with an ejection seat on 13 January 1942 after his control surfaces iced up and became inoperative.  The fighter had been being used in tests of the Argus As 014 impulse jets for Fieseler Fi 103 missile development.  It had its usual Heinkek HeS 8A turbojets removed, and was towed aloft from the Erprobungsstelle Rechlin central test facility of the Luftwaffe in Germany by a pair of Messerschmitt Bf 110C tugs in a heavy snow-shower.  At 7,875 ft (2,400 m), Schenk found he had no control, jettisoned his towline, and ejected. The He 280 was never put into production status. The first operational type built anywhere to provide ejection seats for the crew was the Heinkel He 219 Uhu night fighter in 1942.
In Sweden, a version using compressed air was tested in 1941.  A gunpowder ejection seat was developed by Bofors and tested in 1943 for the Saab 21.  The first test in the air was on a Saab 17 on 27 February 1944, and the first real use occurred by Lt. Bengt Johansson on 29 July 1946 after a mid-air collision between a J 21 and a J 22.
As the first operational military jet in late 1944 to ever feature one, the winner of the German Volksjäger "people's fighter" home defense jet fighter design competition; the lightweight Heinkel He 162A Spatz, featured a new type of ejection seat, this time fired by an explosive cartridge. In this system, the seat rode on wheels set between two pipes running up the back of the cockpit.  When lowered into position, caps at the top of the seat fitted over the pipes to close them.  Cartridges, basically identical to shotgun shells, were placed in the bottom of the pipes, facing upward.  When fired, the gases would fill the pipes, "popping" the caps off the end, and thereby forcing the seat to ride up the pipes on its wheels and out of the aircraft.  By the end of the war, the Dornier Do 335 Pfeil — primarily from it having a rear-mounted engine (of the twin engines powering the design) powering a pusher propeller located at the aft end of the fuselage presenting a hazard to a normal "bailout" escape — and a few late-war prototype aircraft were also fitted with ejection seats.
After World War II, the need for such systems became pressing, as aircraft speeds were getting ever higher, and it was not long before the sound barrier was broken.  Manual escape at such speeds would be impossible. The United States Army Air Forces experimented with downward-ejecting systems operated by a spring, but it was the work of James Martin and his company Martin-Baker that proved crucial.
The first live flight test of the Martin-Baker system took place on 24 July 1946, when fitter Bernard Lynch ejected from a Gloster Meteor Mk III jet. Shortly afterward, on 17 August 1946, 1st Sgt. Larry Lambert was the first live U.S. ejectee. Lynch demonstrated the ejection seat at the Daily Express Air Pageant in 1948, ejecting from a Meteor. Martin-Baker ejector seats were fitted to prototype and production aircraft from the late 1940s, and the first emergency use of such a seat occurred in 1949 during testing of the jet-powered Armstrong Whitworth A.W.52 experimental flying wing.
Early seats used a solid propellant charge to eject the pilot and seat by igniting the charge inside a telescoping tube attached to the seat.  As aircraft speeds increased still further, this method proved inadequate to get the pilot sufficiently clear of the airframe.  Increasing the amount of propellant risked damaging the occupant's spine, so experiments with rocket propulsion began.  In 1958, the Convair F-102 Delta Dagger was the first aircraft to be fitted with a rocket-propelled seat. Martin-Baker developed a similar design, using multiple rocket units feeding a single nozzle.  The greater thrust from this configuration had the advantage of being able to eject the pilot to a safe height even if the aircraft was on or very near the ground.
In the early 1960s, deployment of rocket-powered ejection seats designed for use at supersonic speeds began in such planes as the Convair F-106 Delta Dart.  Six pilots have ejected at speeds exceeding 700 knots (1,300 km/h; 810 mph). The highest altitude at which a Martin-Baker seat was deployed was 57,000 ft (17,400 m) (from a Canberra bomber in 1958). Following an accident on 30 July 1966 in the attempted launch of a D-21 drone, two Lockheed M-21 crew members ejected at Mach 3.25 at an altitude of 80,000 ft (24,000 m).  The pilot was recovered successfully, but the launch control officer drowned after a water landing.  Despite these records, most ejections occur at fairly low speeds and altitudes, when the pilot can see that there is no hope of regaining aircraft control before impact with the ground.
Late in the Vietnam War, the U.S. Air Force and U.S. Navy became concerned about its pilots ejecting over hostile territory and those pilots either being captured or killed and the losses in men and aircraft in attempts to rescue them. Both services began a program titled Air Crew Escape/Rescue Capability or Aerial Escape and Rescue Capability (AERCAB) ejection seats (both terms have been used by the US military and defence industry), where after the pilot ejected, the ejection seat would fly them to a location far enough away from where they ejected to where they could safely be picked up.  A Request for Proposals for concepts for AERCAB ejection seats were issued in the late 1960s.  Three companies submitted papers for further development: A Rogallo wing design by Bell Systems; a gyrocopter design by Kaman Aircraft; and a mini-conventional fixed wing aircraft employing a Princeton Wing (i.e. a wing made of flexible material that rolls out and then becomes rigid by means of internal struts or supports etc. deploying) by Fairchild Hiller.  All three, after ejection, would be propelled by small turbojet engine developed for target drones.  With the exception of the Kaman design, the pilot would still be required to parachute to the ground after reaching a safety-point for rescue.  The AERCAB project was terminated in the 1970s with the end of the Vietnam War.  The Kaman design, in early 1972, was the only one which was to reach the hardware stage. It came close to being tested with a special landing-gear platform attached to the AERCAB ejection seat for first-stage ground take offs and landings with a test pilot.
The purpose of an ejection seat is pilot survival. The pilot typically experiences an acceleration of about 12–14g. Western seats usually impose lighter loads on the pilots; 1960s–70s era Soviet technology often goes up to 20–22 g (with SM-1 and KM-1 gunbarrel-type ejection seats). Compression fractures of vertebrae are a recurrent side effect of ejection.
It was theorised early on that ejection at supersonic speeds would be unsurvivable; extensive tests, including Project Whoosh with chimpanzee test subjects, were undertaken to determine that it was feasible.
The capabilities of the NPP Zvezda K-36 were unintentionally demonstrated at the Fairford Air Show on 24 July 1993 when the pilots of two MiG-29 fighters ejected after a mid-air collision.
The minimal ejection altitude for ACES II seat in inverted flight is about 140 feet (43 m) above ground level at 150 KIAS, while the Russian counterpart – K-36DM has the minimal ejection altitude from inverted flight of 100 feet (30 m) AGL.
When an aircraft is equipped with the NPP Zvezda K-36DM ejection seat and the pilot is wearing the КО-15 protective gear, they are able to eject at airspeeds from 0 to 1,400 kilometres per hour (870 mph) and altitudes of 0 to 25 km (16 mi or about 82,000 ft). The K-36DM ejection seat features drag chutes and a small shield that rises between the pilot's legs to deflect air around the pilot.
Pilots have successfully ejected from underwater in a handful of instances, after being forced to ditch in water. Documented evidence exists that pilots of the US and Indian navies have performed this feat.
As of 20 June 2011 – when two Spanish Air Force pilots ejected over San Javier airport – the number of lives saved by Martin-Baker products was 7,402 from 93 air forces. The company runs a club called the "Ejection Tie Club" and gives survivors a unique tie and lapel pin. The total figure for all types of ejection seats is unknown, but may be considerably higher.
Early models of the ejection seat were equipped with only an overhead ejection handle which doubled in function by forcing the pilot to assume the right posture and by having them pull a screen down to protect both their face and oxygen mask from the subsequent air blast. Martin Baker added a secondary handle in the front of the seat to allow ejection even when pilots weren't able to reach upwards because of high g-force. Later (e.g. in Martin Baker's MK9) the top handle was discarded because the lower handle had proven easier to operate and the technology of helmets had advanced to also protect from the air blast.
The "standard" ejection system operates in two stages.  First, the entire canopy or hatch above the aviator is opened, shattered, or jettisoned, and the seat and occupant are launched through the opening.  In most earlier aircraft this required two separate actions by the aviator, while later egress system designs, such as the Advanced Concept Ejection Seat model 2 (ACES II), perform both functions as a single action.
The ACES II ejection seat is used in most American-built fighters. The A-10 uses connected firing handles that activate both the canopy jettison systems, followed by the seat ejection. The F-15 has the same connected system as the A-10 seat. Both handles accomplish the same task, so pulling either one suffices. The F-16 has only one handle located between the pilot's knees, since the cockpit is too narrow for side-mounted handles.
Non-standard egress systems include Downward Track (used for some crew positions in bomber aircraft, including the B-52 Stratofortress), Canopy Destruct (CD) and Through-Canopy Penetration (TCP), Drag Extraction, Encapsulated Seat, and even Crew Capsule.
Early models of the F-104 Starfighter were equipped with a Downward Track ejection seat due to the hazard of the T-tail. In order to make this work, the pilot was equipped with "spurs" which were attached to cables that would pull the legs inward so the pilot could be ejected. Following this development, some other egress systems began using leg retractors as a way to prevent injuries to flailing legs, and to provide a more stable center of gravity. Some models of the F-104 were equipped with upward-ejecting seats.
Similarly, two of the six ejection seats on the B-52 Stratofortress fire downward, through hatch openings on the bottom of the aircraft; the downward hatches are released from the aircraft by a thruster that unlocks the hatch, while gravity and wind remove the hatch and arm the seat. The four seats on the forward upper deck (two of them, EWO and Gunner, facing the rear of the airplane) fire upwards as usual. Any such downward-firing system is of no use on or near the ground if aircraft is in level flight at the time of the ejection.
Aircraft designed for low-level use sometimes have ejection seats which fire through the canopy, as waiting for the canopy to be ejected is too slow. Many aircraft types (e.g., the BAE Hawk and the Harrier line of aircraft) use Canopy Destruct systems, which have an explosive cord (MDC – Miniature Detonation Cord or FLSC – Flexible Linear Shaped Charge) embedded within the acrylic plastic of the canopy. The MDC is initiated when the eject handle is pulled, and shatters the canopy over the seat a few milliseconds before the seat is launched. This system was developed for the Hawker Siddeley Harrier family of VTOL aircraft as ejection may be necessary while the aircraft was in the hover, and jettisoning the canopy might result in the pilot and seat striking it. This system is also used in the T-6 Texan II and F-35 Lightning II.
Through-Canopy Penetration is similar to Canopy Destruct, but a sharp spike on the top of the seat, known as the "shell tooth", strikes the underside of the canopy and shatters it. The A-10 Thunderbolt II is equipped with canopy breakers on either side of its headrest in the event that the canopy fails to jettison. The T-6 is also equipped with such breakers if the MDC fails to detonate. In ground emergencies, a ground crewman or pilot can use a breaker knife attached to the inside of the canopy to shatter the transparency. The A-6 Intruder and EA-6B Prowler seats were capable of ejecting through the canopy, with canopy jettison a separate option if there is enough time.
CD and TCP systems cannot be used with canopies made of flexible materials, such as the Lexan polycarbonate canopy used on the F-16.
Soviet VTOL naval fighter planes such as the Yakovlev Yak-38 were equipped with ejection seats which were automatically activated during at least some part of the flight envelope.
Drag Extraction is the lightest and simplest egress system available, and has been used on many experimental aircraft. Halfway between simply "bailing out" and using explosive-eject systems, Drag Extraction uses the airflow past the aircraft (or spacecraft) to move the aviator out of the cockpit and away from the stricken craft on a guide rail.  Some operate like a standard ejector seat, by jettisoning the canopy, then deploying a drag chute into the airflow.  That chute pulls the occupant out of the aircraft, either with the seat or following release of the seat straps, who then rides off the end of a rail extending far enough out to help clear the structure.  In the case of the Space Shuttle, the astronauts would have ridden a long, curved rail, blown by the wind against their bodies, then deployed their chutes after free-falling to a safe altitude.
Encapsulated Seat egress systems were developed for use in the B-58 Hustler and B-70 Valkyrie supersonic bombers.  These seats were enclosed in an air-operated clamshell, which permitted the aircrew to escape at airspeeds and altitudes high enough to otherwise cause bodily harm.  These seats were designed to allow the pilot to control the plane even with the clamshell closed, and the capsule would float in case of water landings.
Some aircraft designs, such as the General Dynamics F-111, do not have individual ejection seats, but instead, the entire section of the airframe containing the crew can be ejected as a single capsule. In this system, very powerful rockets are used, and multiple large parachutes are used to bring the capsule down, in a manner similar to the Launch Escape System of the Apollo spacecraft. On landing, an airbag system is used to cushion the landing, and this also acts as a flotation device if the Crew Capsule lands in water.
A zero-zero ejection seat is designed to safely extract upward and land its occupant from a grounded stationary position (i.e., zero altitude and zero airspeed), specifically from aircraft cockpits. The zero-zero capability was developed to help aircrews escape upward from unrecoverable emergencies during low-altitude and/or low-speed flight, as well as ground mishaps. Parachutes require a minimum altitude for opening, to give time for deceleration to a safe landing speed. Thus, prior to the introduction of zero-zero capability, ejections could only be performed above minimum altitudes and airspeeds. If the seat was to work from zero (aircraft) altitude, the seat would have to lift itself to a sufficient altitude.
These early seats were fired from the aircraft with a cannon, providing the high impulse needed over the very short length on the cannon barrel within the seat. This limited the total energy, and thus the additional height possible, as otherwise the high forces needed would crush the pilot.
Modern zero-zero technology use small rockets to propel the seat upward to an adequate altitude and a small explosive charge to open the parachute canopy quickly for a successful parachute descent, so that proper deployment of the parachute no longer relies on airspeed and altitude. The seat cannon clears the seat from the aircraft, then the under-seat rocket pack fires to lift the seat to altitude. As the rockets fire for longer than the cannon, they do not require the same high forces. Zero-zero rocket seats also reduced forces on the pilot during any ejection, reducing injuries and spinal compression.
The Kamov Ka-50, which entered limited service with Russian forces in 1995, was the first production helicopter with an ejection seat. The system is similar to that of a conventional fixed-wing aircraft; however the main rotors are equipped with explosive bolts to jettison the blades moments before the seat is fired.
The only commercial jetliner ever fitted with ejection seats was the Soviet Tupolev Tu-144. However, the seats were present in the prototype only, and were only available for the crew and not the passengers. The  Tu-144 that crashed at the Paris Air Show in 1973 was a production model, and did not have ejection seats.
The Lunar Landing Research Vehicle, (LLRV) and its successor Lunar Landing Training Vehicle (LLTV), used ejection seats. Neil Armstrong ejected on 6 May 1968; following Joe Algranti and Stuart M. Present.
The only spacecraft ever flown with installed ejection seats were Vostok, Gemini, and the Space Shuttle.
Early flights of the Space Shuttle, which used Columbia, were with a crew of two, both provided with ejector seats (STS-1 to STS-4), but the seats were disabled and then removed as the crew size was increased. Columbia and Enterprise were the only two Space Shuttle orbiters fitted with ejection seats. The Buran-class orbiters were planned to be fitted with K-36RB (K-36M-11F35) seats, but as the program was canceled, the seats were never used.
No real life land vehicle has ever been fitted with an ejection seat, though it is a common trope in fiction. A notable example is the Aston Martin DB5 from the James Bond films, which had an ejecting passenger seat.

A launch vehicle or carrier rocket can carry a payload from the surface to outer space, such as spacecraft and satellites. They are often operated with extensive infrastructure such as launch pads, vehicle assembly, fueling systems, range safety, etc. The difficulties of spaceflight demand launch vehicles to be engineered with very advanced aerodynamics and technologies – a big contributor to the vehicle's expensive operating cost.
A suborbital vehicle only need to accelerate near the boundary of space, commonly defined as 100 km (62 mi) of the Kármán line. However, orbital launch vehicles from Earth must reach much higher speed, which makes most practical vehicles to be multistage rocket and use chemical propellant, such as solid propellant mixtures, liquid hydrogen, kerosene, oxygen, hypergolics, etc. These launch vehicles can be classified by their payload capacity, ranging from small-, medium-, heavy- to super-heavy lift.
Launch vehicles are classed by NASA according to low Earth orbit payload capability:
Sounding rockets are similar to small-lift launch vehicles, however they are usually even smaller and do not place payloads into orbit. A modified SS-520 sounding rocket was used to place a 4-kilogram payload (TRICOM-1R) into orbit in 2018.
Orbital spaceflight requires a satellite or spacecraft payload to be accelerated to very high velocity. In the vacuum of space, reaction forces must be provided by the ejection of mass, resulting in the rocket equation. The physics of spaceflight are such that rocket stages are typically required to achieve the desired orbit.
Expendable launch vehicles are designed for one-time use, with boosters that usually separate from their payload and disintegrate during atmospheric reentry or on contact with the ground. In contrast, reusable launch vehicle boosters are designed to be recovered intact and launched again. The Falcon 9 is an example reusable launch vehicle.
For example, the European Space Agency is responsible for the Ariane V, and the United Launch Alliance manufactures and launches the Delta IV and Atlas V rockets.
Launchpads can be located on land (spaceport), on a fixed ocean platform (San Marco), on a mobile ocean platform (Sea Launch), and on a submarine. Launch vehicles can also be launched from the air.
A launch vehicle will start off with its payload at some location on the surface of the Earth. To reach orbit, the vehicle must travel vertically to leave the atmosphere and horizontally to prevent re-contacting the ground. The required velocity varies depending on the orbit but will always be extreme when compared to velocities encountered in normal life.
Launch vehicles provide varying degrees of performance. For example, a satellite bound for Geostationary orbit (GEO) can either be directly inserted by the upper stage of the launch vehicle or launched to a geostationary transfer orbit (GTO). A direct insertion places greater demands on the launch vehicle, while GTO is more demanding of the spacecraft. Once in orbit, launch vehicle upper stages and satellites can have overlapping capabilities, although upper stages tend to have orbital lifetimes measured in hours or days while spacecraft can last decades.
Distributed launch involves the accomplishment of a goal with multiple spacecraft launches. A large spacecraft such as the International Space Station can be constructed by assembling modules in orbit, or in-space propellant transfer conducted to greatly increase the delta-V capabilities of a cislunar or deep space vehicle. Distributed launch enables space missions that are not possible with single launch architectures.
Mission architectures for distributed launch were explored in the 2000s 
and launch vehicles with integrated distributed launch capability built in began development in 2017 with the Starship design. The standard Starship launch architecture is to refuel the spacecraft in low Earth orbit to enable the craft to send high-mass payloads on much more energetic missions.
After 1980, but before the 2010s, two orbital launch vehicles developed the capability to return to the launch site (RTLS).  Both the US Space Shuttle—with one of its abort modes—and the Soviet Buran
had a designed-in capability to return a part of the launch vehicle to the launch site via the mechanism of horizontal-landing of the spaceplane portion of the launch vehicle.  In both cases, the main vehicle thrust structure and the large propellant tank were expendable, as had been the standard procedure for all orbital launch vehicles flown prior to that time.  Both were subsequently demonstrated on actual orbital nominal flights, although both also had an abort mode during launch that could conceivably allow the crew to land the spaceplane following an off-nominal launch.
In the 2000s, both SpaceX and Blue Origin have privately developed a set of technologies to support vertical landing of the booster stage of a launch vehicle. 
After 2010, SpaceX undertook a development program to acquire the ability to bring back and vertically land a part of the Falcon 9 orbital launch vehicle: the first stage.  The first successful landing was done in December 2015, since then several additional rocket stages landed either at a landing pad adjacent to the launch site or on a landing platform at sea, some distance away from the launch site. The Falcon Heavy is similarly designed to reuse the three cores comprising its first stage. On its first flight in February 2018, the two outer cores successfully returned to the launch site landing pads while the center core targeted the landing platform at sea but did not successfully land on it.
Blue Origin developed similar technologies for bringing back and landing their suborbital New Shepard, and successfully demonstrated return in 2015, and successfully reused the same booster on a second suborbital flight in January 2016.  By October 2016, Blue had reflown, and landed successfully, that same launch vehicle a total of five times. It must however be noted that the launch trajectories of both vehicles are very different, with New Shepard going straight up and down, whereas Falcon 9 has to cancel substantial horizontal velocity and return from a significant distance downrange. 
Both Blue Origin and SpaceX also have additional reusable launch vehicles under development.  Blue is developing the first stage of the orbital New Glenn LV to be reusable, with first flight planned for no earlier than 2020. 
SpaceX has a new super-heavy launch vehicle under development for missions to interplanetary space.  The Big Falcon Rocket (BFR) is designed to support RTLS, vertical-landing and full reuse of both the booster stage and the integrated second-stage/large-spacecraft that are designed for use with the BFR. First launch is expected in the early 2020s.

A satellite is an object that is intentionally placed into orbit. These objects are called artificial satellites to distinguish them from natural satellites such as Earth's Moon.
On 4 October 1957, the Soviet Union launched the world's first artificial satellite, Sputnik 1. Since then, about 8,900 satellites from more than 40 countries have been launched. According to a 2018 estimate, about 5,000 remained in orbit. Of those, about 1,900 were operational, while the rest had exceeded their useful lives and become space debris. Approximately 63% of operational satellites are in low Earth orbit, 6% are in medium-Earth orbit (at 20,000 km), 29% are in geostationary orbit (at 36,000 km) and the remaining 2% are in various elliptical orbits. In terms of countries with the most satellites, the United States has the most with 2,944 satellites, China is second with 499, and Russia third with 169.
A few large space stations, including the International Space Station, have been launched in parts and assembled in orbit. Over a dozen space probes have been placed into orbit around other bodies and become artificial satellites of the Moon, Mercury, Venus, Mars, Jupiter, Saturn, a few asteroids, a comet and the Sun.
Satellites are used for many purposes. Among several other applications, they can be used to make star maps and maps of planetary surfaces, and also take pictures of planets they are launched into. Common types include military and civilian Earth observation satellites, communications satellites, navigation satellites, weather satellites, and space telescopes. Space stations and human spacecraft in orbit are also satellites.
Satellites can operate by themselves or as part of a larger system, a satellite formation or satellite constellation.
Satellite orbits have a large range depending on the purpose of the satellite, and are classified in a number of ways. Well-known (overlapping) classes include low Earth orbit, polar orbit, and geostationary orbit.
A launch vehicle is a rocket that places a satellite into orbit. Usually, it lifts off from a launch pad on land. Some are launched at sea from a submarine or a mobile maritime platform, or aboard a plane (see air launch to orbit).
Satellites are usually semi-independent computer-controlled systems. Satellite subsystems attend many tasks, such as power generation, thermal control, telemetry, attitude control, scientific instrumentation, communication, etc.
The first published mathematical study of the possibility of an artificial satellite was Newton's cannonball, a thought experiment by Isaac Newton to explain the motion of natural satellites, in his Philosophiæ Naturalis Principia Mathematica (1687). The first fictional depiction of a satellite being launched into orbit was a short story by Edward Everett Hale, "The Brick Moon" (1869). The idea surfaced again in Jules Verne's The Begum's Fortune (1879).
In 1903, Konstantin Tsiolkovsky (1857–1935) published Exploring Space Using Jet Propulsion Devices, which is the first academic treatise on the use of rocketry to launch spacecraft. He calculated the orbital speed required for a minimal orbit, and that a multi-stage rocket fueled by liquid propellants could achieve this.
In 1928, Herman Potočnik (1892–1929) published his sole book, The Problem of Space Travel – The Rocket Motor. He described the use of orbiting spacecraft for observation of the ground and described how the special conditions of space could be useful for scientific experiments.
In a 1945 Wireless World article, the English science fiction writer Arthur C. Clarke described in detail the possible use of communications satellites for mass communications. He suggested that three geostationary satellites would provide coverage over the entire planet.
In May 1946, the United States Air Force's Project RAND released the Preliminary Design of an Experimental World-Circling Spaceship, which stated that "A satellite vehicle with appropriate instrumentation can be expected to be one of the most potent scientific tools of the Twentieth Century." The United States had been considering launching orbital satellites since 1945 under the Bureau of Aeronautics of the United States Navy. Project RAND eventually released the report, but considered the satellite to be a tool for science, politics, and propaganda, rather than a potential military weapon.
In 1946, American theoretical astrophysicist Lyman Spitzer proposed an orbiting space telescope.
In February 1954 Project RAND released "Scientific Uses for a Satellite Vehicle", written by R.R. Carhart. This expanded on potential scientific uses for satellite vehicles and was followed in June 1955 with "The Scientific Use of an Artificial Satellite", by H.K. Kallmann and W.W. Kellogg.
In the context of activities planned for the International Geophysical Year (1957–58), the White House announced on 29 July 1955 that the U.S. intended to launch satellites by the spring of 1958. This became known as Project Vanguard. On 31 July, the Soviets announced that they intended to launch a satellite by the fall of 1957.
The first artificial satellite was Sputnik 1, launched by the Soviet Union on 4 October 1957 under the Sputnik program, with Sergei Korolev as chief designer. Sputnik 1 helped to identify the density of high atmospheric layers through measurement of its orbital change and provided data on radio-signal distribution in the ionosphere. The unanticipated announcement of Sputnik 1's success precipitated the Sputnik crisis in the United States and ignited the so-called Space Race within the Cold War.
Sputnik 2 was launched on 3 November 1957 and carried the first living passenger into orbit, a dog named Laika.
In early 1955, following pressure by the American Rocket Society, the National Science Foundation, and the International Geophysical Year, the Army and Navy were working on Project Orbiter with two competing programs. The army used the Jupiter C rocket, while the civilian/Navy program used the Vanguard rocket to launch a satellite. Explorer 1 became the United States' first artificial satellite on 31 January 1958.
In June 1961, three-and-a-half years after the launch of Sputnik 1, the United States Space Surveillance Network cataloged 115 Earth-orbiting satellites.
Early satellites were constructed to unique designs. With advancements in technology, multiple satellites began to be built on single model platforms called satellite buses. The first standardized satellite bus design was the HS-333 geosynchronous (GEO) communication satellite launched in 1972. Beginning in 1997, FreeFlyer is a commercial off-the-shelf software application for satellite mission analysis, design and operations.
Currently the largest artificial satellite ever is the International Space Station.
Herman Potočnik explored the idea of using orbiting spacecraft for detailed peaceful and military observation of the ground in his 1928 book, The Problem of Space Travel. He described how the special conditions of space could be useful for scientific experiments. The book described geostationary satellites (first put forward by Konstantin Tsiolkovsky) and discussed communication between them and the ground using radio, but fell short of the idea of using satellites for mass broadcasting and as telecommunications relays.
Satellites can be tracked from Earth stations and also from other satellites.
The United States Space Surveillance Network (SSN), a division of the United States Strategic Command, has been tracking objects in Earth's orbit since 1957 when the Soviet Union opened the Space Age with the launch of Sputnik I. Since then, the SSN has tracked more than 26,000 objects. The SSN currently tracks more than 8,000-artificial orbiting objects. The rest have re-entered Earth's atmosphere and disintegrated, or survived re-entry and impacted the Earth. The SSN tracks objects that are 10 centimeters in diameter or larger; those now orbiting Earth range from satellites weighing several tons to pieces of spent rocket bodies weighing only 10 pounds. About seven percent are operational satellites (i.e. ~560 satellites), the rest are space debris. The United States Strategic Command is primarily interested in the active satellites, but also tracks space debris which upon reentry might otherwise be mistaken for incoming missiles.
There are three basic categories of (non-military) satellite services:
Fixed satellite services handle hundreds of billions of voice, data, and video transmission tasks across all countries and continents between certain points on the Earth's surface.
 Mobile satellite systems help connect remote regions, vehicles, ships, people and aircraft to other parts of the world and/or other mobile or stationary communications units, in addition to serving as navigation systems.
Scientific research satellites provide meteorological information, land survey data (e.g. remote sensing), Amateur (HAM) Radio, and other different scientific research applications such as earth science, marine science, and atmospheric research.
The first satellite, Sputnik 1, was put into orbit around Earth and was therefore in geocentric orbit. This is the most common type of orbit by far, with approximately 3,372 active artificial satellites orbiting the Earth. Geocentric orbits may be further classified by their altitude, inclination and eccentricity.
The commonly used altitude classifications of geocentric orbit are Low Earth orbit (LEO), Medium Earth orbit (MEO) and High Earth orbit (HEO). Low Earth orbit is any orbit below 2,000 km. Medium Earth orbit is any orbit between 2,000 and 35,786 km. High Earth orbit is any orbit higher than 35,786 km.
The satellite's functional versatility is embedded within its technical components and its operations characteristics. Looking at the "anatomy" of a typical satellite, one discovers two modules. Note that some novel architectural concepts such as Fractionated spacecraft somewhat upset this taxonomy.
The bus module consists of the following subsystems:
The structural subsystem provides the mechanical base structure with adequate stiffness to withstand stress and vibrations experienced during launch, maintain structural integrity and stability while on station in orbit, and shields the satellite from extreme temperature changes and micro-meteorite damage.
The telemetry subsystem (aka Command and Data Handling, C&amp;DH) monitors the on-board equipment operations, transmits equipment operation data to the earth control station, and receives the earth control station's commands to perform equipment operation adjustments.
The power subsystem may consist of solar panels to convert solar energy into electrical power, regulation and distribution functions, and batteries that store power and supply the satellite when it passes into the Earth's shadow. Nuclear power sources (Radioisotope thermoelectric generator) have also been used in several successful satellite programs including the Nimbus program (1964–1978).
The thermal control subsystem helps protect electronic equipment from extreme temperatures due to intense sunlight or the lack of sun exposure on different sides of the satellite's body (e.g. optical solar reflector)
The attitude and orbit control subsystem consists of sensors to measure vehicle orientation, control laws embedded in the flight software, and actuators (reaction wheels, thrusters). These apply the torques and forces needed to re-orient the vehicle to the desired altitude, keep the satellite in the correct orbital position, and keep antennas pointed in the right directions.
The second major module is the communication payload, which is made up of transponders. A transponder is capable of :
When satellites reach the end of their mission (this normally occurs within 3 or 4 years after launch), satellite operators have the option of de-orbiting the satellite, leaving the satellite in its current orbit or moving the satellite to a graveyard orbit. Historically, due to budgetary constraints at the beginning of satellite missions, satellites were rarely designed to be de-orbited. One example of this practice is the satellite Vanguard 1. Launched in 1958, Vanguard 1, the 4th artificial satellite to be put in Geocentric orbit, was still in orbit as of February 2022, as well as the upper stage of its launch rocket.
Instead of being de-orbited, most satellites in the first six decades of spaceflight were either left in their current orbit or moved to a graveyard orbit. As of 2002, the FCC requires all geostationary satellites to commit to moving to a graveyard orbit at the end of their operational life prior to launch.
In cases of uncontrolled de-orbiting, the major variable is the solar flux, and minor variables are the components and form factor of the satellite itself, as well as gravitational perturbations generated by the Sun and the Moon. The nominal breakup altitude due to aerodynamic forces and temperatures is 78 km, with a range between 72 and 84 km. Solar panels, however, are destroyed before any other component at altitudes between 90 and 95 km.
After the late 2010s, and especially after the advent and operational fielding of large satellite internet constellations—where on-orbit active satellites more than doubled over a period of five years—the companies building the constellations began to propose regular planned deorbiting of the older satellites that reach end of life, as a part of the regulatory process of obtaining a launch license.
By the early 2000s, and particularly after the advent of CubeSats and increased launches of microsats—frequently launched to the lower altitudes of low Earth orbit (LEO)—satellites began to more frequently be designed to demise, or breakup and burnup entirely in the atmosphere.
For example, SpaceX Starlink satellites, the first large satellite internet constellation to exceed 1000 active satellites on orbit in ~2020, are designed to be 100% demisable and burn up completely on atmospheric reentry at end of life, or in the event of an early satellite failure.
This list includes countries with an independent capability to place satellites in orbit, including production of the necessary launch vehicle. Note: many more countries have the capability to design and build satellites but are unable to launch them, instead relying on foreign launch services. This list does not consider those numerous countries, but only lists those capable of launching satellites indigenously, and the date this capability was first demonstrated. The list does not include the European Space Agency, a multi-national state organization, nor private consortiums.

Orbital Sciences Corporation launched a satellite into orbit on the Pegasus in 1990. SpaceX launched a satellite into orbit on the Falcon 1 in 2008. Rocket Lab launched three cubesats into orbit on the Electron in 2018.
While Canada was the third country to build a satellite which was launched into space, it was launched aboard an American rocket from an American spaceport. The same goes for Australia, who launched first satellite involved a donated U.S. Redstone rocket and American support staff as well as a joint launch facility with the United Kingdom. The first Italian satellite San Marco 1 launched on 15 December 1964 on a U.S. Scout rocket from Wallops Island (Virginia, United States) with an Italian launch team trained by NASA. By similar occasions, almost all further first national satellites was launched by foreign rockets.
†-note: Both Chile and Belarus used Russian companies as principal contractors to build their satellites, they used Russian-Ukrainian manufactured rockets and launched either from Russia or Kazakhstan.
Since the mid-2000s, satellites have been hacked by militant organizations to broadcast propaganda and to pilfer classified information from military communication networks.
For testing purposes, satellites in low earth orbit have been destroyed by ballistic missiles launched from earth. Russia, United States, China and India have demonstrated the ability to eliminate satellites. In 2007 the Chinese military shot down an aging weather satellite, followed by the US Navy shooting down a defunct spy satellite in February 2008. On 27 March 2019 India shot down a live test satellite at 300 km altitude in 3 minutes. India became the fourth country to have the capability to destroy live satellites.
Due to the low received signal strength of satellite transmissions, they are prone to jamming by land-based transmitters. Such jamming is limited to the geographical area within the transmitter's range. GPS satellites are potential targets for jamming, but satellite phone and television signals have also been subjected to jamming.
Also, it is very easy to transmit a carrier radio signal to a geostationary satellite and thus interfere with the legitimate uses of the satellite's transponder. It is common for Earth stations to transmit at the wrong time or on the wrong frequency in commercial satellite space, and dual-illuminate the transponder, rendering the frequency unusable. Satellite operators now have sophisticated monitoring that enables them to pinpoint the source of any carrier and manage the transponder space effectively.
During the last five decades, space agencies have sent thousands of space crafts, space capsules, or satellites to the universe. In fact, weather forecasters make predictions on the weather and natural calamities based on observations from these satellites.
The National Aeronautics and Space Administration (NASA) requested the National Academies to publish a report, "Earth Observations from Space; The First 50 Years of Scientific Achievements", in 2008. It described how the capability to view the whole globe simultaneously from satellite observations revolutionized studies about the planet Earth. This development brought about a new age of combined Earth sciences. The National Academies report concluded that continuing Earth observations from the galaxy are necessary to resolve scientific and social challenges in the future.
The NASA introduced an Earth Observing System (EOS) composed of several satellites, science component, and data system described as the Earth Observing System Data and Information System (EOSDIS). It disseminates numerous science data products as well as services designed for interdisciplinary education. EOSDIS data can be accessed online and accessed through File Transfer Protocol (FTP) and Hyper Text Transfer Protocol Secure (HTTPS). Scientists and researchers perform EOSDIS science operations within a distributed platform of multiple interconnected nodes or Science Investigator-led Processing Systems (SIPS) and discipline-specific Distributed Active Archive Centers (DACCs).
The European Space Agency have been operating Earth Observation satellites since the launch of Meteosat 1 in November 1977. ESA currently has plans to launch a satellite equipped with an artificial intelligence (AI) processor that will allow the spacecraft to make decisions on images to capture and data to transmit to the Earth. BrainSat will use the Intel Myriad X vision processing unit (VPU). The launching will be scheduled in 2019. ESA director for Earth Observation Programs Josef Aschbacher made the announcement during the PhiWeek in November 2018. This is the five-day meet that focused on the future of Earth observation. The conference was held at the ESA Center for Earth Observation in Frascati, Italy. ESA also launched the PhiLab, referring to the future-focused team that works to harness the potentials of AI and other disruptive innovations. Meanwhile, the ESA also announced that it expects to commence the qualification flight of the Space Rider space plane in 2021. This will come after several demonstration missions. Space Rider is the sequel of the Agency's Intermediate Experimental vehicle (IXV) which was launched in 2015. It has the capacity payload of 800 kilograms for orbital missions that will last a maximum of two months.
Issues like space debris, radio and light pollution are increasing in magnitude and at the same time lack progress in national or international regulation. Space debris poses dangers to spacecraft (including satellites) in or crossing geocentric orbits and have the potential to drive a Kessler syndrome which could potentially curtail humanity from conducting space endeavors in the future by making such nearly impossible.
With the increase in numbers of satellite constellations, like SpaceX Starlink, the astronomical community, such as the IAU, report that orbital pollution is getting increased significantly. A report from the SATCON1 workshop in 2020 concluded that the effects of large satellite constellations can severely affect some astronomical research efforts and lists six ways to mitigate harm to astronomy. The IAU is establishing a center (CPS) to coordinate or aggregate measures to mitigate such detrimental effects.
Some notable satellite failures that polluted and dispersed radioactive materials are Kosmos 954, Kosmos 1402 and the Transit 5-BN-3.
Generally liability has been covered by the Liability Convention. Using wood as an alternative material has been posited in order to reduce pollution and debris from satellites that reenter the atmosphere.
Several open source satellites both in terms of open source hardware and open source software were flown or are in development. The satellites have usually form of a CubeSat or PocketQube. In 2013 an amateur radio satellite OSSI-1 was launched and remained in orbit for about 2 months. In 2017 UPSat created by the Greek University of Patras and Libre Space Foundation remained in orbit for 18 months. In 2019 FossaSat-1 was launched. As of February 2021 the Portland State Aerospace Society is developing two open source satellites called OreSat and the Libre Space Foundation also has ongoing satellite projects.
An Earth observation satellite or Earth remote sensing satellite is a satellite used or designed for Earth observation (EO) from orbit, including spy satellites and similar ones intended for non-military uses such as environmental monitoring, meteorology, cartography and others. The most common type are Earth imaging satellites, that take satellite images, analogous to aerial photographs; some EO satellites may perform remote sensing without forming pictures, such as in GNSS radio occultation.
The first occurrence of satellite remote sensing can be dated to the launch of the first artificial satellite, Sputnik 1, by the Soviet Union on October 4, 1957. Sputnik 1 sent back radio signals, which scientists used to study the ionosphere.
The United States Army Ballistic Missile Agency launched the first American satellite, Explorer 1, for NASA’s Jet Propulsion Laboratory on January 31, 1958. The information sent back from its radiation detector led to the discovery of the Earth's Van Allen radiation belts. The TIROS-1 spacecraft, launched on April 1, 1960 as part of NASA's Television Infrared Observation Satellite (TIROS) program, sent back the first television footage of weather patterns to be taken from space.
In 2008, more than 150 Earth observation satellites were in orbit, recording data with both passive and active sensors and acquiring more than 10 terabits of data daily. By 2021, that total had grown to over 950, with the largest number of satellites operated by US-based company Planet Labs.
Most Earth observation satellites carry instruments that should be operated at a relatively low altitude. Most orbit at altitudes above 500 to 600 kilometers (310 to 370 mi). Lower orbits have significant air-drag, which makes frequent orbit reboost maneuvers necessary. The Earth observation satellites ERS-1, ERS-2 and Envisat of European Space Agency as well as the MetOp spacecraft of EUMETSAT are all operated at altitudes of about 800 km (500 mi). The Proba-1, Proba-2 and SMOS spacecraft of European Space Agency are observing the Earth from an altitude of about 700 km (430 mi). The Earth observation satellites of UAE, DubaiSat-1 &amp; DubaiSat-2 are also placed in Low Earth Orbits (LEO) orbits and providing satellite imagery of various parts of the Earth.
To get (nearly) global coverage with a low orbit, a polar orbit is used. A low orbit will have an orbital period of roughly 100 minutes and the Earth will rotate around its polar axis about 25° between successive orbits. The ground track moves towards the west 25° each orbit, allowing a different section of the globe to be scanned with each orbit. Most are in Sun-synchronous orbits.
A geostationary orbit, at 36,000 km (22,000 mi), allows a satellite to hover over a constant spot on the earth since the orbital period at this altitude is 24 hours. This allows uninterrupted coverage of more than 1/3 of the Earth per satellite, so three satellites, spaced 120° apart, can cover the whole Earth except the extreme polar regions. This type of orbit is mainly used for meteorological satellites.
A weather satellite is a type of satellite that is primarily used to monitor the weather and climate of the Earth. These meteorological satellites, however, see more than clouds and cloud systems. City lights, fires, effects of pollution, auroras, sand and dust storms, snow cover, ice mapping, boundaries of ocean currents, energy flows, etc., are other types of environmental information collected using weather satellites.
Weather satellite images helped in monitoring the volcanic ash cloud from Mount St. Helens and activity from other volcanoes such as Mount Etna. Smoke from fires in the western United States such as Colorado and Utah have also been monitored.
Other environmental satellites can assist environmental monitoring by detecting changes in the Earth's vegetation, atmospheric trace gas content, sea state, ocean color, and ice fields. By monitoring vegetation changes over time, droughts can be monitored by comparing the current vegetation state to its long term average. For example, the 2002 oil spill off the northwest coast of Spain was watched carefully by the European ENVISAT, which, though not a weather satellite, flies an instrument (ASAR) which can see changes in the sea surface. Anthropogenic emissions can be monitored by evaluating data of tropospheric NO2 and SO2.
These types of satellites are almost always in Sun-synchronous and "frozen" orbits. A sun-synchronous orbit passes over each spot on the ground at the same time of day, so that observations from each pass can be more easily compared, since the sun is in the same spot in each observation. A "frozen" orbit is the closest possible orbit to a circular orbit that is undisturbed by the oblateness of the Earth, gravitational attraction from the sun and moon, solar radiation pressure, and air drag.
Terrain can be mapped from space with the use of satellites, such as Radarsat-1 and TerraSAR-X.
According to the International Telecommunication Union (ITU), Earth exploration-satellite service (also: Earth exploration-satellite radiocommunication service) is – according to Article 1.51 of the ITU Radio Regulations (RR) – defined as:
A radiocommunication service between earth stations and one or more space stations, which may include links between space stations, in which:

This service may also include feeder links necessary for its operation.This radiocommunication service is classified in accordance with ITU Radio Regulations (article 1) as follows: 
Fixed service (article 1.20)
The allocation of radio frequencies is provided according to Article 5 of the ITU Radio Regulations (edition 2012).
In order to improve harmonisation in spectrum utilisation, the majority of service-allocations stipulated in this document were incorporated in national Tables of Frequency Allocations and Utilisations which is with-in the responsibility of the appropriate national administration. The allocation might be primary, secondary, exclusive, and shared.
However, military usage, in bands where there is civil usage, will be in accordance with the ITU Radio Regulations.

Human spaceflight (also referred to as manned spaceflight or crewed spaceflight) is spaceflight with a crew or passengers aboard a spacecraft, often with the spacecraft being operated directly by the onboard human crew. Spacecraft can also be remotely operated from ground stations on Earth, or autonomously, without any direct human involvement. People trained for spaceflight are called astronauts (American or other), cosmonauts (Russian), or taikonauts (Chinese); and non-professionals are referred to as spaceflight participants or spacefarers.
The first human in space was Soviet cosmonaut Yuri Gagarin, who launched on 12 April 1961 as part of the Soviet Union's Vostok program. This was towards the beginning of the Space Race. On 5 May 1961, Alan Shepard became the first American in space, as part of Project Mercury. Humans traveled to the Moon nine times between 1968 and 1972 as part of the United States' Apollo program, and have had a continuous presence in space for 21 years and 253 days on the International Space Station (ISS). As of 2021, humans have not traveled beyond low Earth orbit since the Apollo 17 lunar mission in December 1972.
Currently, the United States, Russia, and China are the only countries with public or commercial human spaceflight-capable programs. On 15 October 2003, the first Chinese taikonaut, Yang Liwei, went to space as part of Shenzhou 5. Non-governmental spaceflight companies have been working to develop human space programs of their own, e.g. for space tourism or commercial in-space research. The first private human spaceflight launch was a suborbital flight on SpaceShipOne on June 21, 2004. The first commercial orbital crew launch was by SpaceX in May 2020, transporting NASA astronauts to the ISS under United States government contract.
Human spaceflight capability was first developed during the Cold War between the United States and the Soviet Union (USSR). These nations developed intercontinental ballistic missiles for the delivery of nuclear weapons, producing rockets large enough to be adapted to carry the first artificial satellites into low Earth orbit.
After the first satellites were launched in 1957 and 1958 by the Soviet Union, the US began work on Project Mercury, with the aim of launching men into orbit. The USSR was secretly pursuing the Vostok program to accomplish the same thing, and launched the first human into space, the cosmonaut Yuri Gagarin, who, on 12 April 1961, was launched aboard Vostok 1 on a Vostok 3KA rocket and completed a single orbit. On 5 May 1961, the US launched its first astronaut, Alan Shepard, on a suborbital flight aboard Freedom 7 on a Mercury-Redstone rocket. Unlike Gagarin, Shepard manually controlled his spacecraft's attitude. On 20 February 1962, John Glenn became the first American in orbit, aboard Friendship 7 on a Mercury-Atlas rocket. The USSR launched five more cosmonauts in Vostok capsules, including the first woman in space, Valentina Tereshkova aboard Vostok 6, on 16 June 1963. Through 1963, the US launched a total of two astronauts in suborbital flights and four into orbit. The US also made two North American X-15 flights (90 and 91, piloted by Joseph A. Walker) that exceeded the Kármán line, the 100 kilometres (62 mi) altitude used by the Fédération Aéronautique Internationale (FAI) to denote the edge of space.
In 1961, US President John F. Kennedy raised the stakes of the Space Race by setting the goal of landing a man on the Moon and returning him safely to Earth by the end of the 1960s. That same year, the US began the Apollo program of launching three-man capsules atop the Saturn family of launch vehicles to accomplish this; and, in 1962, began Project Gemini, which, in 1965 and 1966, flew 10 missions with two-man crews launched by Titan II rockets, Gemini's objective being to support Apollo by developing American orbital spaceflight experience and techniques to be used during the Moon mission.
Meanwhile, the USSR remained silent about their intentions to send humans to the Moon, and proceeded to stretch the limits of their single-pilot Vostok capsule by adapting it to a two or three-person Voskhod capsule to compete with Gemini. They were able to launch two orbital flights in 1964 and 1965 and achieved the first spacewalk, performed by Alexei Leonov on Voskhod 2 on 8 March 1965. However, the Voskhod did not have Gemini's capability to maneuver in orbit, and the program was terminated. The US Gemini flights did not achieve the first spacewalk, but overcame the early Soviet lead by performing several spacewalks, solving the problem of astronaut fatigue caused by compensating for the lack of gravity, demonstrating the ability of humans to endure two weeks in space, and performing the first space rendezvous and docking of spacecraft.
The US succeeded in developing the Saturn V rocket necessary to send the Apollo spacecraft to the Moon, and sent Frank Borman, James Lovell, and William Anders into 10 orbits around the Moon in Apollo 8 in December 1968. In July 1969, Apollo 11 accomplished Kennedy's goal by landing Neil Armstrong and Buzz Aldrin on the Moon on 21 July and returning them safely on 24 July, along with Command Module pilot Michael Collins. Through 1972, a total of six Apollo missions landed 12 men to walk on the Moon, half of which drove electric powered vehicles on the surface. The crew of Apollo 13—Jim Lovell, Jack Swigert, and Fred Haise—survived a catastrophic in-flight spacecraft failure, orbited the Moon without landing, and returned safely to Earth.
Meanwhile, the USSR secretly pursued crewed lunar orbiting and landing programs. They successfully developed the three-person Soyuz spacecraft for use in the lunar programs, but failed to develop the N1 rocket necessary for a human landing, discontinuing their lunar programs in 1974. Upon losing the Moon race they concentrated on the development of space stations, using the Soyuz as a ferry to take cosmonauts to and from the stations. They started with a series of Salyut sortie stations from 1971 to 1986.
In 1969, Nixon appointed his vice president, Spiro Agnew, to head a Space Task Group to recommend follow-on human spaceflight programs after Apollo. The group proposed an ambitious Space Transportation System based on a reusable Space Shuttle, which consisted of a winged, internally fueled orbiter stage burning liquid hydrogen, launched with a similar, but larger kerosene-fueled booster stage, each equipped with airbreathing jet engines for powered return to a runway at the Kennedy Space Center launch site. Other components of the system included a permanent, modular space station; reusable space tug; and nuclear interplanetary ferry, leading to a human expedition to Mars as early as 1986 or as late as 2000, depending on the level of funding allocated. However, Nixon knew the American political climate would not support congressional funding for such an ambition, and killed proposals for all but the Shuttle, possibly to be followed by the space station. Plans for the Shuttle were scaled back to reduce development risk, cost, and time, replacing the piloted fly back booster with two reusable solid rocket boosters, and the smaller orbiter would use an expendable external propellant tank to feed its hydrogen-fueled main engines. The orbiter would have to make unpowered landings.
In 1973, the US launched the Skylab sortie space station and inhabited it for 171 days with three crews ferried aboard Apollo spacecraft. During that time, President Richard Nixon and Soviet general secretary Leonid Brezhnev were negotiating an easing of Cold War tensions known as détente. As part of this, they negotiated the Apollo-Soyuz program, in which an Apollo spacecraft carrying a special docking adapter module rendezvoused and docked with Soyuz 19 in 1975. The American and Russian crews shook hands in space, but the purpose of the flight was purely symbolic.
The two nations continued to compete rather than cooperate in space, as the US turned to developing the Space Shuttle and planning the space station, which was dubbed Freedom. The USSR launched three Almaz military sortie stations from 1973 to 1977, disguised as Salyuts. They followed Salyut with the development of Mir, the first modular, semi-permanent space station, the construction of which took place from 1986 to 1996. Mir orbited at an altitude of 354 kilometers (191 nautical miles), at an orbital inclination of 51.6°. It was occupied for 4,592 days and made a controlled reentry in 2001.
The Space Shuttle started flying in 1981, but the US Congress failed to approve sufficient funds to make Space Station Freedom a reality. A fleet of four shuttles was built: Columbia, Challenger, Discovery, and Atlantis. A fifth shuttle, Endeavour, was built to replace Challenger, which was destroyed in an accident during launch that killed 7 astronauts on 28 January 1986. From 1983 to 1998, twenty-two Shuttle flights carried components for a European Space Agency sortie space station called Spacelab in the Shuttle payload bay.
The USSR copied the US's reusable Space Shuttle orbiter, which they called Buran-class orbiter or simply Buran, which was designed to be launched into orbit by the expendable Energia rocket, and capable of robotic orbital flight and landing. Unlike the Space Shuttle, Buran had no main rocket engines, but like the Space Shuttle orbiter it used smaller rocket engines to perform its final orbital insertion. A single uncrewed orbital test flight took place in November 1988. A second test flight was planned by 1993, but the program was canceled due to lack of funding and the dissolution of the Soviet Union in 1991. Two more orbiters were never completed, and the one that performed the uncrewed flight was destroyed in a hangar roof collapse in May 2002.
The dissolution of the Soviet Union in 1991 brought an end to the Cold War and opened the door to true cooperation between the US and Russia. The Soviet Soyuz and Mir programs were taken over by the Russian Federal Space Agency, now known as the Roscosmos State Corporation. The Shuttle-Mir Program included American Space Shuttles visiting the Mir space station, Russian cosmonauts flying on the Shuttle, and an American astronaut flying aboard a Soyuz spacecraft for long-duration expeditions aboard Mir.
In 1993, President Bill Clinton secured Russia's cooperation in converting the planned Space Station Freedom into the International Space Station (ISS). Construction of the station began in 1998. The station orbits at an altitude of 409 kilometers (221 nmi) and an orbital inclination of 51.65°. Several of the Space Shuttle's 135 orbital flights were to help assemble, supply, and crew the ISS. Russia has built half of the International Space Station and has continued its cooperation with the US.
China was the third nation in the world, after the USSR and USA, to send humans into space. During the Space Race between the two superpowers, which culminated with Apollo 11 landing humans on the Moon, Mao Zedong and Zhou Enlai decided on 14 July 1967 that China should not be left behind, and initiated their own crewed space program: the top-secret Project 714, which aimed to put two people into space by 1973 with the Shuguang spacecraft. Nineteen PLAAF pilots were selected for this goal in March 1971. The Shuguang-1 spacecraft, to be launched with the CZ-2A rocket, was designed to carry a crew of two. The program was officially cancelled on 13 May 1972 for economic reasons.
In 1992, under China Manned Space Program (CMS), also known as "Project 921", authorization and funding was given for the first phase of a third, successful attempt at crewed spaceflight. To achieve independent human spaceflight capability, China developed Shenzhou spacecraft and Long March 2F rocket dedicated for human spaceflight in the next few years, along with critical infrastructures like new launch site and flight control center being built. The first uncrewed spacecraft, Shenzhou 1, was launched on 20 November 1999 and recovered the next day, marking the first step of the realization of China's human spaceflight capability. Three more uncrewed missions were conducted in the next few years in order to verify the key technologies.  On 15 October 2003 Shenzhou 5, China's first crewed spaceflight mission, put Yang Liwei in orbit for 21 hours and returned safely back to Inner Mongolia, making China the third nation to launch a human into orbit independently.
The goal of the second phase of CMS was to make technology breakthroughs in extravehicular activities (EVA, or spacewalk) as well as space rendezvous and docking to support short-term human activities in space. On 25 September 2008 during the flight of Shenzhou 7, Zhai Zhigang and Liu Boming completed China's first EVA. In 2011, China launched the Tiangong 1 target spacecraft and Shenzhou 8 uncrewed spacecraft. The two spacecraft completed China's first automatic rendezvous and docking on 3 November 2011. About 9 months later, Tiangong 1 completed the first manual rendezvous and docking with Shenzhou 9, which carried China's first female astronaut Liu Yang.
In September 2016, Tiangong 2 was launched into the orbit. It was a space laboratory with more advanced functions and equipment than Tiangong 1. A month later, Shenzhou 11 was launched and docked with Tiangong 2. Two astronauts entered Tiangong 2 and stationed for about 30 days and verified the viability of astronauts' medium-term stay in space. In April 2017, China's first cargo spacecraft, Tianzhou 1 docked with Tiangong 2 and completed multiple in-orbit propellant refueling tests, which marked the successful completion of the second phase of CMS.
The third phase of CMS began in 2020. The goal of this phase is to build China's own space station, Tiangong. The first module of Tiangong, the Tianhe core module, was launched into orbit by China's most powerful rocket Long March 5B on 29 April 2021. It was later visited by multiple cargo and crewed spacecraft and demonstrated China's capability of sustaining Chinese astronauts' long-term stay in space.
According to CMS announcement, all missions of Tiangong Space Station are scheduled to be carried out by the end of 2022. Once the construction is completed, Tiangong will enter the application and development phase, which is poised to last for no less than 10 years.
The European Space Agency began development of the Hermes shuttle spaceplane in 1987, to be launched on the Ariane 5 expendable launch vehicle. It was intended to dock with the European Columbus space station. The projects were canceled in 1992, when it became clear that neither cost nor performance goals could be achieved. No Hermes shuttles were ever built. The Columbus space station was reconfigured as the European module of the same name on the International Space Station.
Japan (NASDA) began development of the HOPE-X experimental shuttle spaceplane in the 1980s, to be launched on its H-IIA expendable launch vehicle. A string of failures in 1998 led to funding reductions, and the project's cancellation in 2003 in favor of participation in the International Space Station program through the Kibō Japanese Experiment Module and H-II Transfer Vehicle cargo spacecraft. As an alternative to HOPE-X, NASDA in 2001 proposed the Fuji crew capsule for independent or ISS flights, but the project did not proceed to the contracting stage.
From 1993 to 1997, the Japanese Rocket Society , Kawasaki Heavy Industries, and Mitsubishi Heavy Industries worked on the proposed Kankoh-maru vertical-takeoff-and-landing single-stage-to-orbit reusable launch system. In 2005, this system was proposed for space tourism.
According to a press release from the Iraqi News Agency dated 5 December 1989, there was only one test of the Al-Abid space launcher, which Iraq intended to use to develop its own crewed space facilities by the end of the century. These plans were put to an end by the Gulf War of 1991 and the economic hardships that followed.
Under the George W. Bush administration, the Constellation program included plans for retiring the Space Shuttle program and replacing it with the capability for spaceflight beyond low Earth orbit. In the 2011 United States federal budget, the Obama administration canceled Constellation for being over budget and behind schedule while not innovating and investing in critical new technologies. As part of the Artemis program, NASA is developing the Orion spacecraft to be launched by the Space Launch System. Under the Commercial Crew Development plan, NASA relies on transportation services provided by the private sector to reach low Earth orbit, such as SpaceX Dragon 2, the Boeing Starliner or Sierra Nevada Corporation's Dream Chaser. The period between the retirement of the Space Shuttle in 2011 and the first launch into space of SpaceShipTwo Flight VP-03 on 13 December 2018 is similar to the gap between the end of Apollo in 1975 and the first Space Shuttle flight in 1981, and is referred to by a presidential Blue Ribbon Committee as the U.S. human spaceflight gap.
Since the early 2000s, a variety of private spaceflight ventures have been undertaken. As of May 2021, SpaceX has launched humans to orbit, while Virgin Galactic has launched crew to a height above 80 km (50 mi) on a suborbital trajectory. Several other companies—including Blue Origin and Sierra Nevada—develop crewed spacecraft. All four companies plan to fly commercial passengers in the emerging space tourism market.
SpaceX has developed Crew Dragon flying on Falcon 9. It first launched astronauts to orbit and to the ISS in May 2020 as part of the Demo-2 mission. Developed as part of NASA's Commercial Crew Development program, the capsule is also available for flights with other customers. A first tourist mission, Inspiration4, launched in September 2021.
Boeing is developing the Starliner capsule as part of NASA's Commercial Crew Development program, which is launched on a United Launch Alliance Atlas V launch vehicle. Starliner made an uncrewed flight in December 2019. A second uncrewed flight attempt was scrubbed in August 2021, with a NASA official saying it would likely not launch until 2022. A crewed flight is not expected before the second half of 2022. Similar to SpaceX, development funding has been provided by a mix of government and private funds.
Virgin Galactic is developing SpaceshipTwo, a commercial suborbital spacecraft aimed at the space tourism market. It reached space in December 2018.
Blue Origin is in a multi-year test program of their New Shepard vehicle and has carried out 16 uncrewed test flights as of September 2021, and one crewed flight carrying founder Jeff Bezos, his brother Mark Bezos, aviator Wally Funk, and 18-year old Oliver Daemen on July 20, 2021.
Over the decades, a number of spacecraft have been proposed for spaceliner passenger travel. Somewhat analogous to travel by airliner after the middle of the 20th century, these vehicles are proposed to transport large numbers of passengers to destinations in space, or on Earth via suborbital spaceflights. To date, none of these concepts have been built, although a few vehicles that carry fewer than 10 persons are currently in the test flight phase of their development process.
One large spaceliner concept currently in early development is the SpaceX Starship, which, in addition to replacing the Falcon 9 and Falcon Heavy launch vehicles in the legacy Earth-orbit market after 2020, has been proposed by SpaceX for long-distance commercial travel on Earth, flying 100+ people suborbitally between two points in under one hour, also known as "Earth-to-Earth".
Small spaceplane or small capsule suborbital spacecraft have been under development for the past decade or so; as of 2017, at least one of each type is under development. Both Virgin Galactic and Blue Origin have craft in active development: the SpaceShipTwo spaceplane and the New Shepard capsule, respectively. Both would carry approximately a half-dozen passengers up to space for a brief time of zero gravity before returning to the launch location. XCOR Aerospace had been developing the Lynx single-passenger spaceplane since the 2000s, but development was halted in 2017.
Participation and representation of humanity in space has been an issue ever since the first phase of space exploration. Some rights of non-spacefaring countries have been secured through international space law, declaring space the "province of all mankind", though the sharing of space by all humanity is sometimes criticized as imperialist and lacking. In addition to the lack of international inclusion, the inclusion of women and people of color has also been lacking. To make spaceflight more inclusive, organizations such as the Justspace Alliance and IAU-featured Inclusive Astronomy have been formed in recent years.
The first woman to ever enter space was Valentina Tereshkova. She flew in 1963, but it was not until the 1980s that another woman entered space. At the time, all astronauts were required to be military test pilots; women were not able to enter this career, which is one reason for the delay in allowing women to join space crews. After the rules were changed, Svetlana Savitskaya became the second woman to enter space; she was also from the Soviet Union. Sally Ride became the next woman to enter space and the first woman to enter space through the United States program.
Since then, eleven other countries have allowed women astronauts. The first all-female space walk occurred in 2018, by Christina Koch and Jessica Meir. These two women had both participated in separate space walks with NASA. The first mission to the Moon with a woman aboard is planned for 2024.
Despite these developments women are still underrepresented among astronauts and especially cosmonauts. More than 600 people have flown in space but only 75 have been women. Issues that block potential applicants from the programs, and limit the space missions they are able to go on, are, for example:
Sally Ride became the first American woman in space, in 1983. Eileen Collins was the first female Shuttle pilot, and with Shuttle mission STS-93 in 1999 she became the first woman to command a U.S. spacecraft.
For many years, only the USSR (later Russia) and the United States were the only countries whose astronauts flew in space. That ended with the 1978 flight of Vladimir Remek. As of 2010, citizens from 38 nations (including space tourists) have flown in space aboard Soviet, American, Russian, and Chinese spacecraft.
Human spaceflight programs have been conducted by the Soviet Union–Russian Federation, the United States, Mainland China, and by American private spaceflight companies.
The following space vehicles and spaceports are currently used for launching human spaceflights:
The following space stations are currently maintained in Earth orbit for human occupation:
Most of the time, the only humans in space are those aboard the ISS, which generally has a crew of 7 except during crew transitions, and those aboard Tiangong, which has a crew complement of 3 but is not always occupied.
NASA and ESA use the term "human spaceflight" to refer to their programs of launching people into space. These endeavors have also been referred to as "manned space missions", though because of gender specificity this is no longer official parlance according to NASA style guides.
Under the Indian Human Spaceflight Program, India was planning to send humans into space on its orbital vehicle Gaganyaan before August 2022, but it has been delayed to 2023, due to the COVID-19 pandemic. The Indian Space Research Organisation (ISRO) began work on this project in 2006. The initial objective is to carry a crew of two or three to low Earth orbit (LEO) for a 3-to-7-day flight in a spacecraft on a GSLV Mk III rocket and return them safely for a water landing at a predefined landing zone. On 15 August 2018, Indian Prime Minister Narendra Modi, declared India will independently send humans into space before the 75th anniversary of independence in 2022. In 2019, ISRO revealed plans for a space station by 2030, followed by a crewed lunar mission. The program envisages the development of a fully-autonomous orbital vehicle capable of carrying 2 or 3 crew members to an about 300 km (190 mi) low Earth orbit and bringing them safely back home.
Since 2008, the Japan Aerospace Exploration Agency has developed the H-II Transfer Vehicle cargo-spacecraft-based crewed spacecraft and Kibō Japanese Experiment Module–based small space laboratory.
NASA is developing a plan to land humans on Mars by the 2030s. The first step will begin with Artemis 1 in 2022, sending an uncrewed Orion spacecraft to a distant retrograde orbit around the Moon and returning it to Earth after a 25-day mission.
SpaceX is developing Starship, a fully reusable two stage system, with near-Earth and cislunar applications and an ultimate goal of landing on Mars. The upper stage of the Starship system, also called Starship, has had 9 atmospheric test flights as of September 2021. A modified version of Starship is being developed for the Artemis program.
Several other countries and space agencies have announced and begun human spaceflight programs using natively developed equipment and technology, including Japan (JAXA), Iran (ISA), and North Korea (NADA). The plans for the Iranian crewed spacecraft are for a small spacecraft and space laboratory. North Korea's space program has plans for crewed spacecraft and small shuttle systems.



There are two main sources of hazard in space flight: those due to the hostile space environment, and those due to possible equipment malfunctions. Addressing these issues is of great importance for NASA and other space agencies before conducting the first extended crewed missions to destinations such as Mars.
Planners of human spaceflight missions face a number of safety concerns.
The basic needs for breathable air and drinkable water are addressed by the life support system of the spacecraft.
Astronauts may not be able to quickly return to Earth or receive medical supplies, equipment, or personnel if a medical emergency occurs. The astronauts may have to rely for long periods on limited resources and medical advice from the ground.
The possibility of blindness and of bone loss have been associated with human space flight.
On 31 December 2012, a NASA-supported study reported that spaceflight may harm the brains of astronauts and accelerate the onset of Alzheimer's disease.
In October 2015, the NASA Office of Inspector General issued a health hazards report related to space exploration, which included the potential hazards of a human mission to Mars.
On 2 November 2017, scientists reported, based on MRI studies, that significant changes in the position and structure of the brain have been found in astronauts who have taken trips in space. Astronauts on longer space trips were affected by greater brain changes.
Researchers in 2018 reported, after detecting the presence on the International Space Station (ISS) of five Enterobacter bugandensis bacterial strains, none pathogenic to humans, that microorganisms on ISS should be carefully monitored to assure a healthy environment for astronauts.
In March 2019, NASA reported that latent viruses in humans may be activated during space missions, possibly adding more risk to astronauts in future deep-space missions.
On 25 September 2021, CNN reported that an alarm had sounded during the Inspiration4 Earth-orbital journey on the SpaceX Dragon 2. The alarm signal was found to be associated with an apparent toilet malfunction.
Medical data from astronauts in low Earth orbits for long periods, dating back to the 1970s, show several adverse effects of a microgravity environment: loss of bone density, decreased muscle strength and endurance, postural instability, and reductions in aerobic capacity. Over time these deconditioning effects can impair astronauts' performance or increase their risk of injury.
In a weightless environment, astronauts put almost no weight on the back muscles or leg muscles used for standing up, which causes the muscles to weaken and get smaller. Astronauts can lose up to twenty per cent of their muscle mass on spaceflights lasting five to eleven days. The consequent loss of strength could be a serious problem in case of a landing emergency. Upon returning to Earth from long-duration flights, astronauts are considerably weakened and are not allowed to drive a car for twenty-one days.
Astronauts experiencing weightlessness will often lose their orientation, get motion sickness, and lose their sense of direction as their bodies try to get used to a weightless environment. When they get back to Earth, they have to readjust and may have problems standing up, focusing their gaze, walking, and turning. Importantly, those motor disturbances only get worse the longer the exposure to weightlessness. These changes can affect the ability to perform tasks required for approach and landing, docking, remote manipulation, and emergencies that may occur while landing.
In addition, after long space flight missions, male astronauts may experience severe eyesight problems, which may be a major concern for future deep space flight missions, including a crewed mission to the planet Mars. Long space flights can also alter a space traveler's eye movements.
Without proper shielding, the crews of missions beyond low Earth orbit might be at risk from high-energy protons emitted by solar particle events (SPEs) associated with solar flares. If estimated correctly, the amount of radiation that astronauts would be exposed to from a solar storm similar to that of the most powerful in recorded history, the Carrington Event, would result in acute radiation sickness at least, and could even be fatal "in a poorly shielded spacecraft". Another storm that could have inflicted a potentially lethal dose of radiation on astronauts outside Earth's protective magnetosphere occurred during the Space Age, shortly after Apollo 16 landed and before Apollo 17 launched. This solar storm, which occurred in August 1972, could potentially have caused any astronauts who were exposed to it to suffer from acute radiation sickness, and may even have been lethal for those engaged in extravehicular activity or on the lunar surface.
Another type of radiation, galactic cosmic rays, presents further challenges to human spaceflight beyond low Earth orbit.
There is also some scientific concern that extended spaceflight might slow down the body's ability to protect itself against diseases, resulting in a weakened immune system and the activation of dormant viruses in the body. Radiation can cause both short- and long-term consequences to the bone marrow stem cells from which blood and immune-system cells are created. Because the interior of a spacecraft is so small, a weakened immune system and more active viruses in the body can lead to a fast spread of infection.
During long missions, astronauts are isolated and confined in small spaces. Depression, anxiety, cabin fever, and other psychological problems may occur more than for an average person and could impact the crew's safety and mission success. NASA spends millions of dollars on psychological treatments for astronauts and former astronauts. To date, there is no way to prevent or reduce mental problems caused by extended periods of stay in space.
Due to these mental disorders, the efficiency of astronauts' work is impaired; and sometimes they are brought back to Earth, incurring the expense of their mission being aborted. A Russian expedition to space in 1976 was returned to Earth after the cosmonauts reported a strong odor that resulted in a fear of fluid leakage; but after a thorough investigation, it became clear that there was no leakage or technical malfunction. It was concluded by NASA that the cosmonauts most likely had hallucinated the smell.
It is possible that the mental health of astronauts can be affected by the changes in the sensory systems while in prolonged space travel.
During astronauts' spaceflight, they are in an extreme environment. This, and the fact that little change is taking place in the environment, will result in the weakening of sensory input to the astronauts' seven senses.
Space flight requires much higher velocities than ground or air transportation, and consequently requires the use of high energy density propellants for launch, and the dissipation of large amounts of energy, usually as heat, for safe reentry through the Earth's atmosphere.
Since rockets have the potential for fire or explosive destruction, space capsules generally employ some sort of launch escape system, consisting either of a tower-mounted solid-fuel rocket to quickly carry the capsule away from the launch vehicle (employed on Mercury, Apollo, and Soyuz, the escape tower being discarded at some point after launch, at a point where an abort can be performed using the spacecraft's engines), or else ejection seats (employed on Vostok and Gemini) to carry astronauts out of the capsule and away for individual parachute landings.
Such a launch escape system is not always practical for multiple-crew-member vehicles (particularly spaceplanes), depending on location of egress hatch(es). When the single-hatch Vostok capsule was modified to become the 2 or 3-person Voskhod, the single-cosmonaut ejection seat could not be used, and no escape tower system was added. The two Voskhod flights in 1964 and 1965 avoided launch mishaps. The Space Shuttle carried ejection seats and escape hatches for its pilot and copilot in early flights; but these could not be used for passengers who sat below the flight deck on later flights, and so were discontinued.
There have been only two in-flight launch aborts of a crewed flight. The first occurred on Soyuz 18a on 5 April 1975. The abort occurred after the launch escape system had been jettisoned when the launch vehicle's spent second stage failed to separate before the third stage ignited and the vehicle strayed off course. The crew finally managed to separate the spacecraft, firing its engines to pull it away from the errant rocket, both cosmonauts landing safely. The second occurred on 11 October 2018 with the launch of Soyuz MS-10. Again, both crew members survived.
In the first use of a launch escape system on the launchpad, before the start of a crewed flight, happened during the planned Soyuz T-10a launch on 26 September 1983, which was aborted by a launch vehicle fire 90 seconds before liftoff. Both cosmonauts aboard landed safely.
The only crew fatality during launch occurred on 28 January 1986, when the Space Shuttle Challenger broke apart 73 seconds after liftoff, due to failure of a solid rocket booster seal, which caused the failure of the external fuel tank, resulting in explosion of the fuel and separation of the boosters. All seven crew members were killed.
Despite the ever-present risks related to mechanical failures while working in open space, no spacewalking astronaut has ever been lost. There is a requirement for spacewalking astronauts to use tethers and sometimes supplementary anchors. If those fail, a spacewalking astronaut would most probably float away impelled by forces that were acting on the astronaut at the time of breaking loose. Such an astronaut would possibly be spinning, as kicking and flailing would be of no use. At the right angle and velocity, the astronaut might even re-enter the Earth's atmosphere and burn up. NASA has protocols for such situations: astronauts would be wearing an emergency jetpack that would automatically counter any tumbling. NASA's plan states that astronauts should then take manual control of the jetpack and fly back to safety.
However, if the jetpack's 3 pounds (1.4 kg) of fuel runs out, and if there is no other astronaut in close proximity to help, or if the air lock is irreparably damaged, the outcome would certainly be fatal. At this time, there is no spacecraft to save an astronaut floating in space, as the only one with a rescue-ready air-locked compartment — the Space Shuttle — retired 11 years ago. There is approximately a litre of water available via straw in an astronaut's helmet. The astronaut would wait roughly 7.5 hours for breathable air to run out before dying of suffocation.
The single pilot of Soyuz 1, Vladimir Komarov, was killed when his capsule's parachutes failed during an emergency landing on 24 April 1967, causing the capsule to crash.
On 1 February 2003, the crew of seven aboard the Space Shuttle Columbia were killed on reentry after completing a successful mission in space. A wing-leading-edge reinforced carbon-carbon heat shield had been damaged by a piece of frozen external tank foam insulation that had broken off and struck the wing during launch. Hot reentry gasses entered and destroyed the wing structure, leading to the breakup of the orbiter vehicle.
There are two basic choices for an artificial atmosphere: either an Earth-like mixture of oxygen and an inert gas such as nitrogen or helium, or pure oxygen, which can be used at lower than standard atmospheric pressure. A nitrogen–oxygen mixture is used in the International Space Station and Soyuz spacecraft, while low-pressure pure oxygen is commonly used in space suits for extravehicular activity.
The use of a gas mixture carries the risk of decompression sickness (commonly known as "the bends") when transitioning to or from the pure oxygen space suit environment. There have been instances of injury and fatalities caused by suffocation in the presence of too much nitrogen and not enough oxygen.
A pure oxygen atmosphere carries the risk of fire. The original design of the Apollo spacecraft used pure oxygen at greater than atmospheric pressure prior to launch. An electrical fire started in the cabin of Apollo 1 during a ground test at Cape Kennedy Air Force Station Launch Complex 34 on 27 January 1967, and spread rapidly. The high pressure, increased by the fire, prevented removal of the plug door hatch cover in time to rescue the crew. All three astronauts—Gus Grissom, Ed White, and Roger Chaffee—were killed. This led NASA to use a nitrogen–oxygen atmosphere before launch, and low pressure pure oxygen only in space.
The March 1966 Gemini 8 mission was aborted in orbit when an attitude control system thruster stuck in the on position, sending the craft into a dangerous spin that threatened the lives of Neil Armstrong and David Scott. Armstrong had to shut the control system off and use the reentry control system to stop the spin. The craft made an emergency reentry and the astronauts landed safely. The most probable cause was determined to be an electrical short due to a static electricity discharge, which caused the thruster to remain powered even when switched off. The control system was modified to put each thruster on its own isolated circuit.
The third lunar landing expedition, Apollo 13, in April 1970, was aborted and the lives of the crew—James Lovell, Jack Swigert, and Fred Haise—were threatened after the failure of a cryogenic liquid oxygen tank en route to the Moon. The tank burst when electrical power was applied to internal stirring fans in the tank, causing the immediate loss of all of its contents, and also damaging the second tank, causing the gradual loss of its remaining oxygen over a period of 130 minutes. This in turn caused loss of electrical power provided by fuel cells to the command spacecraft. The crew managed to return to Earth safely by using the lunar landing craft as a "life boat". The tank failure was determined to be caused by two mistakes: the tank's drain fitting had been damaged when it was dropped during factory testing, necessitating the use of its internal heaters to boil out the oxygen after a pre-launch test; which in turn damaged the fan wiring's electrical insulation because the thermostats on the heaters did not meet the required voltage rating due to a vendor miscommunication.
The crew of Soyuz 11 were killed on 30 June 1971 by a combination of mechanical malfunctions; the crew were asphyxiated due to cabin decompression following separation of their descent capsule from the service module. A cabin ventilation valve had been jolted open at an altitude of 168 kilometres (104 mi) by the stronger-than-expected shock of explosive separation bolts, which were designed to fire sequentially, but in fact had fired simultaneously. The loss of pressure became fatal within about 30 seconds.
As of December 2015, 23 crew members have died in accidents aboard spacecraft. Over 100 others have died in accidents during activity directly related to spaceflight or testing.
Solar System → Local Interstellar Cloud → Local Bubble → Gould Belt → Orion Arm → Milky Way → Milky Way subgroup → Local Group → Local Sheet → Virgo Supercluster → Laniakea Supercluster → KBC Void → Observable universe → UniverseEach arrow (→) may be read as "within" or "part of".

Space exploration is the use of astronomy and space technology to explore outer space. While the exploration of space is carried out mainly by astronomers with telescopes, its physical exploration though is conducted both by uncrewed robotic space probes and human spaceflight. Space exploration, like its classical form astronomy, is one of the main sources for space science.
While the observation of objects in space, known as astronomy, predates reliable recorded history, it was the development of large and relatively efficient rockets during the mid-twentieth century that allowed physical space exploration to become a reality.  The world's first large-scale experimental rocket program was Opel-RAK under the leadership of Fritz von Opel and Max Valier during the late 1920s leading to the first crewed rocket cars and rocket planes,  which paved the way for the Nazi era V2 program and US and Soviet activities from 1950 onwards. The Opel-RAK program and the spectacular public demonstrations of ground and air vehicles drew large crowds, as well as caused global public excitement as so-called "Rocket Rumble" and had a large long-lasting impact on later spaceflight pioneers like Wernher von Braun. Common rationales for exploring space include advancing scientific research, national prestige, uniting different nations, ensuring the future survival of humanity, and developing military and strategic advantages against other countries.
The early era of space exploration was driven by a "Space Race" between the Soviet Union and the United States. The launch of the first human-made object to orbit Earth, the Soviet Union's Sputnik 1, on 4 October 1957, and the first Moon landing by the American Apollo 11 mission on 20 July 1969 are often taken as landmarks for this initial period. The Soviet space program achieved many of the first milestones, including the first living being in orbit in 1957, the first human spaceflight (Yuri Gagarin aboard Vostok 1) in 1961, the first spacewalk (by Alexei Leonov) on 18 March 1965, the first automatic landing on another celestial body in 1966, and the launch of the first space station (Salyut 1) in 1971. 
After the first 20 years of exploration, focus shifted from one-off flights to renewable hardware, such as the Space Shuttle program, and from competition to cooperation as with the International Space Station (ISS).
With the substantial completion of the ISS following STS-133 in March 2011, plans for space exploration by the U.S. remain in flux. Constellation, a Bush Administration program for a return to the Moon by 2020 was judged inadequately funded and unrealistic by an expert review panel reporting in 2009. 
The Obama Administration proposed a revision of Constellation in 2010 to focus on the development of the capability for crewed missions beyond low Earth orbit (LEO), envisioning extending the operation of the ISS beyond 2020, transferring the development of launch vehicles for human crews from NASA to the private sector, and developing technology to enable missions to beyond LEO, such as Earth–Moon L1, the Moon, Earth–Sun L2, near-Earth asteroids, and Phobos or Mars orbit.
In the 2000s, China initiated a successful crewed spaceflight program while India launched Chandraayan 1, while the European Union and Japan have also planned future crewed space missions. China, Russia, and Japan have advocated crewed missions to the Moon during the 21st century, while the European Union has advocated crewed missions to both the Moon and Mars during the 20th and 21st century.
The first telescope is said to have been invented in 1608 in the Netherlands by an eyeglass maker named Hans Lippershey. The Orbiting Astronomical Observatory 2 was the first space telescope launched on 7 December 1968. As of 2 February 2019, there was 3,891 confirmed exoplanets discovered. The Milky Way is estimated to contain 100–400 billion stars and more than 100 billion planets. There are at least 2 trillion galaxies in the observable universe. HD1 is the most distant known object from Earth, reported as 33.4 billion light-years away.
MW 18014 was a German V-2 rocket test launch that took place on 20 June 1944, at the Peenemünde Army Research Center in Peenemünde. It was the first man-made object to reach outer space, attaining an apogee of 176 kilometers, which is well above the Kármán line. It was a vertical test launch. Although the rocket reached space, it did not reach orbital velocity, and therefore returned to Earth in an impact, becoming the first sub-orbital spaceflight.
The first successful orbital launch was of the Soviet uncrewed Sputnik 1 ("Satellite 1") mission on 4 October 1957. The satellite weighed about 83 kg (183 lb), and is believed to have orbited Earth at a height of about 250 km (160 mi). It had two radio transmitters (20 and 40 MHz), which emitted "beeps" that could be heard by radios around the globe. Analysis of the radio signals was used to gather information about the electron density of the ionosphere, while temperature and pressure data was encoded in the duration of radio beeps. The results indicated that the satellite was not punctured by a meteoroid. Sputnik 1 was launched by an R-7 rocket. It burned up upon re-entry on 3 January 1958.
The first successful human spaceflight was Vostok 1 ("East 1"), carrying the 27-year-old Russian cosmonaut, Yuri Gagarin, on 12 April 1961. The spacecraft completed one orbit around the globe, lasting about 1 hour and 48 minutes. Gagarin's flight resonated around the world; it was a demonstration of the advanced Soviet space program and it opened an entirely new era in space exploration: human spaceflight.
The first artificial object to reach another celestial body was Luna 2 reaching the Moon in 1959. The first soft landing on another celestial body was performed by Luna 9 landing on the Moon on 3 February 1966. Luna 10 became the first artificial satellite of the Moon, entering in a lunar orbit on 3 April 1966.
The first crewed landing on another celestial body was performed by Apollo 11 on 20 July 1969, landing on the Moon. There have been a total of six spacecraft with humans landing on the Moon starting from 1969 to the last human landing in 1972.
The first interplanetary flyby was the 1961 Venera 1 flyby of Venus, though the 1962 Mariner 2 was the first flyby of Venus to return data (closest approach 34,773 kilometers). Pioneer 6 was the first satellite to orbit the Sun, launched on 16 December 1965. The other planets were first flown by in 1965 for Mars by Mariner 4, 1973 for Jupiter by Pioneer 10, 1974 for Mercury by Mariner 10, 1979 for Saturn by Pioneer 11, 1986 for Uranus by Voyager 2, 1989 for Neptune by Voyager 2. In 2015, the dwarf planets Ceres and Pluto were orbited by Dawn and passed by New Horizons, respectively. This accounts for flybys of each of the eight planets in the Solar System, the Sun, the Moon and Ceres &amp; Pluto (2 of the 5 recognized dwarf planets).
The first interplanetary surface mission to return at least limited surface data from another planet was the 1970 landing of Venera 7, which returned data to Earth for 23 minutes from Venus. In 1975 the Venera 9 was the first to return images from the surface of another planet, returning images from Venus. In 1971 the Mars 3 mission achieved the first soft landing on Mars returning data for almost 20 seconds. Later much longer duration surface missions were achieved, including over six years of Mars surface operation by Viking 1 from 1975 to 1982 and over two hours of transmission from the surface of Venus by Venera 13 in 1982, the longest ever Soviet planetary surface mission. Venus and Mars are the two planets outside of Earth on which humans have conducted surface missions with uncrewed robotic spacecraft.
Salyut 1 was the first space station of any kind, launched into low Earth orbit by the Soviet Union on 19 April 1971. The International Space Station is currently the only fully functional space station, inhabited continuously since the year 2000.
Voyager 1 became the first human-made object to leave the Solar System into interstellar space on 25 August 2012. The probe passed the heliopause at 121 AU to enter interstellar space.
The Apollo 13 flight passed the far side of the Moon at an altitude of 254 kilometers (158 miles; 137 nautical miles) above the lunar surface, and 400,171 km (248,655 mi) from Earth, marking the record for the farthest humans have ever traveled from Earth in 1970.
Voyager 1 is currently at a distance of 145.11 astronomical units (2.1708×1010 km; 1.3489×1010 mi) (21.708 billion kilometers; 13.489 billion miles) from Earth as of 1 January 2019. It is the most distant human-made object from Earth.
GN-z11 is the most distant known object from Earth, reported as 13.4 billion light-years away.
The dream of stepping into the outer reaches of Earth's atmosphere was driven by the fiction of Jules Verne and H. G. Wells, and rocket technology was developed to try to realize this vision. The German V-2 was the first rocket to travel into space, overcoming the problems of thrust and material failure. During the final days of World War II this technology was obtained by both the Americans and Soviets as were its designers. The initial driving force for further development of the technology was a weapons race for intercontinental ballistic missiles (ICBMs) to be used as long-range carriers for fast nuclear weapon delivery, but in 1961 when the Soviet Union launched the first man into space, the United States declared itself to be in a "Space Race" with the Soviets.
Konstantin Tsiolkovsky, Robert Goddard, Hermann Oberth, and Reinhold Tiling laid the groundwork of rocketry in the early years of the 20th century.
Wernher von Braun was the lead rocket engineer for Nazi Germany's World War II V-2 rocket project. In the last days of the war he led a caravan of workers in the German rocket program to the American lines, where they surrendered and were brought to the United States to work on their rocket development ("Operation Paperclip"). He acquired American citizenship and led the team that developed and launched Explorer 1, the first American satellite. Von Braun later led the team at NASA's Marshall Space Flight Center which developed the Saturn V moon rocket.
Initially the race for space was often led by Sergei Korolev, whose legacy includes both the R7 and Soyuz—which remain in service to this day. Korolev was the mastermind behind the first satellite, first man (and first woman) in orbit and first spacewalk. Until his death his identity was a closely guarded state secret; not even his mother knew that he was responsible for creating the Soviet space program.
After Korolev's death in 1966, Kerim Kerimov was named chairman of the State Commission for Flight Testing of the Soyuz program, partially on the basis of Korolev's personal recommendation. Kerimov, like Korolev, began his career as a rocket engineer; he had worked on several important projects in the space program, including the State Commission for the Vostok program. During his tenure, he oversaw the linkup between Cosmos 186 and Cosmos 188, managed Soyuz spacecraft launches, and oversaw the early Salyut and Mir series space stations. He was demoted in 1974 due to his support for the N1 rocket program and the manner lunar program, both of which had fallen out of favor.
Other key people:
Starting in the mid-20th century probes and then human mission were sent into Earth orbit, and then on to the Moon. Also, probes were sent throughout the known Solar system, and into Solar orbit. Uncrewed spacecraft have been sent into orbit around Saturn, Jupiter, Mars, Venus, and Mercury by the 21st century, and the most distance active spacecraft, Voyager 1 and 2 traveled beyond 100 times the Earth-Sun distance. The instruments were enough though that it is thought they have left the Sun's heliosphere, a sort of bubble of particles made in the Galaxy by the Sun's solar wind.
The Sun is a major focus of space exploration. Being above the atmosphere in particular and Earth's magnetic field gives access to the solar wind and infrared and ultraviolet radiations that cannot reach Earth's surface. The Sun generates most space weather, which can affect power generation and transmission systems on Earth and interfere with, and even damage, satellites and space probes. Numerous spacecraft dedicated to observing the Sun, beginning with the Apollo Telescope Mount, have been launched and still others have had solar observation as a secondary objective. Parker Solar Probe, launched in 2018, will approach the Sun to within 1/8th the orbit of Mercury.
Mercury remains the least explored of the Terrestrial planets. As of May 2013, the Mariner 10 and MESSENGER missions have been the only missions that have made close observations of Mercury. MESSENGER entered orbit around Mercury in March 2011, to further investigate the observations made by Mariner 10 in 1975 (Munsell, 2006b).
A third mission to Mercury, scheduled to arrive in 2025, BepiColombo is to include two probes. BepiColombo is a joint mission between Japan and the European Space Agency. MESSENGER and BepiColombo are intended to gather complementary data to help scientists understand many of the mysteries discovered by Mariner 10's flybys.
Flights to other planets within the Solar System are accomplished at a cost in energy, which is described by the net change in velocity of the spacecraft, or delta-v. Due to the relatively high delta-v to reach Mercury and its proximity to the Sun, it is difficult to explore and orbits around it are rather unstable.
Venus was the first target of interplanetary flyby and lander missions and, despite one of the most hostile surface environments in the Solar System, has had more landers sent to it (nearly all from the Soviet Union) than any other planet in the Solar System. The first flyby was the 1961 Venera 1, though the 1962 Mariner 2 was the first flyby to successfully return data. Mariner 2 has been followed by several other flybys by multiple space agencies often as part of missions using a Venus flyby to provide a gravitational assist en route to other celestial bodies. In 1967 Venera 4 became the first probe to enter and directly examine the atmosphere of Venus. In 1970, Venera 7 became the first successful lander to reach the surface of Venus and by 1985 it had been followed by eight additional successful Soviet Venus landers which provided images and other direct surface data. Starting in 1975 with the Soviet orbiter Venera 9 some ten successful orbiter missions have been sent to Venus, including later missions which were able to map the surface of Venus using radar to pierce the obscuring atmosphere.
Space exploration has been used as a tool to understand Earth as a celestial object in its own right. Orbital missions can provide data for Earth that can be difficult or impossible to obtain from a purely ground-based point of reference.
For example, the existence of the Van Allen radiation belts was unknown until their discovery by the United States' first artificial satellite, Explorer 1. These belts contain radiation trapped by Earth's magnetic fields, which currently renders construction of habitable space stations above 1000 km impractical.
Following this early unexpected discovery, a large number of Earth observation satellites have been deployed specifically to explore Earth from a space based perspective. These satellites have significantly contributed to the understanding of a variety of Earth-based phenomena. For instance, the hole in the ozone layer was found by an artificial satellite that was exploring Earth's atmosphere, and satellites have allowed for the discovery of archeological sites or geological formations that were difficult or impossible to otherwise identify.
The Moon was the first celestial body to be the object of space exploration. It holds the distinctions of being the first remote celestial object to be flown by, orbited, and landed upon by spacecraft, and the only remote celestial object ever to be visited by humans.
In 1959 the Soviets obtained the first images of the far side of the Moon, never previously visible to humans. The U.S. exploration of the Moon began with the Ranger 4 impactor in 1962. Starting in 1966 the Soviets successfully deployed a number of landers to the Moon which were able to obtain data directly from the Moon's surface; just four months later, Surveyor 1 marked the debut of a successful series of U.S. landers. The Soviet uncrewed missions culminated in the Lunokhod program in the early 1970s, which included the first uncrewed rovers and also successfully brought lunar soil samples to Earth for study. This marked the first (and to date the only) automated return of extraterrestrial soil samples to Earth. Uncrewed exploration of the Moon continues with various nations periodically deploying lunar orbiters, and in 2008 the Indian Moon Impact Probe.
Crewed exploration of the Moon began in 1968 with the Apollo 8 mission that successfully orbited the Moon, the first time any extraterrestrial object was orbited by humans. In 1969, the Apollo 11 mission marked the first time humans set foot upon another world. Crewed exploration of the Moon did not continue for long. The Apollo 17 mission in 1972 marked the sixth landing and the most recent human visit. Artemis 2 will flyby the Moon in 2022. Robotic missions are still pursued vigorously.
The exploration of Mars has been an important part of the space exploration programs of the Soviet Union (later Russia), the United States, Europe, Japan and India. Dozens of robotic spacecraft, including orbiters, landers, and rovers, have been launched toward Mars since the 1960s. These missions were aimed at gathering data about current conditions and answering questions about the history of Mars. The questions raised by the scientific community are expected to not only give a better appreciation of the red planet but also yield further insight into the past, and possible future, of Earth.
Mars is the prime candidate where humans could live outside the Earth and the technology to reach Mars is possible.
The exploration of Mars has come at a considerable financial cost with roughly two-thirds of all spacecraft destined for Mars failing before completing their missions, with some failing before they even began. Such a high failure rate can be attributed to the complexity and large number of variables involved in an interplanetary journey, and has led researchers to jokingly speak of The Great Galactic Ghoul which subsists on a diet of Mars probes. This phenomenon is also informally known as the "Mars Curse".
In contrast to overall high failure rates in the exploration of Mars, India has become the first country to achieve success of its maiden attempt.  India's Mars Orbiter Mission (MOM) is one of the least expensive interplanetary missions ever undertaken with an approximate total cost of ₹ 450 Crore (US$73 million). The first mission to Mars by any Arab country has been taken up by the United Arab Emirates. Called the Emirates Mars Mission, it is scheduled for launch in 2020. The uncrewed exploratory probe has been named "Hope Probe" and will be sent to Mars to study its atmosphere in detail.
The Russian space mission Fobos-Grunt, which launched on 9 November 2011 experienced a failure leaving it stranded in low Earth orbit. It was to begin exploration of the Phobos and Martian circumterrestrial orbit, and study whether the moons of Mars, or at least Phobos, could be a "trans-shipment point" for spaceships traveling to Mars.
Until the advent of space travel, objects in the asteroid belt were merely pinpricks of light in even the largest telescopes, their shapes and terrain remaining a mystery.
Several asteroids have now been visited by probes, the first of which was Galileo, which flew past two: 951 Gaspra in 1991, followed by 243 Ida in 1993. Both of these lay near enough to Galileo's planned trajectory to Jupiter that they could be visited at acceptable cost. The first landing on an asteroid was performed by the NEAR Shoemaker probe in 2000, following an orbital survey of the object. The dwarf planet Ceres and the asteroid 4 Vesta, two of the three largest asteroids, were visited by NASA's Dawn spacecraft, launched in 2007.
Hayabusa was a robotic spacecraft developed by the Japan Aerospace Exploration Agency to return a sample of material from the small near-Earth asteroid 25143 Itokawa to Earth for further analysis. Hayabusa was launched on 9 May 2003 and rendezvoused with Itokawa in mid-September 2005. After arriving at Itokawa, Hayabusa studied the asteroid's shape, spin, topography, color, composition, density, and history. In November 2005, it landed on the asteroid twice to collect samples. The spacecraft returned to Earth on 13 June 2010.
The exploration of Jupiter has consisted solely of a number of automated NASA spacecraft visiting the planet since 1973. A large majority of the missions have been "flybys", in which detailed observations are taken without the probe landing or entering orbit; such as in Pioneer and Voyager programs. The Galileo and Juno spacecraft are the only spacecraft to have entered the planet's orbit. As Jupiter is believed to have only a relatively small rocky core and no real solid surface, a landing mission is precluded.
Reaching Jupiter from Earth requires a delta-v of 9.2 km/s, which is comparable to the 9.7 km/s delta-v needed to reach low Earth orbit. Fortunately, gravity assists through planetary flybys can be used to reduce the energy required at launch to reach Jupiter, albeit at the cost of a significantly longer flight duration.
Jupiter has 80 known moons, many of which have relatively little known information about them.
Saturn has been explored only through uncrewed spacecraft launched by NASA, including one mission (Cassini–Huygens) planned and executed in cooperation with other space agencies. These missions consist of flybys in 1979 by Pioneer 11, in 1980 by Voyager 1, in 1982 by Voyager 2 and an orbital mission by the Cassini spacecraft, which lasted from 2004 until 2017.
Saturn has at least 62 known moons, although the exact number is debatable since Saturn's rings are made up of vast numbers of independently orbiting objects of varying sizes. The largest of the moons is Titan, which holds the distinction of being the only moon in the Solar System with an atmosphere denser and thicker than that of Earth.  Titan holds the distinction of being the only object in the Outer Solar System that has been explored with a lander, the Huygens probe deployed by the Cassini spacecraft.
The exploration of Uranus has been entirely through the Voyager 2 spacecraft, with no other visits currently planned. Given its axial tilt of 97.77°, with its polar regions exposed to sunlight or darkness for long periods, scientists were not sure what to expect at Uranus. The closest approach to Uranus occurred on 24 January 1986. Voyager 2 studied the planet's unique atmosphere and magnetosphere. Voyager 2 also examined its ring system and the moons of Uranus including all five of the previously known moons, while discovering an additional ten previously unknown moons.
Images of Uranus proved to have a very uniform appearance, with no evidence of the dramatic storms or atmospheric banding evident on Jupiter and Saturn. Great effort was required to even identify a few clouds in the images of the planet. The magnetosphere of Uranus, however, proved to be unique, being profoundly affected by the planet's unusual axial tilt. In contrast to the bland appearance of Uranus itself, striking images were obtained of the Moons of Uranus, including evidence that Miranda had been unusually geologically active.
The exploration of Neptune began with 25 August 1989 Voyager 2 flyby, the sole visit to the system as of 2014. The possibility of a Neptune Orbiter has been discussed, but no other missions have been given serious thought.
Although the extremely uniform appearance of Uranus during Voyager 2's visit in 1986 had led to expectations that Neptune would also have few visible atmospheric phenomena, the spacecraft found that Neptune had obvious banding, visible clouds, auroras, and even a conspicuous anticyclone storm system rivaled in size only by Jupiter's Great Red Spot. Neptune also proved to have the fastest winds of any planet in the Solar System, measured as high as 2,100 km/h. Voyager 2 also examined Neptune's ring and moon system. It discovered 900 complete rings and additional partial ring "arcs" around Neptune. In addition to examining Neptune's three previously known moons, Voyager 2 also discovered five previously unknown moons, one of which, Proteus, proved to be the last largest moon in the system. Data from Voyager 2 supported the view that Neptune's largest moon, Triton, is a captured Kuiper belt object.
The dwarf planet Pluto presents significant challenges for spacecraft because of its great distance from Earth (requiring high velocity for reasonable trip times) and small mass (making capture into orbit very difficult at present). Voyager 1 could have visited Pluto, but controllers opted instead for a close flyby of Saturn's moon Titan, resulting in a trajectory incompatible with a Pluto flyby. Voyager 2 never had a plausible trajectory for reaching Pluto.
After an intense political battle, a mission to Pluto dubbed New Horizons was granted funding from the United States government in 2003. New Horizons was launched successfully on 19 January 2006. In early 2007 the craft made use of a gravity assist from Jupiter. Its closest approach to Pluto was on 14 July 2015; scientific observations of Pluto began five months prior to closest approach and continued for 16 days after the encounter.
The New Horizons mission did a flyby of the small planetesimal Arrokoth in 2019.
Although many comets have been studied from Earth sometimes with centuries-worth of observations, only a few comets have been closely visited. In 1985, the International Cometary Explorer conducted the first comet fly-by (21P/Giacobini-Zinner) before joining the Halley Armada studying the famous comet. The Deep Impact probe smashed into 9P/Tempel to learn more about its structure and composition and the Stardust mission returned samples of another comet's tail. The Philae lander successfully landed on Comet Churyumov–Gerasimenko in 2014 as part of the broader Rosetta mission.
Deep space exploration is the branch of astronomy, astronautics and space technology that is involved with the exploration of distant regions of outer space. Physical exploration of space is conducted both by human spaceflights (deep-space astronautics) and by robotic spacecraft.
Some of the best candidates for future deep space engine technologies include anti-matter, nuclear power and beamed propulsion. The latter, beamed propulsion, appears to be the best candidate for deep space exploration presently available, since it uses known physics and known technology that is being developed for other purposes.
Breakthrough Starshot is a research and engineering project by the Breakthrough Initiatives to develop a proof-of-concept fleet of light sail spacecraft named StarChip, to be capable of making the journey to the Alpha Centauri star system 4.37 light-years away. It was founded in 2016 by Yuri Milner, Stephen Hawking, and Mark Zuckerberg.
An article in science magazine Nature suggested the use of asteroids as a gateway for space exploration, with the ultimate destination being Mars. In order to make such an approach viable, three requirements need to be fulfilled: first, "a thorough asteroid survey to find thousands of nearby bodies suitable for astronauts to visit"; second, "extending flight duration and distance capability to ever-increasing ranges out to Mars"; and finally, "developing better robotic vehicles and tools to enable astronauts to explore an asteroid regardless of its size, shape or spin". Furthermore, using asteroids would provide astronauts with protection from galactic cosmic rays, with mission crews being able to land on them without great risk to radiation exposure.
The James Webb Space Telescope (JWST or "Webb") is a space telescope that is planned to be the successor to the Hubble Space Telescope. The JWST will provide greatly improved resolution and sensitivity over the Hubble, and will enable a broad range of investigations across the fields of astronomy and cosmology, including observing some of the most distant events and objects in the universe, such as the formation of the first galaxies. Other goals include understanding the formation of stars and planets, and direct imaging of exoplanets and novas.
The primary mirror of the JWST, the Optical Telescope Element, is composed of 18 hexagonal mirror segments made of gold-plated beryllium which combine to create a 6.5-meter (21 ft; 260 in) diameter mirror that is much larger than the Hubble's 2.4-meter (7.9 ft; 94 in) mirror.  Unlike the Hubble, which observes in the near ultraviolet, visible, and near infrared (0.1 to 1 μm) spectra, the JWST will observe in a lower frequency range, from long-wavelength visible light through mid-infrared (0.6 to 27 μm), which will allow it to observe high redshift objects that are too old and too distant for the Hubble to observe. The telescope must be kept very cold in order to observe in the infrared without interference, so it will be deployed in space near the Earth–Sun L2 Lagrangian point, and a large sunshield made of silicon- and aluminum-coated Kapton will keep its mirror and instruments below 50 K (−220 °C; −370 °F).
The Artemis program is an ongoing crewed spaceflight program carried out by NASA, U.S. commercial spaceflight companies, and international partners such as ESA, with the goal of landing "the first woman and the next man" on the Moon, specifically at the lunar south pole region by 2024. Artemis would be the next step towards the long-term goal of establishing a sustainable presence on the Moon, laying the foundation for private companies to build a lunar economy, and eventually sending humans to Mars.
In 2017, the lunar campaign was authorized by Space Policy Directive 1, utilizing various ongoing spacecraft programs such as Orion, the Lunar Gateway, Commercial Lunar Payload Services, and adding an undeveloped crewed lander. The Space Launch System will serve as the primary launch vehicle for Orion, while commercial launch vehicles are planned for use to launch various other elements of the campaign. NASA requested $1.6 billion in additional funding for Artemis for fiscal year 2020, while the Senate Appropriations Committee requested from NASA a five-year budget profile which is needed for evaluation and approval by Congress.
The research that is conducted by national space exploration agencies, such as NASA and Roscosmos, is one of the reasons supporters cite to justify government expenses. Economic analyses of the NASA programs often showed ongoing economic benefits (such as NASA spin-offs), generating many times the revenue of the cost of the program. It is also argued that space exploration would lead to the extraction of resources on other planets and especially asteroids, which contain billions of dollars that worth of minerals and metals. Such expeditions could generate a lot of revenue. In addition, it has been argued that space exploration programs help inspire youth to study in science and engineering. Space exploration also gives scientists the ability to perform experiments in other settings and expand humanity's knowledge.
Another claim is that space exploration is a necessity to mankind and that staying on Earth will lead to extinction. Some of the reasons are lack of natural resources, comets, nuclear war, and worldwide epidemic. Stephen Hawking, renowned British theoretical physicist, said that "I don't think the human race will survive the next thousand years, unless we spread into space. There are too many accidents that can befall life on a single planet. But I'm an optimist. We will reach out to the stars." Arthur C. Clarke (1950) presented a summary of motivations for the human exploration of space in his non-fiction semi-technical monograph Interplanetary Flight. He argued that humanity's choice is essentially between expansion off Earth into space, versus cultural (and eventually biological) stagnation and death.
These motivations could be attributed to one of the first rocket scientists in NASA, Wernher von Braun, and his vision of humans moving beyond Earth. The basis of this plan was to:
"Develop multi-stage rockets capable of placing satellites, animals, and humans in space.
Development of large, winged reusable spacecraft capable of carrying humans and equipment into Earth orbit in a way that made space access routine and cost-effective.
Construction of a large, permanently occupied space station to be used as a platform both to observe Earth and from which to launch deep space expeditions.
Launching the first human flights around the Moon, leading to the first landings of humans on the Moon, with the intent of exploring that body and establishing permanent lunar bases.
Assembly and fueling of spaceships in Earth orbit for the purpose of sending humans to Mars with the intent of eventually colonizing that planet".
Known as the Von Braun Paradigm, the plan was formulated to lead humans in the exploration of space. Von Braun's vision of human space exploration served as the model for efforts in space exploration well into the twenty-first century, with NASA incorporating this approach into the majority of their projects. The steps were followed out of order, as seen by the Apollo program reaching the moon before the space shuttle program was started, which in turn was used to complete the International Space Station. Von Braun's Paradigm formed NASA's drive for human exploration, in the hopes that humans discover the far reaches of the universe.
NASA has produced a series of public service announcement videos supporting the concept of space exploration.
Overall, the public remains largely supportive of both crewed and uncrewed space exploration. According to an Associated Press Poll conducted in July 2003, 71% of U.S. citizens agreed with the statement that the space program is "a good investment", compared to 21% who did not.
Space advocacy and space policy regularly invokes exploration as a human nature.
This advocacy has been criticized by scholars as essentializing and continuation of colonialism, particularly manifest destiny, making space exploration misaligned with science and a less inclusive field.
Spaceflight is the use of space technology to achieve the flight of spacecraft into and through outer space.
Spaceflight is used in space exploration, and also in commercial activities like space tourism and satellite telecommunications. Additional non-commercial uses of spaceflight include space observatories, reconnaissance satellites and other Earth observation satellites.
A spaceflight typically begins with a rocket launch, which provides the initial thrust to overcome the force of gravity and propels the spacecraft from the surface of Earth. Once in space, the motion of a spacecraft—both when unpropelled and when under propulsion—is covered by the area of study called astrodynamics. Some spacecraft remain in space indefinitely, some disintegrate during atmospheric reentry, and others reach a planetary or lunar surface for landing or impact.
Satellites are used for a large number of purposes. Common types include military (spy) and civilian Earth observation satellites, communication satellites, navigation satellites, weather satellites, and research satellites. Space stations and human spacecraft in orbit are also satellites.
The commercialization of space first started out with the launching of private satellites by NASA or other space agencies. Current examples of the commercial satellite use of space include satellite navigation systems, satellite television and satellite radio. The next step of commercialization of space was seen as human spaceflight. Flying humans safely to and from space had become routine to NASA. Reusable spacecraft were an entirely new engineering challenge, something only seen in novels and films like Star Trek and War of the Worlds. Great names like Buzz Aldrin supported the use of making a reusable vehicle like the space shuttle. Aldrin held that reusable spacecraft were the key in making space travel affordable, stating that the use of "passenger space travel is a huge potential market big enough to justify the creation of reusable launch vehicles". How can the public go against the words of one of America's best known heroes in space exploration? After all exploring space is the next great expedition, following the example of Lewis and Clark.Space tourism is the next step reusable vehicles in the commercialization of space. The purpose of this form of space travel is used by individuals for the purpose of personal pleasure.
Private spaceflight companies such as SpaceX and Blue Origin, and commercial space stations such as the Axiom Space and the Bigelow Commercial Space Station have dramatically changed the landscape of space exploration, and will continue to do so in the near future.
Astrobiology is the interdisciplinary study of life in the universe, combining aspects of astronomy, biology and geology. It is focused primarily on the study of the origin, distribution and evolution of life. It is also known as exobiology (from Greek: έξω, exo, "outside"). The term "Xenobiology" has been used as well, but this is technically incorrect because its terminology means "biology of the foreigners". Astrobiologists must also consider the possibility of life that is chemically entirely distinct from any life found on Earth. In the Solar System some of the prime locations for current or past astrobiology are on Enceladus, Europa, Mars, and Titan.
To date, the longest human occupation of space is the International Space Station which has been in continuous use for 21 years, 252 days. Valeri Polyakov's record single spaceflight of almost 438 days aboard the Mir space station has not been surpassed. The health effects of space have been well documented through years of research conducted in the field of aerospace medicine. Analog environments similar to those one may experience in space travel (like deep sea submarines) have been used in this research to further explore the relationship between isolation and extreme environments. It is imperative that the health of the crew be maintained as any deviation from baseline may compromise the integrity of the mission as well as the safety of the crew, hence the reason why astronauts must endure rigorous medical screenings and tests prior to embarking on any missions. However, it does not take long for the environmental dynamics of spaceflight to commence its toll on the human body; for example, space motion sickness (SMS) - a condition which affects the neurovestibular system and culminates in mild to severe signs and symptoms such as vertigo, dizziness, fatigue, nausea, and disorientation - plagues almost all space travelers within their first few days in orbit. Space travel can also have a profound impact on the psyche of the crew members as delineated in anecdotal writings composed after their retirement. Space travel can adversely affect the body's natural biological clock (circadian rhythm); sleep patterns causing sleep deprivation and fatigue; and social interaction; consequently, residing in a Low Earth Orbit (LEO) environment for a prolonged amount of time can result in both mental and physical exhaustion. Long-term stays in space reveal issues with bone and muscle loss in low gravity, immune system suppression, and radiation exposure. The lack of gravity causes fluid to rise upward which can cause pressure to build up in the eye, resulting in vision problems; the loss of bone minerals and densities; cardiovascular deconditioning; and decreased endurance and muscle mass.
Radiation is perhaps the most insidious health hazard to space travelers as it is invisible to the naked eye and can cause cancer. Space craft are no longer protected from the sun's radiation as they are positioned above the Earth's magnetic field; the danger of radiation is even more potent when one enters deep space. The hazards of radiation can be ameliorated through protective shielding on the spacecraft, alerts, and dosimetry.
Fortunately, with new and rapidly evolving technological advancements, those in Mission Control are able to monitor the health of their astronauts more closely utilizing telemedicine. One may not be able to completely evade the physiological effects of space flight, but they can be mitigated. For example, medical systems aboard space vessels such as the International Space Station (ISS) are well equipped and designed to counteract the effects of lack of gravity and weightlessness; on-board treadmills can help prevent muscle loss and reduce the risk of developing premature osteoporosis. Additionally, a crew medical officer is appointed for each ISS mission and a flight surgeon is available 24/7 via the ISS Mission Control Center located in Houston, Texas. Although the interactions are intended to take place in real time, communications between the space and terrestrial crew may become delayed - sometimes by as much as 20 minutes - as their distance from each other increases when the spacecraft moves further out of LEO; because of this the crew are trained and need to be prepared to respond to any medical emergencies that may arise on the vessel as the ground crew are hundreds of miles away. As one can see, travelling and possibly living in space poses many challenges. Many past and current concepts for the continued exploration and colonization of space focus on a return to the Moon as a "stepping stone" to the other planets, especially Mars. At the end of 2006 NASA announced they were planning to build a permanent Moon base with continual presence by 2024.
Beyond the technical factors that could make living in space more widespread, it has been suggested that the lack of private property, the inability or difficulty in establishing property rights in space, has been an impediment to the development of space for human habitation. Since the advent of space technology in the latter half of the twentieth century, the ownership of property in space has been murky, with strong arguments both for and against. In particular, the making of national territorial claims in outer space and on celestial bodies has been specifically proscribed by the Outer Space Treaty, which had been, as of 2012, ratified by all spacefaring nations.
Space colonization, also called space settlement and space humanization, would be the permanent autonomous (self-sufficient) human habitation of locations outside Earth, especially of natural satellites or planets such as the Moon or Mars, using significant amounts of in-situ resource utilization.
Participation and representation of humanity in space is an issue ever since the first phase of space exploration. Some rights of non-spacefaring countries have been secured through international space law, declaring space the "province of all mankind", understanding spaceflight as its resource, though sharing of space for all humanity is still criticized as imperialist and lacking. Additionally to international inclusion, the inclusion of women and people of colour has also been lacking. To reach a more inclusive spaceflight some organizations like the Justspace Alliance and IAU featured Inclusive Astronomy have been formed in recent years.
The first woman to ever enter space was Valentina Tereshkova. She flew in 1963 but it was not until the 1980s that another woman entered space again. All astronauts were required to be military test pilots at the time and women were not able to enter this career, this is one reason for the delay in allowing women to join space crews. After the rule changed, Svetlana Savitskaya became the second woman to enter space, she was also from the Soviet Union. Sally Ride became the next woman to enter space and the first woman to enter space through the United States program.
Since then, eleven other countries have allowed women astronauts. The first all female space walk occurred in 2018, including Christina Koch and Jessica Meir. These two women have both participated in separate space walks with NASA. The first woman to go to the moon is planned for 2024.
Despite these developments women are still underrepresented among astronauts and especially cosmonauts. Issues that block potential applicants from the programs and limit the space missions they are able to go on, are for example:
Artistry in and from space ranges from signals, capturing and arranging material like Yuri Gagarin's selfie in space or the image The Blue Marble, over drawings like the first one in space by cosmonaut and artist Alexei Leonov, music videos like Chris Hadfield's cover of Space Oddity on board the ISS, to permanent installations on celestial bodies like on the Moon.
Solar System → Local Interstellar Cloud → Local Bubble → Gould Belt → Orion Arm → Milky Way → Milky Way subgroup → Local Group → Local Sheet → Virgo Supercluster → Laniakea Supercluster → KBC Void → Observable universe → UniverseEach arrow (→) may be read as "within" or "part of".

A rocket engine uses stored rocket propellants as the reaction mass for forming a high-speed propulsive jet of fluid, usually high-temperature gas. Rocket engines are reaction engines, producing thrust by ejecting mass rearward, in accordance with Newton's third law. Most rocket engines use the combustion of reactive chemicals to supply the necessary energy, but non-combusting forms such as cold gas thrusters and nuclear thermal rockets also exist. Vehicles propelled by rocket engines are commonly called rockets. Rocket vehicles carry their own oxidiser, unlike most combustion engines, so rocket engines can be used in a vacuum to propel spacecraft and ballistic missiles.
Compared to other types of jet engine, rocket engines are the lightest and have the highest thrust, but are the least propellant-efficient (they have the lowest specific impulse). The ideal exhaust is hydrogen, the lightest of all elements, but chemical rockets produce a mix of heavier species, reducing the exhaust velocity.
Rocket engines become more efficient at high speeds, due to the Oberth effect.
Here, "rocket" is used as an abbreviation for "rocket engine".
Thermal rockets use an inert propellant, heated by electricity (electrothermal propulsion) or a nuclear reactor (nuclear thermal rocket).
Chemical rockets are powered by exothermic reduction-oxidation chemical reactions of the propellant:
Rocket engines produce thrust by the expulsion of an exhaust fluid that has been accelerated to high speed through a propelling nozzle. The fluid is usually a gas created by high pressure (150-to-4,350-pound-per-square-inch (10 to 300 bar)) combustion of solid or liquid propellants, consisting of fuel and oxidiser components, within a combustion chamber. As the gases expand through the nozzle, they are accelerated to very high (supersonic) speed, and the reaction to this pushes the engine in the opposite direction. Combustion is most frequently used for practical rockets, as the laws of thermodynamics (specifically Carnot's theorem) dictate that high temperatures and pressures are desirable for the best thermal efficiency. Nuclear thermal rockets are capable of higher efficiencies, but currently have environmental problems which preclude their routine use in the Earth's atmosphere and cislunar space.
For model rocketry, an available alternative to combustion is the water rocket pressurized by compressed air, carbon dioxide, nitrogen, or any other readily available, inert gas.
Rocket propellant is mass that is stored, usually in some form of tank, or within the combustion chamber itself, prior to being ejected from a rocket engine in the form of a fluid jet to produce thrust.
Chemical rocket propellants are the most commonly used. These undergo exothermic chemical reactions  producing a hot gas jet for propulsion. Alternatively, a chemically inert reaction mass can be heated by a high-energy power source through a heat exchanger in lieu of a combustion chamber.
Solid rocket propellants are prepared in a mixture of fuel and oxidising components called grain, and the propellant storage casing effectively becomes the combustion chamber.
Liquid-fuelled rockets force separate fuel and oxidiser components into the combustion chamber, where they mix and burn. Hybrid rocket engines use a combination of solid and liquid or gaseous propellants. Both liquid and hybrid rockets use injectors to introduce the propellant into the chamber. These are often an array of simple jets – holes through which the propellant escapes under pressure; but sometimes may be more complex spray nozzles. When two or more propellants are injected, the jets usually deliberately cause the propellants to collide as this breaks up the flow into smaller droplets that burn more easily.
For chemical rockets the combustion chamber is typically cylindrical, and flame holders, used to hold a part of the combustion in a slower-flowing portion of the combustion chamber, are not needed. The dimensions of the cylinder are such that the propellant is able to combust thoroughly; different rocket propellants require different combustion chamber sizes for this to occur.
This leads to a number called 




L

∗




{\displaystyle L^{*}}

, the characteristic length:
where:
L* is typically in the range of 64–152 centimetres (25–60 in).
The temperatures and pressures typically reached in a rocket combustion chamber in order to achieve practical thermal efficiency are extreme compared to a non-afterburning airbreathing jet engine. No atmospheric nitrogen is present to dilute and cool the combustion, so the propellant mixture can reach true stoichiometric ratios. This, in combination with the high pressures, means that the rate of heat conduction through the walls is very high.
In order for fuel and oxidiser to flow into the chamber, the pressure of the propellants entering the combustion chamber must exceed the pressure inside the combustion chamber itself.  This may be accomplished by a variety of design approaches including turbopumps or, in simpler engines, via sufficient tank pressure to advance fluid flow. Tank pressure may be maintained by several means, including a high-pressure helium pressurization system common to many large rocket engines or, in some newer rocket systems, by a bleed-off of high-pressure gas from the engine cycle to autogenously pressurize the propellant tanks  For example, the self-pressurization gas system of the SpaceX Starship is a critical part of SpaceX strategy to reduce launch vehicle fluids from five in their legacy Falcon 9 vehicle family to just two in Starship, eliminating not only the helium tank pressurant but all hypergolic propellants as well as nitrogen for cold-gas reaction-control thrusters.
The hot gas produced in the combustion chamber is permitted to escape through an opening (the "throat"), and then through a diverging expansion section. When sufficient pressure is provided to the nozzle (about 2.5–3 times ambient pressure), the nozzle chokes and a supersonic jet is formed, dramatically accelerating the gas, converting most of the thermal energy into kinetic energy. Exhaust speeds vary, depending on the expansion ratio the nozzle is designed for, but exhaust speeds as high as ten times the speed of sound in air at sea level are not uncommon. About half of the rocket engine's thrust comes from the unbalanced pressures inside the combustion chamber, and the rest comes from the pressures acting against the inside of the nozzle (see diagram). As the gas expands (adiabatically) the pressure against the nozzle's walls forces the rocket engine in one direction while accelerating the gas in the other.

The most commonly used nozzle is the de Laval nozzle, a fixed geometry nozzle with a high expansion-ratio. The large bell- or cone-shaped nozzle extension beyond the throat gives the rocket engine its characteristic shape.
The exit static pressure of the exhaust jet depends on the chamber pressure and the ratio of exit to throat area of the nozzle. As exit pressure varies from the ambient (atmospheric) pressure, a choked nozzle is said to be
In practice, perfect expansion is only achievable with a variable-exit area nozzle (since ambient pressure decreases as altitude increases), and is not possible above a certain altitude as ambient pressure approaches zero. If the nozzle is not perfectly expanded, then loss of efficiency occurs. Grossly over-expanded nozzles lose less efficiency, but can cause mechanical problems with the nozzle. Fixed-area nozzles become progressively more under-expanded as they gain altitude. Almost all de Laval nozzles will be momentarily grossly over-expanded during startup in an atmosphere.
Nozzle efficiency is affected by operation in the atmosphere because atmospheric pressure changes with altitude; but due to the supersonic speeds of the gas exiting from a rocket engine, the pressure of the jet may be either below or above ambient, and equilibrium between the two is not reached at all altitudes (see diagram).
For optimal performance, the pressure of the gas at the end of the nozzle should just equal the ambient pressure: if the exhaust's pressure is lower than the ambient pressure, then the vehicle will be slowed by the difference in pressure between the top of the engine and the exit; on the other hand, if the exhaust's pressure is higher, then exhaust pressure that could have been converted into thrust is not converted, and energy is wasted.
To maintain this ideal of equality between the exhaust's exit pressure and the ambient pressure, the diameter of the nozzle would need to increase with altitude, giving the pressure a longer nozzle to act on (and reducing the exit pressure and temperature). This increase is difficult to arrange in a lightweight fashion, although is routinely done with other forms of jet engines. In rocketry a lightweight compromise nozzle is generally used and some reduction in atmospheric performance occurs when used at other than the 'design altitude' or when throttled. To improve on this, various exotic nozzle designs such as the plug nozzle, stepped nozzles, the expanding nozzle and the aerospike have been proposed, each providing some way to adapt to changing ambient air pressure and each allowing the gas to expand further against the nozzle, giving extra thrust at higher altitudes.
When exhausting into a sufficiently low ambient pressure (vacuum) several issues arise. One is the sheer weight of the nozzle—beyond a certain point, for a particular vehicle, the extra weight of the nozzle outweighs any performance gained. Secondly, as the exhaust gases adiabatically expand within the nozzle they cool, and eventually some of the chemicals can freeze, producing 'snow' within the jet. This causes instabilities in the jet and must be avoided.
On a de Laval nozzle, exhaust gas flow detachment will occur in a grossly over-expanded nozzle.  As the detachment point will not be uniform around the axis of the engine, a side force may be imparted to the engine. This side force may change over time and result in control problems with the launch vehicle.
Advanced altitude-compensating designs, such as the aerospike or plug nozzle, attempt to minimize performance losses by adjusting to varying expansion ratio caused by changing altitude.
For a rocket engine to be propellant efficient, it is important that the maximum pressures possible be created on the walls of the chamber and nozzle by a specific amount of propellant; as this is the source of the thrust. This can be achieved by all of:
Since all of these things minimise the mass of the propellant used, and since pressure is proportional to the mass of propellant present to be accelerated as it pushes on the engine, and since from Newton's third law the pressure that acts on the engine also reciprocally acts on the propellant, it turns out that for any given engine, the speed that the propellant leaves the chamber is unaffected by the chamber pressure (although the thrust is proportional). However, speed is significantly affected by all three of the above factors and the exhaust speed is an excellent measure of the engine propellant efficiency. This is termed exhaust velocity, and after allowance is made for factors that can reduce it, the effective exhaust velocity is one of the most important parameters of a rocket engine (although weight, cost, ease of manufacture etc. are usually also very important).
For aerodynamic reasons the flow goes sonic ("chokes") at the narrowest part of the nozzle, the 'throat'. Since the speed of sound in gases increases with the square root of temperature, the use of hot exhaust gas greatly improves performance. By comparison, at room temperature the speed of sound in air is about 340 m/s while the speed of sound in the hot gas of a rocket engine can be over 1700 m/s; much of this performance is due to the higher temperature, but additionally rocket propellants are chosen to be of low molecular mass, and this also gives a higher velocity compared to air.
Expansion in the rocket nozzle then further multiplies the speed, typically between 1.5 and 2 times, giving a highly collimated hypersonic exhaust jet. The speed increase of a rocket nozzle is mostly determined by its area expansion ratio—the ratio of the area of the exit to the area of the throat, but detailed properties of the gas are also important. Larger ratio nozzles are more massive but are able to extract more heat from the combustion gases, increasing the exhaust velocity.
Vehicles typically require the overall thrust to change direction over the length of the burn. A number of different ways to achieve this have been flown:
Rocket technology can combine very high thrust (meganewtons), very high exhaust speeds (around 10 times the speed of sound in air at sea level) and very high thrust/weight ratios (&gt;100) simultaneously as well as being able to operate outside the atmosphere, and while permitting the use of low pressure and hence lightweight tanks and structure.
Rockets can be further optimised to even more extreme performance along one or more of these axes at the expense of the others.
The most important metric for the efficiency of a rocket engine is impulse per unit of propellant, this is called specific impulse (usually written 




I

s
p




{\displaystyle I_{sp}}

). This is either measured as a speed (the effective exhaust velocity 




v

e




{\displaystyle v_{e}}

 in metres/second or ft/s) or as a time (seconds). For example, if an engine producing 100 pounds of thrust runs for 320 seconds and burns 100 pounds of propellant, then the specific impulse is 320 seconds. The higher the specific impulse, the less propellant is required to provide the desired impulse.
The specific impulse that can be achieved is primarily a function of the propellant mix (and ultimately would limit the specific impulse), but practical limits on chamber pressures and the nozzle expansion ratios reduce the performance that can be achieved.
Below is an approximate equation for calculating the net thrust of a rocket engine:
Since, unlike a jet engine, a conventional rocket motor lacks an air intake, there is no 'ram drag' to deduct from the gross thrust. Consequently, the net thrust of a rocket motor is equal to the gross thrust (apart from static back pressure).
The 






m
˙





v

e
−
o
p
t





{\displaystyle {\dot {m}}\;v_{e-opt}\,}

 term represents the momentum thrust, which remains constant at a given throttle setting, whereas the 




A

e


(

p

e


−

p

a
m
b


)



{\displaystyle A_{e}(p_{e}-p_{amb})\,}

 term represents the pressure thrust term. At full throttle, the net thrust of a rocket motor improves slightly with increasing altitude, because as atmospheric pressure decreases with altitude, the pressure thrust term increases. At the surface of the Earth the pressure thrust may be reduced by up to 30%, depending on the engine design. This reduction drops roughly exponentially to zero with increasing altitude.
Maximum efficiency for a rocket engine is achieved by maximising the momentum contribution of the equation without incurring penalties from over expanding the exhaust. This occurs when 




p

e


=

p

a
m
b




{\displaystyle p_{e}=p_{amb}}

. Since ambient pressure changes with altitude, most rocket engines spend very little time operating at peak efficiency.
Since specific impulse is force divided by the rate of mass flow, this equation means that the specific impulse varies with altitude.
Due to the specific impulse varying with pressure, a quantity that is easy to compare and calculate with is useful. Because rockets choke at the throat, and because the supersonic exhaust prevents external pressure influences travelling upstream, it turns out that the pressure at the exit is ideally exactly proportional to the propellant flow 






m
˙





{\displaystyle {\dot {m}}}

, provided the mixture ratios and combustion efficiencies are maintained. It is thus quite usual to rearrange the above equation slightly:
and so define the vacuum Isp to be:
where:
And hence:
Rockets can be throttled by controlling the propellant combustion rate 






m
˙





{\displaystyle {\dot {m}}}

 (usually measured in kg/s or lb/s). In liquid and hybrid rockets, the propellant flow entering the chamber is controlled using valves, in solid rockets it is controlled by changing the area of propellant that is burning and this can be designed into the propellant grain (and hence cannot be controlled in real-time).
Rockets can usually be throttled down to an exit pressure of about one-third of ambient pressure (often limited by flow separation in nozzles) and up to a maximum limit determined only by the mechanical strength of the engine.
In practice, the degree to which rockets can be throttled varies greatly, but most rockets can be throttled by a factor of 2 without great difficulty; the typical limitation is combustion stability, as for example, injectors need a minimum pressure to avoid triggering damaging oscillations (chugging or combustion instabilities); but injectors can be optimised and tested for wider ranges.
For example, some more recent liquid-propellant engine designs that have been optimised for greater throttling capability (BE-3, Raptor) can be throttled to as low as 18–20 percent of rated thrust.
Solid rockets can be throttled by using shaped grains that will vary their surface area over the course of the burn.
Rocket engine nozzles are surprisingly efficient heat engines for generating a high speed jet, as a consequence of the high combustion temperature and high compression ratio. Rocket nozzles give an excellent approximation to adiabatic expansion which is a reversible process, and hence they give efficiencies which are very close to that of the Carnot cycle. Given the temperatures reached, over 60% efficiency can be achieved with chemical rockets.
For a vehicle employing a rocket engine the energetic efficiency is very good if the vehicle speed approaches or somewhat exceeds the exhaust velocity (relative to launch); but at low speeds the energy efficiency goes to 0% at zero speed (as with all jet propulsion). See Rocket energy efficiency for more details.
Rockets, of all the jet engines, indeed of essentially all engines, have the highest thrust to weight ratio. This is especially true for liquid rocket engines.
This high performance is due to the small volume of pressure vessels that make up the engine—the pumps, pipes and combustion chambers involved. The lack of inlet duct and the use of dense liquid propellant allows the pressurisation system to be small and lightweight, whereas duct engines have to deal with air which has around three orders of magnitude lower density.
Of the liquid propellants used, density is lowest for liquid hydrogen. Although this propellant has the highest specific impulse, its very low density (about one fourteenth that of water) requires larger and heavier turbopumps and pipework, which decreases the engine's thrust-to-weight ratio (for example the RS-25) compared to those that do not (NK-33).
For efficiency reasons, higher temperatures are desirable, but materials lose their strength if the temperature becomes too high. Rockets run with combustion temperatures that can reach 3,500 K (3,200 °C; 5,800 °F).
Most other jet engines have gas turbines in the hot exhaust. Due to their larger surface area, they are harder to cool and hence there is a need to run the combustion processes at much lower temperatures, losing efficiency. In addition, duct engines use air as an oxidant, which contains 78% largely unreactive nitrogen, which dilutes the reaction and lowers the temperatures. Rockets have none of these inherent combustion temperature limiters.
The temperatures reached by combustion in rocket engines often substantially exceed the melting points of the nozzle and combustion chamber materials (about 1,200 K for copper). Most construction materials will also combust if exposed to high temperature oxidiser, which leads to a number of design challenges. The nozzle and combustion chamber walls must not be allowed to combust, melt, or vaporize (sometimes facetiously termed an "engine-rich exhaust").
Rockets that use the common construction materials such as aluminium, steel, nickel or copper alloys must employ cooling systems to limit the temperatures that engine structures experience. Regenerative cooling, where the propellant is passed through tubes around the combustion chamber or nozzle, and other techniques, such as curtain cooling or film cooling, are employed to give longer nozzle and chamber life. These techniques ensure that a gaseous thermal boundary layer touching the material is kept below the temperature which would cause the material to catastrophically fail.
Two material exceptions that can directly sustain rocket combustion temperatures are graphite and tungsten, although both are subject to oxidation if not protected. Materials technology, combined with the engine design, is a limiting factor in chemical rockets.
In rockets, the heat fluxes that can pass through the wall are among the highest in engineering; fluxes are generally in the range of 100–200 MW/m2. The strongest heat fluxes are found at the throat, which often sees twice that found in the associated chamber and nozzle. This is due to the combination of high speeds (which gives a very thin boundary layer), and although lower than the chamber, the high temperatures seen there. (See § Nozzle above for temperatures in nozzle).
In rockets the coolant methods include
In all cases the cooling effect that prevents the wall from being destroyed is caused by a thin layer of insulating fluid (a boundary layer) that is in contact with the walls that is far cooler than the combustion temperature. Provided this boundary layer is intact the wall will not be damaged.
Disruption of the boundary layer may occur during cooling failures or combustion instabilities, and wall failure typically occurs soon after.
With regenerative cooling a second boundary layer is found in the coolant channels around the chamber. This boundary layer thickness needs to be as small as possible, since the boundary layer acts as an insulator between the wall and the coolant. This may be achieved by making the coolant velocity in the channels as high as possible.
In practice, regenerative cooling is nearly always used in conjunction with curtain cooling and/or film cooling.
Liquid-fuelled engines are often run fuel-rich, which lowers combustion temperatures.  This reduces heat loads on the engine and allows lower cost materials and a simplified cooling system. This can also increase performance by lowering the average molecular weight of the exhaust and increasing the efficiency with which combustion heat is converted to kinetic exhaust energy.
Rocket combustion chambers are normally operated at fairly high pressure, typically 10–200 bar (1–20 MPa, 150–3,000 psi). When operated within significant atmospheric pressure, higher combustion chamber pressures give better performance by permitting a larger and more efficient nozzle to be fitted without it being grossly overexpanded.
However, these high pressures cause the outermost part of the chamber to be under very large hoop stresses – rocket engines are pressure vessels.
Worse, due to the high temperatures created in rocket engines the materials used tend to have a significantly lowered working tensile strength.
In addition, significant temperature gradients are set up in the walls of the chamber and nozzle, these cause differential expansion of the inner liner that create internal stresses.
The extreme vibration and acoustic environment inside a rocket motor commonly result in peak stresses well above mean values, especially in the presence of organ pipe-like resonances and gas turbulence.
The combustion may display undesired instabilities, of sudden or periodic nature. The pressure in the injection chamber may increase until the propellant flow through the injector plate decreases; a moment later the pressure drops and the flow increases, injecting more propellant in the combustion chamber which burns a moment later, and again increases the chamber pressure, repeating the cycle. This may lead to high-amplitude pressure oscillations, often in ultrasonic range, which may damage the motor. Oscillations of ±200 psi at 25 kHz were the cause of failures of early versions of the Titan II missile second stage engines. The other failure mode is a deflagration to detonation transition; the supersonic pressure wave formed in the combustion chamber may destroy the engine.
Combustion instability was also a problem during Atlas development. The Rocketdyne engines used in the Atlas family were found to suffer from this effect in several static firing tests, and three missile launches exploded on the pad due to rough combustion in the booster engines. In most cases, it occurred while attempting to start the engines with a "dry start" method whereby the igniter mechanism would be activated prior to propellant injection. During the process of man-rating Atlas for Project Mercury, solving combustion instability was a high priority, and the final two Mercury flights sported an upgraded propulsion system with baffled injectors and a hypergolic igniter.
The problem affecting Atlas vehicles was mainly the so-called "racetrack" phenomenon, where burning propellant would swirl around in a circle at faster and faster speeds, eventually producing vibration strong enough to rupture the engine, leading to complete destruction of the rocket. It was eventually solved by adding several baffles around the injector face to break up swirling propellant.
More significantly, combustion instability was a problem with the Saturn F-1 engines. Some of the early units tested exploded during static firing, which led to the addition of injector baffles.
In the Soviet space program, combustion instability also proved a problem on some rocket engines, including the RD-107 engine used in the R-7 family and the RD-216 used in the R-14 family, and several failures of these vehicles occurred before the problem was solved. Soviet engineering and manufacturing processes never satisfactorily resolved combustion instability in larger RP-1/LOX engines, so the RD-171 engine used to power the Zenit family still used four smaller thrust chambers fed by a common engine mechanism.
The combustion instabilities can be provoked by remains of cleaning solvents in the engine (e.g. the first attempted launch of a Titan II in 1962), reflected shock wave, initial instability after ignition, explosion near the nozzle that reflects into the combustion chamber, and many more factors. In stable engine designs the oscillations are quickly suppressed; in unstable designs they persist for prolonged periods. Oscillation suppressors are commonly used.
Periodic variations of thrust, caused by combustion instability or longitudinal vibrations of structures between the tanks and the engines which modulate the propellant flow, are known as "pogo oscillations" or "pogo", named after the pogo stick.
Three different types of combustion instabilities occur:
This is a low frequency oscillation at a few Hertz in chamber pressure usually caused by pressure variations in feed lines due to variations in acceleration of the vehicle.: 261  
This can cause cyclic variation in thrust, and the effects can vary from merely annoying to actually damaging the payload or vehicle. Chugging can be minimised by using gas-filled damping tubes on feed lines of high density propellants.
This can be caused due to insufficient pressure drop across the injectors.: 261  
It generally is mostly annoying, rather than being damaging. However, in extreme cases combustion can end up being forced backwards through the injectors – this can cause explosions with monopropellants.
This is the most immediately damaging, and the hardest to control. It is due to acoustics within the combustion chamber that often couples to the chemical combustion processes that are the primary drivers of the energy release, and can lead to unstable resonant "screeching" that commonly leads to catastrophic failure due to thinning of the insulating thermal boundary layer. Acoustic oscillations can be excited by thermal processes, such as the flow of hot air through a pipe or combustion in a chamber. Specifically, standing acoustic waves inside a chamber can be intensified if combustion occurs more intensely in regions where the pressure of the acoustic wave is maximal. 
Such effects are very difficult to predict analytically during the design process, and have usually been addressed by expensive, time-consuming and extensive testing, combined with trial and error remedial correction measures.
Screeching is often dealt with by detailed changes to injectors, or changes in the propellant chemistry, or vaporising the propellant before injection, or use of Helmholtz dampers within the combustion chambers to change the resonant modes of the chamber.
Testing for the possibility of screeching is sometimes done by exploding small explosive charges outside the combustion chamber with a tube set tangentially to the combustion chamber near the injectors to determine the engine's impulse response and then evaluating the time response of the chamber pressure- a fast recovery indicates a stable system.
For all but the very smallest sizes, rocket exhaust compared to other engines is generally very noisy. As the hypersonic exhaust mixes with the ambient air, shock waves are formed. The Space Shuttle generated over 200 dB(A) of noise around its base. To reduce this, and the risk of payload damage or injury to the crew atop the stack, the mobile launcher platform was fitted with a Sound Suppression System that sprayed 1.1 million litres (290,000 US gal) of water around the base of the rocket in 41 seconds at launch time. Using this system kept sound levels within the payload bay to 142 dB.
The sound intensity from the shock waves generated depends on the size of the rocket and on the exhaust velocity. Such shock waves seem to account for the characteristic crackling and popping sounds produced by large rocket engines when heard live. These noise peaks typically overload microphones and audio electronics, and so are generally weakened or entirely absent in recorded or broadcast audio reproductions. For large rockets at close range, the acoustic effects could actually kill.
More worryingly for space agencies, such sound levels can also damage the launch structure, or worse, be reflected back at the comparatively delicate rocket above. This is why so much water is typically used at launches. The water spray changes the acoustic qualities of the air and reduces or deflects the sound energy away from the rocket.
Generally speaking, noise is most intense when a rocket is close to the ground, since the noise from the engines radiates up away from the jet, as well as reflecting off the ground. Also, when the vehicle is moving slowly, little of the chemical energy input to the engine can go into increasing the kinetic energy of the rocket (since useful power P transmitted to the vehicle is 



P
=
F
∗
V


{\displaystyle P=F*V}

 for thrust F and speed V). Then the largest portion of the energy is dissipated in the exhaust's interaction with the ambient air, producing noise. This noise can be reduced somewhat by flame trenches with roofs, by water injection around the jet and by deflecting the jet at an angle.
Rocket engines are usually statically tested at a test facility before being put into production. For high altitude engines, either a shorter nozzle must be used, or the rocket must be tested in a large vacuum chamber.
Rocket vehicles have a reputation for unreliability and danger; especially catastrophic failures. Contrary to this reputation, carefully designed rockets can be made arbitrarily reliable. In military use, rockets are not unreliable. However, one of the main non-military uses of rockets is for orbital launch. In this application, the premium has typically been placed on minimum weight, and it is difficult to achieve high reliability and low weight simultaneously. In addition, if the number of flights launched is low, there is a very high chance of a design, operations or manufacturing error causing destruction of the vehicle.
The Rocketdyne H-1 engine, used in a cluster of eight in the first stage of the Saturn I and Saturn IB launch vehicles, had no catastrophic failures in 152 engine-flights. The Pratt and Whitney RL10 engine, used in a cluster of six in the Saturn I second stage, had no catastrophic failures in 36 engine-flights. The Rocketdyne F-1 engine, used in a cluster of five in the first stage of the Saturn V, had no failures in 65 engine-flights. The Rocketdyne J-2 engine, used in a cluster of five in the Saturn V second stage, and singly in the Saturn IB second stage and Saturn V third stage, had no catastrophic failures in 86 engine-flights.
The Space Shuttle Solid Rocket Booster, used in pairs, caused one notable catastrophic failure in 270 engine-flights.
The RS-25, used in a cluster of three, flew in 46 refurbished engine units. These made a total of 405 engine-flights with no catastrophic in-flight failures. A single in-flight RS-25 engine failure occurred during Space Shuttle Challenger's STS-51-F mission. This failure had no effect on mission objectives or duration.
Rocket propellants require a high energy per unit mass (specific energy), which must be balanced against the tendency of highly energetic propellants to spontaneously explode. Assuming that the chemical potential energy of the propellants can be safely stored, the combustion process results in a great deal of heat being released. A significant fraction of this heat is transferred to kinetic energy in the engine nozzle, propelling the rocket forward in combination with the mass of combustion products released.
Ideally all the reaction energy appears as kinetic energy of the exhaust gases, as exhaust velocity is the single most important performance parameter of an engine. However, real exhaust species are molecules, which typically have translation, vibrational, and rotational modes with which to dissipate energy. Of these, only translation can do useful work to the vehicle, and while energy does transfer between modes this process occurs on a timescale far in excess of the time required for the exhaust to leave the nozzle.
The more chemical bonds an exhaust molecule has, the more rotational and vibrational modes it will have. Consequently, it is generally desirable for the exhaust species to be as simple as possible, with a diatomic molecule composed of light, abundant atoms such as H2 being ideal in practical terms. However, in the case of a chemical rocket, hydrogen is a reactant and reducing agent, not a product. An oxidizing agent, most typically oxygen or an oxygen-rich species, must be introduced into the combustion process, adding mass and chemical bonds to the exhaust species.
An additional advantage of light molecules is that they may be accelerated to high velocity at temperatures that can be contained by currently available materials - the high gas temperatures in rocket engines pose serious problems for the engineering of survivable motors.
Liquid hydrogen (LH2) and oxygen (LOX, or LO2), are the most effective propellants in terms of exhaust velocity that have been widely used to date, though a few exotic combinations involving boron or liquid ozone are potentially somewhat better in theory if various practical problems could be solved.
It is important to note that, when computing the specific reaction energy of a given propellant combination, the entire mass of the propellants (both fuel and oxidiser) must be included. The exception is in the case of air-breathing engines, which use atmospheric oxygen and consequently have to carry less mass for a given energy output. Fuels for car or turbojet engines have a much better effective energy output per unit mass of propellant that must be carried, but are similar per unit mass of fuel.
Computer programs that predict the performance of propellants in rocket engines are available.
With liquid and hybrid rockets, immediate ignition of the propellants as they first enter the combustion chamber is essential.
With liquid propellants (but not gaseous), failure to ignite within milliseconds usually causes too much liquid propellant to be inside the chamber, and if/when ignition occurs the amount of hot gas created can exceed the maximum design pressure of the chamber, causing a catastrophic failure of the pressure vessel.
This is sometimes called a hard start or a rapid unscheduled disassembly (RUD).
Ignition can be achieved by a number of different methods; a pyrotechnic charge can be used, a plasma torch can be used, or electric spark ignition may be employed. Some fuel/oxidiser combinations ignite on contact (hypergolic), and non-hypergolic fuels can be "chemically ignited" by priming the fuel lines with hypergolic propellants (popular in Russian engines).
Gaseous propellants generally will not cause hard starts, with rockets the total injector area is less than the throat thus the chamber pressure tends to ambient prior to ignition and high pressures cannot form even if the entire chamber is full of flammable gas at ignition.
Solid propellants are usually ignited with one-shot pyrotechnic devices and combustion usually proceeds through total consumption of the propellants.
Once ignited, rocket chambers are self-sustaining and igniters are not needed and combustion usually proceeds through total consumption of the propellants.  Indeed, chambers often spontaneously reignite if they are restarted after being shut down for a few seconds. Unless designed for re-ignition, when cooled, many rockets cannot be restarted without at least minor maintenance, such as replacement of the pyrotechnic igniter or even refueling of the propellants.
Rocket jets vary depending on the rocket engine, design altitude, altitude, thrust and other factors.
Carbon-rich exhausts from kerosene-based fuels such as RP-1 are often orange in colour due to the black-body radiation of the unburnt particles, in addition to the blue Swan bands. Peroxide oxidiser-based rockets and hydrogen rocket jets contain largely steam and are nearly invisible to the naked eye but shine brightly in the ultraviolet and infrared ranges. Jets from solid-propellant rockets can be highly visible, as the propellant frequently contains metals such as elemental aluminium which burns with an orange-white flame and adds energy to the combustion process. Rocket engines which burn liquid hydrogen and oxygen will exhibit a nearly transparent exhaust, due to it being mostly superheated steam (water vapour), plus some unburned hydrogen.
The nozzle is usually over-expanded at sea level, and the exhaust can exhibit visible shock diamonds through a schlieren effect caused by the incandescence of the exhaust gas.
The shape of the jet varies for a fixed-area nozzle as the expansion ratio varies with altitude: at high altitude all rockets are grossly under-expanded, and a quite small percentage of exhaust gases actually end up expanding forwards.
The solar thermal rocket would make use of solar power to directly heat reaction mass, and therefore does not require an electrical generator as most other forms of solar-powered propulsion do. A solar thermal rocket only has to carry the means of capturing solar energy, such as concentrators and mirrors. The heated propellant is fed through a conventional rocket nozzle to produce thrust. The engine thrust is directly related to the surface area of the solar collector and to the local intensity of the solar radiation and inversely proportional to the Isp.
Nuclear propulsion includes a wide variety of propulsion methods that use some form of nuclear reaction as their primary power source. Various types of nuclear propulsion have been proposed, and some of them tested, for spacecraft applications:
According to the writings of the Roman Aulus Gellius, the earliest known example of jet propulsion was in c. 400 BC, when a Greek Pythagorean named Archytas, propelled a wooden bird along wires using steam. However, it was not powerful enough to take off under its own thrust.
The aeolipile described in the first century BC, often known as Hero's engine, consisted of a pair of steam rocket nozzles mounted on a bearing. It was created almost two millennia before the Industrial Revolution but the principles behind it were not well understood, and it was not developed into a practical power source.
The availability of black powder to propel projectiles was a precursor to the development of the first solid rocket. Ninth Century Chinese Taoist alchemists discovered black powder in a search for the elixir of life; this accidental discovery led to fire arrows which were the first rocket engines to leave the ground.
It is stated that "the reactive forces of incendiaries were probably not applied to the propulsion of projectiles prior to the 13th century". A turning point in rocket technology emerged with a short manuscript entitled Liber Ignium ad Comburendos Hostes (abbreviated as The Book of Fires). The manuscript is composed of recipes for creating incendiary weapons from the mid-eighth to the end of the thirteenth centuries—two of which are rockets.  The first recipe calls for one part of colophonium and sulfur added to six parts of saltpeter (potassium nitrate) dissolved in laurel oil, then inserted into hollow wood and lit to "fly away suddenly to whatever place you wish and burn up everything".  The second recipe combines one pound of sulfur, two pounds of charcoal, and six pounds of saltpeter—all finely powdered on a marble slab.  This powder mixture is packed firmly into a long and narrow case.  The introduction of saltpeter into pyrotechnic mixtures connected the shift from hurled Greek fire into self-propelled rocketry. .
Articles and books on the subject of rocketry appeared increasingly from the fifteenth through seventeenth centuries.  In the sixteenth century, German military engineer Conrad Haas (1509–1576) wrote a manuscript which introduced the construction of multi-staged rockets.
Rocket engines were also put in use by Tippu Sultan, the king of Mysore. These usually consisted of a tube of soft hammered iron about 8 in (20 cm) long and .mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}1+1⁄2–3 in (3.8–7.6 cm) diameter, closed at one end, packed with black powder propellant and strapped to a shaft of bamboo about 4 ft (120 cm) long. A rocket carrying about one pound of powder could travel almost 1,000 yards (910 m). These 'rockets', fitted with swords, would travel several meters in the air before coming down with sword edges facing the enemy. These were used very effectively against the British empire.
Slow development of this technology continued up to the later 19th century, when Russian Konstantin Tsiolkovsky first wrote about liquid-fueled rocket engines. He was the first to develop the Tsiolkovsky rocket equation, though it was not published widely for some years.
The modern solid- and liquid-fueled engines became realities early in the 20th century, thanks to the American physicist Robert Goddard. Goddard was the first to use a De Laval nozzle on a solid-propellant (gunpowder) rocket engine, doubling the thrust and increasing the efficiency by a factor of about twenty-five. This was the birth of the modern rocket engine. He calculated from his independently derived rocket equation that a reasonably sized rocket, using solid fuel, could place a one-pound payload on the Moon.
Fritz von Opel was instrumental in popularizing rockets as means of propulsion. In the 1920s, he initiated together with Max Valier, co-founder of the "Verein für Raumschiffahrt", the world's first rocket program, Opel-RAK, leading to speed records for automobiles, rail vehicles and the first manned rocket-powered flight in September of 1929. Months earlier in 1928, one of his rocket-powered prototypes, the Opel RAK2, reached piloted by von Opel himself at the AVUS speedway in Berlin a record speed of 238 km/h, watched by 3000 spectators and world media. A world record for rail vehicles was reached with RAK3 and a top speed of 256 km/h. After these successes, von Opel piloted the world's first public rocket-powered flight using Opel RAK.1, a rocket plane designed by Julius Hatry. World media reported on these efforts, including UNIVERSAL Newsreel of the US, causing as "Raketen-Rummel" or "Rocket Rumble" immense global public excitement, and in particular in Germany, where inter alia Wernher von Braun was highly influenced. The Great Depression led to an end of the Opel-RAK program, but Max Valier continued the efforts. After switching from solid-fuel to liquid-fuel rockets, he died while testing and is considered the first fatality of the dawning space age.
Goddard began to use liquid propellants in 1921, and in 1926 became the first to launch a liquid-propellant rocket. Goddard pioneered the use of the De Laval nozzle, lightweight propellant tanks, small light turbopumps, thrust vectoring, the smoothly-throttled liquid fuel engine, regenerative cooling, and curtain cooling.: 247–266 
During the late 1930s, German scientists, such as Wernher von Braun and Hellmuth Walter, investigated installing liquid-fueled rockets in military aircraft (Heinkel He 112, He 111, He 176 and Messerschmitt Me 163).
The turbopump was employed by German scientists in World War II. Until then cooling the nozzle had been problematic, and the A4 ballistic missile used dilute alcohol for the fuel, which reduced the combustion temperature sufficiently.
Staged combustion (Замкнутая схема) was first proposed by Alexey Isaev in 1949. The first staged combustion engine was the S1.5400 used in the Soviet planetary rocket, designed by Melnikov, a former assistant to Isaev. About the same time (1959), Nikolai Kuznetsov began work on the closed cycle engine NK-9 for Korolev's orbital ICBM, GR-1. Kuznetsov later evolved that design into the NK-15 and NK-33 engines for the unsuccessful Lunar N1 rocket.
In the West, the first laboratory staged-combustion test engine was built in Germany in 1963, by Ludwig Boelkow.
Hydrogen peroxide / kerosene fueled engines such as the British Gamma of the 1950s used a closed-cycle process by catalytically decomposing the peroxide to drive turbines before combustion with the kerosene in the combustion chamber proper. This gave the efficiency advantages of staged combustion, without the major engineering problems.
Liquid hydrogen engines were first successfully developed in America: the RL-10 engine first flew in 1962. Its successor, the Rocketdyne J-2, was used in the Apollo program's Saturn V rocket to send humans to the Moon. The high specific impulse and low density of liquid hydrogen lowered the upper stage mass and the overall size and cost of the vehicle.
The record for most engines on one rocket flight is 44, set by NASA in 2016 on a Black Brant.
Combustion, or burning, is a high-temperature exothermic redox chemical reaction between a fuel (the reductant) and an oxidant, usually atmospheric oxygen, that produces oxidized, often gaseous products, in a mixture termed as smoke. Combustion does not always result in fire, because a flame is only visible when substances  undergoing combustion vaporize, but when it does, a flame is a characteristic indicator of the reaction. While the activation energy must be overcome to initiate combustion (e.g., using a lit match to light a fire), the heat from a flame may provide enough energy to make the reaction self-sustaining.  
Combustion is often a complicated sequence of elementary radical reactions. Solid fuels, such as wood and coal, first undergo endothermic pyrolysis to produce gaseous fuels whose combustion then supplies the heat required to produce more of them. Combustion is often hot enough that incandescent light in the form of either glowing or a flame is produced. A simple example can be seen in the combustion of hydrogen and oxygen into water vapor, a reaction which is commonly used to fuel rocket engines. This reaction releases 242 kJ/mol of heat and reduces the enthalpy accordingly (at constant temperature and pressure):
Uncatalyzed combustion in air requires relatively high temperatures. Complete combustion is stoichiometric concerning the fuel, where there is no remaining fuel, and ideally, no residual oxidant. Thermodynamically, the chemical equilibrium of combustion in air is overwhelmingly on the side of the products. However, complete combustion is almost impossible to achieve, since the chemical equilibrium is not necessarily reached, or may contain unburnt products such as carbon monoxide, hydrogen and even carbon (soot or ash). Thus, the produced smoke is usually toxic and contains unburned or partially oxidized products. Any combustion at high temperatures in atmospheric air, which is 78 percent nitrogen, will also create small amounts of several nitrogen oxides, commonly referred to as NOx, since the combustion of nitrogen is thermodynamically favored at high, but not low temperatures. Since burning is rarely clean, fuel gas cleaning or catalytic converters may be required by law.
Fires occur naturally, ignited by lightning strikes or by volcanic products. Combustion (fire) was the first controlled chemical reaction discovered by humans, in the form of campfires and bonfires, and continues to be the main method to produce energy for humanity. Usually, the fuel is carbon, hydrocarbons, or more complicated mixtures such as wood that contains partially oxidized hydrocarbons. The thermal energy produced from combustion of either fossil fuels such as coal or oil, or from renewable fuels such as firewood, is harvested for diverse uses such as cooking, production of electricity or industrial or domestic heating. Combustion is also currently the only reaction used to power rockets. Combustion is also used to destroy (incinerate) waste, both nonhazardous and hazardous.
Oxidants for combustion have high oxidation potential and include atmospheric or pure oxygen, chlorine, fluorine, chlorine trifluoride, nitrous oxide and nitric acid. For instance, hydrogen burns in chlorine to form hydrogen chloride with the liberation of heat and light characteristic of combustion. Although usually not catalyzed, combustion can be catalyzed by platinum or vanadium, as in the contact process.
In complete combustion, the reactant burns in oxygen and produces a limited number of products. When a hydrocarbon burns in oxygen, the reaction will primarily yield carbon dioxide and water.  When elements are burned, the products are primarily the most common oxides. Carbon will yield carbon dioxide, sulfur will yield sulfur dioxide, and iron will yield iron(III) oxide. Nitrogen is not considered to be a combustible substance when oxygen is the oxidant. Still, small amounts of various nitrogen oxides (commonly designated NOx species) form when the air is the oxidative.
Combustion is not necessarily favorable to the maximum degree of oxidation, and it can be temperature-dependent. For example, sulfur trioxide is not produced quantitatively by the combustion of sulfur. .mw-parser-output .template-chem2-su{display:inline-block;font-size:80%;line-height:1;vertical-align:-0.35em}.mw-parser-output .template-chem2-su>span{display:block}.mw-parser-output sub.template-chem2-sub{font-size:80%;vertical-align:-0.35em}.mw-parser-output sup.template-chem2-sup{font-size:80%;vertical-align:0.65em}NOx species appear in significant amounts above about 2,800 °F (1,540 °C), and more is produced at higher temperatures. The amount of NOx is also a function of oxygen excess.
In most industrial applications and in fires, air is the source of oxygen (O2).  In the air, each mole of oxygen is mixed with approximately 3.71 mol of nitrogen.  Nitrogen does not take part in combustion, but at high temperatures some nitrogen will be converted to NOx (mostly NO, with much smaller amounts of NO2). On the other hand, when there is insufficient oxygen to combust the fuel completely, some fuel carbon is converted to carbon monoxide, and some of the hydrogens remain unreacted. A complete set of equations for the combustion of a hydrocarbon in the air, therefore, requires an additional calculation for the distribution of oxygen between the carbon and hydrogen in the fuel.
The amount of air required for complete combustion to take place is known as pure air. However, in practice, the air used is 2-3 times that of pure air.
Incomplete combustion will occur when there is not enough oxygen to allow the fuel to react completely to produce carbon dioxide and water. It also happens when the combustion is quenched by a heat sink, such as a solid surface or flame trap. As is the case with complete combustion, water is produced by incomplete combustion; however, carbon, carbon monoxide, and hydroxide are produced instead of carbon dioxide.
For most fuels, such as diesel oil, coal, or wood, pyrolysis occurs before combustion. In incomplete combustion, products of pyrolysis remain unburnt and contaminate the smoke with noxious particulate matter and gases. Partially oxidized compounds are also a concern; partial oxidation of ethanol can produce harmful acetaldehyde, and carbon can produce toxic carbon monoxide.
The designs of combustion devices can improve the quality of combustion, such as burners and internal combustion engines. Further improvements are achievable by catalytic after-burning devices (such as catalytic converters) or by the simple partial return of the exhaust gases into the combustion process. Such devices are required by environmental legislation for cars in most countries. They may be necessary to enable large combustion devices, such as thermal power stations, to reach legal emission standards.
The degree of combustion can be measured and analyzed with test equipment. HVAC contractors, firefighters and engineers use combustion analyzers to test the efficiency of a burner during the combustion process. Also, the efficiency of an internal combustion engine can be measured in this way, and some U.S. states and local municipalities use combustion analysis to define and rate the efficiency of vehicles on the road today.
Carbon monoxide is one of the products from incomplete combustion. Carbon is released in the normal incomplete combustion reaction, forming soot and dust. Since carbon monoxide is a poisonous gas, complete combustion is preferable, as carbon monoxide may also lead to respiratory troubles when breathed since it takes the place of oxygen and combines with hemoglobin.
These oxides combine with water and oxygen in the atmosphere, creating nitric acid and sulfuric acids, which return to Earth's surface as acid deposition, or "acid rain." Acid deposition harms aquatic organisms and kills trees. Due to its formation of certain nutrients that are less available to plants such as calcium and phosphorus, it reduces the productivity of the ecosystem and farms. An additional problem associated with nitrogen oxides is that they, along with hydrocarbon pollutants, contribute to the formation of ground level ozone, a major component of smog.
Breathing carbon monoxide causes headache, dizziness, vomiting, and nausea. If carbon monoxide levels are high enough, humans become unconscious or die. Exposure to moderate and high levels of carbon monoxide over long periods is positively correlated with risk of heart disease. People who survive severe carbon monoxide poisoning may suffer long-term health problems. Carbon monoxide from air is absorbed in the lungs which then binds with hemoglobin in human's red blood cells. This would reduce the capacity of red blood cells to carry oxygen throughout the body.
Smouldering is the slow, low-temperature, flameless form of combustion, sustained by the heat evolved when oxygen directly attacks the surface of a condensed-phase fuel. It is a typically incomplete combustion reaction. Solid materials that can sustain a smouldering reaction include coal, cellulose, wood, cotton, tobacco, peat, duff, humus, synthetic foams, charring polymers (including polyurethane foam) and dust. Common examples of smoldering phenomena are the initiation of residential fires on upholstered furniture by weak heat sources (e.g., a cigarette, a short-circuited wire) and the persistent combustion of biomass behind the flaming fronts of wildfires.
Rapid combustion is a form of combustion, otherwise known as a fire, in which large amounts of heat and light energy are released, which often results in a flame. This is used in a form of machinery such as internal combustion engines and in thermobaric weapons. Such a combustion is frequently called a Rapid combustion, though for an internal combustion engine this is inaccurate. An internal combustion engine nominally operates on a controlled rapid burn. When the fuel-air mixture in an internal combustion engine explodes, that is known as detonation.
Spontaneous combustion is a type of combustion which occurs by self-heating (increase in temperature due to exothermic internal reactions), followed by thermal runaway (self-heating which rapidly accelerates to high temperatures) and finally, ignition.
For example, phosphorus self-ignites at room temperature without the application of heat.  Organic materials undergoing bacterial composting can generate enough heat to reach the point of combustion.
Combustion resulting in a turbulent flame is the most used for industrial application (e.g. gas turbines, gasoline engines, etc.) because the turbulence helps the mixing process between the fuel and oxidizer.
The term 'micro' gravity refers to a gravitational state that is 'low' (i.e., 'micro' in the sense of 'small' and not necessarily a millionth of Earth's normal gravity) such that the influence of buoyancy on physical processes may be considered small relative to other flow processes that would be present at normal gravity.  In such an environment, the thermal and flow transport dynamics can behave quite differently than in normal gravity conditions (e.g., a candle's flame takes the shape of a sphere.). Microgravity combustion research contributes to the understanding of a wide variety of aspects that are relevant to both the environment of a spacecraft (e.g., fire dynamics relevant to crew safety on the International Space Station) and terrestrial (Earth-based) conditions (e.g., droplet combustion dynamics to assist developing new fuel blends for improved combustion, materials fabrication processes, thermal management of electronic systems, multiphase flow boiling dynamics, and many others).
Combustion processes which happen in very small volumes are considered micro-combustion. The high surface-to-volume ratio increases specific heat loss. Quenching distance plays a vital role in stabilizing the flame in such combustion chambers.
Generally, the chemical equation for stoichiometric combustion of a hydrocarbon in oxygen is:
where 



z
=
x
+


y
4




{\displaystyle z=x+{\frac {y}{4}}}

.
For example, the stoichiometric burning of propane in oxygen is:
If the stoichiometric combustion takes place using air as the oxygen source, the nitrogen present in the air (Atmosphere of Earth) can be added to the equation (although it does not react) to show the stoichiometric composition of the fuel in air and the composition of the resultant flue gas. Note that treating all non-oxygen components in air as nitrogen gives a 'nitrogen' to oxygen ratio of 3.77, i.e. (100% - O2%) / O2% where O2% is 20.95% vol:
where 



z
=
x
+


1
4


y


{\displaystyle z=x+{\frac {1}{4}}y}

.
For example, the stoichiometric combustion of propane (





C

3






H

8








{\displaystyle {\ce {C3H8}}}

) in air is:
The stoichiometric composition of propane in air is 1 / (1 + 5 + 18.87) = 4.02% vol.
The stoichiometric combustion reaction for CαHβOγ in air:
The stoichiometric combustion reaction for CαHβOγSδ:
The stoichiometric combustion reaction for CαHβOγNδSε:
The stoichiometric combustion reaction for CαHβOγFδ:
Various other substances begin to appear in significant amounts in combustion products when the flame temperature is above about 1600 K. When excess air is used, nitrogen may oxidize to NO and, to a much lesser extent, to NO2. CO forms by disproportionation of CO2, and H2 and OH form by disproportionation of H2O.
For example, when 1 mol of propane is burned with 28.6 mol of air (120% of the stoichiometric amount), the combustion products contain 3.3% O2. At 1400 K, the equilibrium combustion products contain 0.03% NO and 0.002% OH. At 1800 K, the combustion products contain 0.17% NO, 0.05% OH, 0.01% CO, and 0.004% H2.
Diesel engines are run with an excess of oxygen to combust small particles that tend to form with only a stoichiometric amount of oxygen, necessarily producing nitrogen oxide emissions. Both the United States and European Union enforce limits to vehicle nitrogen oxide emissions, which necessitate the use of special catalytic converters or treatment of the exhaust with urea (see Diesel exhaust fluid).
The incomplete (partial) combustion of a hydrocarbon with oxygen produces a gas mixture containing mainly CO2, CO, H2O, and H2. Such gas mixtures are commonly prepared for use as protective atmospheres for the heat-treatment of metals and for gas carburizing. The general reaction equation for incomplete combustion of one mole of a hydrocarbon in oxygen is:
When z falls below roughly 50% of the stoichiometric value, CH4 can become an important combustion product; when z falls below roughly 35% of the stoichiometric value, elemental carbon may become stable.
The products of incomplete combustion can be calculated with the aid of a material balance, together with the assumption that the combustion products reach equilibrium. For example, in the combustion of one mole of propane (C3H8) with four moles of O2, seven moles of combustion gas are formed, and z is 80% of the stoichiometric value. The three elemental balance equations are:
These three equations are insufficient in themselves to calculate the combustion gas composition.
However, at the equilibrium position, the water-gas shift reaction gives another equation:
For example, at 1200 K the value of Keq is 0.728. Solving, the combustion gas consists of 42.4% H2O, 29.0% CO2, 14.7% H2, and 13.9% CO. Carbon becomes a stable phase at 1200 K and 1 atm pressure when z is less than 30% of the stoichiometric value, at which point the combustion products contain more than 98% H2 and CO and about 0.5% CH4.
Substances or materials which undergo combustion are called fuels. The most common examples are natural gas, propane, kerosene, diesel, petrol, charcoal, coal, wood, etc.
Combustion of a liquid fuel in an oxidizing atmosphere actually happens in the gas phase. It is the vapor that burns, not the liquid. Therefore, a liquid will normally catch fire only above a certain temperature: its flash point. The flash point of a liquid fuel is the lowest temperature at which it can form an ignitable mix with air. It is the minimum temperature at which there is enough evaporated fuel in the air to start combustion.
Combustion of gaseous fuels may occur through one of four distinctive types of burning: diffusion flame, premixed flame, autoignitive reaction front, or as a detonation. The type of burning that actually occurs depends on the degree to which the fuel and oxidizer are mixed prior to heating: for example, a diffusion flame is formed if the fuel and oxidizer are separated initially, whereas a premixed flame is formed otherwise. Similarly, the type of burning also depends on the pressure: a detonation, for example, is an autoignitive reaction front coupled to a strong shock wave giving it its characteristic high-pressure peak and high detonation velocity.
The act of combustion consists of three relatively distinct but overlapping phases:
Efficient process heating requires recovery of the largest possible part of a fuel's heat of combustion into the material being processed. There are many avenues of loss in the operation of a heating process. Typically, the dominant loss is sensible heat leaving with the offgas (i.e., the flue gas). The temperature and quantity of offgas indicates its heat content (enthalpy), so keeping its quantity low minimizes heat loss.
In a perfect furnace, the combustion air flow would be matched to the fuel flow to give each fuel molecule the exact amount of oxygen needed to cause complete combustion. However, in the real world, combustion does not proceed in a perfect manner. Unburned fuel (usually CO and H2) discharged from the system represents a heating value loss (as well as a safety hazard). Since combustibles are undesirable in the offgas, while the presence of unreacted oxygen there presents minimal safety and environmental concerns, the first principle of combustion management is to provide more oxygen than is theoretically needed to ensure that all the fuel burns. For methane (CH4) combustion, for example, slightly more than two molecules of oxygen are required.
The second principle of combustion management, however, is to not use too much oxygen. The correct amount of oxygen requires three types of measurement: first, active control of air and fuel flow; second, offgas oxygen measurement; and third, measurement of offgas combustibles. For each heating process, there exists an optimum condition of minimal offgas heat loss with acceptable levels of combustibles concentration. Minimizing excess oxygen pays an additional benefit: for a given offgas temperature, the NOx level is lowest when excess oxygen is kept lowest.
Adherence to these two principles is furthered by making material and heat balances on the combustion process. The material balance directly relates the air/fuel ratio to the percentage of O2 in the combustion gas. The heat balance relates the heat available for the charge to the overall net heat produced by fuel combustion. Additional material and heat balances can be made to quantify the thermal advantage from preheating the combustion air, or enriching it in oxygen.
Combustion in oxygen is a chain reaction in which many distinct radical intermediates participate. The high energy required for initiation is explained by the unusual structure of the dioxygen molecule. The lowest-energy configuration of the dioxygen molecule is a stable, relatively unreactive diradical in a triplet spin state. Bonding can be described with three bonding electron pairs and two antibonding electrons, with spins aligned, such that the molecule has nonzero total angular momentum. Most fuels, on the other hand, are in a singlet state, with paired spins and zero total angular momentum. Interaction between the two is quantum mechanically a "forbidden transition", i.e. possible with a very low probability. To initiate combustion, energy is required to force dioxygen into a spin-paired state, or singlet oxygen. This intermediate is extremely reactive. The energy is supplied as heat, and the reaction then produces additional heat, which allows it to continue.
Combustion of hydrocarbons is thought to be initiated by hydrogen atom abstraction (not proton abstraction) from the fuel to oxygen, to give a hydroperoxide radical (HOO). This reacts further to give hydroperoxides, which break up to give hydroxyl radicals. There are a great variety of these processes that produce fuel radicals and oxidizing radicals. Oxidizing species include singlet oxygen, hydroxyl, monatomic oxygen, and hydroperoxyl. Such intermediates are short-lived and cannot be isolated. However, non-radical intermediates are stable and are produced in incomplete combustion. An example is acetaldehyde produced in the combustion of ethanol. An intermediate in the combustion of carbon and hydrocarbons, carbon monoxide, is of special importance because it is a poisonous gas, but also economically useful for the production of syngas.
Solid and heavy liquid fuels also undergo a great number of pyrolysis reactions that give more easily oxidized, gaseous fuels. These reactions are endothermic and require constant energy input from the ongoing combustion reactions. A lack of oxygen or other improperly designed conditions result in these noxious and carcinogenic pyrolysis products being emitted as thick, black smoke.
The rate of combustion is the amount of a material that undergoes combustion over a period of time. It can be expressed in grams per second (g/s) or kilograms per second (kg/s).
Detailed descriptions of combustion processes, from the chemical kinetics perspective, requires the formulation of large and intricate webs of elementary reactions. For instance, combustion of hydrocarbon fuels typically involve hundreds of chemical species reacting according to thousands of reactions.
Inclusion of such mechanisms within computational flow solvers still represents a pretty challenging task mainly in two aspects. First, the number of degrees of freedom (proportional to the number of chemical species) can be dramatically large; second, the source term due to reactions introduces a disparate number of time scales which makes the whole dynamical system stiff. As a result, the direct numerical simulation of turbulent reactive flows with heavy fuels soon becomes intractable even for modern supercomputers.
Therefore, a plethora of methodologies has been devised for reducing the complexity of combustion mechanisms without resorting to high detail level. Examples are provided by:
The kinetic modelling may be explored for insight into the reaction mechanisms of thermal decomposition in the combustion of different materials by using for instance Thermogravimetric analysis.
Assuming perfect combustion conditions, such as complete combustion under adiabatic conditions (i.e., no heat loss or gain), the adiabatic combustion temperature can be determined. The formula that yields this temperature is based on the first law of thermodynamics and takes note of the fact that the heat of combustion is used entirely for heating the fuel, the combustion air or oxygen, and the combustion product gases (commonly referred to as the flue gas).
In the case of fossil fuels burnt in air, the combustion temperature depends on all of the following:
The adiabatic combustion temperature (also known as the adiabatic flame temperature) increases for higher heating values and inlet air and fuel temperatures and for stoichiometric air ratios approaching one.
Most commonly, the adiabatic combustion temperatures for coals are around 2,200 °C (3,992 °F) (for inlet air and fuel at ambient temperatures and for 



λ
=
1.0


{\displaystyle \lambda =1.0}

), around 2,150 °C (3,902 °F) for oil and 2,000 °C (3,632 °F) for natural gas.
In industrial fired heaters, power station steam generators, and large gas-fired turbines, the more common way of expressing the usage of more than the stoichiometric combustion air is percent excess combustion air. For example, excess combustion air of 15 percent means that 15 percent more than the required stoichiometric air is being used.
Combustion instabilities are typically violent pressure oscillations in a combustion chamber. These pressure oscillations can be as high as 180 dB, and long-term exposure to these cyclic pressure and thermal loads reduce the life of engine components. In rockets, such as the F1 used in the Saturn V program, instabilities led to massive damage to the combustion chamber and surrounding components. This problem was solved by re-designing the fuel injector. In liquid jet engines, the droplet size and distribution can be used to attenuate the instabilities. Combustion instabilities are a major concern in ground-based gas turbine engines because of NOx emissions. The tendency is to run lean, an equivalence ratio less than 1, to reduce the combustion temperature and thus reduce the NOx emissions; however, running the combustion lean makes it very susceptible to combustion instability.
The Rayleigh Criterion is the basis for analysis of thermoacoustic combustion instability and is evaluated using the Rayleigh Index over one cycle of instability
where q' is the heat release rate perturbation and p' is the pressure fluctuation.
When the heat release oscillations are in phase with the pressure oscillations, the Rayleigh Index is positive and the magnitude of the thermo acoustic instability is maximised. On the other hand, if the Rayleigh Index is negative, then thermoacoustic damping occurs. The Rayleigh Criterion implies that a thermoacoustic instability can be optimally controlled by having heat release oscillations 180 degrees out of phase with pressure oscillations at the same frequency. This minimizes the Rayleigh Index.



An oxidizing agent (also known as an oxidant, oxidizer, electron recipient, or electron acceptor) is a substance in a redox chemical reaction that gains or "accepts"/"receives" an electron from a reducing agent (called the reductant, reducer, or electron donor). In other words, an oxidizer is any substance that oxidises another substance. The oxidation state, which describes the degree of loss of electrons, of the oxidizer decreases while that of the reductant increases; this is expressed by saying that oxidizers "undergo reduction" and "are reduced" while reducers "undergo oxidation" and "are oxidized".
Common oxidizing agents are oxygen, hydrogen peroxide and the halogens.
In one sense, an oxidizing agent is a chemical species that undergoes a chemical reaction in which it gains one or more electrons. In that sense, it is one component in an oxidation–reduction (redox) reaction. In the second sense, an oxidizing agent is a chemical species that transfers electronegative atoms, usually oxygen, to a substrate. Combustion, many explosives, and organic redox reactions involve atom-transfer reactions.
Electron acceptors participate in electron-transfer reactions. In this context, the oxidizing agent is called an electron acceptor and the reducing agent is called an electron donor. A classic oxidizing agent is the ferrocenium ion Fe(C5H5)+2, which accepts an electron to form Fe(C5H5)2.  One of the strongest acceptors commercially available is "Magic blue", the radical cation derived from N(C6H4-4-Br)3.
Extensive tabulations of ranking the electron accepting properties of various reagents (redox potentials) are available, see Standard electrode potential (data page).
In more common usage, an oxidizing agent transfers oxygen atoms to a substrate. In this context, the oxidizing agent can be called an oxygenation reagent or oxygen-atom transfer (OAT) agent. Examples include MnO−4 (permanganate), CrO2−4 (chromate), OsO4 (osmium tetroxide), and especially ClO−4 (perchlorate). Notice that these species are all oxides.
In some cases, these oxides can also serve as electron acceptors, as illustrated by the conversion of MnO−4 to MnO2−4, manganate.
The dangerous goods definition of an oxidizing agent is a substance that can cause or contribute to the combustion of other material. By this definition some materials that are classified as oxidizing agents by analytical chemists are not classified as oxidizing agents in a dangerous materials sense. An example is potassium dichromate, which does not pass the dangerous goods test of an oxidizing agent.
The U.S. Department of Transportation defines oxidizing agents specifically. There are two definitions for oxidizing agents governed under DOT regulations. These two are Class 5; Division 5.1(a)1 and Class 5; Division 5.1(a)2. Division 5.1 "means a material that may, generally by yielding oxygen, cause or enhance the combustion of other materials." Division 5.(a)1 of the DOT code applies to solid oxidizers "if, when tested in accordance with the UN Manual of Tests and Criteria (IBR, see § 171.7 of this subchapter), its mean burning time is less than or equal to the burning time of a 3:7 potassium bromate/cellulose mixture." 5.1(a)2 of the DOT code applies to liquid oxidizers "if, when tested in accordance with the UN Manual of Tests and Criteria, it spontaneously ignites or its mean time for a pressure rise from 690 kPa to 2070 kPa gauge is less than the time of a 1:1 nitric acid (65 percent)/cellulose mixture."
Liquid fuels are combustible or energy-generating molecules that can be harnessed to create mechanical energy, usually producing kinetic energy; they also must take the shape of their container. It is the fumes of liquid fuels that are flammable instead of the fluid.
Most liquid fuels in widespread use are derived from fossil fuels; however, there are several types, such as hydrogen fuel (for automotive uses), ethanol, and biodiesel, which are also categorized as a liquid fuel. Many liquid fuels play a primary role in transportation and the economy.
Liquid fuels are contrasted with solid fuels and gaseous fuels.
Some common properties of liquid fuels are that they are easy to transport, and can be handled with relative ease. Physical properties of liquid fuels vary by temperature, though not as greatly as for gaseous fuels. Some of these properties are:  flash point, the lowest temperature at which a flammable concentration of vapor is produced; fire point, the temperature at which sustained burning of vapor will occur;  cloud point for diesel fuels, the temperature at which dissolved waxy compounds begin to coalesce, and pour point, the temperature below which the fuel is too thick to pour freely. These properties affect the safety and handling of the fuel.
Most liquid fuels used currently are produced from petroleum. The most notable of these is gasoline. Scientists generally accept that petroleum formed from the fossilized remains of dead plants and animals by exposure to heat and pressure in the Earth's crust.
Gasoline is the most widely used liquid fuel. Gasoline, as it is known in United States and Canada, or petrol virtually everywhere else, is made of hydrocarbon molecules (compounds that contain hydrogen and carbon only) forming aliphatic compounds, or chains of carbons with hydrogen atoms attached. However, many aromatic compounds (carbon chains forming rings) such as benzene are found naturally in gasoline and cause the health risks associated with prolonged exposure to the fuel.
Production of gasoline is achieved by distillation of crude oil. The desirable liquid is separated from the crude oil in refineries. Crude oil is extracted from the ground in several processes, the most commonly seen may be beam pumps. To create gasoline, petroleum must first be removed from crude oil.
Liquid gasoline itself is not actually burned, but its fumes ignite, causing the remaining liquid to evaporate and then burn. Gasoline is extremely volatile and easily combusts, making any leakage potentially extremely dangerous. Gasoline sold in most countries carries a published octane rating. The octane number is an empirical measure of the resistance of gasoline to combusting prematurely, known as knocking. The higher the octane rating, the more resistant the fuel is to autoignition under high pressures, which allows for a higher compression ratio. Engines with a higher compression ratio, commonly used in race cars and high-performance regular-production automobiles, can produce more power; however, such engines require a higher octane fuel.  Increasing the octane rating has, in the past, been achieved by adding 'anti-knock' additives such as lead-tetra-ethyl.  Because of the environmental impact of lead additives, the octane rating is increased today by refining out the impurities that cause knocking.
Conventional diesel is similar to gasoline in that it is a mixture of aliphatic hydrocarbons extracted from petroleum. Diesel may cost more or less than gasoline, but generally costs less to produce because the extraction processes used are simpler. Some countries (particularly Canada, India and Italy) also have lower tax rates on diesel fuels.
After distillation, the diesel fraction is normally processed to reduce the amount of sulfur in the fuel. Sulfur causes corrosion in vehicles, acid rain and higher emissions of soot from the tail pipe (exhaust pipe). Historically, in Europe lower sulfur levels than in the United States were legally required. However, recent US legislation reduced the maximum sulfur content of diesel from 3,000 ppm to 500 ppm in 2007, and 15 ppm by 2010. Similar changes are also underway in Canada, Australia, New Zealand and several Asian countries. See also Ultra-low-sulfur diesel.
A diesel engine is a type of internal combustion engine which ignites fuel by injecting it into a combustion chamber previously compressed with air (which in turn raises the temperature) as opposed to using an outside ignition source, such as a spark plug.
Kerosene is used in kerosene lamps and as a fuel for cooking, heating, and small engines. It displaced whale oil for lighting use.  Jet fuel for jet engines is made in several grades (Avtur, Jet A, Jet A-1, Jet B, JP-4, JP-5, JP-7 or JP-8) that are kerosene-type mixtures. One form of the fuel known as RP-1 is burned with liquid oxygen as rocket fuel. These fuel grade kerosenes meet specifications for smoke points and freeze points.
In the mid-20th century, kerosene or "TVO" (Tractor Vaporising Oil) was used as a cheap fuel for tractors. The engine would start on gasoline, then switch over to kerosene once the engine warmed up. A "heat valve" on the manifold would route the exhaust gases around the intake pipe, heating the kerosene to the point where it can be ignited by an electric spark.
Kerosene is sometimes used as an additive in diesel fuel to prevent gelling or waxing in cold temperatures. However, this is not advisable in some recent vehicle diesel engines, as doing so may interfere with the engine's emissions regulation equipment.
LP gas is a mixture of propane and butane, both of which are easily compressible gases under standard atmospheric conditions. It offers many of the advantages of compressed natural gas (CNG), but does not burn as cleanly, is denser than air and is much more easily compressed. Commonly used for cooking and space heating, LP gas and compressed propane are seeing increased use in motorized vehicles; propane is the third most commonly used motor fuel globally.
Petroleum fuels, when burnt, release carbon dioxide that is harmful to world climate. The amount of carbon dioxide released when one liter of fuel is combusted can be estimated: As a good approximation the chemical formula of e.g. diesel is CnH2n. Note that diesel is a mixture of different molecules. As carbon has a molar mass of 12 g/mol and hydrogen (atomic!) has a molar mass of about 1 g/mol, so the fraction by weight of carbon in diesel is roughly 12/14. The reaction of diesel combustion is given by:
2CnH2n + 3nO2 ⇌ 2nCO2 + 2nH2O
Carbon dioxide has a molar mass of 44g/mol as it consists of 2 atoms of oxygen (16 g/mol) and 1 atom of carbon (12 g/mol). So 12 g of carbon yield 44 g of Carbon dioxide. Diesel has a density of 0.838 kg per liter. Putting everything together the mass of carbon dioxide that is produced by burning 1 liter of diesel can be calculated as:




0.838
k
g

/

l
⋅


12
14


⋅


44
12


=
2.63
k
g

/

l


{\displaystyle 0.838kg/l\cdot {\frac {12}{14}}\cdot {\frac {44}{12}}=2.63kg/l}


The number of 2.63 kg of carbon dioxide from 1 liter of Diesel is close to the values found in the literature. 
For gasoline, with a density of 0.75 kg/l and a ratio of carbon to hydrogen atoms of about 6 to 14, the estimated value of carbon emission if 1 liter of gasoline is burnt gives:




0.75
k
g

/

l
⋅




6
⋅
12


6
⋅
12
+
14



⋅
1

⋅


44
12


=
2.3
k
g

/

l


{\displaystyle 0.75kg/l\cdot {{\frac {6\cdot 12}{6\cdot 12+14}}\cdot 1}\cdot {\frac {44}{12}}=2.3kg/l}


When petroleum is not easily available, chemical processes such as the Fischer–Tropsch process can be used to produce liquid fuels from coal or natural gas. Synthetic fuels from coal were strategically important during World War II for the German military.  Today synthetic fuels produced from natural gas are manufactured, to take advantage of the higher value of liquid fuels in transportation.
Natural gas, composed chiefly of methane, can be compressed to a liquid and used as a substitute for other traditional liquid fuels. Its combustion is very clean compared to other hydrocarbon fuels, but the fuel's low boiling point requires the fuel to be kept at high pressures to keep it in the liquid state. Though it has a much lower flash point than fuels such as gasoline, it is in many ways safer due to its higher autoignition temperature and its low density, which causes it to dissipate when released in air.
Biodiesel is similar to diesel but has differences akin to those between petrol and ethanol. For instance, biodiesel has a higher cetane rating (45-60 compared to 45-50 for crude-oil-derived diesel) and it acts as a cleaning agent to get rid of dirt and deposits. It has been argued that it only becomes economically feasible above oil prices of $80 (£40 or €60 as of late February, 2007) per barrel. This does, however, depend on locality, economic situation, government stance on biodiesel and a host of other factors- and it has been proven to be viable at much lower costs in some countries. Also, it yields about 10% less energy than ordinary diesel. Analogous to the use of higher compression ratios used for engines burning higher octane alcohols and petrol in spark-ignition engines, taking advantage of biodiesel's high cetane rating can potentially overcome the energy deficit compared to ordinary Number 2 diesel.
Generally, the term alcohol refers to ethanol, the first organic chemical produced by humans, but any alcohol can be burned as a fuel. Ethanol and methanol are the most common, being sufficiently inexpensive to be useful.
Methanol is the lightest and simplest alcohol, produced from the natural gas component methane. Its application is limited primarily due to its toxicity (similar to gasoline), but also due to its high corrosivity and miscibility with water. Small amounts are used in some types of gasoline to increase the octane rating. Methanol-based fuels are used in some race cars and model aeroplanes.
Methanol is also called methyl alcohol or wood alcohol, the latter because it was formerly produced from the distillation of wood. It is also known by the name methyl hydrate.
Ethanol, also known as grain alcohol or ethyl alcohol, is commonly found in alcoholic beverages. However, it may also be used as a fuel, most often in combination with gasoline. For the most part, it is used in a 9:1 ratio of gasoline to ethanol to reduce the negative environmental effects of gasoline.
There is increasing interest in the use of a blend of 85% fuel ethanol blended with 15% gasoline. This fuel blend called E85 has a higher fuel octane than most premium types of gasoline. When used in a modern Flexible fuel vehicle, it delivers more performance to the gasoline it replaces at the expense of higher fuel consumption due to ethanol's lesser specific energy content.
Ethanol for use in gasoline and industrial purposes may be considered a fossil fuel because it is often synthesized from the petroleum product ethylene, which is cheaper than production from fermentation of grains or sugarcane.
Butanol is an alcohol which can be used as a fuel in most gasoline internal combustion engines without engine modification. It is typically a product of the fermentation of biomass by the bacterium Clostridium acetobutylicum (also known as the Weizmann organism). This process was first delineated by Chaim Weizmann in 1916 for the production of acetone from starch for making cordite, a smokeless gunpowder.
The advantages of butanol are its high octane rating (over 100) and high energy content, only about 10% lower than gasoline, and subsequently about 50% more energy-dense than ethanol, 100% more so than methanol. Butanol's only major disadvantages are its high flashpoint (35 °C or 95 °F), toxicity (note that toxicity levels exist but are not precisely confirmed), and the fact that the fermentation process for renewable butanol emits a foul odour. The Weizmann organism can only tolerate butanol levels up to 2% or so, compared to 14% for ethanol and yeast. Making butanol from oil produces no such odour, but the limited supply and environmental impact of oil usage defeat the purpose of alternative fuels. The cost of butanol is about $1.25–$1.32 per kilogram ($0.57-$0.58 per pound or $4 approx. per US gallon). Butanol is much more expensive than ethanol (approximately $0.40 per litre or 1.50 per gallon) and methanol.
On June 20, 2006, DuPont and BP announced that they were converting an existing ethanol plant to produce 9 million gallons (34 000 cubic meters) of butanol per year from sugar beets. DuPont stated a goal of being competitive with oil at $30–$40 per barrel ($0.19-$0.25 per liter) without subsidies, so the price gap with ethanol is narrowing.
Liquefied hydrogen is the liquid state of the element hydrogen. It is a common liquid rocket fuel for rocket applications and can be used as a fuel in an internal combustion engine or fuel cell. Various concept hydrogen vehicles have been lower volumetric energy, the hydrogen volumes needed for combustion are large. Hydrogen was liquefied for the first time by James Dewar in 1898.
Ammonia (NH3) has been used as a fuel before at times when gasoline is unavailable (e.g. for buses in Belgium during WWII).  It has a volumetric energy density of 17 Megajoules per liter (compared to 10 for hydrogen, 18 for methanol, 21 for dimethyl ether and 34 for gasoline).  It must be compressed or cooled to be a liquid fuel, although it does not require cryogenic cooling as hydrogen does to be liquefied.
A monopropellant rocket (or "monochemical rocket") is a rocket that uses a single chemical as its propellant.
For monopropellant rockets that depend on a chemical reaction, the power for the propulsive reaction and resultant thrust is provided by the chemical itself.  That is, the energy needed to propel the spacecraft is contained within the chemical bonds of the chemical molecules involved in the reaction.
The most commonly used monopropellant is hydrazine (N2H4), a chemical which is a strong reducing agent.  The most common catalyst is granular alumina (aluminum oxide) coated with iridium. These coated granules are usually under the commercial labels Aerojet S-405 (previously made by Shell) or W.C.Heraeus H-KC 12 GA (previously made by Kali Chemie). There is no igniter with hydrazine.  Aerojet S-405 is a spontaneous catalyst, that is, hydrazine decomposes on contact with the catalyst. The decomposition is highly exothermic and produces a 1,000 °C (1,830 °F) gas that is a mixture of nitrogen, hydrogen and ammonia. The main limiting factor of the monopropellant rocket is its life, which mainly depends on the life of the catalyst. The catalyst may be subject to catalytic poison and catalytic attrition which results in catalyst failure. Another monopropellant is hydrogen peroxide, which, when purified to 90% or higher concentration, is self-decomposing at high temperatures or when a catalyst is present.
Most chemical-reaction monopropellant rocket systems consist of a fuel tank, usually a titanium or aluminium sphere, with an ethylene-propylene rubber container or a surface tension propellant management device filled with the fuel. The tank is then pressurized with helium or nitrogen, which pushes the fuel out to the motors. A pipe leads from the tank to a poppet valve, and then to the decomposition chamber of the rocket motor. Typically, a satellite will have not just one motor, but two to twelve, each with its own valve.
The attitude control rocket motors for satellites and space probes are often very small, 25 mm (0.98 in) or so in diameter, and mounted in groups that point in four directions (within a plane).
The rocket is fired when the computer sends direct current through a small electromagnet that opens the poppet valve. The firing is often very brief, a few milliseconds, and — if operated in air — would sound like a pebble thrown against a metal trash can; if on for long, it would make a piercing hiss.
Chemical-reaction monopropellants are not as efficient as some other propulsion technologies. Engineers choose monopropellant systems when the need for simplicity and reliability outweigh the need for high delivered impulse. If the propulsion system must produce large amounts of thrust, or have a high specific impulse, as on the main motor of an interplanetary spacecraft, other technologies are used.
A concept to provide low Earth orbit (LEO) propellant depots that could be used as way-stations for other spacecraft to stop and refuel on the way to beyond-LEO missions has proposed that waste gaseous hydrogen—an inevitable byproduct of long-term liquid hydrogen storage in the radiative heat environment of space—would be usable as a monopropellant in a solar-thermal propulsion system.  The waste hydrogen would be productively utilized for both orbital stationkeeping and attitude control, as well as providing limited propellant and thrust to use for orbital maneuvers to better rendezvous with other spacecraft that would be inbound to receive fuel from the depot.
Solar-thermal monoprop thrusters are also integral to the design of a next-generation cryogenic upper stage rocket proposed by U.S. company United Launch Alliance (ULA).  The Advanced Common Evolved Stage (ACES) is intended as a lower-cost, more-capable and more-flexible upper stage that would supplement, and perhaps replace, the existing ULA Centaur and ULA Delta Cryogenic Second Stage (DCSS) upper stage vehicles.  The ACES Integrated Vehicle Fluids option eliminates all hydrazine and helium from the space vehicle—normally used for attitude control and station keeping—and depends instead on solar-thermal monoprop thrusters using waste hydrogen.
NASA is developing a new monopropellant propulsion system for small, cost-driven spacecraft with delta-v requirements in the range of 10–150 m/s. This system is based on a hydroxylammonium nitrate (HAN)/water/fuel monopropellant blend which is extremely dense, environmentally benign, and promises good performance and simplicity.
The EURENCO Bofors company produced LMP-103S as a 1-to-1 substitute for hydrazine by dissolving 65% ammonium dinitramide, NH4N(NO2)2, in 35% water solution of methanol and ammonia.  LMP-103S has 6% higher specific impulse and 30% higher impulse density than hydrazine monopropellant. Additionally, hydrazine is highly toxic and carcinogenic, while LMP-103S is only moderately toxic. LMP-103S is UN Class 1.4S allowing for transport on commercial aircraft, and was demonstrated on the Prisma satellite in 2010. Special handling is not required.  LMP-103S could replace hydrazine as the most commonly used monopropellant.
A hypergolic propellant combination used in a rocket engine is one whose components spontaneously ignite when they come into contact with each other.
The two propellant components usually consist of a fuel and an oxidizer. The main advantages of hypergolic propellants are that they can be stored as liquids at room temperature and that engines which are powered by them are easy to ignite reliably and repeatedly. Common hypergolic propellants are difficult to handle due to their extreme toxicity and/or corrosiveness.
In contemporary usage, the terms "hypergol" and "hypergolic propellant" usually mean the most common such propellant combination, dinitrogen tetroxide plus hydrazine and/or its relatives monomethylhydrazine (MMH) and unsymmetrical dimethylhydrazine (UDMH).
In 1935, Hellmuth Walter discovered that hydrazine hydrate was hypergolic with high strength hydrogen peroxide of 80-83 percent. He was probably the first to discover this phenomenon, and set to work developing a fuel. Prof. Otto Lutz  assisted the Walter Company with the development of C-Stoff which contained 30 percent hydrazine hydrate, 57 percent methanol, and 13 percent water, and spontaneously ignited with high strength hydrogen peroxide.: 13  BMW developed engines burning a hypergolic mix of nitric acid with various combinations of amines, xylidines and anilines.
Hypergolic propellants were discovered independently, for the second time, in the U.S. by GALCIT and Navy Annapolis researchers in 1940. They developed engines powered by aniline and red fuming nitric acid (RFNA). Robert Goddard, Reaction Motors, and Curtiss-Wright worked on aniline/nitric acid engines in the early 1940s, for small missiles and jet assisted take-off (JATO).The project resulted in the successful assisted take off of several Martin PBM and PBY bombers, but the project was disliked because of the toxic properties of both fuel and oxidizer, as well as the high freezing point of aniline. The second problem was eventually solved by the addition of small quantities of furfuryl alcohol to the aniline.: 22–23 
In Germany from the mid-1930s through World War II, rocket propellants were broadly classed as monergols, hypergols, non-hypergols and lithergols. The ending ergol is a combination of Greek ergon or work, and Latin oleum or oil, later influenced by the chemical suffix -ol from alcohol.  Monergols were monopropellants, while non-hypergols were bipropellants which required external ignition, and lithergols were solid/liquid hybrids. Hypergolic propellants (or at least hypergolic ignition) were far less prone to hard starts than electric or pyrotechnic ignition. The "hypergole" terminology was coined by Dr. Wolfgang Nöggerath, at the Technical University of Brunswick, Germany.
The only rocket-powered fighter ever deployed was the Messerschmitt Me 163B Komet. The Komet had a HWK 109-509, a rocket motor which consumed methanol/hydrazine as fuel and high test peroxide T-Stoff as oxidizer. The hypergolic rocket motor had the advantage of fast climb and quick-hitting tactics at the cost of being very volatile and capable of exploding with any degree of inattention. Other proposed combat rocket fighters like the Heinkel Julia and reconnaissance aircraft like the DFS 228 were meant to use the Walter 509 series of rocket motors, but besides the Me 163, only the Bachem Ba 349 Natter vertical launch expendable fighter was ever flight-tested with the Walter rocket propulsion system as its primary sustaining thrust system for military-purpose aircraft.
The earliest ballistic missiles, such as the Soviet R-7 that launched Sputnik 1 and the U.S. Atlas and Titan-1, used kerosene and liquid oxygen. Although they are preferred in space launchers, the difficulties of storing a cryogen like liquid oxygen in a missile that had to be kept launch ready for months or years at a time led to a switch to hypergolic propellants in the U.S. Titan II and in most Soviet ICBMs such as the R-36. But the difficulties of such corrosive and toxic materials, including leaks and explosions in Titan-II silos, led to their near universal replacement with solid-fuel boosters, first in Western submarine-launched ballistic missiles and then in land-based U.S. and Soviet ICBMs.: 47 
The Apollo Lunar Module, used in the Moon landings, employed hypergolic fuels in both the descent and ascent rocket engines.
The trend among western space launch agencies is away from large hypergolic rocket engines and toward hydrogen/oxygen engines with higher performance. Ariane 1 through 4, with their hypergolic first and second stages (and optional hypergolic boosters on the Ariane 3 and 4) have been retired and replaced with the Ariane 5, which uses a first stage fueled by liquid hydrogen and liquid oxygen. The Titan II, III and IV, with their hypergolic first and second stages, have also been retired. Hypergolic propellants are still widely used in upper stages when multiple burn-coast periods are required, and in Launch escape systems.
Hypergolically-fueled rocket engines are usually simple and reliable because they need no ignition system. Although larger hypergolic engines in some launch vehicles use turbopumps, most hypergolic engines are pressure-fed. A gas, usually helium, is fed to the propellant tanks under pressure through a series of check and safety valves. The propellants in turn flow through control valves into the combustion chamber; there, their instant contact ignition prevents a mixture of unreacted propellants from accumulating and then igniting in a potentially catastrophic hard start.
As hypergolic rockets do not need an ignition system, they can fire any number of times by simply opening and closing the propellant valves until the propellants are exhausted and are therefore uniquely suited for spacecraft maneuvering and well suited, though not uniquely so, as upper stages of such space launchers as the Delta II and Ariane 5, which must perform more than one burn. Restartable non-hypergolic rocket engines nevertheless exist, notably the cryogenic (oxygen/hydrogen) RL-10 on the Centaur and the J-2 on the Saturn V. The RP-1/LOX Merlin on the Falcon 9 can also be restarted.
The most common hypergolic fuels, hydrazine, monomethylhydrazine and unsymmetrical dimethylhydrazine, and oxidizer, nitrogen tetroxide, are all liquid at ordinary temperatures and pressures.  They are therefore sometimes called storable liquid propellants.  They are suitable for use in spacecraft missions lasting many years. The cryogenity of liquid hydrogen and liquid oxygen has so far limited their practical use to space launch vehicles where they need to be stored only briefly. As the largest issue with the usage of cryogenic propellants in interplanetary space is boil-off, which is largely dependant on the scale of spacecraft, for larger craft such as Starship this is less of an issue.
Another advantage of hypergolic propellants is their high density compared to cryogenic propellants. LOX has a density of 1.14 g/ml, while on the other hand, hypergolic oxidisers such as Nitric Acid or Nitrogen Tetroxide have a density of 1.55 g/ml and 1.45 g/ml respectively. LH2 fuel offers extremely high performance, yet its density only warrants its usage in the largest of rocket stages, while mixtures of Hydrazine and UDMH have a density at least ten times higher. This is of great importance in space probes, as the higher propellant density allows the size of their propellant tank to be reduced significantly, which in turn allows the probe to fit within a smaller payload fairing.
Relative to their mass, traditional hypergolic propellants posess a lower calorific value than cryogenic propellant combinations like LH2 / LOX or LCH4 / LOX. A launch vehicle that uses hypergolic propellant must therefore carry a greater mass of fuel than one that uses these cryogenic fuels.
The corrosivity, toxicity, and carcinogenicity of traditional hypergolics necessitate expensive safety precautions. Failure to follow adequate safety procedures with an exceptionally dangerous UDMH-nitric acid propellant mixture nicknamed "Devil's Venom", for example, resulted in the deadliest rocketry accident in history, the Nedelin catastrophe.
The common hypergolic propellant combinations include:
Less-common and obsolete hypergolic propellants include:
Pyrophoric substances, which ignites spontaneously in the presence of air, are also sometimes used as rocket fuels themselves or to ignite other fuels. For example a mixture of triethylborane and triethylaluminium (which are both separately and even more so together pyrophoric), is used for engine starts in the SR-71 Blackbird and in the F-1 engines on the Saturn V rocket and is used in the Merlin engines on the SpaceX Falcon 9 rockets.
A liquid-propellant rocket or liquid rocket utilizes a rocket engine that uses liquid propellants. Liquids are desirable because they have a reasonably high density and high specific impulse (Isp). This allows the volume of the propellant tanks to be relatively low. It is also possible to use lightweight centrifugal turbopumps to pump the rocket propellant from the tanks into the combustion chamber, which means that the propellants can be kept under low pressure. This permits the use of low-mass propellant tanks that do not need to resist the high pressures needed to store significant amounts of gasses, resulting in a low mass ratio for the rocket.
An inert gas stored in a tank at a high pressure is sometimes used instead of pumps in simpler small engines to force the propellants into the combustion chamber. These engines may have a higher mass ratio, but are usually more reliable, and are therefore used widely in satellites for orbit maintenance.
Liquid rockets can be monopropellant rockets using a single type of propellant, or bipropellant rockets using two types of propellant.  Tripropellant rockets using three types of propellant are rare.  Some designs are throttleable for variable thrust operation and some may be restarted after a previous in-space shutdown.  Liquid propellants are also used in hybrid rockets, with some of the advantages of a solid rocket.
The idea of liquid rocket as understood in the modern context first appears in the book The Exploration of Cosmic Space by Means of Reaction Devices, by the Russian school teacher Konstantin Tsiolkovsky. This seminal treatise on astronautics was published in May 1903, but was not distributed outside  Russia until years later, and Russian scientists paid little attention to it.
Pedro Paulet wrote a letter to El Comercio in Lima in 1927, claiming he had experimented with a liquid rocket engine while he was a student in Paris three decades earlier. Historians of early rocketry experiments, among them Max Valier, Willy Ley, and John D. Clark, have given differing amounts of credence to Paulet's report. Valier applauded Paulet's liquid-propelled rocket design in the Verein für Raumschiffahrt publication Die Rakete, saying the engine had "amazing power" and that his plans were necessary for future rocket development. Wernher von Braun would later describe Paulet as "the pioneer of the liquid fuel propulsion motor" and stated that "Paulet helped man reach the Moon". Paulet was approached by Nazi Germany to help develop rocket technology, though he refused to assist and never shared the formula for his propellant.The first flight of a liquid-propellant rocket took place on March 16, 1926 at Auburn, Massachusetts, when American professor Dr. Robert H. Goddard launched a vehicle using liquid oxygen and gasoline as propellants. The rocket, which was dubbed "Nell", rose just 41 feet during a 2.5-second flight that ended in a cabbage field, but it was an important demonstration that rockets utilizing liquid propulsion were possible. Goddard proposed liquid propellants about fifteen years earlier and began to seriously experiment with them in 1921. The German-Romanian Hermann Oberth published a book in 1922 suggesting the use of liquid propellants.
In Germany, engineers and scientists became enthralled with liquid propulsion, building and testing them in the late 1920s within Opel RAK, the world's first rocket program, in Rüsselsheim. According to Max Valier's account, Opel RAK rocket designer, Friedrich Wilhelm Sander launched two liquid-fuel rockets at Opel Rennbahn in Rüsselsheim on April 10 and April 12, 1929. These Opel RAK rockets have been the first European, and after Goddard the world's second, liquid-fuel rockets in history. In his book “Raketenfahrt” Valier describes the size of the rockets as of 21 cm in diameter and with a length of 74 cm, weighing 7 kg empty and 16 kg with fuel. The maximum thrust was 45 to 50 kp, with a total burning time of 132 seconds. These properties indicate a gas pressure pumping. The main purpose of these tests was to develop the liquid rocket-propulsion system for a Gebrüder-Müller-Griessheim aircraft under construction for a planned flight across the English channel. Also spaceflight historian Frank H. Winter, curator at National Air and Space Museum in Washington, DC, confirms the Opel group was working, in addition to their solid-fuel rockets used for land-speed records and the world's first manned rocket-plane flights with the Opel RAK.1, on liquid-fuel rockets. By May 1929, the engine produced a thrust of 200 kg (440 lb.) "for longer than fifteen minutes and in July 1929, the Opel RAK collaborators were able to attain powered phases of more than thirty minutes for thrusts of 300 kg (660-lb.) at Opel's works in Rüsselsheim," again according to Max Valier's account. The Great Depression brought an end to the Opel RAK activities. After working for the German military in the early 1930s, Sander was arrested by Gestapo in 1935, when private rocket-engineering became forbidden in Germany. He was convicted of treason to 5 years in prison and forced to sell his company, he died in 1938. Max Valier's (via Arthur Rudolph and Heylandt), who died while experimenting in 1930, and Friedrich Sander's work on liquid-fuel rockets was confiscated by the German military, the Heereswaffenamt and integrated into the activities under General Walter Dornberger in the early and mid-1930s in a field near Berlin. Max Valier was a co-founder of an amateur research group, the VfR, working on liquid rockets in the early 1930s, and many of whose members eventually became important rocket technology pioneers, including Wernher von Braun. Von Braun served as head of the army research station that designed the V-2 rocket weapon for the Nazis. 
By the late 1930s, use of rocket propulsion for manned flight began to be seriously experimented with, as Germany's Heinkel He 176 made the first manned rocket-powered flight using a liquid rocket engine, designed by German aeronautics engineer Hellmuth Walter on June 20, 1939.  The only production rocket-powered combat aircraft ever to see military service, the Me 163 Komet in 1944-45, also used a Walter-designed liquid rocket engine, the Walter HWK 109-509, which produced up to 1,700 kgf (16.7 kN) thrust at full power.
After World War II the American government and military finally seriously considered liquid-propellant rockets as weapons and began to fund work on them. The Soviet Union did likewise, and thus began the Space Race.
In 2010s 3D printed engines started being used for spaceflight. Examples of such engines include SuperDraco used in launch escape system of the SpaceX Dragon 2 and also engines used for first or second stages in launch vehicles from Astra, Orbex, Relativity Space, Skyrora, or Launcher.
Liquid rockets have been built as monopropellant rockets using a single type of propellant, bipropellant rockets using two types of propellant, or more exotic tripropellant rockets using three types of propellant.
Bipropellant liquid rockets generally use a liquid fuel, such as liquid hydrogen or a hydrocarbon fuel such as RP-1, and a liquid oxidizer, such as liquid oxygen. The engine may be a cryogenic rocket engine, where the fuel and oxidizer, such as hydrogen and oxygen, are gases which have been liquefied at very low temperatures.
Liquid-propellant rockets can be throttled (thrust varied) in realtime, and have control of mixture ratio (ratio at which oxidizer and fuel are mixed); they can also be shut down, and, with a suitable ignition system or self-igniting propellant, restarted.
Hybrid rockets apply a liquid or gaseous oxidizer to a solid fuel. : 354–356 
All liquid rocket engines have tankage and pipes to store and transfer propellant, an injector system, a combustion chamber which is very typically cylindrical, and one (sometimes two or more) rocket nozzles. Liquid systems enable higher specific impulse than solids and hybrid rocket motors and can provide very high tankage efficiency.
Unlike gases, a typical liquid propellant has a density similar to water, approximately 0.7–1.4g/cm³ (except liquid hydrogen which has a much lower density), while requiring only relatively modest pressure to prevent vaporization. This combination of density and low pressure permits very lightweight tankage; approximately 1% of the contents for dense propellants and around 10% for liquid hydrogen (due to its low density and the mass of the required insulation).
For injection into the combustion chamber, the propellant pressure at the injectors needs to be greater than the chamber pressure; this can be achieved with a pump. Suitable pumps usually use centrifugal turbopumps due to their high power and light weight, although reciprocating pumps have been employed in the past. Turbopumps are usually extremely lightweight and can give excellent performance; with an on-Earth weight well under 1% of the thrust. Indeed, overall rocket engine thrust to weight ratios including a turbopump have been as high as 155:1 with the SpaceX Merlin 1D rocket engine and up to 180:1 with the vacuum version 
Alternatively, instead of pumps, a heavy tank of a high-pressure inert gas such as helium can be used, and the pump forgone; but the delta-v that the stage can achieve is often much lower due to the extra mass of the tankage, reducing performance; but for high altitude or vacuum use the tankage mass can be acceptable.
The major components of a rocket engine are therefore the combustion chamber (thrust chamber), pyrotechnic igniter, propellant feed system, valves, regulators, the propellant tanks, and the rocket engine nozzle. In terms of feeding propellants to the combustion chamber, liquid-propellant engines are either pressure-fed or pump-fed, and pump-fed engines work in either a gas-generator cycle, a staged-combustion cycle, or an expander cycle.
A liquid rocket engine can be tested prior to use, whereas for a solid rocket motor a rigorous quality management must be applied during manufacturing to ensure high reliability. A Liquid rocket engine can also usually be reused for several flights, as in the Space Shuttle and Falcon 9 series rockets, although reuse of solid rocket motors was also effectively demonstrated during the shuttle program.
Use of liquid propellants can be associated with a number of issues:
Thousands of combinations of fuels and oxidizers have been tried over the years. Some of the more common and practical ones are:
One of the most efficient mixtures, oxygen and hydrogen, suffers from the extremely low temperatures required for storing liquid hydrogen (around 20 K or −253.2 °C or −423.7 °F) and very low fuel density (70 kg/m3 or 4.4 lb/cu ft, compared to RP-1 at 820 kg/m3 or 51 lb/cu ft), necessitating large tanks that must also be lightweight and insulating. Lightweight foam insulation on the Space Shuttle external tank led to the Space Shuttle Columbia's destruction, as a piece broke loose, damaged its wing and caused it to break up on atmospheric reentry.
Liquid methane/LNG has several advantages over LH2. Its performance (max. specific impulse) is lower than that of LH2 but higher than that of RP1 (kerosene) and solid propellants, and its higher density, similarly to other hydrocarbon fuels, provides higher thrust to volume ratios than LH2, although its density is not as high as that of RP1.  This makes it specially attractive for reusable launch systems because higher density allows for smaller motors, propellant tanks and associated systems. LNG also burns with less or no soot (less or no coking) than RP1, which eases reusability when compared with it, and LNG and RP1 burn cooler than LH2 so LNG and RP1 do not deform the interior structures of the engine as much. This means that engines that burn LNG can be reused more than those that burn RP1 or LH2. Unlike engines that burn LH2, both RP1 and LNG engines can be designed with a shared shaft with a single turbine and two turbopumps, one each for LOX and LNG/RP1. In space, LNG does not need heaters to keep it liquid, unlike RP1. LNG is less expensive, being readily available in large quantities. It can be stored for more prolonged periods of time, and is less explosive than LH2.
Many non-cryogenic bipropellants are hypergolic (self igniting).
For storable ICBMs and most spacecraft, including crewed vehicles, planetary probes, and satellites, storing cryogenic propellants over extended periods is unfeasible. Because of this, mixtures of hydrazine or its derivatives in combination with nitrogen oxides are generally used for such applications, but are toxic and carcinogenic. Consequently, to improve handling, some crew vehicles such as Dream Chaser and Space Ship Two plan to use hybrid rockets with non-toxic fuel and oxidizer combinations.
The injector implementation in liquid rockets determines the percentage of the theoretical performance of the nozzle that can be achieved. A poor injector performance causes unburnt propellant to leave the engine, giving poor efficiency.
Additionally, injectors are also usually key in reducing thermal loads on the nozzle; by increasing the proportion of fuel around the edge of the chamber, this gives much lower temperatures on the walls of the nozzle.
Injectors can be as simple as a number of small diameter holes arranged in carefully constructed patterns through which the fuel and oxidizer travel. The speed of the flow is determined by the square root of the pressure drop across the injectors, the shape of the hole and other details such as the density of the propellant.
The first injectors used on the V-2 created parallel jets of fuel and oxidizer which then combusted in the chamber. This gave quite poor efficiency.
Injectors today classically consist of a number of small holes which aim jets of fuel and oxidizer so that they collide at a point in space a short distance away from the injector plate. This helps to break the flow up into small droplets that burn more easily.
The main types of injectors are
The pintle injector permits good mixture control of fuel and oxidizer over a wide range of flow rates. The pintle injector was used in the Apollo Lunar Module engines (Descent Propulsion System) and the Kestrel engine, it is currently used in the Merlin engine on Falcon 9 and Falcon Heavy rockets.
The RS-25 engine designed for the Space Shuttle uses a system of fluted posts, which use heated hydrogen from the preburner to vaporize the liquid oxygen flowing through the center of the posts and this improves the rate and stability of the combustion process; previous engines such as the F-1 used for the Apollo program had significant issues with oscillations that led to destruction of the engines, but this was not a problem in the RS-25 due to this design detail.
Valentin Glushko invented the centripetal injector in the early 1930s, and it has been almost universally used in Russian engines. Rotational motion is applied to the liquid (and sometimes the two propellants are mixed), then it is expelled through a small hole, where it forms a cone-shaped sheet that rapidly atomizes.  Goddard's first liquid engine used a single impinging injector.  German scientists in WWII experimented with impinging injectors on flat plates, used successfully in the Wasserfall missile.
To avoid instabilities such as chugging, which is a relatively low speed oscillation, the engine must be designed with enough pressure drop across the injectors to render the flow largely independent of the chamber pressure. This pressure drop is normally achieved by using at least 20% of the chamber pressure across the injectors.
Nevertheless, particularly in larger engines, a high speed combustion oscillation is  easily triggered, and these are not well understood. These high speed oscillations tend to disrupt the gas side boundary layer of the engine, and this can cause the cooling system to rapidly fail, destroying the engine. These kinds of oscillations are much more common on large engines, and plagued the development of the Saturn V, but were finally overcome.
Some combustion chambers, such as those of the RS-25 engine, use Helmholtz resonators as damping mechanisms to stop particular resonant frequencies from growing.
To prevent these issues the RS-25 injector design instead went to a lot of effort to vaporize the propellant prior to injection into the combustion chamber. Although many other features were used to ensure that instabilities could not occur, later research showed that these other features were unnecessary, and the gas phase combustion worked reliably.
Testing for stability often involves the use of small explosives. These are detonated within the chamber during operation, and causes an impulsive excitation. By examining the pressure trace of the chamber to determine how quickly the effects of the disturbance die away, it is possible to estimate the stability and redesign features of the chamber if required.
For liquid-propellant rockets, four different ways of powering the injection of the propellant into the chamber are in common use.
Fuel and oxidizer must be pumped into the combustion chamber against the pressure of the hot gasses being burned, and engine power is limited by the rate at which propellant can be pumped into the combustion chamber. For atmospheric or launcher use, high pressure, and thus high power, engine cycles are desirable to minimize gravity drag. For orbital use, lower power cycles are usually fine.
Selecting an engine cycle is one of the earlier steps to rocket engine design.  A number of tradeoffs arise from this selection, some of which include:
Injectors are commonly laid out so that a fuel-rich layer is created at the combustion chamber wall. This reduces the temperature there, and downstream to the throat and even into the nozzle and permits the combustion chamber to be run at higher pressure, which permits a higher expansion ratio nozzle to be used which gives a higher ISP and better system performance. A liquid rocket engine often employs regenerative cooling, which uses the fuel or less commonly the oxidizer to cool the chamber and nozzle.
Ignition can be performed in many ways, but perhaps more so with liquid propellants than other rockets a consistent and significant ignitions source is required; a delay of ignition (in some cases as small as a few tens of milliseconds) can cause overpressure of the chamber due to excess propellant. A hard start can even cause an engine to explode.
Generally, ignition systems try to apply flames across the injector surface, with a mass flow of approximately 1% of the full mass flow of the chamber.
Safety interlocks are sometimes used to ensure the presence of an ignition source before the main valves open; however reliability of the interlocks can in some cases be lower than the ignition system. Thus it depends on whether the system must fail safe, or whether overall mission success is more important. Interlocks are rarely used for upper, unmanned stages where failure of the interlock would cause loss of mission, but are present on the RS-25 engine, to shut the engines down prior to liftoff of the Space Shuttle. In addition, detection of successful ignition of the igniter is surprisingly difficult, some systems use thin wires that are cut by the flames, pressure sensors have also seen some use.
Methods of ignition include pyrotechnic, electrical (spark or hot wire), and chemical.  Hypergolic propellants have the advantage of self igniting, reliably and with less chance of hard starts.  In the 1940s, the Russians began to start engines with hypergols, to then switch over to the primary propellants after ignition. This was also used on the American F-1 rocket engine on the Apollo program.
Ignition with a pyrophoric agent: Triethylaluminium ignites on contact with air and will ignite and/or decompose on contact with water, and with any other oxidizer—it is one of the few substances sufficiently pyrophoric to ignite on contact with cryogenic liquid oxygen. The enthalpy of combustion, ΔcH°, is −5,105.70 ± 2.90 kJ/mol (−1,220.29 ± 0.69 kcal/mol). Its easy ignition makes it particularly desirable as a rocket engine ignitor.  May be used in conjunction with triethylborane to create triethylaluminum-triethylborane, better known as TEA-TEB.
Bibliography.mw-parser-output .refbegin{font-size:90%;margin-bottom:0.5em}.mw-parser-output .refbegin-hanging-indents>ul{margin-left:0}.mw-parser-output .refbegin-hanging-indents>ul>li{margin-left:0;padding-left:3.2em;text-indent:-3.2em}.mw-parser-output .refbegin-hanging-indents ul,.mw-parser-output .refbegin-hanging-indents ul li{list-style:none}@media(max-width:720px){.mw-parser-output .refbegin-hanging-indents>ul>li{padding-left:1.6em;text-indent:-1.6em}}.mw-parser-output .refbegin-columns{margin-top:0.3em}.mw-parser-output .refbegin-columns ul{margin-top:0}.mw-parser-output .refbegin-columns li{page-break-inside:avoid;break-inside:avoid-column}
A solid-propellant rocket or solid rocket is a rocket with a rocket engine that uses solid propellants (fuel/oxidizer). The earliest rockets were solid-fuel rockets powered by gunpowder; they were used in warfare by the Chinese, Persians, Mongols, and Indians as early as the 13th century.
All rockets used some form of solid or powdered propellant up until the 20th century, when liquid-propellant rockets offered more efficient and controllable alternatives. Solid rockets are still used today in military armaments worldwide, model rockets, solid rocket boosters and on larger applications for their simplicity and reliability.
Since solid-fuel rockets can remain in storage for a long time without much propellant degradation and because they almost always launch reliably, they have been frequently used in military applications such as missiles. The lower performance of solid propellants (as compared to liquids) does not favor their use as primary propulsion in modern medium-to-large launch vehicles customarily used to orbit commercial satellites and launch major space probes. Solids are, however, frequently used as strap-on boosters to increase payload capacity or as spin-stabilized add-on upper stages when higher-than-normal velocities are required. Solid rockets are used as light launch vehicles for low Earth orbit (LEO) payloads under 2 tons or escape payloads up to 500 kilograms (1,100 lb).
A simple solid rocket motor consists of a casing, nozzle, grain (propellant charge), and igniter.
The solid grain mass burns in a predictable fashion to produce exhaust gases, the flow of which is described by Taylor–Culick flow. The nozzle dimensions are calculated to maintain a design chamber pressure, while producing thrust from the exhaust gases.
Once ignited, a simple solid rocket motor cannot be shut off, because it contains all the ingredients necessary for combustion within the chamber in which they are burned. More advanced solid rocket motors can be throttled, and also be extinguished, and then re-ignited by control of the nozzle geometry, or through the use of vent ports. Further, pulsed rocket motors that burn in segments, and that can be ignited upon command are available.
Modern designs may also include a steerable nozzle for guidance, avionics, recovery hardware (parachutes), self-destruct mechanisms, APUs, controllable tactical motors, controllable divert and attitude control motors, and thermal management materials.
The medieval Song dynasty Chinese invented a very primitive form of solid-propellant rocket. Illustrations and descriptions in the 14th century Chinese military treatise Huolongjing by the Ming dynasty military writer and philosopher Jiao Yu confirm that the Chinese in 1232 used proto solid propellant rockets then known as "fire arrows" to drive back the Mongols during the Mongol siege of Kaifeng. Each arrow took a primitive form of a simple, solid-propellant rocket tube that was filled with gunpowder. One open end allowed the gas to escape and was attached to a long stick that acted as a guidance system for flight direction control.
The first rockets with tubes of cast iron were used by the Kingdom of Mysore under Hyder Ali and Tipu Sultan in the 1750s. These rockets had a reach of targets up to a mile and a half away. These were extremely effective in the Second Anglo-Mysore War that ended in a humiliating defeat for the British Empire. Word of the success of the Mysore rockets against the British Imperial power triggered research in England, France, Ireland and elsewhere. When the British finally conquered the fort of Srirangapatana in 1799, hundreds of rockets were shipped off to the Royal Arsenal near London to be reverse-engineered. This led to the first industrial manufacture of military rockets with the Congreve rocket in 1804.
In 1921 the Soviet research and development laboratory Gas Dynamics Laboratory began developing solid-propellant rockets, which resulted in the first launch in 1928, that flew for approximately 1,300 metres. These rockets were used in 1931 for the world's first successful use of rockets to assist take-off of aircraft.  The research continued from 1933 by the Reactive Scientific Research Institute (RNII) with the development of the RS-82 and RS-132 rockets, including designing several variations for ground-to-air, ground-to-ground, air-to-ground and air-to-air combat. The earliest known use by the Soviet Air Force of aircraft-launched unguided anti-aircraft rockets in combat against heavier-than-air aircraft took place in August 1939, during the Battle of Khalkhin Gol.  In June 1938, the RNII began developing a multiple rocket launcher based on the RS-132 rocket. In August 1939, the completed product was the BM-13 / Katyusha rocket launcher. Towards the end of 1938 the first significant large scale testing of the rocket launchers took place, 233 rockets of various types were used. A salvo of rockets could completely straddle a target at a range of 5,500 metres (3.4 mi). By the end of World War II total production of rocket launchers reached about 10,000. with 12 million rockets of the RS type produced for the Soviet armed forces.
In the United States modern castable composite solid rocket motors were invented by the American aerospace engineer Jack Parsons at Caltech in 1942 when he replaced double base propellant with roofing asphalt and potassium perchlorate. This made possible slow-burning rocket motors of adequate size and with sufficient shelf-life for jet-assisted take off applications. Charles Bartley, employed at JPL (Caltech), substituted curable synthetic rubber for the gooey asphalt, creating a flexible but geometrically stable load-bearing propellant grain that bonded securely to the motor casing. This made possible much larger solid rocket motors. Atlantic Research Corporation significantly boosted composite propellant Isp in 1954 by increasing the amount of powdered aluminium in the propellant to as much as 20%.
Solid-propellant rocket technology got its largest boost in technical innovation, size and capability with the various mid-20th century government initiatives to develop increasingly capable military missiles. After initial designs of ballistic missile military technology designed with liquid-propellant rockets in the 1940s and 1950s, both the Soviet Union and the United States embarked on major initiatives to develop solid-propellant local, regional, and intercontinental ballistic missiles, including solid-propellant missiles that could be launched from air or sea. Many other governments also developed these military technologies over the next 50 years.
By the later 1980s and continuing to 2020, these government-developed highly-capable solid rocket technologies have been applied to orbital spaceflight by many government-directed programs, most often as booster rockets to add extra thrust during the early ascent of their primarily liquid rocket launch vehicles.  Some designs have had solid rocket upper stages as well.  Examples flying in the 2010s include the European Ariane 5, US Atlas V and Space Shuttle, and Japan's H-II.
The largest solid rocket motors ever built were Aerojet's three 6.60-meter (260 in) monolithic solid motors cast in Florida. Motors 260 SL-1 and SL-2 were 6.63 meters (261 in) in diameter, 24.59 meters (80 ft 8 in) long, weighed 842,900 kilograms (1,858,300 lb), and had a maximum thrust of 16 MN (3,500,000 lbf). Burn duration was two minutes. The nozzle throat was large enough to walk through standing up. The motor was capable of serving as a 1-to-1 replacement for the 8-engine Saturn I liquid-propellant first stage but was never used as such. Motor 260 SL-3 was of similar length and weight but had a maximum thrust of 24 MN (5,400,000 lbf) and a shorter duration.
Design begins with the total impulse required, which determines the fuel and oxidizer mass. Grain geometry and chemistry are then chosen to satisfy the required motor characteristics.
The following are chosen or solved simultaneously. The results are exact dimensions for grain, nozzle, and case geometries:
The grain may or may not be bonded to the casing. Case-bonded motors are more difficult to design, since the deformation of the case and the grain under flight must be compatible.
Common modes of failure in solid rocket motors include fracture of the grain, failure of case bonding, and air pockets in the grain. All of these produce an instantaneous increase in burn surface area and a corresponding increase in exhaust gas production rate and pressure, which may rupture the casing.
Another failure mode is casing seal failure. Seals are required in casings that have to be opened to load the grain. Once a seal fails, hot gas will erode the escape path and result in failure. This was the cause of the Space Shuttle Challenger disaster.
Solid rocket fuel deflagrates from the surface of exposed propellant in the combustion chamber. In this fashion, the geometry of the propellant inside the rocket motor plays an important role in the overall motor performance. As the surface of the propellant burns, the shape evolves (a subject of study in internal ballistics), most often changing the propellant surface area exposed to the combustion gases. Since the propellant volume is equal to the cross sectional area 




A

s




{\displaystyle A_{s}}

 times the fuel length, the volumetric propellant consumption rate is the cross section area times the linear burn rate 






b
˙





{\displaystyle {\dot {b}}}

, and the instantaneous mass flow rate of combustion gases generated is equal to the volumetric rate times the fuel density 



ρ


{\displaystyle \rho }

:
Several geometric configurations are often used depending on the application and desired thrust curve:
Circular bore simulation
C-slot simulation
Moon burner simulation
5-point finocyl simulation
The casing may be constructed from a range of materials. Cardboard is used for small black powder model motors, whereas aluminium is used for larger composite-fuel hobby motors. Steel was used for the space shuttle boosters. Filament-wound graphite epoxy casings are used for high-performance motors.
The casing must be designed to withstand the pressure and resulting stresses of the rocket motor, possibly at elevated temperature. For design, the casing is considered a pressure vessel.
To protect the casing from corrosive hot gases, a sacrificial thermal liner on the inside of the casing is often implemented, which ablates to prolong the life of the motor casing.
A convergent-divergent design accelerates the exhaust gas out of the nozzle to produce thrust. The nozzle must be constructed from a material that can withstand the heat of the combustion gas flow. Often, heat-resistant carbon-based materials are used, such as amorphous graphite or carbon-carbon.
Some designs include directional control of the exhaust. This can be accomplished by gimballing the nozzle, as in the Space Shuttle SRBs, by the use of jet vanes in the exhaust as in the V-2 rocket, or by liquid injection thrust vectoring (LITV).
LITV consists of injecting a liquid into the exhaust stream after the nozzle throat. The liquid then vaporizes, and in most cases chemically reacts, adding mass flow to one side of the exhaust stream and thus providing a control moment. For example, the Titan IIIC solid boosters injected nitrogen tetroxide for LITV; the tanks can be seen on the sides of the rocket between the main center stage and the boosters.
An early Minuteman first stage used a single motor with four gimballed nozzles to provide pitch, yaw, and roll control.
A typical, well-designed ammonium perchlorate composite propellant (APCP) first-stage motor may have a vacuum specific impulse (Isp) as high as 285.6 seconds (2.801 km/s) (Titan IVB SRMU). This compares to 339.3 s (3.327 km/s) for RP1/LOX (RD-180) and 452.3 s (4.436 km/s) for LH2/LOX (Block II RS-25) bipropellant engines. Upper stage specific impulses are somewhat greater: as much as 303.8 s (2.979 km/s) for APCP (Orbus 6E), 359 s (3.52 km/s) for RP1/LOX (RD-0124) and 465.5 s (4.565 km/s) for LH2/LOX (RL10B-2). Propellant fractions are usually somewhat higher for (non-segmented) solid propellant first stages than for upper stages. The 53,000-kilogram (117,000 lb) Castor 120 first stage has a propellant mass fraction of 92.23% while the 14,000-kilogram (31,000 lb) Castor 30 upper stage developed for Orbital Science's Taurus II COTS(Commercial Off The Shelf) (International Space Station resupply) launch vehicle has a 91.3% propellant fraction with 2.9% graphite epoxy motor casing, 2.4% nozzle, igniter and thrust vector actuator, and 3.4% non-motor hardware including such things as payload mount, interstage adapter, cable raceway, instrumentation, etc. Castor 120 and Castor 30 are 2.36 and 2.34 meters (93 and 92 in) in diameter, respectively, and serve as stages on the Athena IC and IIC commercial launch vehicles. A four-stage Athena II using Castor 120s as both first and second stages became the first commercially developed launch vehicle to launch a lunar probe (Lunar Prospector) in 1998.
Solid rockets can provide high thrust for relatively low cost. For this reason, solids have been used as initial stages in rockets (for example the Space Shuttle), while reserving high specific impulse engines, especially less massive hydrogen-fueled engines, for higher stages. In addition, solid rockets have a long history as the final boost stage for satellites due to their simplicity, reliability, compactness and reasonably high mass fraction. A spin-stabilized solid rocket motor is sometimes added when extra velocity is required, such as for a mission to a comet or the outer solar system, because a spinner does not require a guidance system (on the newly added stage). Thiokol's extensive family of mostly titanium-cased Star space motors has been widely used, especially on Delta launch vehicles and as spin-stabilized upper stages to launch satellites from the cargo bay of the Space Shuttle. Star motors have propellant fractions as high as 94.6% but add-on structures and equipment reduce the operating mass fraction by 2% or more.
Higher performing solid rocket propellants are used in large strategic missiles (as opposed to commercial launch vehicles). HMX, C4H8N4(NO2)4, a nitramine with greater energy than ammonium perchlorate, was used in the propellant of the Peacekeeper ICBM and is the main ingredient in NEPE-75 propellant used in the Trident II D-5 Fleet Ballistic Missile. It is because of explosive hazard that the higher energy military solid propellants containing HMX are not used in commercial launch vehicles except when the LV is an adapted ballistic missile already containing HMX propellant (Minotaur IV and V based on the retired Peacekeeper ICBMs). The Naval Air Weapons Station at China Lake, California, developed a new compound, C6H6N6(NO2)6, called simply CL-20 (China Lake compound #20). Compared to HMX, CL-20 has 14% more energy per mass, 20% more energy per volume, and a higher oxygen-to-fuel ratio. One of the motivations for development of these very high energy density military solid propellants is to achieve mid-course exo-atmospheric ABM capability from missiles small enough to fit in existing ship-based below-deck vertical launch tubes and air-mobile truck-mounted launch tubes. CL-20 propellant compliant with Congress' 2004 insensitive munitions (IM) law has been demonstrated and may, as its cost comes down, be suitable for use in commercial launch vehicles, with a very significant increase in performance compared with the currently favored APCP solid propellants. With a specific impulse of 309 s already demonstrated by Peacekeeper's second stage using HMX propellant, the higher energy of CL-20 propellant can be expected to increase specific impulse to around 320 s in similar ICBM or launch vehicle upper stage applications, without the explosive hazard of HMX.
An attractive attribute for military use is the ability for solid rocket propellant to remain loaded in the rocket for long durations and then be reliably launched at a moment's notice.
Black powder (gunpowder) is composed of charcoal (fuel), potassium nitrate (oxidizer), and sulfur (fuel and catalyst). It is one of the oldest pyrotechnic compositions with application to rocketry. In modern times, black powder finds use in low-power model rockets (such as Estes and Quest rockets), as it is cheap and fairly easy to produce. The fuel grain is typically a mixture of pressed fine powder (into a solid, hard slug), with a burn rate that is highly dependent upon exact composition and operating conditions. The performance or specific impulse of black powder is low, around 80 seconds. The grain is sensitive to fracture and, therefore, catastrophic failure. Black powder does not typically find use in motors above 40 newtons (9.0 pounds-force).
Composed of powdered zinc metal and powdered sulfur (oxidizer), ZS or "micrograin" is another pressed propellant that does not find any practical application outside specialized amateur rocketry circles due to its poor performance (as most ZS burns outside the combustion chamber) and fast linear burn rates on the order of 2 m/s. ZS is most often employed as a novelty propellant as the rocket accelerates extremely quickly leaving a spectacular large orange fireball behind it.
In general, rocket candy propellants are an oxidizer (typically potassium nitrate) and a sugar fuel (typically dextrose, sorbitol, or sucrose) that are cast into shape by gently melting the propellant constituents together and pouring or packing the amorphous colloid into a mold. Candy propellants generate a low-medium specific impulse of roughly 130 s and, thus, are used primarily by amateur and experimental rocketeers.
DB propellants are composed of two monopropellant fuel components where one typically acts as a high-energy (yet unstable) monopropellant and the other acts as a lower-energy stabilizing (and gelling) monopropellant. In typical circumstances, nitroglycerin is dissolved in a nitrocellulose gel and solidified with additives. DB propellants are implemented in applications where minimal smoke is required yet medium-high performance (Isp of roughly 235 s) is required. The addition of metal fuels (such as aluminium) can increase the performance (around 250 s), though metal oxide nucleation in the exhaust can turn the smoke opaque.
A powdered oxidizer and powdered metal fuel are intimately mixed and immobilized with a rubbery binder (that also acts as a fuel). Composite propellants are often either ammonium nitrate-based (ANCP) or ammonium perchlorate-based (APCP). Ammonium nitrate composite propellant often uses magnesium and/or aluminium as fuel and delivers medium performance (Isp of about 210 s) whereas ammonium perchlorate composite propellant often uses aluminium fuel and delivers high performance (vacuum Isp up to 296 s with a single piece nozzle or 304 s with a high area ratio telescoping nozzle). Aluminium is used as fuel because it has a reasonable specific energy density, a high volumetric energy density, and is difficult to ignite accidentally. Composite propellants are cast, and retain their shape after the rubber binder, such as Hydroxyl-terminated polybutadiene (HTPB), cross-links (solidifies) with the aid of a curative additive. Because of its high performance, moderate ease of manufacturing, and moderate cost, APCP finds widespread use in space rockets, military rockets, hobby and amateur rockets, whereas cheaper and less efficient ANCP finds use in amateur rocketry and gas generators. Ammonium dinitramide, NH4N(NO2)2, is being considered as a 1-to-1 chlorine-free substitute for ammonium perchlorate in composite propellants. Unlike ammonium nitrate, ADN can be substituted for AP without a loss in motor performance.
Polyurethane-bound aluminium-APCP solid fuel was used in the submarine launched Polaris missiles. APCP used in the space shuttle Solid Rocket Boosters consisted of ammonium perchlorate (oxidizer, 69.6% by weight), aluminium (fuel, 16%), iron oxide (a catalyst, 0.4%), polybutadiene acrylonitrile (PBAN) polymer (a non-urethane rubber binder that held the mixture together and acted as secondary fuel, 12.04%), and an epoxy curing agent (1.96%). It developed a specific impulse of 242 seconds (2.37 km/s) at sea level or 268 seconds (2.63 km/s) in a vacuum. The 2005-2009 Constellation Program was to use a similar PBAN-bound APCP.
In 2009, a group succeeded in creating a propellant of water and nanoaluminium (ALICE).
Typical HEC propellants start with a standard composite propellant mixture (such as APCP) and add a high-energy explosive to the mix. This extra component usually is in the form of small crystals of RDX or HMX, both of which have higher energy than ammonium perchlorate. Despite a modest increase in specific impulse, implementation is limited due to the increased hazards of the high-explosive additives.
Composite modified double base propellants start with a nitrocellulose/nitroglycerin double base propellant as a binder and add solids (typically ammonium perchlorate (AP) and powdered aluminium) normally used in composite propellants. The ammonium perchlorate makes up the oxygen deficit introduced by using nitrocellulose, improving the overall specific impulse. The aluminium improves specific impulse as well as combustion stability. High performing propellants such as NEPE-75 used to fuel the Trident II D-5, SLBM replace most of the AP with polyethylene glycol-bound HMX, further increasing specific impulse. The mixing of composite and double base propellant ingredients has become so common as to blur the functional definition of double base propellants.
One of the most active areas of solid propellant research is the development of high-energy, minimum-signature propellant using C6H6N6(NO2)6 CL-20 nitroamine (China Lake compound #20), which has 14% higher energy per mass and 20% higher energy density than HMX. The new propellant has been successfully developed and tested in tactical rocket motors. The propellant is non-polluting: acid-free, solid particulates-free, and lead-free. It is also smokeless and has only a faint shock diamond pattern that is visible in the otherwise transparent exhaust. Without the bright flame and dense smoke trail produced by the burning of aluminized propellants, these smokeless propellants all but eliminate the risk of giving away the positions from which the missiles are fired. The new CL-20 propellant is shock-insensitive (hazard class 1.3) as opposed to current HMX smokeless propellants which are highly detonable (hazard class 1.1). CL-20 is considered a major breakthrough in solid rocket propellant technology but has yet to see widespread use because costs remain high.
Electric solid propellants (ESPs) are a family of high performance plastisol solid propellants that can be ignited and throttled by the application of electric current. Unlike conventional rocket motor propellants that are difficult to control and extinguish, ESPs can be ignited reliably at precise intervals and durations. It requires no moving parts and the propellant is insensitive to flames or electrical sparks.
Solid propellant rocket motors can be bought for use in model rocketry; they are normally small cylinders of black powder fuel with an integral nozzle and optionally a small charge that is set off when the propellant is exhausted after a time delay. This charge can be used to trigger a camera, or deploy a parachute. Without this charge and delay, the motor may ignite a second stage (black powder only).
In mid- and high-power rocketry, commercially made APCP motors are widely used. They can be designed as either single-use or reloadables. These motors are available in impulse ranges from "A" (1.26Ns– 2.50Ns) to "O"(20.48KNs – 40.96KNs), from several manufacturers. They are manufactured in standardized diameters, and varying lengths depending on required impulse. Standard motor diameters are 13, 18, 24, 29, 38, 54, 75, 98, and 150 millimeters. Different propellant formulations are available to produce different thrust profiles, as well as "special effects" such as colored flames, smoke trails, or large quantities of sparks (produced by adding titanium sponge to the mix).
Almost all sounding rockets use solid motors.
Due to reliability, ease of storage and handling, solid rockets are used on missiles and ICBMs.
Solid rockets are suitable for launching small payloads to orbital velocities, especially if three or more stages are used. Many of these are based on repurposed ICBMs.
Larger liquid-fueled orbital rockets often use solid rocket boosters to gain enough initial thrust to launch the fully fueled rocket.
Solid fuel is also used for some upper stages, particularly the Star 37 (sometimes referred to as the "Burner" upper stage) and the Star 48 (sometimes referred to as the "Payload Assist Module", or PAM), both manufactured originally by Thiokol, and today by Northrop Grumman. They are used to lift large payloads to intended orbits (such as the Global Positioning System satellites), or smaller payloads to interplanetary—or even interstellar—trajectories. Another solid-fuel upper stage, used by the Space Shuttle and the Titan IV, was the Boeing-manufactured Inertial Upper Stage (IUS).
Some rockets, like the Antares (manufactured by Northrop Grumman), have mandatory solid-fuel upper stages. The Antares rocket uses the Northrop Grumman-manufactured Castor 30 as an upper stage.

A hybrid-propellant rocket is a rocket with a rocket motor that uses rocket propellants in two different phases: one solid and the other either gas or liquid. The hybrid rocket concept can be traced back to at least the 1930s.
Hybrid rockets avoid some of the disadvantages of solid rockets like the dangers of propellant handling, while also avoiding some disadvantages of liquid rockets like their mechanical complexity. Because it is difficult for the fuel and oxidizer to be mixed intimately (being different states of matter), hybrid rockets tend to fail more benignly than liquids or solids. Like liquid rocket engines, hybrid rocket motors can be shut down easily and the thrust is throttleable. The theoretical specific impulse (




I

s
p




{\displaystyle I_{sp}}

) performance of hybrids is generally higher than solid motors and lower than liquid engines. 




I

s
p




{\displaystyle I_{sp}}

 as high as 400 s has been measured in a hybrid rocket using metalized fuels. Hybrid systems are more complex than solid ones, but they avoid significant hazards of manufacturing, shipping and handling solid rocket motors by storing the oxidizer and the fuel separately.
The first work on hybrid rockets was performed in the late 1930s at IG Farben in Germany and concurrently at the California Rocket Society in the United States. Leonid Andrussow, working in Germany, first theorized hybrid propellant rockets. O. Lutz, W. Noeggerath, and Andrussow tested a 10 kilonewtons (2,200 lbf) hybrid rocket motor using coal and gaseous N2O as the propellants. Oberth also worked on a hybrid rocket motor using LOX as the oxidizer and graphite as the fuel. The high heat of sublimation of carbon prevented these rocket motors from operating efficiently, as it resulted in a negligible burning rate.
In the 1940s, the California Pacific Rocket Society used LOX in combination with several different fuel types, including wood, wax, and rubber. The most successful of these tests was with the rubber fuel, which is still the dominant fuel in use today. In June 1951, a LOX / rubber rocket was flown to an altitude of 9 kilometres (5.6 mi).
Two major efforts occurred in the 1950s. One of these efforts was by G. Moore and K. Berman at General Electric. The duo used 90% high test peroxide (HTP, or H2O2) and polyethylene (PE) in a rod and tube grain design. They drew several significant conclusions from their work. The fuel grain had uniform burning. Grain cracks did not affect combustion, like it does with solid rocket motors. No hard starts were observed (a hard start is a pressure spike seen close to the time of ignition, typical of liquid rocket engines). The fuel surface acted as a flame holder, which encouraged stable combustion. The oxidizer could be throttled with one valve, and a high oxidizer to fuel ratio helped simplify combustion. The negative observations were low burning rates and that the thermal instability of peroxide was problematic for safety reasons. Another effort that occurred in the 1950s was the development of a reverse hybrid. In a standard hybrid rocket motor, the solid material is the fuel. In a reverse hybrid rocket motor, the oxidizer is solid. William Avery of the Applied Physics Laboratory used jet fuel and ammonium nitrate, selected for their low cost. His O/F ratio was 0.035, which was 200 times smaller than the ratio used by Moore and Berman.
In 1953 Pacific Rocket Society (est. 1943) was developing the XDF-23, a 4 inches (10 cm) × 72 inches (180 cm) hybrid rocket, designed by Jim Nuding, using LOX and rubber polymer called "Thiokol". They had already tried other fuels in prior iterations including cotton, paraffin wax and wood. The XDF name itself comes from "experimental Douglas fir" from one of the first units.
In the 1960s, European organizations also began work on hybrid rockets. ONERA, based in France, and Volvo Flygmotor, based in Sweden, developed sounding rockets using hybrid rocket motor technology. The ONERA group focused on a hypergolic rocket motor, using nitric acid and an amine fuel. The company flew eight rockets: Once in April 1964, three times in June 1965, and four times in 1967. The maximum altitude the flights achieved was over 100 kilometres (62 mi). The Volvo Flygmotor group also used a hypergolic propellant combination. They also used nitric acid for their oxidizer, but used Tagaform (polybutadiene with an aromatic amine) as their fuel. Their flight was in 1969, lofting a 20 kilograms (44 lb) payload to 80 kilometres (50 mi).
Meanwhile, in the United States, United Technologies Center (Chemical Systems Division) and Beech Aircraft were working on a supersonic target drone, known as Sandpiper. It used MON-25 (mixed 25% NO, 75% N2O4) as the oxidizer and polymethyl methacrylate (PMM) and Mg for the fuel. The drone flew six times in 1968, for more than 300 seconds and to an altitude greater than 160 kilometres (99 mi). The second iteration of the rocket, known as the HAST, had IRFNA-PB/PMM for its propellants and was throttleable over a 10/1 range. HAST could carry a heavier payload than the Sandpiper. Another iteration, which used the same propellant combination as the HAST, was developed by Chemical Systems Division and Teledyne Aircraft. Development for this program ended in the mid-1980s. Chemical Systems Division also worked on a propellant combination of lithium and FLOx (mixed F2 and O2). This was an efficient hypergolic rocket that was throttleable. The vacuum specific impulse was 380 seconds at 93% combustion efficiency.
American Rocket Company (AMROC) developed the largest hybrid rockets ever created in the late 1980s and early 1990s. The first version of their engine, fired at the Air Force Phillips Laboratory, produced 312,000 newtons (70,000 lbf) of thrust for 70 seconds with a propellant combination of LOX and hydroxyl-terminated polybutadiene (HTPB) rubber. The second version of the motor, known as the H-250F, produced more than 1,000,000 newtons (220,000 lbf) of thrust.
Korey Kline of Environmental Aeroscience Corporation (eAc) first fired a gaseous oxygen and rubber hybrid in 1982 at Lucerne Dry Lake, CA, after discussions on the technology with Bill Wood, formerly with Westinghouse. The first SpaceShipOne hybrid tests were successfully conducted by Kline and eAc at Mojave, CA.
In 1994, the U.S. Air Force Academy flew a hybrid sounding rocket to an altitude of 5 kilometres (3.1 mi). The 6.4 metres (21 ft) rocket used HTPB and LOX for its propellant, and reached a peak thrust of 4,400 newtons (990 lbf) and had a thrust duration of 16 seconds.
In its simplest form, a hybrid rocket consists of a pressure vessel (tank) containing the liquid oxidiser, the combustion chamber containing the solid propellant, and a mechanical device separating the two. When thrust is desired, a suitable ignition source is introduced in the combustion chamber and the valve is opened. The liquid oxidiser (or gas) flows into the combustion chamber where it is vaporized and then reacted with the solid propellant. Combustion occurs in a boundary layer diffusion flame adjacent to the surface of the solid propellant.
Generally, the liquid propellant is the oxidizer and the solid propellant is the fuel because solid oxidizers are extremely dangerous and lower performing than liquid oxidizers. Furthermore, using a solid fuel such as Hydroxyl-terminated polybutadiene (HTPB) or paraffin wax allows for the incorporation of high-energy fuel additives such as aluminium, lithium, or metal hydrides.
The governing equation for hybrid rocket combustion shows that the regression rate is dependent on the oxidizer mass flux rate, which means the rate that the fuel will burn is proportional to the amount of oxidizer flowing through the port. This differs from a solid rocket motor, in which the regression rate is proportional to the chamber pressure of the motor.
As the motor burns, the increase in diameter of the fuel port results in an increased fuel mass flow rate. This phenomenon makes the oxidizer to fuel ratio (O/F) shift during the burn. The increased fuel mass flow rate can be compensated for by also increasing the oxidizer mass flow rate. In addition to the O/F varying as a function of time, it also varies based on the position down the fuel grain. The closer the position is to the top of the fuel grain, the higher the O/F ratio. Since the O/F varies down the port, a point called the stoichiometric point may exist at some point down the grain.
Hybrid rocket motors exhibit some obvious as well as some subtle advantages over liquid-fuel rockets and solid-fuel rockets. A brief summary of some of these is given below:
Hybrid rockets also exhibit some disadvantages when compared with liquid and solid rockets. These include:
In general, much less development work has been completed with hybrids than liquids or solids and it is likely that some of these disadvantages could be rectified through further investment in research and development.
One problem in designing large hybrid orbital rockets is that turbopumps become necessary to achieve high flow rates and pressurization of the oxidizer. This turbopump must be powered by something. In a traditional liquid-propellant rocket, the turbopump uses the same fuel and oxidizer as the rocket, since they are both liquid and can be fed to the pre-burner. But in a hybrid, the fuel is solid and cannot be fed to a turbopump's engine. Some hybrids use an oxidizer that can also be used as a monopropellant, such as nitromethane or hydrogen peroxide, and so a turbopump can run on it alone. But nitromethane and hydrogen peroxide are significantly less efficient than liquid oxygen, which cannot be used alone to run a turbopump. Another fuel would be needed, requiring its own tank and decreasing rocket performance.
A reverse-hybrid rocket, which is not very common, is one where the engine uses a solid oxidizer and a liquid fuel. Some liquid fuel options are kerosene, hydrazine, and LH2. Common fuels for a typical hybrid rocket engine include polymers such as acrylics, polyethylene (PE), cross-linked rubber, such as HTPB, or liquefying fuels such as paraffin wax. Plexiglass was a common fuel, since the combustion could be visible through the transparent combustion chamber. Hydroxyl-terminated polybutadiene (HTPB) synthetic rubber is currently the most popular fuel for hybrid rocket engines, due to its energy, and due to how safe it is to handle. Tests have been performed in which HTPB was soaked in liquid oxygen, and it still did not become explosive. These fuels are generally not as dense as solid rocket motors, so they are often doped with aluminum to increase the density and therefore the rocket performance.: 404 
Hybrid rocket fuel grains can be manufactured via casting techniques, since they are typically a plastic or a rubber. Complex geometries, which are driven by the need for higher fuel mass flow rates, makes casting fuel grains for hybrid rockets expensive and time-consuming due in part to equipment costs. On a larger scale, cast grains must be supported by internal webbing, so that large chunks of fuel do not impact or even potentially block the nozzle. Grain defects are also an issue in larger grains. Traditional fuels that are cast are hydroxyl-terminated polybutadiene (HTPB) and paraffin waxes.
Additive manufacturing is currently being used to create grain structures that were otherwise not possible to manufacture. Helical ports have been shown to increase fuel regression rates while also increasing volumetric efficiency. An example of material used for a hybrid rocket fuel is acrylonitrile butadiene styrene (ABS). The printed material is also typically enhanced with additives to improve rocket performance. Recent work at the University of Tennessee Knoxville has shown that, due to the increased surface area, the use of powdered fuels (i.e. graphite, coal, aluminum) encased in a 3D printed, ABS matrix can significantly increase the fuel burn rate and thrust level as compared to traditional polymer grains.
Common oxidizers include gaseous or liquid oxygen, nitrous oxide, and hydrogen peroxide. For a reverse hybrid, oxidizers such as frozen oxygen and ammonium perchlorate are used.: 405–406 
Proper oxidizer vaporization is important for the rocket to perform efficiently. Improper vaporization can lead to very large regression rate differences at the head end of the motor when compared to the aft end. One method is to use a hot gas generator to heat the oxidizer in a pre-combustion chamber. Another method is to use an oxidizer that can also be used as a monopropellant. A good example is hydrogen peroxide, which can be catalytically decomposed over a silver bed into hot oxygen and steam. A third method is to inject a propellant that is hypergolic with the oxidizer into the flow. Some of the oxidizer will decompose, heating up the rest of the oxidizer in the flow.: 406–407 
Generally, well designed and carefully constructed hybrids are very safe. The primary hazards associated with hybrids are:
Because the fuel in a hybrid does not contain an oxidizer, it will not combust explosively on its own. For this reason, hybrids are classified as having no TNT equivalent explosive power. In contrast, solid rockets often have TNT equivalencies similar in magnitude to the mass of the propellant grain. Liquid-fuel rockets typically have a TNT equivalence calculated based on the amount of fuel and oxidizer which could realistically intimately combine before igniting explosively; this is often taken to be 10–20% of the total propellant mass. For hybrids, even filling the combustion chamber with oxidizer prior to ignition will not generally create an explosion with the solid fuel, the explosive equivalence is often quoted as 0%.
In 1998 SpaceDev acquired all of the intellectual property, designs, and test results generated by over 200 hybrid rocket motor firings by the American Rocket Company over its eight-year life. SpaceShipOne, the first private manned spacecraft, was powered by SpaceDev's hybrid rocket motor burning HTPB with nitrous oxide. However, nitrous oxide was the prime substance responsible for the explosion that killed three in the development of the successor of SpaceShipOne at Scaled Composites in 2007. The Virgin Galactic SpaceShipTwo follow-on commercial suborbital spaceplane uses a scaled-up hybrid motor.
SpaceDev was developing the SpaceDev Streaker, an expendable small launch vehicle, and SpaceDev Dream Chaser, capable of both suborbital and orbital human space flight. Both Streaker and Dream Chaser use hybrid rocket motors that burn nitrous oxide and the synthetic HTPB rubber. SpaceDev was acquired by Sierra Nevada Corporation in 2009, becoming its Space Systems division, which continues to develop Dream Chaser for NASA's Commercial Crew Development contract. Sierra Nevada also developed RocketMotorTwo, the hybrid engine for SpaceShipTwo. On October 31, 2014, when SpaceShipTwo was lost, initial speculation had suggested that its hybrid engine had in fact exploded and killed one test pilot and seriously injured the other. However, investigation data now indicates an early deployment of the SpaceShip-Two feather system was the cause for aerodynamic breakup of the vehicle.
U.S. Rockets manufactured and deployed hybrids using self-pressurizing nitrous oxide (N2O) and hydroxyl-terminated polybutadiene (HTPB) as well as mixed High-test peroxide (HTP) and HTPB. The High-test peroxide (H2O2) 86% and (HTPB) and aluminum hybrids developed by U.S. Rockets produced a sea level delivered specific impulse (Isp) of 240, well above the typical 180 of N2O-HTPB hybrids. In addition to that, they were self-starting, restartable, had considerably lower combustion instability making them suitable for fragile or manned missions such as Bloodhound SSC, SpaceShipTwo or SpaceShipThree. The company had successfully tested and deployed both pressure fed and pump fed versions of the latter HTP-HTPB style. Deliverables to date have ranged from 6 inch to 18 inch diameter, and developed units up to 54 inch diameter. The vendor claimed scalability to over 5 meters diameter with regression rates approaching solids, according to literature distributed at the November 2013 Defense Advanced Research Projects Agency (DARPA) meeting for XS-1. U.S. Rockets is no longer manufacturing large-scale rockets.
Gilmour Space Technologies began testing Hybrid rocket engines in 2015 with both N2O and HP with HDPE and HDPE+wax blends. For 2016 testing includes a 5000 Lb HP/PE engine. The company is planning to use hybrids for both sounding and orbital rockets.
Orbital Technologies Corporation (Orbitec) has been involved in some U.S. government-funded research on hybrid rockets including the "Vortex Hybrid" concept.
Environmental Aeroscience Corporation (eAc) was incorporated in 1994 to develop hybrid rocket propulsion systems. It was included in the design competition for the SpaceShipOne motor but lost the contract to SpaceDev. Environmental Aeroscience Corporation still supplied parts to SpaceDev for the oxidizer fill, vent, and dump system.
Rocket Lab formerly sold hybrid sounding rockets and related technology.
The Reaction Research Society (RRS), although known primarily for their work with liquid rocket propulsion, has a long history of research and development with hybrid rocket propulsion.
Copenhagen Suborbitals, a Danish rocket group, has designed and test-fired several hybrids using N2O at first and currently LOX. Their fuel is epoxy, paraffin wax, or polyurethane. The group eventually moved away from hybrids because of thrust instabilities, and now uses a motor similar to that of the V-2 rocket.
TiSPACE is a Taiwanese company which is developing a family of hybrid-propellant rockets.
bluShift Aerospace in Brunswick, Maine, won a NASA SBIR grant to develop a modular hybrid rocket engine for its proprietary bio-derived fuel in June 2019. Having completed the grant bluShift has launched its first sounding rocket using the technology.
Vaya Space based out of Cocoa, Florida, is expected to launch its hybrid fuel rocket Dauntless in 2023.
Space Propulsion Group was founded in 1999 by Arif Karabeyoglu, Brian Cantwell, and others from Stanford University to develop high regression-rate liquefying hybrid rocket fuels. They have successfully fired motors as large as 12.5 in. diameter which produce 13,000 lbf. using the technology and are currently developing a 24 in. diameter, 25,000 lbf. motor to be initially fired in 2010. Stanford University is the institution where liquid-layer combustion theory for hybrid rockets was developed. The SPaSE group at Stanford is currently working with NASA Ames Research Center developing the Peregrine sounding rocket which will be capable of 100 km altitude. Engineering challenges include various types of combustion instabilities. Although the proposed motor was test fired in 2013, the Peregrine program eventually switched to a standard solid rocket for its 2016 debut.
The University of Tennessee Knoxville has carried out hybrid rocket research since 1999, working in collaboration with NASA Marshall Space Flight Center and private industry. This work has included the integration of a water-cooled calorimeter nozzle, one of the first 3D-printed, hot section components successfully used in a rocket motor. Other work at the university has focused on the use of  helical oxidizer injection, bio-derived fuels and powdered fuels encased in a 3D-printed, ABS matrix, including the successful launch of a coal-fired hybrid at the 2019 Spaceport America Cup.
At the Delft University of Technology, the student team Delft Aerospace Rocket Engineering (DARE) is very active in the design and building of hybrid rockets. In October 2015, DARE broke the European student altitude record with the Stratos II+ sounding rocket. Stratos II+ was propelled by the DHX-200 hybrid rocket engine, using a nitrous oxide oxidizer and fuel blend of paraffin, sorbitol and aluminium powder. On July 26, 2018 DARE attempted to launch the Stratos III hybrid rocket. This rocket used the same fuel/oxidizer combination as its predecessor, but with an increased impulse of around 360 kNs. At the time of development, this was the most powerful hybrid rocket engine ever developed by a student team in terms of total impulse. Unfortunately, the Stratos III vehicle was lost 20 seconds into the flight.
Florida Institute of Technology has successfully tested and evaluated hybrid technologies with their Panther Project. The WARR student-team at the Technical University of Munich has been developing hybrid engines and rockets since the early 1970s. Using acids, oxygen, or nitrous oxide in combination with polyethylene, or HTPB. The development includes test stand engines as well as airborne versions, like the first German hybrid rocket Barbarella. They are currently working on a hybrid rocket with Liquid oxygen as its oxidizer, to break the European height record of amateur rockets. They are also working with Rocket Crafters and testing their hybrid rockets.
Boston University's student-run "Rocket Propulsion Group", which in the past has launched only solid motor rockets, is attempting to design and build a single-stage hybrid sounding rocket to launch into sub-orbital space by July 2015.
Brigham Young University (BYU), the University of Utah, and Utah State University launched a student-designed rocket called Unity IV in 1995 which burned the solid fuel hydroxyl-terminated polybutadiene (HTPB) with an oxidizer of gaseous oxygen, and in 2003 launched a larger version which burned HTPB with nitrous oxide.
University of Brasilia's Hybrid Team has extensive research in paraffin wax / N2O hybrids having already made more than 50 tests fires. Hybrid Team is currently working liquefied propellant, numeric optimization and rocket design. Nowadays the rocket design team, called Capital Rocket Team, is developing high power hybrid rockets and researching about some additives. The Chemical Propulsion Laboratory has already made some researches and is developing the motor for SARA platform.
University of California, Los Angeles's student-run "University Rocket Project" launches hybrid propulsion rockets utilizing nitrous oxide as an oxidizer and HTPB as the fuel. They are currently in the development process of their third student-built hybrid rocket engine.
University of Toronto's student-run "University of Toronto Aerospace Team", designs and builds hybrid engine powered rockets. They are currently constructing a new engine testing facility at the University of Toronto Institute for Aerospace Studies, and are working towards breaking the Canadian amateur rocketry altitude record with their new rocket, Defiance MKIII, currently under rigorous testing. Defiance MK III's engine, QUASAR, is a Nitrous-Paraffin hybrid engine, capable of producing 7 kN of thrust for a period of 9 seconds.
In 2016, Pakistan's DHA Suffa University successfully developed Raheel-1, hybrid rocket engines in 1 kN class, using paraffin wax and liquid oxygen, thereby becoming the first university run rocket research program in the country. In India, Birla Institute of Technology, Mesra Space engineering and rocketry department has been working on Hybrid Projects with various fuels and oxidizers.
Pars Rocketry Group from Istanbul Technical University has designed and built the first hybrid rocket engine of Turkey, the rocket engine extensively tested in May 2015.
A United Kingdom-based team (laffin-gas) is using four N2O hybrid rockets in a drag-racing style car. Each rocket has an outer diameter of 150 mm and is 1.4 m long. They use a fuel grain of high-density wound paper soaked in cooking oil. The N2O supply is provided by Nitrogen-pressurised piston accumulators which provide a higher rate of delivery than N2O gas alone and also provide damping of any reverse shock.
In Italy one of the leading centers for research in hybrid propellants rockets is CISAS (Center of Studies and Activities for Space) "G. Colombo", University of Padua. The activities cover all stages of the development: from theoretical analysis of the combustion process to numerical simulation using CFD codes, and then by conducting ground tests of small scale and large-scale rockets (up to 20 kN, N2O-Paraffin wax based motors). One of these engines flew successfully in 2009. Since 2014, the research group is focused on the use of high test peroxide as oxidizer, in partnership with "Technology for Propulsion and Innovation", a university of Padua spin-off company.
In Taiwan, hybrid rocket system developments began in 2009 through R&amp;D projects of NSPO with two university teams. Both teams employed nitrous oxide / HTPB propellant system with different improvement schemes. Several hybrid rockets have been successfully launched by NCKU and NCTU teams so far, reaching altitudes of 10–20 km. Their plans include attempting 100–200 km altitude launch to test nanosatellites, and developing orbital launch capabilities for nanosatellites in the long run. A sub-scale N2O/PE dual-vortical-flow (DVF) hybrid engine hot-fire test in 2014 has delivered an averaged Isp of 280 sec, which indicates that the system has reached around 97% combustion efficiency.
In (Germany) the University of Stuttgart's Student team HyEnd is the current world record holder for the highest-flying student-built hybrid rocket with their HEROS rockets.
The Aerospace Team of the TU Graz, Austria, is also developing a hybrid-propellant rocket.
The Polish Student team PWr in Space at Wrocław University of Science and Technology has developed three hybrid rockets: R2 "Setka", R3 "Dziewięćdziesiątka dziewiątka" and the most powerful of all - R4 "Lynx" with a successful test at their test stand 
Many other universities, such as Embry-Riddle Aeronautical University, the University of Washington, Purdue University, the University of Michigan at Ann Arbor, the University of Arkansas at Little Rock, Hendrix College, the University of Illinois, Portland State University, University of KwaZulu-Natal, Texas A&amp;M University, Aarhus University, Rice University, and AGH University of Science and Technology have hybrid motor test stands that allow for student research with hybrid rockets.
There are a number of hybrid rocket motor systems available for amateur/hobbyist use in high-powered model rocketry. These include the popular HyperTek systems and a number of 'Urbanski-Colburn Valved' (U/C) systems such as RATTWorks, Contrail Rockets, and Propulsion Polymers.
All of these systems use nitrous oxide as the oxidizer and a plastic fuel (such as Polyvinyl chloride (PVC), Polypropylene), or a polymer-based fuel such as HTPB. This reduces the cost per flight compared to solid rocket motors, although there is generally more ground support equipment required with hybrids.
An October 26, 2005 episode of the television show MythBusters entitled "Confederate Rocket"  featured a hybrid rocket motor using liquid nitrous oxide and paraffin wax. The myth purported that during the American Civil War, the Confederate Army was able to construct a rocket of this type. The myth was revisited in a later episode entitled Salami Rocket, using hollowed out dry salami as the solid fuel.
In the February 18, 2007, episode of Top Gear, a Reliant Robin was used by Richard Hammond and James May in an attempt to modify a normal K-reg Robin into a reusable Space Shuttle. Steve Holland, a professional radio-controlled aircraft pilot, helped Hammond to work out how to land a Robin safely. The craft was built by senior members of the United Kingdom Rocketry Association (UKRA) and achieved a successful launch, flew for several seconds into the air and managed to successfully jettison the solid-fuel rocket boosters on time. This was the largest rocket launched by a non-government organisation in Europe. It used 6 × 40960 NS O motors by Contrail Rockets giving a maximum thrust of 8 tonnes.  However, the car failed to separate from the large external fuel tank due to faulty explosive bolts between the Robin and the external tank, and the Robin subsequently crashed into the ground and seemed to have exploded soon after. This explosion was added for dramatic effect as neither Reliant Robins nor hybrid rocket motors explode in the way depicted.

To ensure that all Wikipedia content is verifiable, Wikipedia provides a means for anyone to question an uncited claim. If your work has been tagged, please provide a reliable source for the statement, and discuss if needed.
You can add a citation by selecting from the drop-down  menu at the top of the editing box. In markup, you can add a citation manually using ref tags. There are also more elaborate ways to cite sources.
In wiki markup, you can question an uncited claim by inserting a simple {{Citation needed}} tag, or a more comprehensive {{Citation needed|reason=Your explanation here|date=July 2022}}. Alternatively, {{fact}} and {{cn}} will produce the same result. These all display as: 
Example: 87 percent of statistics are made up on the spot. For information on adding citations in articles, see Help:Referencing for beginners. For information on when to remove this template messages, see Help:Maintenance template removal.
A "citation needed" tag is a request for another editor to supply a source for the tagged fact: a form of communication between members of a collaborative editing community. It is never, in itself, an "improvement" of an article. Though readers may be alerted by a "citation needed" that a particular statement is not supported, and even doubted by some, many readers don't fully understand the community's processes. Not all tags get addressed in a timely manner, staying in place for months or years, forming an ever-growing Wikipedia backlog—this itself can be a problem. Best practice recommends the following: 
Before adding a tag, at least consider the following alternatives, one of which may prove much more constructive:
Currently, there are 460,976 articles with "Citation needed" statements. You can browse the whole list of these articles at Category:All articles with unsourced statements.
Frequently the authors of statements do not return to Wikipedia to support the statement with citations, so other Wikipedia editors have to do work checking those statements. With 460,976 statements that need WP:Verification, sometimes it's hard to choose which article to work on. The tool Citation Hunt makes that easier by suggesting random articles, which you can sort by topical category membership.
Gunpowder, also commonly known as black powder to distinguish it from modern smokeless powder, is the earliest known chemical explosive. It consists of a mixture of sulfur, carbon (in the form of charcoal) and potassium nitrate (saltpeter). The sulfur and carbon act as fuels while the saltpeter is an oxidizer. Gunpowder has been widely used as a propellant in firearms, artillery, rocketry, and pyrotechnics, including use as a blasting agent for explosives in quarrying, mining, and road building.
Gunpowder is classified as a low explosive because of its relatively slow decomposition rate and consequently low brisance. Low explosives deflagrate (i.e., burn at subsonic speeds), whereas high explosives detonate producing a supersonic shockwave. Ignition of gunpowder packed behind a projectile generates enough pressure to force the shot from the muzzle at high speed, but usually not enough force to rupture the gun barrel. It thus makes a good propellant, but is less suitable for shattering rock or fortifications with its low-yield explosive power. Nonetheless it was widely used to fill fused artillery shells (and used in mining and civil engineering projects) until the second half of the 19th century, when the first high explosives were put into use.
Its use in weapons has declined due to smokeless powder replacing it, and it is no longer used for industrial purposes due to its relative inefficiency compared to newer alternatives such as dynamite and ammonium nitrate/fuel oil.
A simple, commonly cited, chemical equation for the combustion of gunpowder is:
A balanced, but still simplified, equation is:
The exact percentages of ingredients varied greatly through the medieval period as the recipes were developed by trial and error, and needed to be updated for changing military technology.
Gunpowder does not burn as a single reaction, so the byproducts are not easily predicted. One study showed that it produced (in order of descending quantities) 55.91% solid products: potassium carbonate, potassium sulfate, potassium sulfide, sulfur, potassium nitrate, potassium thiocyanate, carbon, ammonium carbonate and 42.98% gaseous products: carbon dioxide, nitrogen, carbon monoxide, hydrogen sulfide, hydrogen, methane, 1.11% water.
Gunpowder made with less-expensive and more plentiful sodium nitrate instead of potassium nitrate (in appropriate proportions) works just as well. However, it is more hygroscopic than powders made from potassium nitrate. Muzzleloaders have been known to fire after hanging on a wall for decades in a loaded state, provided they remained dry. By contrast, gunpowder made with sodium nitrate must be kept sealed to remain stable.
Gunpowder releases 3 megajoules per kilogram and contains its own oxidant. This is less than TNT (4.7 megajoules per kilogram), or gasoline (47.2 megajoules per kilogram in combustion, but gasoline requires an oxidant; for instance, an optimized gasoline and O2 mixture releases 10.4 megajoules per kilogram, taking into account the mass of the oxygen).
Gunpowder also has a low energy density compared to modern "smokeless" powders, and thus to achieve high energy loadings, large amounts are needed with heavy projectiles.
Gunpowder is a low explosive: it does not detonate, but rather deflagrates (burns quickly). This is an advantage in a propellant device, where one does not desire a shock that would shatter the gun and potentially harm the operator; however, it is a drawback when an explosion is desired. In that case, the propellant (and most importantly, gases produced by its burning) must be confined. Since it contains its own oxidizer and additionally burns faster under pressure, its combustion is capable of bursting containers such as a shell, grenade, or improvised "pipe bomb" or "pressure cooker" casings to form shrapnel.
In quarrying, high explosives are generally preferred for shattering rock. However, because of its low brisance, gunpowder causes fewer fractures and results in more usable stone compared to other explosives, making it useful for blasting slate, which is fragile, or monumental stone such as granite and marble. Gunpowder is well suited for blank rounds, signal flares, burst charges, and rescue-line launches. It is also used in fireworks for lifting shells, in rockets as fuel, and in certain special effects.
Combustion converts less than half the mass of gunpowder to gas; most of it turns into particulate matter. Some of it is ejected, wasting propelling power, fouling the air, and generally being a nuisance (giving away a soldier's position, generating fog that hinders vision, etc.). Some of it ends up as a thick layer of soot inside the barrel, where it also is a nuisance for subsequent shots, and a cause of jamming an automatic weapon. Moreover, this residue is hygroscopic, and with the addition of moisture absorbed from the air forms a corrosive substance. The soot contains potassium oxide or sodium oxide that turns into potassium hydroxide, or sodium hydroxide, which corrodes wrought iron or steel gun barrels. Gunpowder arms therefore require thorough and regular cleaning to remove the residue.
The first confirmed reference to what can be considered gunpowder in China occurred in the 9th century AD during the Tang dynasty, first in a formula contained in the Taishang Shengzu Jindan Mijue (太上聖祖金丹秘訣) in 808, and then about 50 years later in a Taoist text known as the Zhenyuan miaodao yaolüe (真元妙道要略). The Taishang Shengzu Jindan Mijue mentions a formula composed of six parts sulfur to six parts saltpeter to one part birthwort herb. According to the Zhenyuan miaodao yaolüe, "Some have heated together sulfur, realgar and saltpeter with honey; smoke and flames result, so that their hands and faces have been burnt, and even the whole house where they were working burned down." Based on these Taoist texts, the invention of gunpowder by Chinese alchemists was likely an accidental byproduct from experiments seeking to create the elixir of life. This experimental medicine origin is reflected in its Chinese name huoyao (Chinese: 火药/火藥; pinyin: huǒ yào /xuo yɑʊ/), which means "fire medicine". Saltpeter was known to the Chinese by the mid-1st century AD and was primarily produced in the provinces of Sichuan, Shanxi, and Shandong. There is strong evidence of the use of saltpeter and sulfur in various medicinal combinations. A Chinese alchemical text dated 492 noted saltpeter burnt with a purple flame, providing a practical and reliable means of distinguishing it from other inorganic salts, thus enabling alchemists to evaluate and compare purification techniques; the earliest Latin accounts of saltpeter purification are dated after 1200.
The earliest chemical formula for gunpowder appeared in the 11th century Song dynasty text, Wujing Zongyao (Complete Essentials from the Military Classics), written by Zeng Gongliang between 1040 and 1044. The Wujing Zongyao provides encyclopedia references to a variety of mixtures that included petrochemicals—as well as garlic and honey. A slow match for flame throwing mechanisms using the siphon principle and for fireworks and rockets is mentioned. The mixture formulas in this book do not contain enough saltpeter to create an explosive however; being limited to at most 50% saltpeter, they produce an incendiary. The Essentials was written by a Song dynasty court bureaucrat and there is little evidence that it had any immediate impact on warfare; there is no mention of its use in the chronicles of the wars against the Tanguts in the 11th century, and China was otherwise mostly at peace during this century. However it had already been used for fire arrows since at least the 10th century. Its first recorded military application dates its use to the year 904 in the form of incendiary projectiles. In the following centuries various gunpowder weapons such as bombs, fire lances, and the gun appeared in China. Explosive weapons such as bombs have been discovered in a shipwreck off the shore of Japan dated from 1281, during the Mongol invasions of Japan.
By 1083 the Song court was producing hundreds of thousands of fire arrows for their garrisons. Bombs and the first proto-guns, known as "fire lances", became prominent during the 12th century and were used by the Song during the Jin-Song Wars. Fire lances were first recorded to have been used at the Siege of De'an in 1132 by Song forces against the Jin. In the early 13th century the Jin utilized iron-casing bombs. Projectiles were added to fire lances, and re-usable fire lance barrels were developed, first out of hardened paper, and then metal. By 1257 some fire lances were firing wads of bullets. In the late 13th century metal fire lances became 'eruptors', proto-cannons firing co-viative projectiles (mixed with the propellant, rather than seated over it with a wad), and by 1287 at the latest, had become true guns, the hand cannon.
According to Iqtidar Alam Khan, it was invading Mongols who introduced gunpowder to the Islamic world. The Muslims acquired knowledge of gunpowder some time between 1240 and 1280, by which point the Syrian Hasan al-Rammah had written recipes, instructions for the purification of saltpeter, and descriptions of gunpowder incendiaries. It is implied by al-Rammah's usage of "terms that suggested he derived his knowledge from Chinese sources" and his references to saltpeter as "Chinese snow" (Arabic: ثلج الصين thalj al-ṣīn), fireworks as "Chinese flowers", and rockets as "Chinese arrows" that knowledge of gunpowder arrived from China. However, because al-Rammah attributes his material to "his father and forefathers", al-Hassan argues that gunpowder became prevalent in Syria and Egypt by "the end of the twelfth century or the beginning of the thirteenth". In Persia saltpeter was known as "Chinese salt" (Persian: نمک چینی) namak-i chīnī) or "salt from Chinese salt marshes" (نمک شوره چینی namak-i shūra-yi chīnī).
Hasan al-Rammah included 107 gunpowder recipes in his text al-Furusiyyah wa al-Manasib al-Harbiyya (The Book of Military Horsemanship and Ingenious War Devices), 22 of which are for rockets. If one takes the median of 17 of these 22 compositions for rockets (75% nitrates, 9.06% sulfur, and 15.94% charcoal), it is nearly identical to the modern reported ideal recipe of 75% potassium nitrate, 10% sulfur, and 15% charcoal. The text also mentions fuses, incendiary bombs, naphtha pots, fire lances, and an illustration and description of the earliest torpedo. The torpedo was called the "egg which moves itself and burns". Two iron sheets were fastened together and tightened using felt. The flattened pear shaped vessel was filled with gunpowder, metal filings, "good mixtures", two rods, and a large rocket for propulsion. Judging by the illustration, it was evidently supposed to glide across the water. Fire lances were used in battles between the Muslims and Mongols in 1299 and 1303.
Al-Hassan claims that in the Battle of Ain Jalut of 1260, the Mamluks used against the Mongols, in "the first cannon in history", formula with near-identical ideal composition ratios for explosive gunpowder. Other historians urge caution regarding claims of Islamic firearms use in the 1204–1324 period as late medieval Arabic texts used the same word for gunpowder, naft, that they used for an earlier incendiary, naphtha.
The earliest surviving documentary evidence for cannons in the Islamic world is from an Arabic manuscript dated to the early 14th century. The author's name is uncertain but may have been Shams al-Din Muhammad, who died in 1350. Dating from around 1320-1350, the illustrations show gunpowder weapons such as gunpowder arrows, bombs, fire tubes, and fire lances or proto-guns. The manuscript describes a type of gunpowder weapon called a midfa which uses gunpowder to shoot projectiles out of a tube at the end of a stock. Some consider this to be a cannon while others do not. The problem with identifying cannons in early 14th century Arabic texts is the term midfa, which appears from 1342 to 1352 but cannot be proven to be true hand-guns or bombards. Contemporary accounts of a metal-barrel cannon in the Islamic world do not occur until 1365. Needham believes that in its original form the term midfa refers to the tube or cylinder of a naphtha projector (flamethrower), then after the invention of gunpowder it meant the tube of fire lances, and eventually it applied to the cylinder of hand-gun and cannon.
According to Paul E. J. Hammer, the Mamluks certainly used cannons by 1342. According to J. Lavin, cannons were used by Moors at the siege of Algeciras in 1343. A metal cannon firing an iron ball was described by Shihab al-Din Abu al-Abbas al-Qalqashandi between 1365-1376.
The musket appeared in the Ottoman Empire by 1465. In 1598, Chinese writer Zhao Shizhen described Turkish muskets as being superior to European muskets. The Chinese military book Wu Pei Chih (1621) later described Turkish muskets that used a rack-and-pinion mechanism, which was not known to have been used in European or Chinese firearms at the time.
The state-controlled manufacture of gunpowder by the Ottoman Empire through early supply chains to obtain nitre, sulfur and high-quality charcoal from oaks in Anatolia contributed significantly to its expansion between the 15th and 18th century. It was not until later in the 19th century when the syndicalist production of Turkish gunpowder was greatly reduced, which coincided with the decline of its military might.
Some sources mention possible gunpowder weapons being deployed by the Mongols against European forces at the Battle of Mohi in 1241. Professor Kenneth Warren Chase credits the Mongols for introducing into Europe gunpowder and its associated weaponry. However, there is no clear route of transmission, and while the Mongols are often pointed to as the likeliest vector, Timothy May points out that "there is no concrete evidence that the Mongols used gunpowder weapons on a regular basis outside of China." However, Timothy May also points out "However... the Mongols used the gunpowder weapon in their wars against the Jin, the Song and in their invasions of Japan."
The earliest Western accounts of gunpowder appears in texts written by English philosopher Roger Bacon in 1267 called Opus Majus and Opus Tertium. The oldest written recipes in continental Europe were recorded under the name Marcus Graecus or Mark the Greek between 1280 and 1300 in the Liber Ignium, or Book of Fires.
Records show that, in England, gunpowder was being made in 1346 at the Tower of London; a powder house existed at the Tower in 1461; and in 1515 three King's gunpowder makers worked there. Gunpowder was also being made or stored at other Royal castles, such as Portchester. The English Civil War (1642–1645) led to an expansion of the gunpowder industry, with the repeal of the Royal Patent in August 1641.
In late 14th century Europe, gunpowder was improved by corning, the practice of drying it into small clumps to improve combustion and consistency. During this time, European manufacturers also began regularly purifying saltpeter, using wood ashes containing potassium carbonate to precipitate calcium from their dung liquor, and using ox blood, alum, and slices of turnip to clarify the solution.
During the Renaissance, two European schools of pyrotechnic thought emerged, one in Italy and the other at Nuremberg, Germany. In Italy, Vannoccio Biringuccio, born in 1480, was a member of the guild Fraternita di Santa Barbara but broke with the tradition of secrecy by setting down everything he knew in a book titled De la pirotechnia, written in vernacular.  It was published posthumously in 1540, with 9 editions over 138 years, and also reprinted by MIT Press in 1966.
By the mid-17th century fireworks were used for entertainment on an unprecedented scale in Europe, being popular even at resorts and public gardens. With the publication of Deutliche Anweisung zur Feuerwerkerey (1748), methods for creating fireworks were sufficiently well-known and well-described that "Firework making has become an exact science." In 1774 Louis XVI ascended to the throne of France at age 20.  After he discovered that France was not self-sufficient in gunpowder, a Gunpowder Administration was established; to head it, the lawyer Antoine Lavoisier was appointed. Although from a bourgeois family, after his degree in law Lavoisier became wealthy from a company set up to collect taxes for the Crown; this allowed him to pursue experimental natural science as a hobby.
Without access to cheap saltpeter (controlled by the British), for hundreds of years France had relied on saltpetremen with royal warrants, the droit de fouille or "right to dig", to seize nitrous-containing soil and demolish walls of barnyards, without compensation to the owners. This caused farmers, the wealthy, or entire villages to bribe the petermen and the associated bureaucracy to leave their buildings alone and the saltpeter uncollected.  Lavoisier instituted a crash program to increase saltpeter production, revised (and later eliminated) the droit de fouille, researched best refining and powder manufacturing methods, instituted management and record-keeping, and established pricing that encouraged private investment in works.  Although saltpeter from new Prussian-style putrefaction works had not been produced yet (the process taking about 18 months), in only a year France had gunpowder to export. A chief beneficiary of this surplus was the American Revolution.  By careful testing and adjusting the proportions and grinding time, powder from mills such as at Essonne outside Paris became the best in the world by 1788, and inexpensive.
Two British physicists, Andrew Noble and Frederick Abel, worked to improve the properties of gunpowder during the late 19th century. This formed the basis for the Noble-Abel gas equation for internal ballistics.
The introduction of smokeless powder in the late 19th century led to a contraction of the gunpowder industry. After the end of World War I, the majority of the British gunpowder manufacturers merged into a single company, "Explosives Trades limited"; and a number of sites were closed down, including those in Ireland. This company became Nobel Industries Limited; and in 1926 became a founding member of Imperial Chemical Industries. The Home Office removed gunpowder from its list of Permitted Explosives; and shortly afterwards, on 31 December 1931, the former Curtis &amp; Harvey's Glynneath gunpowder factory at Pontneddfechan, in Wales, closed down, and it was demolished by fire in 1932. The last remaining gunpowder mill at the Royal Gunpowder Factory, Waltham Abbey was damaged by a German parachute mine in 1941 and it never reopened. This was followed by the closure of the gunpowder section at the Royal Ordnance Factory, ROF Chorley, the section was closed and demolished at the end of World War II; and ICI Nobel's Roslin gunpowder factory, which closed in 1954. This left ICI Nobel's Ardeer site in Scotland, which included a gunpowder factory, as the only factory in Great Britain producing gunpowder. The gunpowder area of the Ardeer site closed in October 1976.
Gunpowder and gunpowder weapons were transmitted to India through the Mongol invasions of India. The Mongols were defeated by Alauddin Khalji of the Delhi Sultanate, and some of the Mongol soldiers remained in northern India after their conversion to Islam. It was written in the Tarikh-i Firishta (1606–1607) that Nasiruddin Mahmud the ruler of the Delhi Sultanate presented the envoy of the Mongol ruler Hulegu Khan with a dazzling pyrotechnics display upon his arrival in Delhi in 1258. Nasiruddin Mahmud tried to express his strength as a ruler and tried to ward off any Mongol attempt similar to the Siege of Baghdad (1258). Firearms known as top-o-tufak also existed in many Muslim kingdoms in India by as early as 1366. From then on the employment of gunpowder warfare in India was prevalent, with events such as the "Siege of Belgaum" in 1473 by Sultan Muhammad Shah Bahmani.
The shipwrecked Ottoman Admiral Seydi Ali Reis is known to have introduced the earliest type of matchlock weapons, which the Ottomans used against the Portuguese during the Siege of Diu (1531). After that, a diverse variety of firearms, large guns in particular, became visible in Tanjore, Dacca, Bijapur, and Murshidabad. Guns made of bronze were recovered from Calicut (1504)- the former capital of the Zamorins
The Mughal emperor Akbar mass-produced matchlocks for the Mughal Army. Akbar is personally known to have shot a leading Rajput commander during the Siege of Chittorgarh. The Mughals began to use bamboo rockets (mainly for signalling) and employ sappers: special units that undermined heavy stone fortifications to plant gunpowder charges.
The Mughal Emperor Shah Jahan is known to have introduced much more advanced matchlocks, their designs were a combination of Ottoman and Mughal designs. Shah Jahan also countered the British and other Europeans in his province of Gujarāt, which supplied Europe saltpeter for use in gunpowder warfare during the 17th century. Bengal and Mālwa participated in saltpeter production. The Dutch, French, Portuguese, and English used Chhapra as a center of saltpeter refining.
Ever since the founding of the Sultanate of Mysore by Hyder Ali, French military officers were employed to train the Mysore Army. Hyder Ali and his son Tipu Sultan were the first to introduce modern cannons and muskets, their army was also the first in India to have official uniforms. During the Second Anglo-Mysore War Hyder Ali and his son Tipu Sultan unleashed the Mysorean rockets at their British opponents effectively defeating them on various occasions. The Mysorean rockets inspired the development of the Congreve rocket, which the British widely utilized during the Napoleonic Wars and the War of 1812.
Cannons were introduced to Majapahit when Kublai Khan's Chinese army under the leadership of Ike Mese sought to invade Java in 1293. History of Yuan mentioned that the Mongol used cannons (Chinese: Pao) against Daha forces.: 1–2 : 220  Cannons were used by the Ayutthaya Kingdom in 1352 during its invasion of the Khmer Empire. Within a decade large quantities of gunpowder could be found in the Khmer Empire. By the end of the century firearms were also used by the Trần dynasty.
Even though the knowledge of making gunpowder-based weapon has been known after the failed Mongol invasion of Java, and the predecessor of firearms, the pole gun (bedil tombak), was recorded as being used by Java in 1413,: 245  the knowledge of making "true" firearms came much later, after the middle of the 15th century. It was brought by the Islamic nations of West Asia, most probably the Arabs. The precise year of introduction is unknown, but it may be safely concluded to be no earlier than 1460.: 23  Before the arrival of the Portuguese in Southeast Asia, the natives already possessed primitive firearms, the Java arquebus. Portuguese influence to local weaponry, particularly after the capture of Malacca (1511), resulted in a new type of hybrid tradition matchlock firearm, the istinggar.
Portuguese and Spanish invaders were unpleasantly surprised and even outgunned on occasion. Circa 1540, the Javanese, always alert for new weapons found the newly arrived Portuguese weaponry superior to that of the locally made variants. Majapahit-era cetbang cannons were further improved and used in the Demak Sultanate period during the Demak invasion of Portuguese Malacca. During this period, the iron for manufacturing Javanese cannons was imported from Khorasan in northern Persia. The material was known by Javanese as wesi kurasani (Khorasan iron). When the Portuguese came to the archipelago, they referred to it as Berço, which was also used to refer to any breech-loading swivel gun, while the Spaniards call it Verso.: 151  By the early 16th century, the Javanese already locally-producing large guns, some of them still survived until the present day and dubbed as "sacred cannon" or "holy cannon". These cannons varied between 180-260-pounders, weighing anywhere between 3–8 tons, length of them between 3–6 m. Javanese bronze breech-loaded swivel-guns, known as cetbang, or erroneously as lantaka, was used widely by the Majapahit navy as well as by pirates and rival lords. Following the decline of the Majapahit, particularly after the paregreg civil war (1404–1406),: 174–175  the consequent decline in demand for gunpowder weapons caused many weapon makers and bronze-smiths to move to Brunei, Sumatra, Malaysia and the Philippines lead to widespread use, especially in the Makassar Strait. It led to near universal use of the swivel-gun and cannons in the Nusantara archipelago.
Saltpeter harvesting was recorded by Dutch and German travelers as being common in even the smallest villages and was collected from the decomposition process of large dung hills specifically piled for the purpose. The Dutch punishment for possession of non-permitted gunpowder appears to have been amputation. Ownership and manufacture of gunpowder was later prohibited by the colonial Dutch occupiers. According to colonel McKenzie quoted in Sir Thomas Stamford Raffles', The History of Java (1817), the purest sulfur was supplied from a crater from a mountain near the straits of Bali.
On the origins of gunpowder technology, historian Tonio Andrade remarked, "Scholars today overwhelmingly concur that the gun was invented in China." Gunpowder and the gun are widely believed by historians to have originated from China due to the large body of evidence that documents the evolution of gunpowder from a medicine to an incendiary and explosive, and the evolution of the gun from the fire lance to a metal gun, whereas similar records do not exist elsewhere. As Andrade explains, the large amount of variation in gunpowder recipes in China relative to Europe is "evidence of experimentation in China, where gunpowder was at first used as an incendiary and only later became an explosive and a propellant... in contrast, formulas in Europe diverged only very slightly from the ideal proportions for use as an explosive and a propellant, suggesting that gunpowder was introduced as a mature technology."
However, the history of gunpowder is not without controversy. A major problem confronting the study of early gunpowder history is ready access to sources close to the events described. Often the first records potentially describing use of gunpowder in warfare were written several centuries after the fact, and may well have been colored by the contemporary experiences of the chronicler. Translation difficulties have led to errors or loose interpretations bordering on artistic licence. Ambiguous language can make it difficult to distinguish gunpowder weapons from similar technologies that do not rely on gunpowder. A commonly cited example is a report of the Battle of Mohi in Eastern Europe that mentions a "long lance" sending forth "evil-smelling vapors and smoke", which has been variously interpreted by different historians as the "first-gas attack upon European soil" using gunpowder, "the first use of cannon in Europe", or merely a "toxic gas" with no evidence of gunpowder. It is difficult to accurately translate original Chinese alchemical texts, which tend to explain phenomena through metaphor, into modern scientific language with rigidly defined terminology in English.  Early texts potentially mentioning gunpowder are sometimes marked by a linguistic process where semantic change occurred. For instance, the Arabic word naft transitioned from denoting naphtha to denoting gunpowder, and the Chinese word pào changed in meaning from trebuchet to a cannon. This has led to arguments on the exact origins of gunpowder based on etymological foundations. Science and technology historian Bert S. Hall makes the observation that, "It goes without saying, however, that historians bent on special pleading, or simply with axes of their own to grind, can find rich material in these terminological thickets."
Another major area of contention in modern studies of the history of gunpowder is regarding the transmission of gunpowder. While the literary and archaeological evidence supports a Chinese origin for gunpowder and guns, the manner in which gunpowder technology was transferred from China to the West is still under debate. It is unknown why the rapid spread of gunpowder technology across Eurasia took place over several decades whereas other technologies such as paper, the compass, and printing did not reach Europe until centuries after they were invented in China.
Gunpowder is a granular mixture of:
Potassium nitrate is the most important ingredient in terms of both bulk and function because the combustion process releases oxygen from the potassium nitrate, promoting the rapid burning of the other ingredients. To reduce the likelihood of accidental ignition by static electricity, the granules of modern gunpowder are typically coated with graphite, which prevents the build-up of electrostatic charge.
Charcoal does not consist of pure carbon; rather, it consists of partially pyrolyzed cellulose, in which the wood is not completely decomposed. Carbon differs from ordinary charcoal. Whereas charcoal's autoignition temperature is relatively low, carbon's is much greater. Thus, a gunpowder composition containing pure carbon would burn similarly to a match head, at best.
The current standard composition for the gunpowder manufactured by pyrotechnicians was adopted as long ago as 1780. Proportions by weight are 75% potassium nitrate (known as saltpeter or saltpetre), 15% softwood charcoal, and 10% sulfur. These ratios have varied over the centuries and by country, and can be altered somewhat depending on the purpose of the powder. For instance, power grades of black powder, unsuitable for use in firearms but adequate for blasting rock in quarrying operations, are called blasting powder rather than gunpowder with standard proportions of 70% nitrate, 14% charcoal, and 16% sulfur; blasting powder may be made with the cheaper sodium nitrate substituted for potassium nitrate and proportions may be as low as 40% nitrate, 30% charcoal, and 30% sulfur. In 1857, Lammot du Pont solved the main problem of using cheaper sodium nitrate formulations when he patented DuPont "B" blasting powder. After manufacturing grains from press-cake in the usual way, his process tumbled the powder with graphite dust for 12 hours. This formed a graphite coating on each grain that reduced its ability to absorb moisture.
Neither the use of graphite nor sodium nitrate was new. Glossing gunpowder corns with graphite was already an accepted technique in 1839, and sodium nitrate-based blasting powder had been made in Peru for many years using the sodium nitrate mined at Tarapacá (now in Chile). Also, in 1846, two plants were built in south-west England to make blasting powder using this sodium nitrate. The idea may well have been brought from Peru by Cornish miners returning home after completing their contracts. Another suggestion is that it was William Lobb, the plant collector, who recognised the possibilities of sodium nitrate during his travels in South America. Lammot du Pont would have known about the use of graphite and probably also knew about the plants in south-west England. In his patent he was careful to state that his claim was for the combination of graphite with sodium nitrate-based powder, rather than for either of the two individual technologies.
French war powder in 1879 used the ratio 75% saltpeter, 12.5% charcoal, 12.5% sulfur. English war powder in 1879 used the ratio 75% saltpeter, 15% charcoal, 10% sulfur. The British Congreve rockets used 62.4% saltpeter, 23.2% charcoal and 14.4% sulfur, but the British Mark VII gunpowder was changed to 65% saltpeter, 20% charcoal and 15% sulfur. The explanation for the wide variety in formulation relates to usage. Powder used for rocketry can use a slower burn rate since it accelerates the projectile for a much longer time—whereas powders for weapons such as flintlocks, cap-locks, or matchlocks need a higher burn rate to accelerate the projectile in a much shorter distance. Cannons usually used lower burn-rate powders, because most would burst with higher burn-rate powders.
Besides black powder, there are other historically important types of gunpowder. "Brown gunpowder" is cited as composed of 79% nitre, 3% sulfur, and 18% charcoal per 100 of dry powder, with about 2% moisture. Prismatic Brown Powder is a large-grained product the Rottweil Company introduced in 1884 in Germany, which was adopted by the British Royal Navy shortly thereafter. The French navy adopted a fine, 3.1 millimeter, not prismatic grained product called Slow Burning Cocoa (SBC) or "cocoa powder". These brown powders reduced burning rate even further by using as little as 2 percent sulfur and using charcoal made from rye straw that had not been completely charred, hence the brown color.
Lesmok powder was a product developed by DuPont in 1911, one of several semi-smokeless products in the industry containing a mixture of black and nitrocellulose powder. It was sold to Winchester and others primarily for .22 and .32 small calibers. Its advantage was that it was believed at the time to be less corrosive than smokeless powders then in use.  It was not understood in the U.S. until the 1920s that the actual source of corrosion was the potassium chloride residue from potassium chlorate sensitized primers.  The bulkier black powder fouling better disperses primer residue.  Failure to mitigate primer corrosion by dispersion caused the false impression that nitrocellulose-based powder caused corrosion. Lesmok had some of the bulk of black powder for dispersing primer residue, but somewhat less total bulk than straight black powder, thus requiring less frequent bore cleaning. It was last sold by Winchester in 1947.
The development of smokeless powders, such as cordite, in the late 19th century created the need for a spark-sensitive priming charge, such as gunpowder. However, the sulfur content of traditional gunpowders caused corrosion problems with Cordite Mk I and this led to the introduction of a range of sulfur-free gunpowders, of varying grain sizes. They typically contain 70.5 parts of saltpeter and 29.5 parts of charcoal. Like black powder, they were produced in different grain sizes. In the United Kingdom, the finest grain was known as sulfur-free mealed powder (SMP). Coarser grains were numbered as sulfur-free gunpowder (SFG n): 'SFG 12', 'SFG 20', 'SFG 40' and 'SFG 90', for example; where the number represents the smallest BSS sieve mesh size, which retained no grains.
Sulfur's main role in gunpowder is to decrease the ignition temperature. A sample reaction for sulfur-free gunpowder would be:
The term black powder was coined in the late 19th century, primarily in the United States, to distinguish prior gunpowder formulations from the new smokeless powders and semi-smokeless powders. Semi-smokeless powders featured bulk volume properties that approximated black powder, but had significantly reduced amounts of smoke and combustion products. Smokeless powder has different burning properties (pressure vs. time) and can generate higher pressures and work per gram. This can rupture older weapons designed for black powder. Smokeless powders ranged in color from brownish tan to yellow to white. Most of the bulk semi-smokeless powders ceased to be manufactured in the 1920s.
The original dry-compounded powder used in 15th-century Europe was known as "Serpentine", either a reference to Satan or to a common artillery piece that used it. The ingredients were ground
together with a mortar and pestle, perhaps for 24 hours, resulting in a fine flour.  Vibration during transportation could cause the components to separate again, requiring remixing in the field. Also if the quality of the saltpeter was low (for instance if it was contaminated with highly hygroscopic calcium nitrate), or if the powder was simply old (due to the mildly hygroscopic nature of potassium nitrate), in humid weather it would need to be re-dried. The dust from "repairing" powder in the field was a major hazard.
Loading cannons or bombards before the powder-making advances of the Renaissance was a skilled art. Fine powder loaded haphazardly or too tightly would burn incompletely or too slowly. Typically, the breech-loading powder chamber in the rear of the piece was filled only about half full, the serpentine powder neither too compressed nor too loose, a wooden bung pounded in to seal the chamber from the barrel when assembled, and the projectile placed on.  A carefully determined empty space was necessary for the charge to burn effectively. When the cannon was fired through the touchhole, turbulence from the initial surface combustion caused the rest of the powder to be rapidly exposed to the flame.
The advent of much more powerful and easy to use corned powder changed this procedure, but serpentine was used with older guns into the 17th century.
For propellants to oxidize and burn rapidly and effectively, the combustible ingredients must be reduced to the smallest possible particle sizes, and be as thoroughly mixed as possible. Once mixed, however, for better results in a gun, makers discovered that the final product should be in the form of individual dense grains that spread the fire quickly from grain to grain, much as straw or twigs catch fire more quickly than a pile of sawdust.
In late 14th century Europe and China, gunpowder was improved by wet grinding; liquid, such as distilled spirits was added during the grinding-together of the ingredients and the moist paste dried afterwards. The principle of wet mixing to prevent the separation of dry ingredients, invented for gunpowder, is used today in the pharmaceutical industry. It was discovered that if the paste was rolled into balls before drying the resulting gunpowder absorbed less water from the air during storage and traveled better. The balls were then crushed in a mortar by the gunner immediately before use, with the old problem of uneven particle size and packing causing unpredictable results. If the right size particles were chosen, however, the result was a great improvement in power.  Forming the damp paste into corn-sized clumps by hand or with the use of a sieve instead of larger balls produced a product after drying that loaded much better, as each tiny piece provided its own surrounding air space that allowed much more rapid combustion than a fine powder.  This "corned" gunpowder was from 30% to 300% more powerful. An example is cited where 15 kilograms (34 lb) of serpentine was needed to shoot a 21-kilogram (47 lb) ball, but only 8.2 kilograms (18 lb) of corned powder.
Because the dry powdered ingredients must be mixed and bonded together for extrusion and cut into grains to maintain the blend, size reduction and mixing is done while the ingredients are damp, usually with water. After 1800, instead of forming grains by hand or with sieves, the damp mill-cake was pressed in molds to increase its density and extract the liquid, forming press-cake. The pressing took varying amounts of time, depending on conditions such as atmospheric humidity.  The hard, dense product was broken again into tiny pieces, which were separated with sieves to produce a uniform product for each purpose: coarse powders for cannons, finer grained powders for muskets, and the finest for small hand guns and priming. Inappropriately fine-grained powder often caused cannons to burst before the projectile could move down the barrel, due to the high initial spike in pressure. Mammoth powder with large grains, made for Rodman's 15-inch cannon, reduced the pressure to only 20 percent as high as ordinary cannon powder would have produced.
In the mid-19th century, measurements were made determining that the burning rate within a grain of black powder (or a tightly packed mass) is about 6 cm/s (0.20 feet/s), while the rate of ignition propagation from grain to grain is around 9 m/s (30 feet/s), over two orders of magnitude faster.
Modern corning first compresses the fine black powder meal into blocks with a fixed density (1.7 g/cm3). In the United States, gunpowder grains were designated F (for fine) or C (for coarse). Grain diameter decreased with a larger number of Fs and increased with a larger number of Cs, ranging from about .mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}2 mm (1⁄16 in) for 7F to 15 mm (9⁄16 in) for 7C. Even larger grains were produced for artillery bore diameters greater than about 17 cm (6.7 in). The standard DuPont Mammoth powder developed by Thomas Rodman and Lammot du Pont for use during the American Civil War had grains averaging 15 mm (0.6 in) in diameter with edges rounded in a glazing barrel. Other versions had grains the size of golf and tennis balls for use in 20-inch (51 cm) Rodman guns. In 1875 DuPont introduced Hexagonal powder for large artillery, which was pressed using shaped plates with a small center core—about 38 mm (1+1⁄2 in) diameter, like a wagon wheel nut, the center hole widened as the grain burned. By 1882 German makers also produced hexagonal grained powders of a similar size for artillery.
By the late 19th century manufacturing focused on standard grades of black powder from Fg used in large bore rifles and shotguns, through FFg (medium and small-bore arms such as muskets and fusils), FFFg (small-bore rifles and pistols), and FFFFg (extreme small bore, short pistols and most commonly for priming flintlocks). A coarser grade for use in military artillery blanks was designated A-1. These grades were sorted on a system of screens with oversize retained on a mesh of 6 wires per inch, A-1 retained on 10 wires per inch, Fg retained on 14, FFg on 24, FFFg on 46, and FFFFg on 60. Fines designated FFFFFg were usually reprocessed to minimize explosive dust hazards. In the United Kingdom, the main service gunpowders were classified RFG (rifle grained fine) with diameter of one or two millimeters and RLG (rifle grained large) for grain diameters between two and six millimeters. Gunpowder grains can alternatively be categorized by mesh size: the BSS sieve mesh size, being the smallest mesh size, which retains no grains. Recognized grain sizes are Gunpowder G 7, G 20, G 40, and G 90.
Owing to the large market of antique and replica black-powder firearms in the US, modern  black powder substitutes like Pyrodex, Triple Seven and Black Mag3 pellets have been developed since the 1970s. These products, which should not be confused with smokeless powders, aim to produce less fouling (solid residue), while maintaining the traditional volumetric measurement system for charges. Claims of less corrosiveness of these products have been controversial however. New cleaning products for black-powder guns have also been developed for this market.
For the most powerful black powder, meal powder, a wood charcoal, is used. The best wood for the purpose is Pacific willow, but others such as alder or buckthorn can be used. In Great Britain between the 15th and 19th centuries charcoal from alder buckthorn was greatly prized for gunpowder manufacture; cottonwood was used by the American Confederate States. The ingredients are reduced in particle size and mixed as intimately as possible. Originally, this was with a mortar-and-pestle or a similarly operating stamping-mill, using copper, bronze or other non-sparking materials, until supplanted by the rotating ball mill principle with non-sparking bronze or lead. Historically, a marble or limestone edge runner mill, running on a limestone bed, was used in Great Britain; however, by the mid 19th century this had changed to either an iron-shod stone wheel or a cast iron wheel running on an iron bed. The mix was dampened with alcohol or water during grinding to prevent accidental ignition. This also helps the extremely soluble saltpeter to mix into the microscopic pores of the very high surface-area charcoal.
Around the late 14th century, European powdermakers first began adding liquid during grinding to improve mixing, reduce dust, and with it the risk of explosion. The powder-makers would then shape the resulting paste of dampened gunpowder, known as mill cake, into corns, or grains, to dry. Not only did corned powder keep better because of its reduced surface area, gunners also found that it was more powerful and easier to load into guns. Before long, powder-makers standardized the process by forcing mill cake through sieves instead of corning powder by hand.
The improvement was based on reducing the surface area of a higher density composition. At the beginning of the 19th century, makers increased density further by static pressing. They shoveled damp mill cake into a two-foot square box, placed this beneath a screw press and reduced it to half its volume. "Press cake" had the hardness of slate. They broke the dried slabs with hammers or rollers, and sorted the granules with sieves into different grades.  In the United States, Eleuthere Irenee du Pont, who had learned the trade from Lavoisier, tumbled the dried grains in rotating barrels to round the edges and increase durability during shipping and handling. (Sharp grains rounded off in transport, producing fine "meal dust" that changed the burning properties.)
Another advance was the manufacture of kiln charcoal by distilling wood in heated iron retorts instead of burning it in earthen pits. Controlling the temperature influenced the power and consistency of the finished gunpowder.  In 1863, in response to high prices for Indian saltpeter, DuPont chemists developed a process using potash or mined potassium chloride to convert plentiful Chilean sodium nitrate to potassium nitrate.
The following year (1864) the Gatebeck Low Gunpowder Works in Cumbria (Great Britain) started a plant to manufacture potassium nitrate by essentially the same chemical process. This is nowadays called the 'Wakefield Process', after the owners of the company. It would have used potassium chloride from the Staßfurt mines, near Magdeburg, Germany, which had recently become available in industrial quantities.
During the 18th century, gunpowder factories became increasingly dependent on mechanical energy. Despite mechanization, production difficulties related to humidity control, especially during the pressing, were still present in the late 19th century. A paper from 1885 laments that "Gunpowder is such a nervous and sensitive spirit, that in almost every process of manufacture it changes under our hands as the weather changes." Pressing times to the desired density could vary by a factor of three depending on the atmospheric humidity.
The United Nations Model Regulations on the Transportation of Dangerous Goods and national transportation authorities, such as United States Department of Transportation, have classified gunpowder (black powder) as a Group A: Primary explosive substance for shipment because it ignites so easily. Complete manufactured devices containing black powder are usually classified as Group D: Secondary detonating substance, or black powder, or article containing secondary detonating substance, such as firework, class D model rocket engine, etc., for shipment because they are harder to ignite than loose powder. As explosives, they all fall into the category of Class 1 .
Besides its use as a propellant in firearms and artillery, black powder's other main use has been as a blasting powder in quarrying, mining, and road construction (including railroad construction). During the 19th century, outside of war emergencies such as the Crimean War or the American Civil War, more black powder was used in these industrial uses than in firearms and artillery. Dynamite gradually replaced it for those uses. Today, industrial explosives for such uses are still a huge market, but most of the market is in newer explosives rather than black powder.
Beginning in the 1930s, gunpowder or smokeless powder was used in rivet guns, stun guns for animals, cable splicers and other industrial construction tools. The "stud gun", a powder-actuated tool, drove nails or screws into solid concrete, a function not possible with hydraulic tools, and today is still an important part of various industries, but the cartridges usually use smokeless powders. Industrial shotguns have been used to eliminate persistent material rings in operating rotary kilns (such as those for cement, lime, phosphate, etc.) and clinker in operating furnaces, and commercial tools make the method more reliable.
Gunpowder has occasionally been employed for other purposes besides weapons, mining, fireworks and construction:

The Song dynasty (; Chinese: 宋朝; pinyin: Sòng cháo; 960–1279) was an imperial dynasty of China that began in 960 and lasted until 1279. The dynasty was founded by Emperor Taizu of Song following his usurpation of the throne of the Later Zhou, ending the Five Dynasties and Ten Kingdoms period. The Song often came into conflict with the contemporaneous Liao, Western Xia and Jin dynasties in northern China. After retreating to southern China, the Song was eventually conquered by the Mongol-led Yuan dynasty.
The dynasty is divided into two periods: Northern Song and Southern Song. During the Northern Song (Chinese: 北宋; 960–1127), the capital was in the northern city of Bianjing (now Kaifeng) and the dynasty controlled most of what is now Eastern China. The Southern Song (Chinese: 南宋; 1127–1279) refers to the period after the Song lost control of its northern half to the Jurchen-led Jin dynasty in the Jin–Song Wars. At that time, the Song court retreated south of the Yangtze and established its capital at Lin'an (now Hangzhou). Although the Song dynasty had lost control of the traditional Chinese heartlands around the Yellow River, the Southern Song Empire contained a large population and productive agricultural land, sustaining a robust economy. In 1234, the Jin dynasty was conquered by the Mongols, who took control of northern China, maintaining uneasy relations with the Southern Song. Möngke Khan, the fourth Great Khan of the Mongol Empire, died in 1259 while besieging the mountain castle Diaoyucheng, Chongqing. His younger brother Kublai Khan was proclaimed the new Great Khan and in 1271 founded the Yuan dynasty. After two decades of sporadic warfare, Kublai Khan's armies conquered the Song dynasty in 1279 after defeating the Southern Song in the Battle of Yamen, and reunited China under the Yuan dynasty.
Technology, science, philosophy, mathematics, and engineering flourished during the Song era. The Song dynasty was the first in world history to issue banknotes or true paper money and the first Chinese government to establish a permanent standing navy. This dynasty saw the first recorded chemical formula of gunpowder, the invention of gunpowder weapons such as fire arrows, bombs, and the fire lance. It also saw the first discernment of true north using a compass, first recorded description of the pound lock, and improved designs of astronomical clocks. Economically, the Song dynasty was unparalleled with a gross domestic product three times larger than that of Europe during the 12th century. China's population doubled in size between the 10th and 11th centuries. This growth was made possible by expanded rice cultivation, use of early-ripening rice from Southeast and South Asia, and production of widespread food surpluses. The Northern Song census recorded 20 million households, double of the Han and Tang dynasties. It is estimated that the Northern Song had a population of 90 million people, and 200 million by the time of the Ming dynasty. This dramatic increase of population fomented an economic revolution in pre-modern China.
The expansion of the population, growth of cities, and emergence of a national economy led to the gradual withdrawal of the central government from direct involvement in economic affairs. The lower gentry assumed a larger role in local administration and affairs. Social life during the Song was vibrant. Citizens gathered to view and trade precious artworks, the populace intermingled at public festivals and private clubs, and cities had lively entertainment quarters. The spread of literature and knowledge was enhanced by the rapid expansion of woodblock printing and the 11th-century invention of movable-type printing. Philosophers such as Cheng Yi and Zhu Xi reinvigorated Confucianism with new commentary, infused with Buddhist ideals, and emphasized a new organization of classic texts that established the doctrine of Neo-Confucianism. Although civil service examinations had existed since the Sui dynasty, they became much more prominent in the Song period. Officials gaining power through imperial examination led to a shift from a military-aristocratic elite to a scholar-bureaucratic elite.
After usurping the throne of the Later Zhou dynasty, Emperor Taizu of Song (r.  960–976) spent sixteen years conquering the rest of China, reuniting much of the territory that had once belonged to the Han and Tang empires and ending the upheaval of the Five Dynasties and Ten Kingdoms period. In Kaifeng, he established a strong central government over the empire. The establishment of this capital marked the start of the Northern Song period. He ensured administrative stability by promoting the civil service examination system of drafting state bureaucrats by skill and merit (instead of aristocratic or military position) and promoted projects that ensured efficiency in communication throughout the empire. In one such project, cartographers created detailed maps of each province and city that were then collected in a large atlas. Emperor Taizu also promoted groundbreaking scientific and technological innovations by supporting works like the astronomical clock tower designed and built by the engineer Zhang Sixun.
The Song court maintained diplomatic relations with Chola India, the Fatimid Caliphate of Egypt, Srivijaya, the Kara-Khanid Khanate in Central Asia, the Goryeo kingdom in Korea, and other countries that were also trade partners with Japan. Chinese records even mention an embassy from the ruler of "Fu lin" (拂菻, i.e. the Byzantine Empire), Michael VII Doukas, and its arrival in 1081. However, China's closest neighbouring states had the greatest impact on its domestic and foreign policy. From its inception under Taizu, the Song dynasty alternated between warfare and diplomacy with the ethnic Khitans of the Liao dynasty in the northeast and with the Tanguts of the Western Xia in the northwest. The Song dynasty used military force in an attempt to quell the Liao dynasty and to recapture the Sixteen Prefectures, a territory under Khitan control since 938 that was traditionally considered to be part of China proper (Most parts of today's Beijing and Tianjin). Song forces were repulsed by the Liao forces, who engaged in aggressive yearly campaigns into Northern Song territory until 1005, when the signing of the Shanyuan Treaty ended these northern border clashes. The Song were forced to provide tribute to the Khitans, although this did little damage to the Song economy since the Khitans were economically dependent upon importing massive amounts of goods from the Song. More significantly, the Song state recognized the Liao state as its diplomatic equal. The Song created an extensive defensive forest along the Song-Liao border to thwart potential Khitan cavalry attacks.
The Song dynasty managed to win several military victories over the Tanguts in the early 11th century, culminating in a campaign led by the polymath scientist, general, and statesman Shen Kuo (1031–1095). However, this campaign was ultimately a failure due to a rival military officer of Shen disobeying direct orders, and the territory gained from the Western Xia was eventually lost. The Song fought against the Vietnamese kingdom of Đại Việt twice, the first conflict in 981 and later a significant war from 1075 to 1077 over a border dispute and the Song's severing of commercial relations with Đại Việt. After the Vietnamese forces inflicted heavy damages in a raid on Guangxi, the Song commander Guo Kui (1022–1088) penetrated as far as Thăng Long (modern Hanoi). Heavy losses on both sides prompted the Vietnamese commander Thường Kiệt (1019–1105) to make peace overtures, allowing both sides to withdraw from the war effort; captured territories held by both Song and Vietnamese were mutually exchanged in 1082, along with prisoners of war.
During the 11th century, political rivalries divided members of the court due to the ministers' differing approaches, opinions, and policies regarding the handling of the Song's complex society and thriving economy. The idealist Chancellor, Fan Zhongyan (989–1052), was the first to receive a heated political backlash when he attempted to institute the Qingli Reforms, which included measures such as improving the recruitment system of officials, increasing the salaries for minor officials, and establishing sponsorship programs to allow a wider range of people to be well educated and eligible for state service.
After Fan was forced to step down from his office, Wang Anshi (1021–1086) became Chancellor of the imperial court. With the backing of Emperor Shenzong (1067–1085), Wang Anshi severely criticized the educational system and state bureaucracy. Seeking to resolve what he saw as state corruption and negligence, Wang implemented a series of reforms called the New Policies. These involved land value tax reform, the establishment of several government monopolies, the support of local militias, and the creation of higher standards for the Imperial examination to make it more practical for men skilled in statecraft to pass.
The reforms created political factions in the court. Wang Anshi's "New Policies Group" (Xin Fa), also known as the "Reformers", were opposed by the ministers in the "Conservative" faction led by the historian and Chancellor Sima Guang (1019–1086). As one faction supplanted another in the majority position of the court ministers, it would demote rival officials and exile them to govern remote frontier regions of the empire. One of the prominent victims of the political rivalry, the famous poet and statesman Su Shi (1037–1101), was jailed and eventually exiled for criticizing Wang's reforms.
The continual alternation between reform and conservatism had effectively weakened the dynasty. This decline can also be attributed to Cai Jing (1047–1126), who was appointed by Emperor Zhezong (1085–1100) and who remained in power until 1125. He revived the New Policies and pursued political opponents, tolerated corruption and encouraged Emperor Huizong (1100–1126) to neglect his duties to focus on artistic pursuits. Later, a peasant rebellion broke out in Zhejiang and Fujian, headed by Fang La in 1120. The rebellion may have been caused by an increasing tax burden, the concentration of landownership and oppressive government measures.
While the central Song court remained politically divided and focused upon its internal affairs, alarming new events to the north in the Liao state finally came to its attention. The Jurchen, a subject tribe of the Liao, rebelled against them and formed their own state, the Jin dynasty (1115–1234). The Song official Tong Guan (1054–1126) advised Emperor Huizong to form an alliance with the Jurchens, and the joint military campaign under this Alliance Conducted at Sea toppled and completely conquered the Liao dynasty by 1125. During the joint attack, the Song's northern expedition army removed the defensive forest along the Song-Liao border.
However, the poor performance and military weakness of the Song army was observed by the Jurchens, who immediately broke the alliance, beginning the Jin–Song Wars of 1125 and 1127. Because of the removal of the previous defensive forest, the Jin army marched quickly across the North China Plain to Kaifeng. In the Jingkang Incident during the latter invasion, the Jurchens captured not only the capital, but the retired Emperor Huizong, his successor Emperor Qinzong, and most of the Imperial court.
The remaining Song forces regrouped under the self-proclaimed Emperor Gaozong of Song (1127–1162) and withdrew south of the Yangtze to establish a new capital at Lin'an (modern Hangzhou). The Jurchen conquest of North China and shift of capitals from Kaifeng to Lin'an was the dividing line between the Northern and Southern Song dynasties.
After their fall to the Jin, the Song lost control of North China. Now occupying what has been traditionally known as "China Proper," the Jin regarded themselves the rightful rulers of China. The Jin later chose earth as their dynastic element and yellow as their royal color. According to the theory of the Five Elements (wuxing), the earth element follows the fire, the dynastic element of the Song, in the sequence of elemental creation. Therefore, their ideological move showed that the Jin considered Song reign in China complete, with the Jin replacing the Song as the rightful rulers of China Proper.
Although weakened and pushed south beyond the Huai River, the Southern Song found new ways to bolster its strong economy and defend itself against the Jin dynasty. It had able military officers such as Yue Fei and Han Shizhong. The government sponsored massive shipbuilding and harbor improvement projects, and the construction of beacons and seaport warehouses to support maritime trade abroad, including at the major international seaports, such as Quanzhou, Guangzhou, and Xiamen, that were sustaining China's commerce.
To protect and support the multitude of ships sailing for maritime interests into the waters of the East China Sea and Yellow Sea (to Korea and Japan), Southeast Asia, the Indian Ocean, and the Red Sea, it was necessary to establish an official standing navy. The Song dynasty therefore established China's first permanent navy in 1132, with a headquarters at Dinghai. With a permanent navy, the Song were prepared to face the naval forces of the Jin on the Yangtze River in 1161, in the Battle of Tangdao and the Battle of Caishi. During these battles the Song navy employed swift paddle wheel driven naval vessels armed with traction trebuchet catapults aboard the decks that launched gunpowder bombs. Although the Jin forces commanded by Wanyan Liang (the Prince of Hailing) boasted 70,000 men on 600 warships, and the Song forces only 3,000 men on 120 warships, the Song dynasty forces were victorious in both battles due to the destructive power of the bombs and the rapid assaults by paddlewheel ships. The strength of the navy was heavily emphasized after that. A century after the navy was founded it had grown in size to 52,000 fighting marines.
The Song government confiscated portions of land owned by the landed gentry in order to raise revenue for these projects, an act which caused dissension and loss of loyalty amongst leading members of Song society but did not stop the Song's defensive preparations. Financial matters were made worse by the fact that many wealthy, land-owning families—some of which had officials working for the government—used their social connections with those in office in order to obtain tax-exempt status.
Although the Song dynasty was able to hold back the Jin, a new foe came to power over the steppe, deserts, and plains north of the Jin dynasty. The Mongols, led by Genghis Khan (r. 1206–1227), initially invaded the Jin dynasty in 1205 and 1209, engaging in large raids across its borders, and in 1211 an enormous Mongol army was assembled to invade the Jin. The Jin dynasty was forced to submit and pay tribute to the Mongols as vassals; when the Jin suddenly moved their capital city from Beijing to Kaifeng, the Mongols saw this as a revolt. Under the leadership of Ögedei Khan (r.1229–1241), both the Jin dynasty and Western Xia dynasty were conquered by Mongol forces in 1233/34.
The Mongols were allied with the Song, but this alliance was broken when the Song recaptured the former imperial capitals of Kaifeng, Luoyang, and Chang'an at the collapse of the Jin dynasty. After the first Mongol invasion of Vietnam in 1258, Mongol general Uriyangkhadai attacked Guangxi from Hanoi as part of a coordinated Mongol attack in 1259 with armies attacking in Sichuan under Mongol leader Möngke Khan and other Mongol armies attacking in modern-day Shandong and Henan. On August 11, 1259, Möngke Khan died during the siege of Diaoyu Castle in Chongqing.
Möngke's death and the ensuing succession crisis prompted Hulagu Khan to pull the bulk of the Mongol forces out of the Middle East where they were poised to fight the Egyptian Mamluks (who defeated the remaining Mongols at Ain Jalut). Although Hulagu was allied with Kublai Khan, his forces were unable to help in the assault against the Song, due to Hulagu's war with the Golden Horde.
Kublai continued the assault against the Song, gaining a temporary foothold on the southern banks of the Yangtze. By the winter of 1259, Uriyangkhadai's army fought its way north to meet Kublai Khan's army, which was besieging Ezhou in Hubei. Kublai made preparations to take Ezhou, but a pending civil war with his brother Ariq Böke—a rival claimant to the Mongol Khaganate—forced Kublai to move back north with the bulk of his forces. In Kublai's absence, the Song forces were ordered by Chancellor Jia Sidao to make an immediate assault and succeeded in pushing the Mongol forces back to the northern banks of the Yangtze. There were minor border skirmishes until 1265, when Kublai won a significant battle in Sichuan.
From 1268 to 1273, Kublai blockaded the Yangtze River with his navy and besieged Xiangyang, the last obstacle in his way to invading the rich Yangtze River basin. Kublai officially declared the creation of the Yuan dynasty in 1271. In 1275, a Song force of 130,000 troops under Chancellor Jia Sidao was defeated by Kublai's newly appointed commander-in-chief, general Bayan. By 1276, most of the Song territory had been captured by Yuan forces, including the capital Lin'an.
In the Battle of Yamen on the Pearl River Delta in 1279, the Yuan army, led by the general Zhang Hongfan, finally crushed the Song resistance. The last remaining ruler, the 13-year-old emperor Zhao Bing, committed suicide, along with Prime Minister Lu Xiufu and 1300 members of the royal clan. On Kublai's orders, carried out by his commander Bayan, the rest of the former imperial family of Song were unharmed; the deposed Emperor Gong was demoted, being given the title 'Duke of Ying', but was eventually exiled to Tibet where he took up a monastic life. The former emperor would eventually be forced to commit suicide under the orders of Kublai's great-great grandson, Gegeen Khan, out of fear that Emperor Gong would stage a coup to restore his reign. Other members of the Song Imperial Family continued to live in the Yuan dynasty, including Zhao Mengfu and Zhao Yong.
The Song dynasty was an era of administrative sophistication and complex social organization. Some of the largest cities in the world were found in China during this period (Kaifeng and Hangzhou had populations of over a million). People enjoyed various social clubs and entertainment in the cities, and there were many schools and temples to provide the people with education and religious services. The Song government supported social welfare programs including the establishment of retirement homes, public clinics, and paupers' graveyards. The Song dynasty supported a widespread postal service that was modeled on the earlier Han dynasty (202 BCE – CE 220) postal system to provide swift communication throughout the empire. The central government employed thousands of postal workers of various ranks to provide service for post offices and larger postal stations. In rural areas, farming peasants either owned their own plots of land, paid rents as tenant farmers, or were serfs on large estates.
Although women were on a lower social tier than men (according to Confucian ethics), they enjoyed many social and legal privileges and wielded considerable power at home and in their own small businesses. As Song society became more and more prosperous and parents on the bride's side of the family provided larger dowries for her marriage, women naturally gained many new legal rights in ownership of property. Under certain circumstances, an unmarried daughter without brothers, or a surviving mother without sons, could inherit one-half of her father's share of undivided family property. There were many notable and well-educated women, and it was a common practice for women to educate their sons during their earliest youth. The mother of the scientist, general, diplomat, and statesman Shen Kuo taught him essentials of military strategy. There were also exceptional women writers and poets, such as Li Qingzhao (1084–1151), who became famous even in her lifetime.
Religion in China during this period had a great effect on people's lives, beliefs, and daily activities, and Chinese literature on spirituality was popular. The major deities of Daoism and Buddhism, ancestral spirits, and the many deities of Chinese folk religion were worshipped with sacrificial offerings. Tansen Sen asserts that more Buddhist monks from India traveled to China during the Song than in the previous Tang dynasty (618–907). With many ethnic foreigners travelling to China to conduct trade or live permanently, there came many foreign religions; religious minorities in China included Middle Eastern Muslims, the Kaifeng Jews, and Persian Manichaeans.
The populace engaged in a vibrant social and domestic life, enjoying such public festivals as the Lantern Festival and the Qingming Festival. There were entertainment quarters in the cities providing a constant array of amusements. There were puppeteers, acrobats, theatre actors, sword swallowers, snake charmers, storytellers, singers and musicians, prostitutes, and places to relax, including tea houses, restaurants, and organized banquets. People attended social clubs in large numbers; there were tea clubs, exotic food clubs, antiquarian and art collectors' clubs, horse-loving clubs, poetry clubs, and music clubs. Like regional cooking and cuisines in the Song, the era was known for its regional varieties of performing arts styles as well. Theatrical drama was very popular amongst the elite and general populace, although Classical Chinese—not the vernacular language—was spoken by actors on stage. The four largest drama theatres in Kaifeng could hold audiences of several thousand each. There were also notable domestic pastimes, as people at home enjoyed activities such as the go and xiangqi board games.
During this period greater emphasis was laid upon the civil service system of recruiting officials; this was based upon degrees acquired through competitive examinations, in an effort to select the most capable individuals for governance. Selecting men for office through proven merit was an ancient idea in China. The civil service system became institutionalized on a small scale during the Sui and Tang dynasties, but by the Song period, it became virtually the only means for drafting officials into the government. The advent of widespread printing helped to widely circulate Confucian teachings and to educate more and more eligible candidates for the exams. This can be seen in the number of exam takers for the low-level prefectural exams rising from 30,000 annual candidates in the early 11th century to 400,000 candidates by the late 13th century. The civil service and examination system allowed for greater meritocracy, social mobility, and equality in competition for those wishing to attain an official seat in government. Using statistics gathered by the Song state, Edward A. Kracke, Sudō Yoshiyuki, and Ho Ping-ti supported the hypothesis that simply having a father, grandfather, or great-grandfather who had served as an official of state did not guarantee one would obtain the same level of authority. Robert Hartwell and Robert P. Hymes criticized this model, stating that it places too much emphasis on the role of the nuclear family and considers only three paternal ascendants of exam candidates while ignoring the demographic reality of Song China, the significant proportion of males in each generation that had no surviving sons, and the role of the extended family. Many felt disenfranchised by what they saw as a bureaucratic system that favored the land-holding class able to afford the best education. One of the greatest literary critics of this was the official and famous poet Su Shi. Yet Su was a product of his times, as the identity, habits, and attitudes of the scholar-official had become less aristocratic and more bureaucratic with the transition of the periods from Tang to Song. At the beginning of the dynasty, government posts were disproportionately held by two elite social groups: a founding elite who had ties with the founding emperor and a semi-hereditary professional elite who used long-held clan status, family connections, and marriage alliances to secure appointments. By the late 11th century, the founding elite became obsolete, while political partisanship and factionalism at court undermined the marriage strategies of the professional elite, which dissolved as a distinguishable social group and was replaced by a multitude of gentry families.
Due to Song's enormous population growth and the body of its appointed scholar-officials being accepted in limited numbers (about 20,000 active officials during the Song period), the larger scholarly gentry class would now take over grassroots affairs on the vast local level. Excluding the scholar-officials in office, this elite social class consisted of exam candidates, examination degree-holders not yet assigned to an official post, local tutors, and retired officials. These learned men, degree-holders, and local elites supervised local affairs and sponsored necessary facilities of local communities; any local magistrate appointed to his office by the government relied upon the cooperation of the few or many local gentry in the area. For example, the Song government—excluding the educational-reformist government under Emperor Huizong—spared little amount of state revenue to maintain prefectural and county schools; instead, the bulk of the funds for schools was drawn from private financing. This limited role of government officials was a departure from the earlier Tang dynasty (618–907), when the government strictly regulated commercial markets and local affairs; now the government withdrew heavily from regulating commerce and relied upon a mass of local gentry to perform necessary duties in their communities.
The gentry distinguished themselves in society through their intellectual and antiquarian pursuits, while the homes of prominent landholders attracted a variety of courtiers, including artisans, artists, educational tutors, and entertainers. Despite the disdain for trade, commerce, and the merchant class exhibited by the highly cultured and elite exam-drafted scholar-officials, commercialism played a prominent role in Song culture and society. A scholar-official would be frowned upon by his peers if he pursued means of profiteering outside of his official salary; however, this did not stop many scholar-officials from managing business relations through the use of intermediary agents.
The Song judicial system retained most of the legal code of the earlier Tang dynasty, the basis of traditional Chinese law up until the modern era. Roving sheriffs maintained law and order in the municipal jurisdictions and occasionally ventured into the countryside. Official magistrates overseeing court cases were not only expected to be well-versed in written law but also to promote morality in society. Magistrates such as the famed Bao Zheng (999–1062) embodied the upright, moral judge who upheld justice and never failed to live up to his principles. Song judges specified the guilty person or party in a criminal act and meted out punishments accordingly, often in the form of caning. A guilty individual or parties brought to court for a criminal or civil offense were not viewed as wholly innocent until proven otherwise, while even accusers were viewed with a high level of suspicion by the judge. Due to costly court expenses and immediate jailing of those accused of criminal offenses, people in the Song preferred to settle disputes and quarrels privately, without the court's interference.
Shen Kuo's Dream Pool Essays argued against traditional Chinese beliefs in anatomy (such as his argument for two throat valves instead of three); this perhaps spurred the interest in the performance of post-mortem autopsies in China during the 12th century. The physician and judge known as Song Ci (1186–1249) wrote a pioneering work of forensic science on the examination of corpses in order to determine cause of death (strangulation, poisoning, drowning, blows, etc.) and to prove whether death resulted from murder, suicide, or accidental death. Song Ci stressed the importance of proper coroner's conduct during autopsies and the accurate recording of the inquest of each autopsy by official clerks.
The Song military was chiefly organized to ensure that the army could not threaten Imperial control, often at the expense of effectiveness in war. Northern Song's Military Council operated under a Chancellor, who had no control over the imperial army. The imperial army was divided among three marshals, each independently responsible to the Emperor. Since the Emperor rarely led campaigns personally, Song forces lacked unity of command. The imperial court often believed that successful generals endangered royal authority, and relieved or even executed them (notably Li Gang, Yue Fei, and Han Shizhong).
Although the scholar-officials viewed military soldiers as lower members in the hierarchic social order, a person could gain status and prestige in society by becoming a high-ranking military officer with a record of victorious battles. At its height, the Song military had one million soldiers divided into platoons of 50 troops, companies made of two platoons, battalions composed of 500 soldiers. Crossbowmen were separated from the regular infantry and placed in their own units as they were prized combatants, providing effective missile fire against cavalry charges. The government was eager to sponsor new crossbow designs that could shoot at longer ranges, while crossbowmen were also valuable when employed as long-range snipers. Song cavalry employed a slew of different weapons, including halberds, swords, bows, spears, and 'fire lances' that discharged a gunpowder blast of flame and shrapnel.
Military strategy and military training were treated as sciences that could be studied and perfected; soldiers were tested in their skills of using weaponry and in their athletic ability. The troops were trained to follow signal standards to advance at the waving of banners and to halt at the sound of bells and drums.
The Song navy was of great importance during the consolidation of the empire in the 10th century; during the war against the Southern Tang state, the Song navy employed tactics such as defending large floating pontoon bridges across the Yangtze River in order to secure movements of troops and supplies. There were large ships in the Song navy that could carry 1,000 soldiers aboard their decks, while the swift-moving paddle-wheel craft were viewed as essential fighting ships in any successful naval battle.
In a battle on January 23, 971, massive arrow fire from Song dynasty crossbowmen decimated the war elephant corps of the Southern Han army. This defeat not only marked the eventual submission of the Southern Han to the Song dynasty, but also the last instance where a war elephant corps was employed as a regular division within a Chinese army.
There was a total of 347 military treatises written during the Song period, as listed by the history text of the Song Shi (compiled in 1345). However, only a handful of these military treatises have survived, which includes the Wujing Zongyao written in 1044. It was the first known book to have listed formulas for gunpowder; it gave appropriate formulas for use in several different kinds of gunpowder bombs. It also provided detailed descriptions and illustrations of double-piston pump flamethrowers, as well as instructions for the maintenance and repair of the components and equipment used in the device.
The visual arts during the Song dynasty were heightened by new developments such as advances in landscape and portrait painting. The gentry elite engaged in the arts as accepted pastimes of the cultured scholar-official, including painting, composing poetry, and writing calligraphy. The poet and statesman Su Shi and his associate Mi Fu (1051–1107) enjoyed antiquarian affairs, often borrowing or buying art pieces to study and copy. Poetry and literature profited from the rising popularity and development of the ci poetry form. Enormous encyclopedic volumes were compiled, such as works of historiography and dozens of treatises on technical subjects. This included the universal history text of the Zizhi Tongjian, compiled into 1000 volumes of 9.4 million written Chinese characters. The genre of Chinese travel literature also became popular with the writings of the geographer Fan Chengda (1126–1193) and Su Shi, the latter of whom wrote the 'daytrip essay' known as Record of Stone Bell Mountain that used persuasive writing to argue for a philosophical point. Although an early form of the local geographic gazetteer existed in China since the 1st century, the matured form known as "treatise on a place", or fangzhi, replaced the old "map guide", or transl. zho – transl. tujing, during the Song dynasty.
The imperial courts of the emperor's palace were filled with his entourage of court painters, calligraphers, poets, and storytellers. Emperor Huizong was the eighth emperor of the Song dynasty and he was a renowned artist as well as a patron of the art and the catalogue of his collection listed over 6,000 known paintings. A prime example of a highly venerated court painter was Zhang Zeduan (1085–1145) who painted an enormous panoramic painting, Along the River During the Qingming Festival. Emperor Gaozong of Song initiated a massive art project during his reign, known as the Eighteen Songs of a Nomad Flute from the life story of Cai Wenji (b. 177). This art project was a diplomatic gesture to the Jin dynasty while he negotiated for the release of his mother from Jurchen captivity in the north.
In philosophy, Chinese Buddhism had waned in influence but it retained its hold on the arts and on the charities of monasteries. Buddhism had a profound influence upon the budding movement of Neo-Confucianism, led by Cheng Yi (1033–1107) and Zhu Xi (1130–1200). Mahayana Buddhism influenced Fan Zhongyan and Wang Anshi through its concept of ethical universalism, while Buddhist metaphysics deeply affected the pre–Neo-Confucian doctrine of Cheng Yi. The philosophical work of Cheng Yi in turn influenced Zhu Xi. Although his writings were not accepted by his contemporary peers, Zhu's commentary and emphasis upon the Confucian classics of the Four Books as an introductory corpus to Confucian learning formed the basis of the Neo-Confucian doctrine. By the year 1241, under the sponsorship of Emperor Lizong, Zhu Xi's Four Books and his commentary on them became standard requirements of study for students attempting to pass the civil service examinations. The neighbouring East Asian countries of Japan and Korea also adopted Zhu Xi's teaching, known as the Shushigaku (朱子學, School of Zhu Xi) of Japan, and in Korea the Jujahak (주자학). Buddhism's continuing influence can be seen in painted artwork such as Lin Tinggui's Luohan Laundering. However, the ideology was highly criticized and even scorned by some. The statesman and historian Ouyang Xiu (1007–1072) called the religion a "curse" that could only be remedied by uprooting it from Chinese culture and replacing it with Confucian discourse. The Chan sect experienced a literary flourishing in the Song period, which saw the publication of several major classical koan collections which remain influential in Zen philosophy and practice to the present day. A true revival of Buddhism in Chinese society would not occur until the Mongol rule of the Yuan dynasty, with Kublai Khan's sponsorship of Tibetan Buddhism and Drogön Chögyal Phagpa as the leading lama. The Christian sect of Nestorianism, which had entered China in the Tang era, would also be revived in China under Mongol rule.
Sumptuary laws regulated the food that one consumed and the clothes that one wore according to status and social class. Clothing was made of hemp or cotton cloths, restricted to a color standard of black and white. Trousers were the acceptable attire for peasants, soldiers, artisans, and merchants, although wealthy merchants might choose to wear more ornate clothing and male blouses that came down below the waist. Acceptable apparel for scholar-officials was rigidly defined by the social ranking system. However, as time went on this rule of rank-graded apparel for officials was not as strictly enforced. Each official was able to display his awarded status by wearing different-colored traditional silken robes that hung to the ground around his feet, specific types of headgear, and even specific styles of girdles that displayed his graded-rank of officialdom.
Women wore long dresses, blouses that came down to the knee, skirts, and jackets with long or short sleeves, while women from wealthy families could wear purple scarves around their shoulders. The main difference in women's apparel from that of men was that it was fastened on the left, not on the right.
The main food staples in the diet of the lower classes remained rice, pork, and salted fish. In 1011, Emperor Zhenzong of Song introduced Champa rice to China from Vietnam's Kingdom of Champa, which sent 30,000 bushels as a tribute to Song. Champa rice was drought-resistant and able to grow fast enough to offer two harvests a year instead of one.
Song restaurant and tavern menus are recorded. They list entrees for feasts, banquets, festivals, and carnivals. They reveal a diverse and lavish diet for those of the upper class. They could choose from a wide variety of meats and seafood, including shrimp, geese, duck, mussel, shellfish, fallow deer, hare, partridge, pheasant, francolin, quail, fox, badger, clam, crab, and many others. Dairy products were rare in Chinese cuisine at this time. Beef was rarely consumed since the bull was a valuable draft animal, and dog meat was absent from the diet of the wealthy, although the poor could choose to eat dog meat if necessary (yet it was not part of their regular diet). People also consumed dates, raisins, jujubes, pears, plums, apricots, pear juice, lychee-fruit juice, honey and ginger drinks, spices and seasonings of Sichuan pepper, ginger, soy sauce, oil, sesame oil, salt, and vinegar.
The Song dynasty had one of the most prosperous and advanced economies in the medieval world. Song Chinese invested their funds in joint stock companies and in multiple sailing vessels at a time when monetary gain was assured from the vigorous overseas trade and domestic trade along the Grand Canal and Yangtze River. Prominent merchant families and private businesses were allowed to occupy industries that were not already government-operated monopolies. Both private and government-controlled industries met the needs of a growing Chinese population in the Song. Artisans and merchants formed guilds that the state had to deal with when assessing taxes, requisitioning goods, and setting standard workers' wages and prices on goods.
The iron industry was pursued by both private entrepreneurs who owned their own smelters as well as government-supervised smelting facilities. The Song economy was stable enough to produce over a hundred million kilograms (over two hundred million pounds) of iron product a year. Large-scale Deforestation in China would have continued if not for the 11th-century innovation of the use of coal instead of charcoal in blast furnaces for smelting cast iron. Much of this iron was reserved for military use in crafting weapons and armouring troops, but some was used to fashion the many iron products needed to fill the demands of the growing domestic market. The iron trade within China was advanced by the construction of new canals, facilitating the flow of iron products from production centres to the large market in the capital city.
The annual output of minted copper currency in 1085 reached roughly six billion coins. The most notable advancement in the Song economy was the establishment of the world's first government issued paper-printed money, known as Jiaozi (see also Huizi). For the printing of paper money, the Song court established several government-run factories in the cities of Huizhou, Chengdu, Hangzhou, and Anqi. The size of the workforce employed in paper money factories was large; it was recorded in 1175 that the factory at Hangzhou employed more than a thousand workers a day.
The economic power of Song China heavily influenced foreign economies abroad. The Moroccan geographer al-Idrisi wrote in 1154 of the prowess of Chinese merchant ships in the Indian Ocean and of their annual voyages that brought iron, swords, silk, velvet, porcelain, and various textiles to places such as Aden (Yemen), the Indus River, and the Euphrates in modern-day Iraq. Foreigners, in turn, affected the Chinese economy. For example, many West Asian and Central Asian Muslims went to China to trade, becoming a preeminent force in the import and export industry, while some were even appointed as officers supervising economic affairs. Sea trade with the South-west Pacific, the Hindu world, the Islamic world, and East Africa brought merchants great fortune and spurred an enormous growth in the shipbuilding industry of Song-era Fujian province. However, there was risk involved in such long overseas ventures. In order to reduce the risk of losing money on maritime trade missions abroad, wrote historians Ebrey, Walthall, and Palais:
 investors usually divided their investment among many ships, and each ship had many investors behind it. One observer thought eagerness to invest in overseas trade was leading to an outflow of copper cash. He wrote, "People along the coast are on intimate terms with the merchants who engage in overseas trade, either because they are fellow-countrymen or personal acquaintances. ...  money to take with them on their ships for purchase and return conveyance of foreign goods. They invest from ten to a hundred strings of cash, and regularly make profits of several hundred percent".Advancements in weapons technology enhanced by gunpowder, including the evolution of the early flamethrower, explosive grenade, firearm, cannon, and land mine, enabled the Song Chinese to ward off their militant enemies until the Song's ultimate collapse in the late 13th century. The Wujing Zongyao manuscript of 1044 was the first book in history to provide formulas for gunpowder and their specified use in different types of bombs. While engaged in a war with the Mongols, in 1259 the official Li Zengbo wrote in his Kezhai Zagao, Xugaohou that the city of Qingzhou was manufacturing one to two thousand strong iron-cased bombshells a month, dispatching to Xiangyang and Yingzhou about ten to twenty thousand such bombs at a time. In turn, the invading Mongols employed northern Chinese soldiers and used these same types of gunpowder weapons against the Song. By the 14th century the firearm and cannon could also be found in Europe, India, and the Middle East, during the early age of gunpowder warfare.
As early as the Han dynasty, when the state needed to accurately measure distances traveled throughout the empire, the Chinese relied on a mechanical odometer. The Chinese odometer was a wheeled carriage, its gearwork being driven by the rotation of the carriage's wheels; specific units of distance—the Chinese li—were marked by the mechanical striking of a drum or bell as an auditory signal. The specifications for the 11th-century odometer were written by Chief Chamberlain Lu Daolong, who is quoted extensively in the historical text of the Song Shi (compiled by 1345). In the Song period, the odometer vehicle was also combined with another old complex mechanical device known as the south-pointing chariot. This device, originally crafted by Ma Jun in the 3rd century, incorporated a differential gear that allowed a figure mounted on the vehicle to always point in the southern direction, no matter how the vehicle's wheels turned about. The concept of the differential gear that was used in this navigational vehicle is now found in modern automobiles in order to apply an equal amount of torque to a car's wheels even when they are rotating at different speeds.
Polymath figures such as the scientists and statesmen Shen Kuo (1031–1095) and Su Song (1020–1101) embodied advancements in all fields of study, including botany, zoology, geology, mineralogy, metallurgy, mechanics, magnetics, meteorology, horology, astronomy, pharmaceutical medicine, archeology, mathematics, cartography, optics, art criticism, hydraulics, and many other fields.
Shen Kuo was the first to discern magnetic declination of true north while experimenting with a compass. Shen theorized that geographical climates gradually shifted over time. He created a theory of land formation involving concepts accepted in modern geomorphology. He performed optical experiments with camera obscura just decades after Ibn al-Haytham was the first to do so. He also improved the designs of astronomical instruments such as the widened astronomical sighting tube, which allowed Shen Kuo to fix the position of the pole star (which had shifted over centuries of time). Shen Kuo was also known for hydraulic clockworks, as he invented a new overflow-tank clepsydra which had more efficient higher-order interpolation instead of linear interpolation in calibrating the measure of time.
Su Song was best known for his horology treatise written in 1092, which described and illustrated in great detail his hydraulic-powered, 12 m (39 ft) tall astronomical clock tower built in Kaifeng. The clock tower featured large astronomical instruments of the armillary sphere and celestial globe, both driven by an early intermittently working escapement mechanism (similarly to the western verge escapement of true mechanical clocks appeared in medieval clockworks, derived from ancient clockworks of classical times). Su's tower featured a rotating gear wheel with 133 clock jack mannequins who were timed to rotate past shuttered windows while ringing gongs and bells, banging drums, and presenting announcement plaques. In his printed book, Su published a celestial atlas of five star charts. These star charts feature a cylindrical projection similar to Mercator projection, the latter being a cartographic innovation of Gerardus Mercator in 1569.
The Song Chinese observed supernovae, including SN 1054, the remnants of which would form the Crab Nebula. Moreover, the Soochow Astronomical Chart on Chinese planispheres was prepared in 1193 for instructing the crown prince on astronomical findings. The planispheres were engraved in stone several decades later.
There were many notable improvements to Chinese mathematics during the Song era. Mathematician Yang Hui's 1261 book provided the earliest Chinese illustration of Pascal's triangle, although it had earlier been described by Jia Xian in around 1100. Yang Hui also provided rules for constructing combinatorial arrangements in magic squares, provided theoretical proof for Euclid's forty-third proposition about parallelograms, and was the first to use negative coefficients of 'x' in quadratic equations. Yang's contemporary Qin Jiushao (c. 1202–1261) was the first to introduce the zero symbol into Chinese mathematics; before this blank spaces were used instead of zeroes in the system of counting rods. He is also known for working with the Chinese remainder theorem, Heron's formula, and astronomical data used in determining the winter solstice. Qin's major work was the Mathematical Treatise in Nine Sections published in 1247.
Geometry was essential to surveying and cartography. The earliest extant Chinese maps date to the 4th century BCE, yet it was not until the time of Pei Xiu (224–271) that topographical elevation, a formal rectangular grid system, and use of a standard graduated scale of distances was applied to terrain maps. Following a long tradition, Shen Kuo created a raised-relief map, while his other maps featured a uniform graduated scale of 1:900,000. A 3 ft (0.91 m) squared map of 1137—carved into a stone block—followed a uniform grid scale of 100 li for each gridded square, and accurately mapped the outline of the coasts and river systems of China, extending all the way to India. Furthermore, the world's oldest known terrain map in printed form comes from the edited encyclopedia of Yang Jia in 1155, which displayed western China without the formal grid system that was characteristic of more professionally made Chinese maps. Although gazetteers had existed since 52 CE during the Han dynasty and gazetteers accompanied by illustrative maps (Chinese: tujing) since the Sui dynasty, the illustrated gazetteer became much more common in the Song dynasty, when the foremost concern was for illustrative gazetteers to serve political, administrative, and military purposes.
The innovation of movable type printing was made by the artisan Bi Sheng (990–1051), first described by the scientist and statesman Shen Kuo in his Dream Pool Essays of 1088. The collection of Bi Sheng's original clay-fired typeface was passed on to one of Shen Kuo's nephews, and was carefully preserved. Movable type enhanced the already widespread use of woodblock methods of printing thousands of documents and volumes of written literature, consumed eagerly by an increasingly literate public. The advancement of printing deeply affected education and the scholar-official class, since more books could be made faster while mass-produced, printed books were cheaper in comparison to laborious handwritten copies. The enhancement of widespread printing and print culture in the Song period was thus a direct catalyst in the rise of social mobility and expansion of the educated class of scholar elites, the latter which expanded dramatically in size from the 11th to 13th centuries.
The movable type invented by Bi Sheng was ultimately trumped by the use of woodblock printing due to the limitations of the enormous Chinese character writing system, yet movable type printing continued to be used and was improved in later periods. The Yuan dynasty scholar-official Wang Zhen (fl. 1290–1333) implemented a faster typesetting process, improved Bi's baked-clay movable type character set with a wooden one, and experimented with tin-metal movable type. The wealthy printing patron Hua Sui (1439–1513) of the Ming dynasty established China's first metal movable type (using bronze) in 1490. In 1638, the Peking Gazette switched their printing process from woodblock to movable type printing. Yet it was during the Qing dynasty that massive printing projects began to employ movable type printing. This includes the printing of sixty-six copies of a 5,020 volume long encyclopedia in 1725, the Gujin Tushu Jicheng (Complete Collection of Illustrations and Writings from the Earliest to Current Times), which necessitated the crafting of 250,000 movable type characters cast in bronze. By the 19th century the European style printing press replaced the old Chinese methods of movable type, while traditional woodblock printing in modern East Asia is used sparsely and for aesthetic reasons.
The most important nautical innovation of the Song period seems to have been the introduction of the magnetic mariner's compass, which permitted accurate navigation on the open sea regardless of the weather. The magnetized compass needle – known in Chinese as the "south-pointing needle" – was first described by Shen Kuo in his 1088 Dream Pool Essays and first mentioned in active use by sailors in Zhu Yu's 1119 Pingzhou Table Talks.
There were other considerable advancements in hydraulic engineering and nautical technology during the Song dynasty. The 10th-century invention of the pound lock for canal systems allowed different water levels to be raised and lowered for separated segments of a canal, which significantly aided the safety of canal traffic and allowed for larger barges. There was the Song-era innovation of watertight bulkhead compartments that allowed damage to hulls without sinking the ships. If ships were damaged, the Chinese of the 11th century employed drydocks to repair them while suspended out of the water. The Song used crossbeams to brace the ribs of ships in order to strengthen them in a skeletal-like structure. Stern-mounted rudders had been mounted on Chinese ships since the 1st century, as evidenced with a preserved Han tomb model of a ship. In the Song period, the Chinese devised a way to mechanically raise and lower rudders in order for ships to travel in a wider range of water depths. The Song arranged the protruding teeth of anchors in a circular pattern instead of in one direction. David Graff and Robin Higham state that this arrangement " them more reliable" for anchoring ships.
Architecture during the Song period reached new heights of sophistication. Authors such as Yu Hao and Shen Kuo wrote books outlining the field of architectural layouts, craftsmanship, and structural engineering in the 10th and 11th centuries, respectively. Shen Kuo preserved the written dialogues of Yu Hao when describing technical issues such as slanting struts built into pagoda towers for diagonal wind bracing. Shen Kuo also preserved Yu's specified dimensions and units of measurement for various building types. The architect Li Jie (1065–1110), who published the Yingzao Fashi ('Treatise on Architectural Methods') in 1103, greatly expanded upon the works of Yu Hao and compiled the standard building codes used by the central government agencies and by craftsmen throughout the empire. He addressed the standard methods of construction, design, and applications of moats and fortifications, stonework, greater woodwork, lesser woodwork, wood-carving, turning and drilling, sawing, bamboo work, tiling, wall building, painting and decoration, brickwork, glazed tile making, and provided proportions for mortar formulas in masonry. In his book, Li provided detailed and vivid illustrations of architectural components and cross-sections of buildings. These illustrations displayed various applications of corbel brackets, cantilever arms, mortise and tenon work of tie beams and cross beams, and diagrams showing the various building types of halls in graded sizes. He also outlined the standard units of measurement and standard dimensional measurements of all building components described and illustrated in his book.
Grandiose building projects were supported by the government, including the erection of towering Buddhist Chinese pagodas and the construction of enormous bridges (wood or stone, trestle or segmental arch bridge). Many of the pagoda towers built during the Song period were erected at heights that exceeded ten stories. Some of the most famous are the Iron Pagoda built in 1049 during the Northern Song and the Liuhe Pagoda built in 1165 during the Southern Song, although there were many others. The tallest is the Liaodi Pagoda of Hebei built in 1055, towering 84 m (276 ft) in total height. Some of the bridges reached lengths of 1,220 m (4,000 ft), with many being wide enough to allow two lanes of cart traffic simultaneously over a waterway or ravine. The government also oversaw construction of their own administrative offices, palace apartments, city fortifications, ancestral temples, and Buddhist temples.
The professions of the architect, craftsman, carpenter, and structural engineer were not seen as professionally equal to that of a Confucian scholar-official. Architectural knowledge had been passed down orally for thousands of years in China, in many cases from a father craftsman to his son. Structural engineering and architecture schools were known to have existed during the Song period; one prestigious engineering school was headed by the renowned bridge-builder Cai Xiang (1012–1067) in medieval Fujian province.
Besides existing buildings and technical literature of building manuals, Song dynasty artwork portraying cityscapes and other buildings aid modern-day scholars in their attempts to reconstruct and realize the nuances of Song architecture. Song dynasty artists such as Li Cheng, Fan Kuan, Guo Xi, Zhang Zeduan, Emperor Huizong of Song, and Ma Lin painted close-up depictions of buildings as well as large expanses of cityscapes featuring arched bridges, halls and pavilions, pagoda towers, and distinct Chinese city walls. The scientist and statesman Shen Kuo was known for his criticism of artwork relating to architecture, saying that it was more important for an artist to capture a holistic view of a landscape than it was to focus on the angles and corners of buildings. For example, Shen criticized the work of the painter Li Cheng for failing to observe the principle of "seeing the small from the viewpoint of the large" in portraying buildings.
There were also pyramidal tomb structures in the Song era, such as the Song imperial tombs located in Gongxian, Henan province. About 100 km (62 mi) from Gongxian is another Song dynasty tomb at Baisha, which features "elaborate facsimiles in brick of Chinese timber frame construction, from door lintels to pillars and pedestals to bracket sets, that adorn interior walls." The two large chambers of the Baisha tomb also feature conical-shaped roofs. Flanking the avenues leading to these tombs are lines of Song dynasty stone statues of officials, tomb guardians, animals, and legendary creatures.
In addition to the Song gentry's antiquarian pursuits of art collecting, scholar-officials during the Song became highly interested in retrieving ancient relics from archaeological sites, in order to revive the use of ancient vessels in ceremonies of state ritual. Scholar-officials of the Song period claimed to have discovered ancient bronze vessels that were created as far back as the Shang dynasty (1600–1046 BCE), which bore the written characters of the Shang era. Some attempted to recreate these bronze vessels by using imagination alone, not by observing tangible evidence of relics; this practice was criticized by Shen Kuo in his work of 1088. Yet Shen Kuo had much more to criticize than this practice alone. Shen objected to the idea of his peers that ancient relics were products created by famous "sages" in lore or the ancient aristocratic class; Shen rightfully attributed the discovered handicrafts and vessels from ancient times as the work of artisans and commoners from previous eras. He also disapproved of his peers' pursuit of archaeology simply to enhance state ritual, since Shen not only took an interdisciplinary approach with the study of archaeology, but he also emphasized the study of functionality and investigating what was the ancient relics' original processes of manufacture. Shen used ancient texts and existing models of armillary spheres to create one based on ancient standards; Shen described ancient weaponry such as the use of a scaled sighting device on crossbows; while experimenting with ancient musical measures, Shen suggested hanging an ancient bell by using a hollow handle.
Despite the gentry's overriding interest in archaeology simply for reviving ancient state rituals, some of Shen's peers took a similar approach to the study of archaeology. His contemporary Ouyang Xiu (1007–1072) compiled an analytical catalogue of ancient rubbings on stone and bronze which pioneered ideas in early epigraphy and archaeology. During the 11th century, Song scholars discovered the ancient shrine of Wu Liang (78–151 CE), a scholar of the Han dynasty (202 BCE – 220 CE); they produced rubbings of the carvings and bas-reliefs decorating the walls of his tomb so that they could be analyzed elsewhere. On the unreliability of historical works written after the fact, the epigrapher and poet Zhao Mingcheng (1081–1129) stated "... the inscriptions on stone and bronze are made at the time the events took place and can be trusted without reservation, and thus discrepancies may be discovered." Historian R.C. Rudolph states that Zhao's emphasis on consulting contemporary sources for accurate dating is parallel with the concern of the German historian Leopold von Ranke (1795–1886), and was in fact emphasized by many Song scholars. The Song scholar Hong Mai (1123–1202) heavily criticized what he called the court's "ridiculous" archaeological catalogue Bogutu compiled during the Huizong reign periods of Zheng He and Xuan He (1111–1125). Hong Mai obtained old vessels from the Han dynasty and compared them with the descriptions offered in the catalogue, which he found so inaccurate he stated he had to "hold my sides with laughter." Hong Mai pointed out that the erroneous material was the fault of Chancellor Cai Jing, who prohibited scholars from reading and consulting written histories.

A multiple rocket launcher (MRL) is a type of rocket artillery system that contains multiple launchers which are fixed to a single platform, and shoots its rocket ordnance in a fashion similar to a volley gun. Rockets are self-propelled in flight and have different capabilities than conventional artillery shells, such as longer effective range, lower recoil, typically considerably higher payload than a similarly sized gun artillery platform, or even carrying multiple warheads.
Unguided rocket artillery is notoriously inaccurate and slow to reload compared to gun artillery. A multiple rocket launcher helps compensate for this with its ability to launch multiple rockets in rapid succession, which, coupled with the large kill zone of each warhead, can easily deliver saturation fire over a target area. However, modern rockets can use GPS or inertial guidance to combine the advantages of rockets with the higher accuracy of precision-guided munitions.
The first multiple rocket launchers were invented during the medieval Chinese Song dynasty, in which the Chinese fire lance was fixed backward on a pike or arrow and shot at enemy in 1180.  This form of a rocket evolved from the Chinese fire lance was used during the Mongol siege of Kaifeng.
Later, Chinese military created and employed multiple rocket launchers that fired up to 100 small fire-arrow rockets simultaneously. The typical powder section of such an arrow-rocket was 1/3 to 1/2 ft (10 to 15 cm) long. Bamboo arrow shafts varied from 1.5 ft (45 cm) to 2.5 ft (75 cm) long and the striking distance reached 300 to 400 paces. The Chinese also enhanced rocket tips with poison and made sure that the launchers were also mobile. The designers designed a multiple rocket launcher that could be carried and operated by a single soldier. Various forms of multiple rocket launchers evolved, including the launcher mounted on wheelbarrows.  The Joseon Dynasty of Korea used an expanded variant of such a launcher (called the hwacha) to great effect against invading armies during the Japanese invasions of 1592–1598, most notably the Battle of Haengju. The launchpad made of 100 to 200 holes containing rocket arrows was placed on a two-wheeled cart. The range of the fired arrows is estimated to be 2000 meters. Hwacha was a key weapon used against the invading armies during the Japanese invasions of 1592–1598. The most notable example was Battle of Haengju, in which 40 hwachas were deployed to repel 30,000 Japanese soldiers.
European armies preferred relatively large single-launch rockets prior to World War II. Napoleonic armies of both sides followed British adoption of Mysorean rockets as the Congreve rocket. These were explosive steel-cased bombardment rockets with minimal launchers. European navies developed naval multiple launcher mounts with steadily improving explosive rockets for light and coastal vessels. These weapons were largely replaced by conventional light artillery during the late nineteenth century.
The first self-propelled multiple rocket launcher –  and arguably the most famous –  was the Soviet BM-13 Katyusha, first used during World War II and exported to Soviet allies afterwards. They were simple systems in which a rack of launch rails was mounted on the back of a truck. This set the template for modern multiple rocket launchers. The Americans mounted tubular launchers atop M4 Sherman tanks to create the T34 Calliope rocket launching tank, only used in small numbers, as their closest equivalent to the Katyusha. The Germans began using a towed six-tube multiple rocket launcher during World War II, the Nebelwerfer, called the "Screaming Mimi" by the Allies. The system was developed before the war to skirt the limitations of the Treaty of Versailles. Later in the war, the 15 cm Nebelwerfer 41 was mounted on modified Opel Maultier "Mule" halftracks, becoming the Panzerwerfer 42 4/1. Another version produced in limited numbers towards the end of the war was a conversion of the Schwerer Wehrmachtschlepper ("heavy military transport", sWS) halftrack to a configuration similar to the Panzerwerfer 42 4/1, mounting the 10-barreled 15 cm Nebelwerfer.
Another German halftrack MRL system was inspired by the Russian BM-13. Keeping the Soviet 82mm rocket calibre as well as the launch and rocket stabilisation designs, it was developed into a system of 2 rows of 12 guide rails mounted to the Maultier chassis, each row providing the capacity for 24 rockets, underslung as well as on top of the rails, for 48 rockets total. This vehicle was designated 8 cm Raketen-Vielfachwerfer (8 cm multiple rocket launcher). As the launch system was inspired by and looked similar to the BM-13, which the Germans had nicknamed "Stalin-Orgel" or "Stalin-Organ", the Vielfachwerfer soon became known as the "Himmler-Orgel", or "Himmler-Organ".
There are two main types of multiple rocket launchers:
Like all artillery, MRLs have a reputation of devastating morale on ill-disciplined or already shaken troops. The material effect depends on circumstances, as well-covered field fortifications may provide reasonable protection.
MRLs are still unable to properly engage reverse slope positions in mountain warfare because it is more difficult to determine the trajectory compared to that of a howitzer by adding or removing propellant increments. Simple MRL rocket types have a rather long minimum firing range for the same reason. An approach to lessen this limit is the addition of drag rings to the rocket nose. The increased drag slows the rocket down relative to a clean configuration and creates a less flat trajectory. Pre-packaged MRL munitions do not offer this option but some MRL types with individually-loaded rockets do.
Improvised multiple rocket launchers based on helicopter or aircraft-mounted rocket pods (typically of 57–80 mm calibre) especially on light trucks and pickups (so-called "technicals") are often seen in civil wars, when rebels make use of captured launchers and munitions.
Modern MRL systems can use modern land navigation (especially satellite navigation such as GPS) for quick and accurate positioning. The accurate determination of the battery position previously required such an extent of effort that making a dispersed operation of the battery was at times impractical. MRL systems with GPS can have their MRLs dispersed and fire from dispersed positions at a single target, just as previously multiple batteries were often united on one target area.
Radar may be used to track weather balloons to determine winds or to track special rockets which self-destruct in the air. The tracking allows determination of the influence of winds and propellant temperatures on the rocket's flight path. These observations can then be factored into the firing solution for the rocket salvo for effect.
Such tracking radars can also be used to predict the range error of individual rockets. Trajectory-correcting munitions may then benefit from this, as a directional radio may send a coded message to the rocket to deploy air brakes at just the right time to correct most of the range error. This requires that the rockets were originally aimed too far, as the range can only be shortened by the air brakes, not extended.
A more sophisticated system makes use of radar data and one-way radio datalink to initiate a two dimensional (range and azimuth) correction of the rocket's flight-path with steering by fins or nose thrusters. The latter is more common with systems which can be used to upgrade old rockets and the IMI ACCULAR is an example. MRL capable of using thermobaric warheads, mounted on a T-72 tank chassis in action]]
Fin-stabilised rockets also allow for easy course corrections using rudders or minute charges. Precision-guided munitions have been introduced to exploit this. Guidance principles such as GPS satellite navigation, inertial navigation systems and semi-active laser seekers are used for this. This improves dispersion from a CEP of hundreds of meters at dozens of kilometers range to just a few meters and largely independent of the range of the round (except for INS, as INS navigation creates a small dispersion that's about proportional to range). This in turn made great increases of rocket (or missile) ranges useful; previously dispersion had made rockets too inefficient and often too dangerous to friendly troops at long ranges.
Long range MRL missiles often fly a higher quasiballistic trajectory than shorter ranged rockets and thus pose a deconfliction challenge, as they might collide with friendly aircraft in the air.
The differences between an MRL missile and a large anti-tank guided missile such as Nimrod have blurred due to guided MRL missiles such as M31 GMLRS (Guided Unitary Multiple Launch Rocket System), which passed flight tests in 2014.
The Mongol invasions and conquests took place during the 13th and 14th centuries, creating history's largest contiguous empire: the Mongol Empire, which by 1300 covered large parts of Eurasia. Historians regard the Mongol devastation as one of the deadliest episodes in history. In addition, Mongol expeditions may have spread the bubonic plague across much of Eurasia, helping to spark the Black Death of the 14th century.
The Mongol Empire developed in the course of the 13th century through a series of victorious campaigns throughout Asia, reaching Eastern Europe by the 1240s. In contrast with later "empires of the sea" such as European colonial powers, the Mongol Empire was a land power, fueled by the grass-foraging Mongol cavalry and cattle. Thus most Mongol conquest and plundering took place during the warmer seasons, when there was sufficient grazing for their herds. The rise of the Mongols was preceded by 15 years of wet and warm weather conditions from 1211 to 1225 that allowed favourable conditions for the breeding of horses, which greatly assisted their expansion.
As the Mongol Empire began to fragment from 1260, conflict between the Mongols and Eastern European polities continued for centuries. Mongols continued to rule China into the 14th century under the Yuan dynasty, while Mongol rule in Persia persisted into the 15th century under the Timurid Empire. In India, the later Mughal Empire survived into the 19th century.
Genghis Khan forged the initial Mongol Empire in Central Asia, starting with the unification of the nomadic tribes Merkits, Tatars, Keraites, Turks, Naimans and Mongols. The Uighur Buddhist Qocho Kingdom surrendered and joined the empire. He then continued expansion via conquest of the Qara Khitai and the Khwarazmian dynasty.
Large areas of Islamic Central Asia and northeastern Iran were seriously depopulated, as every city or town that resisted the Mongols was destroyed. Each soldier was given a quota of enemies to execute according to circumstances. For example, after the conquest of Urgench, each Mongol warrior – in an army of perhaps two tumens (20,000 troops) – was required to execute 24 people, or nearly half a million people per said army.
Against the Alans and the Cumans (Kipchaks), the Mongols used divide-and-conquer tactics by first warning the Cumans to end their support of the Alans, whom they then defeated, before rounding on the Cumans. Alans were recruited into the Mongol forces with one unit called "Right Alan Guard" which was combined with "recently surrendered" soldiers. Mongols and Chinese soldiers stationed in the area of the former Kingdom of Qocho and in Besh Balikh established a Chinese military colony led by Chinese general Qi Kongzhi (Ch'i Kung-chih).
During the Mongol attack on the Mamluks in the Middle East, most of the Mamluk military was composed of Kipchaks, and the Golden Horde's supply of Kipchak fighters replenished the Mamluk armies and helped them fight off the Mongols.
Hungary became a refuge for fleeing Cumans.
The decentralized, stateless Kipchaks only converted to Islam after the Mongol conquest, unlike the centralized Karakhanid entity comprising the Yaghma, Qarluqs, and Oghuz who converted earlier to world religions.
The Mongol conquest of the Kipchaks led to a merged society with a Mongol ruling class over a Kipchak-speaking populace which came to be known as Tatar, and which eventually absorbed Armenians, Italians, Greeks, and Goths on the Crimean peninsula to form the modern day Crimean Tatar people.
The Mongols conquered, by battle or voluntary surrender, the areas of present-day Iran, Iraq, the Caucasus, and parts of Syria and Turkey, with further Mongol raids reaching southwards into Palestine as far as Gaza in 1260 and 1300. The major battles were the Siege of Baghdad (1258), when the Mongols sacked the city which had been the center of Islamic power for 500 years, and the Battle of Ain Jalut in 1260, when the Muslim Mamluks were able to defeat the Mongols in the battle at Ain Jalut in the southern part of the Galilee — the first time the Mongols had been decisively stopped. One thousand northern Chinese engineer squads accompanied the Mongol Khan Hulagu during his conquest of the Middle East.
Genghis Khan and his descendants launched progressive invasions of China, subjugating the Western Xia in 1209 before destroying them in 1227, defeating the Jin dynasty in 1234 and defeating the Song dynasty in 1279. They made the Kingdom of Dali into a vassal state in 1253 after the Dali King Duan Xingzhi defected to the Mongols and helped them conquer the rest of Yunnan, forced Korea to capitulate through nine invasions, but failed in their attempts to invade Japan, their fleets scattered by kamikaze storms.
The Mongols' greatest triumph was when Kublai Khan established the Yuan dynasty in China in 1271. The dynasty created a "Han Army" (漢軍) out of defected Jin troops and an army of defected Song troops called the "Newly Submitted Army" (新附軍).
The Mongol force which invaded southern China was far greater than the force they sent to invade the Middle East in 1256.
The Yuan dynasty established the top-level government agency Bureau of Buddhist and Tibetan Affairs to govern Tibet, which was conquered by the Mongols and put under Yuan rule. The Mongols also invaded Sakhalin Island between 1264 and 1308. Likewise, Korea (Goryeo) became a semi-autonomous vassal state of the Yuan dynasty for about 80 years.
By 1206, Genghis Khan had conquered all Mongol and Turkic tribes in Mongolia and southern Siberia. In 1207 his eldest son Jochi subjugated the Siberian forest people, the Uriankhai, the Oirats, Barga, Khakas, Buryats, Tuvans, Khori-Tumed, and Kyrgyz. He then organized the Siberians into three tumens. Genghis Khan gave the Telengit and Tolos along the Irtysh River to an old companion, Qorchi. While the Barga, Tumed, Buriats, Khori, Keshmiti, and Bashkirs were organized in separate thousands, the Telengit, Tolos, Oirats and Yenisei Kirghiz were numbered into the regular tumens Genghis created a settlement of Chinese craftsmen and farmers at Kem-kemchik after the first phase of the Mongol conquest of the Jin dynasty. The Great Khans favored gyrfalcons, furs, women and Kyrgyz horses for tribute.
Western Siberia came under the Golden Horde. The descendants of Orda Khan, the eldest son of Jochi, directly ruled the area. In the swamps of western Siberia, dog sled Yam stations were set up to facilitate collection of tribute.
In 1270, Kublai Khan sent a Chinese official, with a new batch of settlers, to serve as judge of the Kyrgyz and Tuvan basin areas (益蘭州 and 謙州). Ogedei's grandson Kaidu occupied portions of Central Siberia from 1275 on. The Yuan dynasty army under Kublai's Kipchak general Tutugh reoccupied the Kyrgyz lands in 1293. From then on the Yuan dynasty controlled large portions of Central and Eastern Siberia.
The Mongols invaded and destroyed Volga Bulgaria and Kievan Rus', before invading Poland, Hungary, Bulgaria, and other territories. Over the course of three years (1237–1240), the Mongols razed all the major cities of Russia with the exceptions of Novgorod and Pskov.
Giovanni da Pian del Carpine, the Pope's envoy to the Mongol Great Khan, traveled through Kiev in February 1246 and wrote:.mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 40px}.mw-parser-output .templatequote .templatequotecite{line-height:1.5em;text-align:left;padding-left:1.6em;margin-top:0}They  attacked Rus, where they made great havoc, destroying cities and fortresses and slaughtering men; and they laid siege to Kiev, the capital of Rus; after they had besieged the city for a long time, they took it and put the inhabitants to death. When we were journeying through that land we came across countless skulls and bones of dead men lying about on the ground. Kiev had been a very large and thickly populated town, but now it has been reduced almost to nothing, for there are at the present time scarce two hundred houses there and the inhabitants are kept in complete slavery.The Mongol invasions displaced populations on a scale never seen before in central Asia or eastern Europe. Word of the Mongol hordes' approach spread terror and panic. The violent character of the invasions acted as a catalyst for further violence between Europe's elites and sparked additional conflicts. The increase in violence in the affected eastern European regions correlates with a decrease in the elite's numerical skills, and has been postulated as a root of the Great Divergence.
From 1221 to 1327, the Mongol Empire launched several invasions into the Indian subcontinent. The Mongols occupied parts of Punjab region for decades. However, they failed to penetrate past the outskirts of Delhi and were repelled from the interior of India. Centuries later, the Mughals, whose founder Babur had Mongol roots, established their own empire in India.
Kublai Khan's Yuan dynasty invaded Burma between 1277 and 1287, resulting in the capitulation and disintegration of the Pagan Kingdom. However, the invasion of 1301 was repulsed by the Burmese Myinsaing Kingdom. The Mongol invasions of Vietnam (Đại Việt) and Java resulted in defeat for the Mongols, although much of Southeast Asia agreed to pay tribute to avoid further bloodshed.
The Mongol invasions played an indirect role in the establishment of major Tai states in the region by recently migrated Tais, who originally came from Southern China, in the early centuries of the second millennium. Major Tai states such as Lan Na, Sukhothai, and Lan Xang appeared around this time.
Due to the lack of contemporary records, estimates of the violence associated with the Mongol conquests vary considerably. Not including the mortality from the Plague in Europe, West Asia, or China it is possible that between 20 and 57 million people were killed between 1206 and 1405 during the various campaigns of Genghis Khan, Kublai Khan, and Timur. The havoc included battles, sieges, early biological warfare, and massacres.

There are no forbidden words or expressions on Wikipedia, but certain expressions should be used with caution because they may introduce bias. Strive to eliminate expressions that are flattering, disparaging, vague, clichéd, or endorsing of a particular viewpoint.
The advice in this guideline is not limited to the examples provided and should not be applied rigidly.  If a word can be replaced by one with less potential for misunderstanding, it should be.  Some words have specific technical meanings in some contexts and are acceptable in those contexts, e.g. claim in law.  What matters is that articles should be well-written and be consistent with the core content policies – Wikipedia:Neutral point of view, Wikipedia:No original research, and Wikipedia:Verifiability. The guideline does not apply to quotations, which should be faithfully reproduced from the original sources (.mw-parser-output div.crossreference{padding-left:0}see Wikipedia:Manual of Style § Quotations).
If you do not feel you can improve the problematic wording of an article yourself, a template message can be added to draw the attention of other editors to an article needing a cleanup.

Words to watch: legendary, best, great, acclaimed, iconic, visionary, outstanding, leading, celebrated, popular, award-winning, landmark, cutting-edge, innovative, revolutionary, extraordinary, brilliant, hit, famous, renowned, remarkable, prestigious, world-class, respected, notable, virtuoso, honorable, awesome, unique, pioneering, phenomenal ... 
Words such as these are often used without attribution to promote the subject of an article, while neither imparting nor plainly summarizing verifiable information. They are known as "peacock terms" by Wikipedia contributors. Instead of making subjective proclamations about a subject's importance, use facts and attribution to demonstrate it.
An article suffering from such language should be rewritten to correct the problem or, if an editor is unsure how best to make a correction, the article may be tagged with an appropriate template, such as {{Peacock term}}.
Puffery is an example of positively loaded language; negatively loaded language should be avoided just as much. People responsible for "public spending" (the neutral term) can be loaded both ways, as "tax-and-spend politicians borrowing off the backs of our grandchildren" or "public servants ensuring crucial investment in our essential infrastructure for the public good".
Words to watch: cult, racist, perverted, sexist, homophobic, transphobic, misogynistic, sect, fundamentalist, heretic, extremist, denialist, terrorist, freedom fighter, bigot, myth, neo-Nazi, -gate, pseudo-, controversial ... 
Value-laden labels – such as calling an organization a cult, an individual a racist, sexist, terrorist, or freedom fighter, or a sexual practice a perversion – may express contentious opinion and are best avoided unless widely used by reliable sources to describe the subject, in which case use in-text attribution. Avoid myth in its informal sense, and establish the scholarly context for any formal use of the term.
The prefix pseudo- indicates something false or spurious, which may be debatable. The suffix ‑gate suggests the existence of a scandal. Use these in articles only when they are in wide use externally, e.g. Gamergate (harassment campaign), with in-text attribution if in doubt. Rather than describing an individual using the subjective and vague term controversial, instead give readers information about relevant controversies. Make sure, as well, that reliable sources establish the existence of a controversy and that the term is not used to grant a fringe viewpoint undue weight.
For the term pseudoscience: per the policy Wikipedia:Neutral point of view, pseudoscientific views "should be clearly described as such". Per the content guideline Wikipedia:Fringe theories, the term pseudoscience, if supported by reliable sources, may be used to distinguish fringe theories from mainstream science.
For additional guidance on -ist/-ism terms, see § Neologisms and new compounds, below.
Words to watch: some people say, many scholars state, it is believed/regarded/considered, many are of the opinion, most feel, experts declare, it is often reported, it is widely thought, research has shown, science says, scientists claim, it is often said, officially, is widely regarded as, X has been described as Y ... 
Weasel words are words and phrases aimed at creating an impression that something specific and meaningful has been said, when in fact only a vague or ambiguous claim has been communicated. A common form of weasel wording is through vague attribution, where a statement is dressed with authority, yet has no substantial basis. Phrases such as those above present the appearance of support for statements but can deny the reader the opportunity to assess the source of the viewpoint. They may disguise a biased view. Claims about what people say, think, feel, or believe, and what has been shown, demonstrated, or proved should be clearly attributed.
The examples above are not automatically weasel words. They may also be used in the lead section of an article or in a topic sentence of a paragraph, and the article body or the rest of the paragraph can supply attribution. Likewise, views that are properly attributed to a reliable source may use similar expressions, if those expressions accurately represent the opinions of the source. Reliable sources may analyze and interpret, but for editors to do so would violate the Wikipedia:No original research or Wikipedia:Neutral point of view policies. Equally, editorial irony such as "Despite the fact that fishermen catch fish, they don't tend to find any" and damning with faint praise, like "It is known that person X is skilled in golf, but is inferior to person Y." have no place in Wikipedia articles.
Articles including weasel words should ideally be rewritten such that they are supported by reliable sources; alternatively, they may be tagged with the {{Weasel}}, {{By whom}}, or similar templates to identify the problem to future readers (who may elect to fix the problem).
Words to watch: supposed, apparent, purported, alleged, accused, so-called ...   Also, scare-quoting: a Yale "report"; undue emphasis: "... a Baptist church" 
Words such as supposed, apparent, alleged, and purported can imply that a given point is inaccurate, although alleged and accused are appropriate when wrongdoing is asserted but undetermined, such as with people awaiting or undergoing a criminal trial; when these are used, ensure that the source of the accusation is clear. So-called can mean commonly named, falsely named, or contentiously named, and it can be difficult to tell these apart. Simply called is preferable for the first meaning; detailed and attributed explanations are preferable for the others.
Misused punctuation can also have similar effects. Quotation marks, when not marking an actual quotation, may be interpreted as "scare quotes", indicating that the writer is distancing themselves from the otherwise common interpretation of the quoted expression. The use of emphasis may turn an innocuous word into a loaded expression, so such occurrences should also be considered carefully.
Words to watch: notably, it should be noted, arguably, interestingly, essentially, utterly, actually, clearly, absolutely, of course, without a doubt, indeed, happily, sadly, tragically, aptly, fortunately, unfortunately, untimely ... 
Use of adverbs such as notably and interestingly, and phrases such as it should be noted, to highlight something as particularly significant or certain without attributing that opinion, should usually be avoided so as to maintain impartial tone. Words such as fundamentally, essentially, and basically can indicate particular interpretive viewpoints and thus should also be attributed in controversial cases. Care should be used with actually, which implies something contrary to expectations; make sure this is verifiable and not just assumed. Clearly, obviously, naturally, and of course all presume too much about the reader's knowledge and perspective and often amount to verbiage. Wikipedia should not take a view on whether an event was fortunate or not.
This kind of persuasive writing approach is also against the Wikipedia:No original research policy (Wikipedia does not try to steer the reader to a particular interpretation or conclusion) and the Instructional and presumptuous language guideline (Wikipedia does not break the fourth wall and write at the reader, other than with navigational hatnotes.)
Words to watch: but, despite, however, though, although, furthermore, while ... 
More subtly, editorializing can produce implications that are not supported by the sources. When used to link two statements, words such as but, despite, however, and although may imply a relationship where none exists, possibly unduly calling the validity of the first statement into question while giving undue weight to the credibility of the second.
Words to watch: reveal, point out, clarify, expose, explain, find, note, observe, insist, speculate, surmise, claim, assert, admit, confess, deny ... 
In some types of writing, repeated use of said is considered tedious, and writers are encouraged to employ synonyms (see WP:The problem with elegant variation). But on Wikipedia, it is more important to avoid language that makes undue implications.
Said, stated, described, wrote, commented, and according to are almost always neutral and accurate. Extra care is needed with more loaded terms. For example, to write that a person clarified, explained, exposed, found, pointed out, showed, or revealed something can imply it is true, instead of simply conveying the fact that it was said. To write that someone insisted, noted, observed, speculated, or surmised can suggest the degree of the person's carefulness, resoluteness, or access to evidence, even when such things are unverifiable.
To say that someone asserted or claimed something can call their statement's credibility into question, by emphasizing any potential contradiction or implying disregard for evidence. Similarly, be judicious in using admit, confess, reveal, and deny, particularly for living persons, because these verbs can inappropriately imply culpability.
Words to watch: passed away, gave her life, eternal rest, make love, an issue with, collateral damage ... 
Euphemisms should generally be avoided in favor of more neutral and precise terms. Died and had sex are neutral and accurate; passed away and made love are euphemisms. Some words and phrases that are proper in many contexts also have euphemistic senses that should be avoided: civilian casualties should not be masked as collateral damage.
If a person has an affliction, or is afflicted, say just that. See Wikipedia:Manual of Style/Medicine-related articles § Careful language  for more guidance on writing about medical conditions.
Norms vary for expressions about disabilities and disabled people. Do not assume that plain language is inappropriate. The goal is to express ideas clearly and directly without causing unnecessary offense. See also this essay by editors involved in WikiProject Disability.
Words to watch: lion's share, tip of the iceberg, white elephant, gild the lily, take the plunge, ace up the sleeve, bird in the hand, twist of fate, at the end of the day ... 
Clichés and idioms are generally to be avoided in favor of direct, literal expressions. Lion's share is often misunderstood; instead use a term such as all, most, two-thirds, or whatever matches the context. The tip of the iceberg should be reserved for discussions of icebergs.  If something is seen as wasteful excess, do not call it gilding the lily or white elephant; instead, describe the wasteful thing in terms of the actions or events that led to the excess. Instead of writing that someone took the plunge, state their action matter-of-factly.
In general, if a literal reading of a phrase makes no sense given the context, the sentence needs rewording. Some idioms are only common in certain parts of the world, and many readers are not native speakers of English; articles should not presume familiarity with particular phrases. Wiktionary has a long list of English idioms, some of which should be avoided.
Words to watch: recently, lately, currently, today, presently, to date, 15 years ago, formerly, in the past, traditionally, this/last/next (year/month/winter/spring/summer/fall/autumn), yesterday, tomorrow, in the future, now, soon, since ... 
Absolute specifications of time are preferred to relative constructions using recently, currently, and so on, because the latter may go out of date. "By July 2022 contributions had dropped" has the same meaning as "Recently, contributions have dropped" but the first sentence retains its meaning as time passes. And recently type constructions may be ambiguous even at the time of writing: Was it in the last week? Month? Year? The information that "The current president, Cristina Fernández, took office in 2007", or "Cristina Fernández has been president since 2007", is better rendered "Cristina Fernández became president in 2007". Wordings such as "17 years ago" or "Jones is 65 years old" should be rewritten as "in 2005", "Jones was 65 years old at the time of the incident", or "Jones was born in 1957." If a direct quote contains relative time, ensure the date of the quote is clear, such as "Joe Bloggs in 2007 called it 'one of the best books of the last decade'." 
When material in an article may become out of date, follow the Wikipedia:As of guideline, which allows information to be written in a less time-dependent way. There are also several templates for alerting readers to time-sensitive wording problems.
Expressions like "former(ly)", "in the past", and "traditional(ly)" lump together unspecified periods in the past. "Traditional" is particularly pernicious because it implies immemorial established usage. It is better to use explicit dates supported by sources. Instead of "hamburgers are a traditional American food," say "the hamburger was invented in about 1900 and became widely popular in the United States in the 1930s." Because seasons differ between the northern and southern hemisphere, try to use months, quarters, or other non-seasonal terms such as mid-year unless the season itself is pertinent (spring blossoms, autumn harvest); see Wikipedia:Manual of Style/Dates and numbers § Seasons of the year.
Words to watch: this country, here, there, somewhere, sometimes, often, occasionally, somehow ... 
As in the previous section, prefer specific statements to general ones. It is better to use explicit descriptions, based on reliable sources, of when, where, or how an event occurred. Instead of saying "In April 2012, Senator Smith somehow managed to increase his approval rating by 10%", say "In April 2012, Senator Smith's approval rating increased by 10%, which respondents attributed to his new position on foreign policy." Instead of saying "Senator Smith often discusses foreign policy in his speeches", say "Senator Smith discussed foreign policy during his election campaign, and subsequently during his victory speech at the State Convention Center."
Remember that Wikipedia is a global encyclopedia, and does not assume particular places or times are the "default". We emphasize facts and viewpoints to the same degree that they are emphasized by the reliable sources. Terms like this country should not be used.
Words to watch: is/was survived by, 's survivors include,  ... 
Phrasing such as "Smith died in 1982, survived by her husband Jack and two sons" should be avoided; this information can be made more complete and spread out through the article. The "survived by" phrasing is a common way to end newspaper obituaries and legal death notices, and is relevant at the time of death or for inheritance purposes. But an encyclopedia article covers the subject's entire life, not just the event of their death. Information about children and spouses might be presented in an infobox or in sections about the subject's personal life. Readers can generally infer which family members died after the subject. Usually this information is not worth highlighting explicitly, except for unusual situations (for example where children predecease their parents, or where the inheritance was disputed).
Even in a stub article, a different arrangement with more details sounds more like an encyclopedia and less like an obituary: "Smith married Jack in 1957. The couple had two sons, Bill and Ted. She died in 1982."
It is necessary for a reference work to distinguish carefully between an office (such as president of the United States) and an incumbent (such as Joe Biden); a newspaper does not usually need to make this distinction, for a newspaper "President Biden" and "the President" are one and the same during his tenure.
Neologisms are expressions coined recently or in isolated circumstances to which they have remained restricted. In most cases, they do not appear in general-interest dictionaries, though they may be used routinely within certain communities or professions. They should generally be avoided because their definitions tend to be unstable and many do not last. Where the use of a neologism is necessary to describe recent developments in a certain field, its meaning must be supported by reliable sources.
Adding common prefixes or suffixes such as pre-, post-, non-, anti-, or -like to existing words to create new compounds can aid brevity, but make sure the resulting terms are not misleading or offensive, and that they do not lend undue weight to a point of view. For instance, adding -ism or -ist to a word may suggest that a tenuous belief system is well-established, that a belief's adherents are particularly dogmatic or ideological (as in abortionism), or that factual statements are actually a matter of doctrine (as in evolutionism).  Some words, by their structure, can suggest extended forms that may turn out to be contentious (e.g. lesbian and transgender imply the longer words lesbianism and transgenderism, which are sometimes taken as offensive for seeming to imply a belief system or agenda).
For additional guidance on -ist/-ism terms, see § Contentious labels, above.
Do not use similar or related words in a way that blurs meaning or is incorrect or distorting.
For example, the adjective Arab refers to people and things of ethnic Arab origin. The term Arabic generally refers to the Arabic language or writing system, and related concepts. Arabian relates to the Arabian peninsula or historical Arabia. (These terms are all capitalized, e.g. Arabic script and Arabian horse, aside from a few conventionalized exceptions that have lost their cultural connection, such as gum arabic.) Do not substitute these terms for Islamic, Muslim, Islamist, Middle-eastern, etc.; a Muslim Arab is someone who is in both categories.
Similar concerns pertain to many cultural, scientific, and other topics and the terminology used about them. When in doubt about a term, consult major modern dictionaries.
Wikipedia is not censored, and the inclusion of material that might offend is part of its purpose as an encyclopedia. Quotes should always be verbatim and as they appear in the original source. However, language that is vulgar, obscene, or profane should be used only if its omission would make an article less accurate or relevant, and if there is no non-obscene alternative. Such words should not be used outside quotations and names except where they are themselves an article topic.

Fireworks are a class of low explosive pyrotechnic devices used for aesthetic and entertainment purposes. The most common use of a firework is as part of a fireworks display (also called a fireworks show or pyrotechnics), a display of the effects produced by firework devices.
Fireworks take many forms to produce the four primary effects: noise, light, smoke, as well as floating materials (confetti most notably). They may be designed to burn with colored flames and sparks including red, orange, yellow, green, blue, purple and silver. Displays are common throughout the world and are the focal point of many cultural and religious celebrations.
Fireworks are generally classified as to where they perform, either as a ground or aerial firework. In the latter case they may provide their own propulsion (skyrocket) or be shot into the air by a mortar (aerial shell).
The most common feature of fireworks is a paper or pasteboard tube or casing filled with the combustible material, often pyrotechnic stars. A number of these tubes or cases are often combined so as to make when kindled, a great variety of sparkling shapes, often variously colored. A skyrocket is a common form of firework, although the first skyrockets were used in warfare. The aerial shell, however, is the backbone of today's commercial aerial display, and a smaller version for consumer use is known as the festival ball in the United States.
Fireworks were originally invented in China. Cultural events and festivities such as the Chinese New Year and the Mid-Autumn Moon Festival were and still are times when fireworks are guaranteed sights. China is the largest manufacturer and exporter of fireworks in the world.
Silent fireworks are becoming popular for providing all the beauty without the added explosive sounds imitating artillery and warfare that traumatize pets, wildlife, and many humans. The Italian town of Collecchio switched to silent fireworks in 2015, mandating the switch.
The earliest fireworks came from China during the Song dynasty (960–1279). Fireworks were used to accompany many festivities. The art and science of firework making has developed into an independent profession. In China, pyrotechnicians were respected for their knowledge of complex techniques in mounting firework displays.
During the Han dynasty (202 BC – 220 AD), people threw bamboo stems into a fire to produce an explosion with a loud sound. In later times, gunpowder packed into small containers was used to mimic the sounds of burning bamboo. Exploding bamboo stems and gunpowder firecrackers were interchangeably known as baozhu (爆竹) or baogan (爆竿). It was during the Song dynasty that people manufactured the first firecrackers comprising tubes made from rolled sheets of paper containing gunpowder and a fuse. They also strung these firecrackers together into large clusters, known as bian (lit. "whip") or bianpao (lit. "whip cannon"), so the firecrackers could be set off one by one in close sequence. By the 12th and possibly the 11th century, the term baozhang (爆仗) was used to specifically refer to gunpowder firecrackers.
During the Song dynasty, many of the common people could purchase various kinds of fireworks from market vendors. Grand displays of fireworks were also known to be held. In 1110, a large fireworks display in a martial demonstration was held to entertain Emperor Huizong of Song (r. 1100–1125) and his court. A record from 1264 states that a rocket-propelled firework went off near the Empress Dowager Gong Sheng and startled her during a feast held in her honor by her son Emperor Lizong of Song (r. 1224–1264). Rocket propulsion was common in warfare, as evidenced by the Huolongjing compiled by Liu Bowen (1311–1375) and Jiao Yu (fl. c. 1350–1412). In 1240 the Arabs acquired knowledge of gunpowder and its uses from China. A Syrian named Hasan al-Rammah wrote of rockets, fireworks, and other incendiaries, using terms that suggested he derived his knowledge from Chinese sources, such as his references to fireworks as "Chinese flowers".
In regards to colored fireworks, this was derived and developed from earlier (possibly Han dynasty or soon thereafter) Chinese application of chemical substances to create colored smoke and fire. Such application appears in the Huolongjing (14th century) and Wubeizhi (preface of 1621, printed 1628), which describes recipes, several of which used low-nitrate gunpowder, to create military signal smokes with various colors. In the Wubei Huolongjing (武備火龍經; Ming, completed after 1628), two formulas appears for firework-like signals, the sanzhangju (三丈菊) and baizhanglian (百丈蓮), that produces silver sparkles in the smoke. In the Huoxilüe (火戲略; 1753) by Zhao Xuemin (趙學敏), there are several recipes with low-nitrate gunpowder and other chemical substances to tint flames and smoke. These included, for instance, arsenical sulphide for yellow, copper acetate (verdigris) for green, lead carbonate for lilac-white, and mercurous chloride (calomel) for white. The Chinese pyrotechnics were described by the French author Antoine Caillot (1818): "It is certain that the variety of colours which the Chinese have the secret of giving to flame is the greatest mystery of their fireworks." Similarly, the English geographer Sir John Barrow (ca. 1797) wrote "The diversity of colours indeed with which the Chinese have the secret of cloathing fire seems to be the chief merit of their pyrotechny."
Fireworks were produced in Europe by the 14th century, becoming popular by the 17th century. Lev Izmailov, ambassador of Peter the Great, once reported from China: "They make such fireworks that no one in Europe has ever seen." In 1758, the Jesuit missionary Pierre Nicolas le Chéron d'Incarville, living in Beijing, wrote about the methods and composition on how to make many types of Chinese fireworks to the Paris Academy of Sciences, which revealed and published the account five years later. Amédée-François Frézier published his revised work Traité des feux d'artice pour le spectacle (Treatise on Fireworks) in 1747 (originally 1706), covering the recreational and ceremonial uses of fireworks, rather than their military uses. Music for the Royal Fireworks was composed by George Frideric Handel in 1749 to celebrate the Peace treaty of Aix-la-Chapelle, which had been declared the previous year.
"Prior to the nineteenth century and the advent of modern chemistry they  must have been relatively dull and unexciting." Bertholet in 1786 discovered that oxidations with potassium chlorate resulted in a violet emission. Subsequent developments revealed that oxidations with the chlorates of barium, strontium, copper, and sodium result in intense emission of bright colors. The isolation of metallic magnesium and aluminium marked another breakthrough as these metals burn with an intense silvery light.
Improper use of fireworks may be dangerous, both to the person operating them (risks of burns and wounds) and to bystanders; in addition, they may start fires after landing on flammable material. For this reason, the use of fireworks is generally legally restricted. Display fireworks are restricted by law for use by professionals; consumer items, available to the public, are smaller versions containing limited amounts of explosive material to reduce potential danger.
Fireworks are also a problem for animals, both domestic and wild, which can be frightened by their noise, leading to them running away, often into danger, or hurting themselves on fences or in other ways in an attempt to escape. Frightened birds also may abandon nests and not return to complete rearing their young.
Pyrotechnical competitions involving fireworks are held in many countries. The most prestigious fireworks competition is the Montreal Fireworks Festival, an annual competition held in Montreal, Quebec, Canada. Another magnificent competition is Le Festival d'Art Pyrotechnique held in the summer annually at the Bay of Cannes in Côte d'Azur, France. The World Pyro Olympics is an annual competition amongst the top fireworks companies in the world. It is held in Manila, Philippines. The event is one of the largest and most intense international fireworks competitions.
Enthusiasts in the United States have formed clubs which unite hobbyists and professionals. The groups provide safety instruction and organize meetings and private "shoots" at remote premises where members shoot commercial fireworks as well as fire pieces of their own manufacture. Clubs secure permission to fire items otherwise banned by state or local ordinances. Competition among members and between clubs, demonstrating everything from single shells to elaborate displays choreographed to music, are held.
One of the oldest clubs is Crackerjacks, Inc., organized in 1976 in the Eastern Seaboard region of the U.S.
The Pyrotechnics Guild International, Inc. or PGI, founded in 1969, is an independent worldwide nonprofit organization of amateur and professional fireworks enthusiasts. It is notable for its large number of members, around 3,500 in total. The PGI exists solely to further the safe usage and enjoyment of both professional grade and consumer grade fireworks while both advancing the art and craft of pyrotechnics and preserving its historical aspects. Each August the PGI conducts its annual week-long convention, where some the world's biggest and best fireworks displays occur. Vendors, competitors, and club members come from around the US and from various parts of the globe to enjoy the show and to help out at this all-volunteer event. Aside from the nightly firework shows, the competition is a highlight of the convention. This is a completely unique event where individual classes of hand-built fireworks are competitively judged, ranging from simple fireworks rockets to extremely large and complex aerial shells. Some of the biggest, best, most intricate fireworks displays in the United States take place during the convention week.
Amateur and professional members can come to the convention to purchase fireworks, paper goods, novelty items, non-explosive chemical components and much more at the PGI trade show. Before the nightly fireworks displays and competitions, club members have a chance to enjoy open shooting of any and all legal consumer or professional grade fireworks, as well as testing and display of hand-built fireworks. The week ends with the Grand Public Display on Friday night, which gives the chosen display company a chance to strut their stuff in front of some of the world's biggest fireworks aficionados. The stakes are high and much planning is put into the show. In 1994 a shell of 36 inches (914 mm) in diameter was fired during the convention, more than twice as large as the largest shell usually seen in the US, and shells as large as 24 inches (610 mm) are frequently fired.
Both fireworks and firecrackers are a popular tradition during Halloween in Vancouver, although apparently this is not the custom elsewhere in Canada.
In the Republic of Ireland and Northern Ireland there are many fireworks displays, during the Halloween season. The largest are in the cities of Belfast, Derry, and Dublin. The 2010 Derry Halloween fireworks attracted an audience of more than 20,000 people. The sale of fireworks is strongly restricted in the Republic of Ireland, although many illegal fireworks are sold throughout October or smuggled from Northern Ireland. In the Republic the maximum punishment for possessing fireworks without a licence, or lighting fireworks in a public place, is a €10,000 fine  and a five-year prison sentence.
Two firework displays on All Hallows' Eve in the United States are the annual "Happy Hallowishes" show at Walt Disney World's Magic Kingdom "Mickey's Not-So-Scary Halloween Party" event, which began in 2005, and the "Halloween Screams" at Disneyland Park, which began in 2009.
In Australia, fireworks displays are used in the public celebration of major events such as New Year's Eve and Australia Day. Notable annual fireworks events include the Sydney New Year's Eve Midnight Fireworks show and the City of Perth Skyworks. In the Northern Territory, "Cracker Night" is celebrated every 1 July on Territory Day, where residents are allowed to buy and use fireworks without a permit.
In France, fireworks are traditionally displayed on the eve of Bastille day (14 July) to commemorate the French revolution and the storming of the Bastille on that same day in 1789. Every city in France lights up the sky for the occasion with a special mention to Paris that offers a spectacle around the Eiffel Tower.
In Hungary fireworks are used on 20 August, which is a national celebration day 
Indians throughout the world celebrate with fireworks as part of their popular "festival of lights" (Diwali) in Oct-Nov every year.
During the summer in Japan, fireworks festivals (花火大会, hanabi taikai) are held nearly every day someplace in the country, in total numbering more than 200 during August. The festivals consist of large fireworks shows, the largest of which use between 100,000 and 120,000 rounds (Tondabayashi, Osaka), and can attract more than 800,000 spectators. Street vendors set up stalls to sell various drinks and staple Japanese food (such as yakisoba, okonomiyaki, takoyaki, kakigōri (shaved ice), and traditionally held festival games, such as kingyo-sukui, or goldfish scooping.
Even today, men and women attend these events wearing the traditional yukata, summer kimono, or jinbei, and gather in large social circles of family or friends to sit picnic-like, eating and drinking, while watching the show.
The first fireworks festival in Japan was held in 1733.
Sumidagawa Fireworks Festival is one of the many being celebrated annually throughout Japan in summer.
Fireworks have been used in Malta for hundreds of years. When the islands were ruled by the Order of St John, fireworks were used on special occasions such as the election of a new Grand Master, the appointment of a new Pope or the birth of a prince.
Nowadays, fireworks are used in village feasts throughout the summer. The Malta International Fireworks Festival is also held annually.
Pyrotechnics experts from around the world have competed in Monte Carlo, Monaco, since 1966. The festival runs from July to August every year, and the winner returns in 18 November for the fireworks display on the night before the National Day of Monaco. The event is held in Port Hercule, beginning at around 9:30pm every night, depending on the sunset.
The Singapore Fireworks Celebrations (previously the Singapore Fireworks Festival) is an annual event held in Singapore as part of its National Day celebrations. The festival features local and foreign teams which launch displays on different nights. While currently non-competitive in nature, the organizer has plans to introduce a competitive element in the future.
The annual festival has grown in magnitude, from 4,000 rounds used in 2004, to 6,000 in 2005, to more than 9,100 in 2006.
Busan International Fireworks Festival is one of the most significant fireworks festivals in Asia.
In Switzerland fireworks are often used on 1 August, which is a national celebration day.
One of the biggest occasions for fireworks in the UK is Guy Fawkes Night held each year on 5 November, to celebrate the foiling of the Catholic Gunpowder Plot on 5 November 1605, an attempt to kill King James I. The Guardian newspaper said in 2008 that Britain's biggest Guy Fawkes night events were:
The main firework celebrations in the UK are by the public who buy from many suppliers.
America's earliest settlers brought their enthusiasm for fireworks to the United States. Fireworks and black ash were used to celebrate important events long before the American Revolutionary War. The very first celebration of Independence Day was in 1777, six years before Americans knew whether or not the new nation would survive the war; fireworks were a part of all festivities. In 1789, George Washington's inauguration was accompanied by a fireworks display.. George Marshall was an American naval hero during the War of 1812 and other campaigns. He was a Master Gunner and pyrotechnics specialist who wrote Marshall's Practical Marine Gunnery  in 1822.  The book outlines chemical formulas for the composition of fireworks.  This early fascination with fireworks' noise and color continues today with fireworks displays commonly included in Independence Day celebrations.
In 2004, Disneyland, in Anaheim, California, pioneered the commercial use of aerial fireworks launched with compressed air rather than gunpowder. The display shell explodes in the air using an electronic timer. The advantages of compressed air launch are a reduction in fumes, and much greater accuracy in height and timing. The Walt Disney Company is now the largest consumer of fireworks in the world.
In addition to large public displays, people often buy small quantities of fireworks for their own celebrations. Fireworks on general sale are usually less powerful than professional fireworks. Types include firecrackers, rockets, cakes (multishot aerial fireworks), and smoke balls.
Fireworks can also be used in an agricultural capacity as to frighten away birds.
Colors in fireworks are usually generated by pyrotechnic stars—usually just called stars—which produce intense light when ignited. Stars contain four basic types of ingredients.
Some of the more common color-producing compounds are tabulated here. The color of a compound in a firework will be the same as its color in a flame test (shown at right). Not all compounds that produce a colored flame are appropriate for coloring fireworks, however. Ideal colorants will produce a pure, intense color when present in moderate concentration.
The color of sparks is limited to red/orange, yellow/gold and white/silver. This is explained by light emission from an incandescent solid particle in contrast to the element-specific emission from the vapor phase of a flame. Light emitted from a solid particle is defined by black-body radiation. Low boiling metals can form sparks with an intensively colored glowing shell surrounding the basic particle. This is caused by vapor phase combustion of the metal.
Lithium (medium red)
Li2CO3 (lithium carbonate) LiCl (lithium chloride)
Rubidium (violet-red)
RbNO3 (rubidium nitrate)
The brightest stars, often called Mag Stars, are fueled by aluminium. Magnesium is rarely used in the fireworks industry due to its lack of ability to form a protective oxide layer. Often an alloy of both metals called magnalium is used.
Many of the chemicals used in the manufacture of fireworks are non-toxic, while many more have some degree of toxicity, can cause skin sensitivity, or exist in dust form and are thereby inhalation hazards. Still others are poisons if directly ingested or inhaled.
The following table lists the principal elements used in modern pyrotechnics. Some elements are used in their elemental form such as particles of titanium, aluminium, iron, zirconium, and magnesium. These elements burn in the presence of air (O2) or oxidants (perchlorate, chlorate). Most elements in pyrotechnics are in the form of salts.
A cake is a cluster of individual tubes linked by fuse that fires a series of aerial effects. Tube diameters can range in size from .mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}1⁄4–4 inches (6.4–101.6 mm), and a single cake can have more than 1,000 shots. The variety of effects within individual cakes is often such that they defy descriptive titles and are instead given cryptic names such as "Bermuda Triangle", "Pyro Glyphics", "Waco Wakeup", and "Poisonous Spider", to name a few. Others are simply quantities of 2.5–4 in (64–102 mm) shells fused together in single-shot tubes.
A shell containing several large stars that travel a short distance before breaking apart into smaller stars, creating a crisscrossing grid-like effect. Strictly speaking, a crossette star should split into 4 pieces which fly off symmetrically, making a cross. Once limited to silver or gold effects, colored crossettes such as red, green, or white are now very common.
A spherical break of colored stars, similar to a peony, but with stars that leave a visible trail of sparks.
Essentially the same as a peony shell, but with fewer and larger stars. These stars travel a longer-than-usual distance from the shell break before burning out. For instance, if a 3 in (76 mm) peony shell is made with a star size designed for a 6 in (152 mm)  shell, it is then considered a dahlia. Some dahlia shells are cylindrical rather than spherical to allow for larger stars.
A type of Chrysanthemum or Peony, with a center cluster of non-moving stars, normally of a contrasting color or effect.
Inserts that propel themselves rapidly away from the shell burst, often resembling fish swimming away.
Named for the shape of its break, this shell features heavy long-burning tailed stars that only travel a short distance from the shell burst before free-falling to the ground. Also known as a waterfall shell. Sometimes there is a glittering through the "waterfall".
Kamuro is a Japanese word meaning "boys haircut", which is what this shell resembles when fully exploded in the air. It is a dense burst of glittering silver or gold stars which leave a heavy glitter trail and shine bright in the night's sky.
A mine (a.k.a. pot à feu) is a ground firework that expels stars and/or other garnitures into the sky. Shot from a mortar like a shell, a mine consists of a canister with the lift charge on the bottom with the effects placed on top. Mines can project small reports, serpents, small shells, as well as just stars. Although mines up to 12 inches (305 mm) diameter appear on occasion, they are usually between 3–5 inches (76–127 mm) in diameter.
A large shell containing several smaller shells of various sizes and types. The initial burst scatters the shells across the sky before they explode. Also called a bouquet shell. When a shell contains smaller shells of the same size and type, the effect is usually referred to as "Thousands". Very large bouquet shells (up to 48 inches ) are frequently used in Japan.
A shell containing a relatively few large comet stars arranged in such a way as to burst with large arms or tendrils, producing a palm tree-like effect. Proper palm shells feature a thick rising tail that displays as the shell ascends, thereby simulating the tree trunk to further enhance the "palm tree" effect. One might also see a burst of color inside the palm burst (given by a small insert shell) to simulate coconuts.
A spherical break of colored stars that burn without a tail effect. The peony is the most commonly seen shell type.
A shell with stars specially arranged so as to create a ring. Variations include smiley faces, hearts, and clovers.
A Roman candle is a long tube containing several large stars which fire at a regular interval. These are commonly arranged in fan shapes or crisscrossing shapes, at a closer proximity to the audience. Some larger Roman candles contain small shells (bombettes) rather than stars.
A shell intended to produce a loud report rather than a visual effect. Salute shells usually contain flash powder, producing a quick flash followed by a very loud report resembling military artillery. Titanium may be added to the flash powder mix to produce a cloud of bright sparks around the flash. Salutes are commonly used in large quantities during finales to create intense noise and brightness. They are often cylindrical in shape to allow for a larger payload of flash powder, but ball shapes are common and cheaper as well. Salutes are also called Maroons.
A shell containing a fast burning tailed or charcoal star that is burst very hard so that the stars travel in a straight and flat trajectory before slightly falling and burning out. This appears in the sky as a series of radial lines much like the legs of a spider.
An effect created by large, slow-burning stars within a shell that leave a trail of large glittering sparks behind and make a sizzling noise. The "time" refers to the fact that these stars burn away gradually, as opposed to the standard brocade "rain" effect where a large amount of glitter material is released at once.
Similar to a chrysanthemum, but with long-burning silver or gold stars that produce a soft, dome-shaped weeping willow-like effect.
Farfalle is an effect in Italian fireworks with spinning silver sprays in the air.
Similar to a Farfalle but has spinning stars
Fireworks pose risks of injury to people, and of damage, largely as a fire hazard. The explosions added to fireworks may frighten and traumatize animals and people. Wildlife may die while fleeing in a panic and in affected areas birds may abandon nests containing their young.
Fireworks produce smoke and dust that may contain residues of heavy metals, sulfur-coal compounds and some low concentration toxic chemicals. These by-products of fireworks combustion will vary depending on the mix of ingredients of a particular firework. (The color green, for instance, may be produced by adding the various compounds and salts of barium, some of which are toxic, and some of which are not.) Some fishers have noticed and reported to environmental authorities that firework residues can hurt fish and other water-life because some may contain toxic compounds (such as antimony sulfide or arsenic). This is a subject of much debate due to the fact that large-scale pollution from other sources makes it difficult to measure the amount of pollution that comes specifically from fireworks. The possible toxicity of any fallout may also be affected by the amount of black powder used, type of oxidizer, colors produced and launch method.
Perchlorate salts, when in solid form, dissolve and move rapidly in groundwater and surface water. Even in low concentrations in drinking water supplies, perchlorate ions are known to inhibit the uptake of iodine by the thyroid gland. As of 2010, there are no federal drinking water standards for perchlorates in the United States, but the
US Environmental Protection Agency has studied the impacts of perchlorates on the environment as well as drinking water.
Several U.S. states have enacted drinking water standard for perchlorates, including Massachusetts in 2006. California's legislature enacted AB 826, the Perchlorate Contamination Prevention Act of 2003, requiring California's Department of Toxic Substance Control (DTSC) to adopt regulations specifying best management practices for perchlorate-containing substances. The Perchlorate Best Management Practices were adopted on 31 December 2005 and became operative on 1 July 2006. California issued drinking water standards in 2007. Several other states, including Arizona, Maryland, Nevada, New Mexico, New York, and Texas have established non-enforceable, advisory levels for perchlorates.
The courts have also taken action with regard to perchlorate contamination. For example, in 2003, a federal district court in California found that Comprehensive Environmental Response, Compensation and Liability Act (CERCLA) applied because perchlorate is ignitable and therefore a "characteristic" hazardous waste.
Pollutants from fireworks raise concerns because of potential health risks associated with hazardous by-products. For most people the effects of exposure to low levels of toxins from many sources over long periods are unknown. For persons with asthma or multiple chemical sensitivity the smoke from fireworks may aggravate existing health problems.
Pollution is also a concern because fireworks often contain heavy metals as source of color. However, gunpowder smoke and the solid residues are basic, and as such the net effect of fireworks on acid rain is debatable. What is not disputed is that most consumer fireworks leave behind a considerable amount of solid debris, including both readily biodegradable components as well as nondegradable plastic items. Concerns over pollution, consumer safety, and debris have restricted the sale and use of consumer fireworks in many countries. Professional displays, on the other hand, remain popular around the world.
Others argue that alleged concern over pollution from fireworks constitutes a red herring, since the amount of contamination from fireworks is minuscule in comparison to emissions from sources such as the burning of fossil fuels. In the US, some states and local governments restrict the use of fireworks in accordance with the Clean Air Act which allows laws relating to the prevention and control of outdoor air pollution to be enacted. Few governmental entities, by contrast, effectively limit pollution from burning fossil fuels such as diesel fuel or coal. Coal-fueled electricity generation alone is a much greater source of heavy metal contamination in the environment than fireworks.
Some companies within the U.S. fireworks industry claim they are working with Chinese manufacturers to reduce and ultimately hope to eliminate of the pollutant perchlorate.
Fireworks are illegal in most Australian states and territories, unless part of a display by a licensed pyrotechnician and with a permit. However Tasmania, ACT and Northern Territory allow consumer use with a permit (dependent on calendar date and circumstances). On 1 July for Territory Day you can freely use fireworks without a permit in the Northern Territory.
Small novelties such as party poppers and sparklers are legal for consumers across Australia.
On 24 August 2009, the ACT Government announced a complete ban on backyard fireworks.
The use, storage and sale of commercial-grade fireworks in Canada is licensed by Natural Resources Canada's Explosive Regulatory Division (ERD). Unlike their consumer counterpart, commercial-grade fireworks function differently, and come in a wide range of sizes from 50 mm (2 inches) up to 300 mm (11+13⁄16 inches) or more in diameter. Commercial grade fireworks require a Fireworks Operator Certificate (FOC), obtained from the ERD by completing a one-day safety course. There are two categories of FOC: one for pyrotechnics (those used on stage and in movies) and another for display fireworks (those used in dedicated fireworks shows). Each requires completion of its own course, although there are special categories of FOC which allow visiting operators to run their shows with the assistance of a Canadian supervisor.
The display fireworks FOC has two levels: assistant (which allows you to work under a qualified supervisor until you are familiar with the basics), and fully licensed. A fully licensed display fireworks operator can also be further endorsed for marine launch, flying saucers, and other more technically demanding fireworks displays.
The pyrotechnician FOC has three levels: pyrotechnician (which allows work under a supervisor), supervising pyrotechnician, and special effects pyrotechnician (which allows the fabrication of certain types of pyrotechnic devices). Additionally, a special effects pyrotechnician can be endorsed for the use of detonating cord.
Since commercial-grade fireworks are shells which are loaded into separate mortars by hand, there is danger in every stage of the setup. Setup of these fireworks involves the placement and securing of mortars on wooden or wire racks; loading of the shells; and if electronically firing, wiring and testing. The mortars are generally made of FRE (fiber-reinforced epoxy) or HDPE (high-density polyethylene). Older mortars made of sheet steel have been banned by most countries due to the problem of shrapnel produced during a misfire.
Setup of mortars in Canada for an oblong firing site require that a mortar be configured at an angle of 10 to 15 degrees down-range with a safety distance of at least 200 meters (660 ft) down-range and 100 meters (330 ft) surrounding the mortars, plus distance adjustments for wind speed and direction. In June 2007, the ERD approved circular firing sites for use with vertically fired mortars with a safety distance of at least 175-meter (574 ft) radius, plus distance adjustments for wind speed and direction.
Loading of shells is a delicate process, and must be done with caution, and a loader must ensure not only the mortar is clean, but also make sure that no part of their body is directly over the mortar in case of a premature fire. Wiring the shells is a painstaking process; whether the shells are being fired manually or electronically, any "chain fusing" or wiring of electrical ignitors, care must be taken to prevent the fuse (an electrical match, often incorrectly called a squib) from igniting. If the setup is wired electrically, the electrical matches are usually plugged into a "firing rail" or "breakout box" that runs back to the main firing board; from there, the Firing Board is simply hooked up to a car battery, and can proceed with firing the show when ready.
Since commercial-grade fireworks are so much larger and more powerful, setup, and firing crews are always under great pressure to ensure they safely set up, fire, and clean up after a show.
In Chile, the manufacture, importation, possession and use of fireworks is prohibited to unauthorized individuals; only certified firework companies can legally use fireworks. As they are considered a type of explosive, offenders can in principle be tried before military courts, although this is unusual in practice.
The European Union's policy is aimed at harmonising and standardising the EU member states' policies on the regulation of production, transportation, sale, consumption and overall safety of fireworks across Europe.
In Belgium, each municipality can decide how to regulate fireworks. During New Year's Eve, lighting fireworks without a licence is allowed in 35% of the 308 Flemish municipalities, in around 50% a permit from the burgemeester (mayor) is required, and around 14% of municipalities have banned consumer fireworks altogether.
In Finland those under 18 years old haven't been allowed to buy any fireworks since 2009. Safety goggles are required. The use of fireworks is generally allowed on the evening and night of New Year's Eve, 31 December. In some municipalities of Western Finland it is allowed to use fireworks without a fire station's permission on the last weekend of August. With the fire station's permission, fireworks can be used year-round.
In Germany, amateurs over 18 years old are allowed to buy and ignite fireworks of Category F2 for several hours on 31 December and 1 January; each German municipality is authorised to limit the number of hours this may last locally. The sale of Category F3 and F4 fireworks to consumers is prohibited. Lighting fireworks is forbidden near churches, hospitals, retirement homes and wooden or thatch-roofed buildings. All major German cities organise professional fireworks shows.
In addition to the previously existing regulations, there was a nationwide ban on the sale of category F2 fireworks to consumers on New Year's Eve 2020/2021 during the COVID-19 pandemic, with the aim to relieve the burden on hospitals by reducing the number of emergencies due to injuries caused by fireworks on New Year's Eve.
In 2015, the Italian town of Collecchio mandated silent fireworks, being among the first to make the switch without losing the beauty of the visual displays.
In the Netherlands, fireworks cannot be sold to anyone under the age of 16. It may only be sold during a period of three days before a new year. If one of these days is a Sunday, that day is excluded from sale and sale may commence one day earlier.
In the Republic of Ireland, fireworks are illegal and possession is punishable by huge fines and/or prison. However, around Halloween a large amount of fireworks are set off, due to the ease of being able to purchase from Northern Ireland.
In Sweden, fireworks can only be purchased and used by people 18 or older. Fire crackers used to be banned, but are now allowed under European Union fireworks policy.
In Iceland, the Icelandic law states that anyone may purchase and use fireworks during a certain period around New Year's Eve. Most places that sell fireworks in Iceland make their own rules about age of buyers, usually it is around 16. The people of Reykjavík spend enormous sums of money on fireworks, most of which are fired as midnight approaches on 31 December. As a result, every New Year's Eve the city is lit up with fireworks displays.
Fireworks in New Zealand are available from 2 to 5 November, around Guy Fawkes Day, and may be purchased only by those 18 years of age and older (up from 14 years pre-2007). Despite the restriction on when fireworks may be sold, there is no restriction regarding when fireworks may be used. The types of fireworks available to the public are multi-shot "cakes", Roman candles, single shot shooters, ground and wall spinners, fountains, cones, sparklers, and various novelties, such as smoke bombs and Pharaoh's serpents. Consumer fireworks are also not allowed to be louder than 90 decibels.
In Norway, fireworks can only be purchased and used by people 18 or older. Sale is restricted to a few days before New Year's Eve. Rockets are not allowed.
Fireworks in the UK have become more strictly regulated since 1997. Since 2005, the law has been harmonised gradually, in accordance with other EU member state laws.
Fireworks are mostly used in England, Scotland and Wales around Diwali, in late October or early November, and Guy Fawkes Night, 5 November. In the UK, responsibility for the safety of firework displays is shared between the Health and Safety Executive, fire brigades and local authorities. Currently, there is no national system of licensing for fireworks operators, but in order to purchase display fireworks, operators must have licensed explosives storage and public liability insurance.
Fireworks cannot be sold to people under the age of 18 and are not permitted to be set off between 11pm and 7am with exceptions only for:
The maximum legal NEC (net explosive content) of a UK firework available to the public is two kilograms. Jumping jacks, strings of firecrackers, shell firing tubes, bangers and mini-rockets were all banned during the late 1990s. In 2004, single-shot air bombs and bottle rockets were banned, and rocket sizes were limited. From March 2008 any firework with more than 5% flashpowder per tube has been classified 1.3G. The aim of these measures was to eliminate "pocket money" fireworks, and to limit the disruptive effects of loud bangs.
In the United States, the laws governing fireworks vary widely from state to state, or from county to county. Federal, state, and local authorities govern the use of display fireworks in the United States. At the federal level, the Consumer Product Safety Commission (CPSC) regulates consumer fireworks through the Federal Hazardous Substances Act (FHSA). The National Fire Protection Association (NFPA) sets forth a set of codes that give the minimum standards of display fireworks use and safety in the U.S. Both state and local jurisdictions can further add restrictions on the use and safety requirements of display fireworks. There are currently 46 states in the United States in which fireworks are legal for consumer use.

The imperial examination, or keju (Chinese: 科舉; lit. "subject recommendation") was a civil-service examination system in Imperial China, administered for the purpose of selecting candidates for the state bureaucracy. The concept of choosing bureaucrats by merit rather than by birth started early in Chinese history, but using written examinations as a tool of selection started in earnest during the Sui dynasty then into the Tang dynasty of 618–907. The system became dominant during the Song dynasty (960–1279) and lasted for almost a millennium until its abolition in the late Qing dynasty reforms in 1905. Aspects of the imperial examination still exist for entry into the civil service of contemporary China, in both the People's Republic of China (PRC) and the Republic of China (ROC).
The exams served to ensure a common knowledge of writing, Chinese classics, and literary style among state officials. This common culture helped to unify the empire, and the ideal of achievement by merit gave legitimacy to imperial rule. The examination system played a significant role in tempering the power of hereditary aristocracy and military authority, and in the rise of a gentry class of scholar-bureaucrats.
Starting with the Song dynasty, the imperial examination system became a more formal system and developed into a roughly three-tiered ladder from local to provincial to court exams. During the Ming dynasty (1368–1644), authorities narrowed the content down to mostly texts on Neo-Confucian orthodoxy; the highest degree, the jinshi (Chinese: 進士), became essential for the highest offices. On the other hand, holders of the basic degree, the shengyuan (生員), became vastly oversupplied, resulting in holders who could not hope for office. Wealthy families, especially from the merchant class, could opt into the system by educating their sons or by purchasing degrees. During the late 19th century, some critics within Qing China blamed the examination system for stifling science and technical knowledge, and urged for some reforms. At the time, China had about one civil licentiate per 1000 people. Due to the stringent requirements, out of the two or three million annual applicants who took the exams, there was only a 1% passing rate.
The Chinese examination system has made a profound influence in the development of modern civil service administrative functions in other countries. These include analogous structures that exist in Japan, Korea, Ryūkyū, and Vietnam. In addition to Asia, reports by European missionaries and diplomats introduced the Chinese examination system to the Western world and encouraged France, Germany as well as the British East India Company (EIC) to use similar methods to select prospective employees. Seeing its initial success within the EIC, the British government adopted a similar testing system for screening civil servants across the board throughout the United Kingdom in 1855. The United States would also establish such programs for certain government jobs after 1883.
Tests of skill such as archery contests have existed since the Zhou dynasty (or, more mythologically, Yao). The Confucian characteristic of the later imperial exams was largely due to the reign of Emperor Wu of Han during the Han dynasty. Although some examinations did exist from the Han to the Sui dynasty, they did not offer an official avenue to government appointment, the majority of which were filled through recommendations based on qualities such as social status, morals, and ability.
The bureaucratic imperial examinations as a concept has its origins in the year 605 during the short-lived Sui dynasty. Its successor, the Tang dynasty, implemented imperial examinations on a relatively small scale until the examination system was extensively expanded during the reign of Wu Zetian. Included in the expanded examination system was a military exam, but the military exam never had a significant impact on the Chinese officer corps and military degrees were seen as inferior to their civil counterpart. The exact nature of Wu's influence on the examination system is still a matter of scholarly debate.
During the Song dynasty the emperors expanded both examinations and the government school system, in part to counter the influence of military aristocrats, increasing the number of degree holders to more than four to five times that of the Tang. From the Song dynasty onward, the examinations played the primary role in selecting scholar-officials, who formed the literati elite of society. However the examinations co-existed with other forms of recruitment such as direct appointments for the ruling family, nominations, quotas, clerical promotions, sale of official titles, and special procedures for eunuchs. The regular higher level degree examination cycle was decreed in 1067 to be three years but this triennial cycle only existed in nominal terms. In practice both before and after this, the examinations were irregularly implemented for significant periods of time: thus, the calculated statistical averages for the number of degrees conferred annually should be understood in this context. The jinshi exams were not a yearly event and should not be considered so; the annual average figures are a necessary artifact of quantitative analysis. The operations of the examination system were part of the imperial record keeping system, and the date of receiving the jinshi degree is often a key biographical datum: sometimes the date of achieving jinshi is the only firm date known for even some of the most historically prominent persons in Chinese history.
A brief interruption to the examinations occurred at the beginning of the Yuan dynasty in the 13th century, but was later brought back with regional quotas which favored the Mongols and disadvantaged Southern Chinese. During the Ming and Qing dynasties, the system contributed to the narrow and focused nature of intellectual life and enhanced the autocratic power of the emperor. The system continued with some modifications until its abolition in 1905 during the last years of the Qing dynasty. The modern examination system for selecting civil servants also indirectly evolved from the imperial one.
Candidates for offices recommended by the prefect of a prefecture were examined by the Ministry of Rites and then presented to the emperor. Some candidates for clerical positions would be given a test to determine whether they could memorize nine thousand Chinese characters. The tests administered during the Han dynasty did not offer formal entry into government posts. Recruitment and appointment in the Han dynasty were primarily through recommendations by aristocrats and local officials. Recommended individuals were also primarily aristocrats. In theory, recommendations were based on a combination of reputation and ability but it is not certain how well this worked in practice. Oral examinations on policy issues were sometimes conducted personally by the emperor himself during Western Han times.
In 165 BC Emperor Wen of Han introduced recruitment to the civil service through examinations, however these did not heavily emphasize Confucian material. Previously, potential officials never sat for any sort of academic examinations.
Emperor Wu of Han's early reign saw the creation of a series of posts for academicians in 136 BC. Ardently promoted by Dong Zhongshu, the Taixue and Imperial examination came into existence by recommendation of Gongsun Hong, chancellor under Wu. Officials would select candidates to take part in an examination of the Confucian classics, from which Emperor Wu would select officials to serve by his side. Gongsun intended for the Taixue's graduates to become imperial officials but they usually only started off as clerks and attendants, and mastery of only one canonical text was required upon its founding, changing to all five in the Eastern Han. Starting with only 50 students, Emperor Zhao expanded it to 100, Emperor Xuan to 200, and Emperor Yuan to 1000.
While the examinations expanded under the Han, the number of graduates who went on to hold office were few. The examinations did not offer a formal route to commissioned office and the primary path to office remained through recommendations. Though connections and recommendations remained more meaningful than the exam, the initiation of the examination system by Emperor Wu was crucial for the Confucian nature of later imperial examinations. During the Han dynasty, these examinations were primarily used for the purpose of classifying candidates who had been specifically recommended. Even during the Tang dynasty the quantity of placements into government service through the examination system only averaged about nine persons per year, with the known maximum being less than 25 in any given year.
The first standardized method of recruitment in Chinese history was introduced during the Three Kingdoms period in the Kingdom of Wei. It was called the nine-rank system. In the nine-rank system, each office was given a rank from highest to lowest in descending order from one to nine. Imperial officials were responsible for assessing the quality of the talents recommended by local elites. The criteria for recruitment included qualities such as morals and social status, which in practice meant that influential families monopolized all high ranking posts while men of poorer means filled the lower ranks.
The local zhongzheng (lit. central and impartial) officials assessed the status of households or families in nine categories; only the sons of the fifth categories and above were entitled to offices. The method obviously contradicted the ideal of meritocracy. It was, however, convenient in a time of constant wars among the various contending states, all of them relying on an aristocratic political and social structure. For nearly three hundred years, noble young men were afforded government higher education in the Imperial Academy and carefully prepared for public service. The Jiupin guanren fa was closely related to this kind of educational practice and only began to decline after the second half of the sixth century.The Sui dynasty continued the tradition of recruitment through recommendation but modified it in 587 with the requirement for every prefecture (fu) to supply three scholars a year. In 599, all capital officials of rank five and above were required to make nominations for consideration in several categories.
During the Sui dynasty, examinations for "classicists" (mingjing ke) and "cultivated talents" (xiucai ke) were introduced. Classicists were tested on the Confucian canon, which was considered an easy task at the time, so those who passed were awarded posts in the lower rungs of officialdom. Cultivated talents were tested on matters of statecraft as well as the Confucian canon. In AD 607, Emperor Yang of Sui established a new category of examinations for the "presented scholar" (jinshike 进士科). These three categories of examination were the origins of the imperial examination system that would last until 1905. Consequently, the year 607 is also considered by many to be the real beginning of the imperial examination system. The Sui dynasty was itself short lived however and the system was not developed further until much later.
The imperial examinations did not significantly shift recruitment selection in practice during the Sui dynasty. Schools at the capital still produced students for appointment. Inheritance of official status was also still practiced. Men of the merchant and artisan classes were still barred from officialdom. However the reign of Emperor Wen of Sui did see much greater expansion of government authority over officials. Under Emperor Wen (r. 581–604), all officials down to the district level had to be appointed by the Department of State Affairs in the capital and were subjected to annual merit rating evaluations. Regional Inspectors and District Magistrates had to be transferred every three years and their subordinates every four years. They were not allowed to bring their parents or adult children with them upon reassignment of territorial administration. The Sui did not establish any hereditary kingdoms or marquisates (hóu) of the Han sort. To compensate, nobles were given substantial stipends and staff. Aristocratic officials were ranked based on their pedigree with distinctions such as "high expectations", "pure", and "impure" so that they could be awarded offices appropriately.
The Tang dynasty and the Zhou interregnum of Empress Wu (Wu Zetian) expanded examinations beyond the basic process of qualifying candidates based on questions of policy matters followed by an interview. Oral interviews as part of the selection process were theoretically supposed to be an unbiased process, but in practice favored candidates from elite clans based in the capitals of Chang'an and Luoyang (speakers of solely non-elite dialects could not succeed).
Under the Tang, six categories of regular civil-service examinations were organized by the Department of State Affairs and held by the Ministry of Rites: cultivated talents, classicists, presented scholars, legal experts, writing experts, and arithmetic experts. Emperor Xuanzong of Tang also added categories for Daoism and apprentices. The hardest of these examination categories, the presented scholar jinshi degree, became more prominent over time until it superseded all other examinations. By the late Tang the jinshi degree became a prerequisite for appointment into higher offices. Appointments by recommendation were also required to take examinations.
The examinations were carried out in the first lunar month. After the results were completed, the list of results was submitted to the Grand Chancellor, who had the right to alter the results. Sometimes the list was also submitted to the Secretariat-Chancellery for additional inspection. The emperor could also announce a repeat of the exam. The list of results was then published in the second lunar month.
Classicists were tested by being presented phrases from the classic texts. Then they had to write the whole paragraph to complete the phrase. If the examinee was able to correctly answer five of ten questions, they passed. This was considered such an easy task that a 30-year-old candidate was said to be old for a classicist examinee, but young to be a jinshi. An oral version of the classicist examination known as moyi also existed but consisted of 100 questions rather than just ten. In contrast, the jinshi examination not only tested the Confucian classics, but also history, proficiency in compiling official documents, inscriptions, discursive treatises, memorials, and poems and rhapsodies. Because the number of jinshi graduates were so low they acquired great social standing in society. The judicial, arithmetic, and clerical examinations were also held but these graduates only qualified for their specific agencies.
Candidates who passed the exam were not automatically granted office. They still had to pass a quality evaluation by the Ministry of Rites, after which they were allowed to wear official robes.
Wu Zetian's reign was a pivotal moment for the imperial examination system. The reason for this was because up until that point, the Tang rulers had all been male members of the Li family. Wu Zetian, who officially took the title of emperor in 690, was a woman outside the Li family who needed an alternative base of power. Reform of the imperial examinations featured prominently in her plan to create a new class of elite bureaucrats derived from humbler origins. Both the palace and military examinations were created under Wu Zetian.
In 655, Wu Zetian graduated 44 candidates with the jìnshì degree (進士), and during one seven-year period the annual average of exam takers graduated with a jinshi degree was greater than 58 persons per year. Wu lavished favors on the newly graduated jinshi degree-holders, increasing the prestige associated with this path of attaining a government career, and clearly began a process of opening up opportunities to success for a wider population pool, including inhabitants of China's less prestigious southeast area. Wu Zetian's government further expanded the civil service examination system by allowing certain commoners and gentry previously disqualified by their non-elite backgrounds to take the tests. Most of the Li family supporters were located to the northwest, particularly around the capital city of Chang'an. Wu's progressive accumulation of political power through enhancement of the examination system involved attaining the allegiance of previously under-represented regions, alleviating frustrations of the literati, and encouraging education in various locales so even people in the remote corners of the empire would study to pass the imperial exams. These degree holders would then become a new nucleus of elite bureaucrats around which the government could center itself.
In 681, a fill in the blank test based on knowledge of the Confucian classics was introduced.
Examples of officials whom she recruited through her reformed examination system include Zhang Yue, Li Jiao, and Shen Quanqi.
Despite the rise in importance of the examination system, the Tang society was still heavily influenced by aristocratic ideals, and it was only after the ninth century that the situation changed. As a result, it was common for candidates to visit examiners before the examinations in order to win approval. The aristocratic influence declined after the ninth century, when the examination degree holders also increased in numbers. They now began to play a more decisive role in the Court. At the same time, a quota system was established which could enhance the equitable representation, geographically, of successful candidates.From 702 onward, the names of examinees were hidden to prevent examiners from knowing who was tested. Prior to this, it was even a custom for candidates to present their examiner with their own literary works in order to impress him.
Sometime between 730 and 740, after the Tang restoration, a section requiring the composition of original poetry (including both shi and fu) was added to the tests, with rather specific set requirements: this was for the jinshi degree, as well as certain other tests. The less-esteemed examinations tested for skills such as mathematics, law, and calligraphy. The success rate on these tests of knowledge on the classics was between 10 and 20 percent, but for the thousand or more candidates going for a jinshi degree each year in which it was offered, the success rate for the examinees was only between 1 and 2 percent: a total of 6504 jinshi were created during course of the Tang dynasty (an average of only about 23 jinshi awarded per year). After 755, up to 15 percent of civil service officials were recruited through the examinations.
During the early years of the Tang restoration, the following emperors expanded on Wu's policies since they found them politically useful, and the annual averages of degrees conferred continued to rise. This led to the formation of new court factions consisting of examiners and their graduates. With the upheavals which later developed and the disintegration of the Tang empire into the "Five Dynasties and Ten Kingdoms period", the examination system gave ground to other traditional routes to government positions and favoritism in grading reduced the opportunities of examinees who lacked political patronage. Ironically this period of fragmentation resulted in the utter destruction of old networks established by elite families that had ruled China throughout its various dynasties since its conception. With the disappearance of the old aristocracy, Wu's system of bureaucrat recruitment once more became the dominant model in China, and eventually coalesced into the class of nonhereditary elites who would become known to the West as "mandarins," in reference to Mandarin, the dialect of Chinese employed in the imperial court.
In the Song dynasty (960–1279) the imperial examinations became the primary method of recruitment for official posts. More than a hundred palace examinations were held during the dynasty, resulting in a greater number of jinshi degrees rewarded. The examinations were opened to adult Chinese males, with some restrictions, including even individuals from the occupied northern territories of the Liao and Jin dynasties. Figures given for the number of examinees record 70–80,000 in 1088 and 79,000 at the turn of the 12th century. In the mid-11th century, between 5,000 and 10,000 took the metropolitan examinations in a given year. By the mid-12th century, 100,000 candidates registered for the prefectural examinations each year, and by the mid-13th century, more than 400,000. The number of active jinshi degree holders ranged from 5,000 to 10,000 between the 11th and 13th centuries, representing 7,085 of 18,700 posts in 1046 and 8,260 of 38,870 posts in 1213. Statistics indicate that the Song imperial government degree-awards eventually more than doubled the highest annual averages of those awarded during the Tang dynasty, with 200 or more per year on average being common, and at times reaching a per annum figure of almost 240.
The examination hierarchy was formally divided into prefectural, metropolitan, and palace examinations. The prefectural examination was held on the 15th day of the eighth lunar month. Graduates of the prefectural examination were then sent to the capital for metropolitan examination, which took place in Spring, but had no fixed date. Graduates of the metropolitan examination were then sent to the palace examination.
Many individuals of low social status were able to rise to political prominence through success in the imperial examination. According to studies of degree-holders in the years 1148 and 1256, approximately 57 percent originated from families without a father, grandfather, or great-grandfather who had held official rank. However most did have some sort of relative in the bureaucracy. Prominent officials who went through the imperial examinations include Wang Anshi, who proposed reforms to make the exams more practical, and Zhu Xi (1130-1200), whose interpretations of the Four Classics became the orthodox Neo-Confucianism which dominated later dynasties. Two other prominent successful entries into politics through the examination system were Su Shi (1037-1101) and his brother Su Zhe (1039-1112): both of whom became political opponents of Wang Anshi. The process of studying for the examination tended to be time-consuming and costly, requiring time to spare and tutors. Most of the candidates came from the numerically small but relatively wealthy land-owning scholar-official class.
Since 937, by the decision of the Emperor Taizu of Song, the palace examination was supervised by the emperor himself. In 992, the practice of anonymous submission of papers during the palace examination was introduced; it was spread to the departmental examinations in 1007, and to the prefectural level in 1032. From 1037 on, it was forbidden for examiners to supervise examinations in their home prefecture. Examiners and high officials were also forbidden from contacting each other prior to the exams. The practice of recopying papers in order to prevent revealing the candidate's calligraphy was introduced at the capital and departmental level in 1015, and in the prefectures in 1037.
In 1009, Emperor Zhenzong of Song (r. 997–1022) introduced quotas on degrees awarded. In 1090, only 40 degrees were being awarded to 3,000 candidates in Fuzhou, which meant only one degree would be awarded for every 75 candidates. The quota system became even more stringent in the 13th century when only one percent of candidates were allowed to pass the prefectural examination. Even graduates of the lowest tier of examinations represented an elite class.
In 1071, Emperor Shenzong of Song (r. 1067–1085) abolished the classicist as well as various other examinations on law and arithmetics. The jinshi examination became the primary gateway to officialdom. Judicial and classicist examinations were revived shortly after. However the judicial examination was classified as a special examination and not many people took the classicist examination. The oral version of the classicist exam was abolished. Other special examinations for household and family member of officials, Minister of Personnel, and subjects such as history as applied to current affairs (shiwu ce Policy Questions), translation, and judicial matters were also administered by the state. Policy Questions became an essential part of following examinations. An exam called the cewen which focused on contemporary matters such as politics, economics, and military affairs was introduced.
The Song also saw the introduction of a new examination essay, that of jing yi; (exposition on the meaning of the Classics). This required candidates to compose a logically coherent essay by juxtaposing quotations from the Classics or sentences of similar meaning to certain passages. This reflected the stress the Song placed on creative understanding of the Classics. It would eventually develop into the so-called ‘eight-legged essays’ essays (bagu wen ) that gave the defining character to the Ming and Qing examinations.Various reforms or attempts to reform the examination system were made during the Song dynasty by individuals such as Fan Zhongyan, Zhu Xi, and by Wang Anshi. Wang and Zhu successfully argued that poems and rhapsodies should be excluded from the examinations because they were of no use to administration or cultivation of virtue. The poetry section of the examination was removed in the 1060s. Fan's memorial to the throne initiated a process which lead to major educational reform through the establishment of a comprehensive public school system.
The Khitans who ruled the Liao dynasty only held imperial examinations for regions with large Han populations. The Liao examinations focused on lyric-meter poetry and rhapsodies. The Khitans themselves did not take the exams until 1115 when it became an acceptable avenue for advancing their careers.
The Jurchens of the Jin dynasty held two separate examinations to accommodate their former Liao and Song subjects. In the north examinations focused on lyric-meter poetry and rhapsodies while in the south, Confucian Classics were tested. During the reign of Emperor Xizong of Jin (r. 1135–1150), the contents of both examinations were unified and examinees were tested on both genres. Emperor Zhangzong of Jin (r. 1189–1208) abolished the prefectural examinations. Emperor Shizong of Jin (r. 1161–1189) created the first examination conducted in the Jurchen language, with a focus on political writings and poetry. Graduates of the Jurchen examination were called "treatise graduates" (celun jinshi) to distinguish them from the regular Chinese jinshi.
Imperial examinations were ceased for a time with the defeat of the Song in 1279 by Kublai Khan and his Yuan dynasty. One of Kublai's main advisers, Liu Bingzhong, submitted a memorial recommending the restoration of the examination system: however, this was not done. Kublai ended the imperial examination system, as he believed that Confucian learning was not needed for government jobs. Also, Kublai was opposed to such a commitment to the Chinese language and to the ethnic Han scholars who were so adept at it, as well as its accompanying ideology: he wished to appoint his own people without relying on an apparatus inherited from a newly conquered and sometimes rebellious country. The discontinuation of the exams had the effect of reducing the prestige of traditional learning, reducing the motivation for doing so, as well as encouraging new literary directions not motivated by the old means of literary development and success.
The examination system was revived in 1315, with significant changes, during the reign of Ayurbarwada Buyantu Khan. The new examination system organized its examinees into regional categories in a way which favored Mongols and severely disadvantaged Southern Chinese. A quota system both for number of candidates and degrees awarded was instituted based on the classification of the four groups, those being the Mongols, their non-Han allies (Semu-ren), Northern Chinese, and Southern Chinese, with further restrictions by province favoring the northeast of the empire (Mongolia) and its vicinities. A quota of 300 persons was fixed for provincial examinations with 75 persons from each group. The metropolitan exam had a quota of 100 persons with 25 persons from each group. Candidates were enrolled on two lists with the Mongols and Semu-ren located on the left and the Northern and Southern Chinese on the right. Examinations were written in Chinese and based on Confucian and Neo-Confucian texts but the Mongols and Semu-ren received easier questions to answer than the Han. Successful candidates were awarded one of three ranks. All graduates were eligible for official appointment.
The Yuan decision to use Zhu Xi’s classical scholarship as the examination standard was critical in enhancing the integration of the examination system with Confucian educational experience. Both Chinese and non-Chinese candidates were recruited separately, to guarantee that non-Chinese officials could control the government, but this also furthered Confucianisation of the conquerors.Under the revised system the yearly averages for examination degrees awarded was about 21. The way in which the four regional racial categories were divided tended to favor the Mongols, Semu-ren, and North Chinese, despite the South Chinese being by far the largest portion of the population. The 1290 census figures record some 12,000,000 households (about 48% of the total Yuan population) for South China, versus 2,000,000 North Chinese households, and the populations of Mongols and Semu-ren were both less. While South China was technically allotted 75 candidates for each provincial exam, only 28 Han Chinese from South China were included among the 300 candidates, the rest of the South China slots (47) being occupied by resident Mongols or Semu-ren, although 47 "racial South Chinese" who were not residents of South China were approved as candidates.
The Ming dynasty (1368–1644) retained and expanded the system it inherited. The Hongwu Emperor was initially reluctant to restart the examinations, considering their curriculum to be lacking in practical knowledge. In 1370 he declared that the exams would follow the Neo-Confucian canon put forth by Zhu Xi in the Song dynasty: the Four Books, discourses, and political analysis. Then he abolished the examinations two years later because he preferred appointment by referral. In 1384, the examinations were revived again, however in addition to the Neo-Confucian canon, Hongwu added another portion to the exams to be taken by successful candidates five days after the first exam. These new exams emphasized shixue (practical learning), including subjects such as law, mathematics, calligraphy, horse riding, and archery. The emperor was particularly adamant about the inclusion of archery, and for a few days after issuing the edict, he personally commanded the Guozijian and county-level schools to practice it diligently. As a result of the new focus on practical learning, from 1384 to 1756/7, all provincial and metropolitan examinations incorporated material on legal knowledge and the palace examinations included policy questions on current affairs. The first palace examination of the Ming dynasty was held in 1385.
Provincial and metropolitan exams were organized in three sessions. The first session consisted of three questions on the examinee's interpretation of the Four Books, and four on the Classics corpus. The second session took place three days later, and consisted of a discursive essay, five critical judgments, and one in the style of an edict, an announcement and a memorial. Three days after that, the third session was held, consisting of five essays on the Classics, historiography, and contemporary affairs. The palace exam was just one session, consisting of questions on critical matters in the Classics or current affairs. Written answers were expected to follow a predefined structure called the eight-legged essay, which consisted of eight parts: opening, amplification, preliminary exposition, initial argument, central argument, latter argument, final argument, and conclusion. The length of the essay ranged between 550 and 700 characters. Gu Yanwu considered the eight-legged essay to be worse than the book burning of Qin Shi Huang and his burying alive of 460 Confucian scholars.
The content of the examinations in the Ming and Qing times remained very much the same as that in the Song, except that literary composition was now widened to include government documents. The most important was the weight given to eight-legged essays. As a literary style, they are constructed on logical reasoning for coherent exposition. However, as the format evolved, they became excessively rigid, to ensure fair grading. Candidates often only memorised ready essays in the hope that the ones they memorised might be the examination questions. Since all questions were taken from the Classics, there were just so many possible passages that the examiners could use for questions. More often than not, the questions could be a combination of two or more totally unrelated passages. Candidates could be at a complete loss as to how to make out their meaning, let alone writing a logically coherent essay. This aroused strong criticism, but the use of the style remained until the end of the examination system.The Hanlin Academy played a central role in the careers of examination graduates during the Ming dynasty. Graduates of the metropolitan exam with honors were directly appointed senior compiler in the Hanlin Academy. Regular metropolitan exam graduates were appointed junior compilers or examining editors. In 1458, appointment in the Hanlin Academy and the Grand Secretariat was restricted to jinshi graduates. Posts such as minister or vice minister of rites or right vice ministers of personnel were also restricted to jinshi graduates. The training jinshi graduates underwent in the Hanlin Academy allowed them insight into a wide range of central government agencies. Ninety percent of Grand Chancellors during the Ming dynasty were jinshi degree holders.
The Neo-Confucian orthodoxy became the new guideline for literati learning, narrowing the way in which they could politically and socially interpret the Confucian canon. At the same time, commercialization of the economy and booming population growth resulted in an inflation of the number of degree candidates at the lower levels. The Ming bureaucracy did not increase degree quotas in proportion to the increased population. Near the end of the Ming dynasty, in 1600, there were roughly 500,000 shengyuan in a population of 150 million, that is, one per 300 people. This trend of booming population but artificial limitation of degrees awarded continued into the Qing dynasty, when during the mid-19th century, the ratio of shengyuan to population had shrunk to one per each thousand people. Access to government office became not only extremely difficult, but officials also became more orthodox in their thinking. The higher and more prestigious offices were still dominated by jinshi degree-holders, similar to the Song dynasty, but tended to come from elite families.
The social background of metropolitan graduates also narrowed as time went on. In the early years of the Ming dynasty only 14 percent of metropolitan graduates came from families that had a history of providing officials, while in the last years of the Ming roughly 60 percent of metropolitan exam graduates came from established elite families.
The ambition of Hong Taiji, the first emperor of the Qing dynasty, was to use the examinations to foster a cadre of Manchu Bannermen who were both martial and literate to administer the government. He initiated the first exam for bannermen in 1638, offered in both Manchu and Chinese, even before his troops took Beijing in 1644. But the Manchu bannermen had no time or money to prepare for the exams, especially since they could gain advancement on the battlefield, and the dynasty went on to rely on both Manchu and Han Chinese officials chosen through the system inherited with minor adaptation from the Ming.  During the dynasty a total of 26,747 jinshi degrees were earned in 112 examinations held over the 261 years 1644–1905, an average of 238.8 jinshi degrees conferred per examination.
Racial quotas were placed on the number of graduates permitted. In the early Qing period, a 4:6 Manchu to Han quota was placed on the palace examination, and was in effect until 1655. Separate examinations were held for bannermen from 1652 to 1655 with a ten-point racial quota of 4:2:4 for Manchus, Mongols, and Han Chinese. In 1651, "translation" examinations were implemented for bannermen, however the purpose of these exams was not to create translators, but to service those Manchus and bannermen who did not understand Classical Chinese. During the reign of the Qianlong Emperor (1736–95), Manchus and Mongols were encouraged to take the examinations in Classical Chinese. The translation examination was abolished in 1840 because there weren't enough candidates to justify it. After 1723, Han Chinese graduates of the palace examination were required to learn the Manchu language. Bilingualism in Chinese and Manchu languages was favored in the bureaucracy and people who fulfilled the language requirements were given preferential appointments. For example, in 1688, a candidate from Hangzhou who was able to answer policy questions at the palace examination in both Chinese and Manchu was appointed as compiler at the Hanlin Academy, despite finishing bottom of the second tier of jinshi graduates. Ethnic minorities such as the Peng people in Jiangxi Province were given a quota of 1:50 for the shengyuan degree to encourage them to settle down and give up their nomadic way of life. In 1767, a memorial from Guangxi Province noted that some Han Chinese took advantage of the ethnic quotas to become shengyuan and that it was hard to verify who was a native. In 1784, quotas were recommended for Muslims to incorporate them into mainstream society. In 1807, a memorial from Hunan Province requested higher quotas for Miao people so that they would not have to compete with Han Chinese candidates.
The Chinese Christian Taiping Heavenly Kingdom was created in rebellion against the Qing dynasty led by a failed examination candidate Hong Xiuquan, which established its capital in Nanjing in 1851. Following the imperial example, the Taipings held exams starting in 1851. They replaced the Confucian Classes, however, with the Taiping Bible, the Old and New Testaments as edited by Hong. Candidates were expected to write eight-legged essays using quotations.
In 1853, women were for the first time in Chinese history able to become examination candidates. Fu Shanxiang took the exam and became the first (and last) female zhuangyuan in Chinese history.
By the 1830s and 1840s, proposals emerged from officials calling for reforms to the Imperial Examinations to include Western technology. In 1864, Li Hongzhang submitted proposals to add a new subject into the Imperial examinations involving Western technology, that scholars may focus their efforts entirely on this. A similar proposal was tabled by Feng Guifen in 1861 and Ding Richang (mathematics and science) in 1867. In 1872, and again in 1874, Shen Baozhen submitted proposals to the throne for the reform of the Imperial Examinations to include Mathematics. Shen also proposed the abolition of the military examinations, which were based on obsolete weaponry such as archery. He proposed the idea that Tongwen Guan students who performed well in mathematics could be directly appointed to the Zongli Yamen as if they were Imperial examination graduates. Li Hongzhang, in an 1874 memorial, tabled the concept of "Bureaus of Western Learning" (洋学局) in coastal provinces, participation in which was to be accorded the honour of Imperial examination degrees. In 1888, the Imperial examinations was expanded to include the subject of international commerce.
With the military defeats in the 1890s and pressure to develop a national school system, reformers such as Kang Youwei and Liang Qichao called for abolition of the exams, and the Hundred Days' Reform of 1898 proposed a set of modernizations. After the Boxer Rebellion, the government drew up plans to reform under the name of New Policies. Reformers memorialized the throne to abolish the system. The key sponsors were Yuan Shikai, Yin Chang, and Zhang Zhidong (Chang Chih-tung). On 2 September 1905, the throne ordered the examination system be discontinued, beginning at the first level in 1905. The new system provided equivalents to the old degrees; a bachelor's degree, for instance, would be considered equivalent to the xiu cai.
Those who had at least the degree of shengyuan remained fairly well off since they retained their social status. Older students who had failed to even become shengyuan were more damaged because they could not easily absorb new learning, were too proud to turn to commerce, and too weak for physical labor.
The original purpose of the imperial examinations as they were implemented during the Sui dynasty was to strike a blow against the hereditary aristocracy and to centralize power around the emperor. The era preceding the Sui dynasty, the period of Northern and Southern dynasties, was a golden age for the Chinese aristocracy. The power they wielded seriously constrained the emperor's ability to exercise his power in court, especially when it came to appointing officials. The Sui emperor created the imperial examinations to bypass and mitigate aristocratic interests. This was the origin of the Chinese examination system.
Although quite a few Northern Song families or lineages succeeded in producing high officials over several generations, none could begin to rival the great families of the Six Dynasties and Tang in longevity, prestige, or perhaps even power. Most important, the promise of the examinations transformed learning from an elite concern to a preoccupation. Education became less the domain of scholarly families comprising one portion of elite society and more an activity urged upon academically promising boys and young men throughout elite society.The short lived Sui dynasty was soon replaced by the Tang, who built on the examination system. The emperor placed the palace exam graduates, the jinshi, in important government posts, where they came into conflict with hereditary elites. During the reign of Emperor Xuanzong of Tang (713-56), about a third of the Grand Chancellors appointed were jinshi, but by the time of Emperor Xianzong of Tang (806-21), three fifths of the Grand Chancellors appointed were jinshi. This change in the way government was organized dealt a real blow to the aristocrats, but they did not sit idly by and wait to become obsolete. Instead they themselves entered the examinations to gain the privileges associated with it. By the end of the dynasty, the aristocratic class had produced 116 jinshi, so that they remained a significant influence in the government. Hereditary privileges were also not completely done away with. The sons of high ministers and great generals had the right to hold minor offices without taking the examinations. This privilege was introduced in 963 and allowed high officials to nominate their sons, nephews, and grandsons for civil service. However, after 1009, nominated candidates also had to study at the Guozijian and after completing a course, sit an examination. More than 50 percent passed. Lower level posts in the capital were awarded to these graduates.
In addition, the number of graduates were not only small, but also formed their own clique in the government based around the examiners and the men they passed. In effect the graduates became another interest group the emperor had to contend with. This problem was greatly mitigated by the increase in candidates and graduates during the Song dynasty, made possible by its robust economy. Since the entire upper echelon of the Song dynasty was filled by jinshi, and imperial clan members were barred from posts of substance, there was no longer any conflict of the type relating to different preparatory backgrounds. Efforts were made to break the link between examiner and examinee, removing another factor contributing to the formation of scholar bureaucrat cliques. While the influence of certain scholar officials never disappeared, they no longer held any influence in organizing men.
The scholar-bureaucrats, later known as Mandarins in the West, continued to exert significant influence throughout the rest of Chinese imperial history. The relationship between the emperor and his officials as seen from the side of the officials is encapsulated by Zhang Fangping's statement to Emperor Renzong of Song in the 1040s: "The empire cannot be ruled by Your Majesty alone; the empire can only be governed by Your Majesty collaborating with the officials." In 1071, Emperor Shenzong of Song remarked that Wang Anshi's New Policies were for the benefit of the people and not the shidafu, the elite literate class. His grand chancellor, Wen Yanbo, retorted, "You govern the nation with us, the officials, not with the people."
... the true characteristic of the Song political system was not autocracy but "scholar-official (shidafu) government," made possible by the existence of the examination system as a means of validating political authority. Over the long run, the emperor shifted from being a figure with administrative power to one with symbolic power, tightly constrained by the system he was part of, even if he did not always act according to the ideals his ministers urged upon him; the emperor was not the top of a pyramid but the keystone in an arch, whose successful functioning depended on his staying in his place.The importance of clan identity as the chief marker of status seems to have declined by the 9th century, when the genealogical section of epitaphs excluded choronyms (combination of clan and place names) in favor of office titles. For the aristocracy, although status was still an important factor in marriage, wealth had become far more important by the time of the Southern Song (1127–1279) than the Tang era. During the Yuanyou era (1086-1093), it was reported that an imperial clanswoman had married a man surnamed Liu from the foreign quarter of Guangzhou. In 1137, a complaint was lodged against a military official for marrying his sister to the "great merchant" Pu Yali (a two-time envoy from Arabia). The foreign merchant family of Pu also sought marriage into the imperial clan.
Song Taizong possessed all of his older brother’s ambitions and suspicions but almost none of his positive qualities. As a child he had no friends, and as an adult he was a poor leader of men. Liu Jingzhen has characterized him as self-confident and self-reliant, but I think a more accurate assessment would be arrogant and insecure. All of these qualities, combined with an energetic constitution, led Taizong to attempt to read every document and make every decision necessary for the operations of government. The immense quantities of paper generated by the Song bureaucracy inevitably defeated him in this project, though he continued heroically trying to master it until his death. The central irony of Taizong’s reign was that the more he succeeded in centralizing the power of government and increasing the authority of the emperor, the less any individual emperor, no matter how energetic, could control that power by himself. Taizong thus unintentionally presided over the shift of the de jure nearly absolute imperial power to the de facto control of a rapidly professionalizing bureaucracy.Although there was a military version of the exam, it was neglected by the government. The importance of the regular imperial examinations in governance had the effect of subordinating the military to civil government. By the time of the Song dynasty, the two highest military posts of Minister of War and Chief of Staff were both reserved for civil servants. It became routine for civil officials to be appointed as front-line commanders in the army. The highest rank for a dedicated military career was reduced to unit commander. To further reduce the influence of military leaders, they were routinely reassigned at the end of a campaign, so that no lasting bond occurred between commander and soldier. The policy of appointing civil officials as ad hoc military leaders was maintained by both the Ming and Qing dynasties after the initial phase of conquest. Successful commanders entrenched solely in the military tradition such as Yue Fei were viewed with distrust and apprehension. He was eventually executed by the Song government despite successfully leading Song forces against the Jin dynasty. It's possible that foreigners also looked down on Chinese military men in contrast to their civil counterparts. In 1042, the general Fan Zhongyan refused a military title because he thought it would demean him in the eyes of the Tibetans and Tanguts. Although it negatively impacted the military's performance at times, the new relationship between the civil and military sectors of the government avoided the endemic military coups of preceding dynasties for the rest of imperial Chinese history.
During the Tang period, a set curricular schedule took shape where the three steps of reading, writing, and the composition of texts had to be learnt before students could enter state academies. As the number of graduates increased, competition for government posts became more fierce. Whereas at the beginning of the Tang dynasty, there were few jinshi, during the Song dynasty, there were more than the government needed. Several reforms to education were suggested to thin the number of candidates and improve their quality. In 1044, Han Qi, Fan Zhongyan, Ouyang Xiu, and Song Qi implemented the Qingli Reforms, which included hiring experts in the classics to teach at government schools, setting up schools in every prefecture, and requiring every candidate to have attended the prefectural school for at least 300 days to qualify for the prefectural exam. Prior to the Qingli Reforms, the role of the state was limited to supporting a few institutions, such as in 1022 when the government granted a prefectural school in Yanzhou with 151 acres of land. Some of the reformers directly participated in the creation of schools. Between 1035 and 1046, Fan Zhongyan and his followers set up 16 schools. Ouyang Xiu donated 1.5 million cash for the construction of a school in his home prefecture. Han Qi did the same as well. The Qingli Reforms were abandoned after only one year.
By the end of the Northern Song... government schools connected in a hierarchical empire-wide system, but there was a high degree of organizational uniformity as well. Prefectural school preceptorships became respected posts in local administration which, as far as we can tell, were routinely filled; educational support fields and student support provisions became the rule rather than the exception; and to cope with the rising demand of schooling, school entrance examinations became common.While the Qingli Reforms failed, the ideal of a statewide education system was taken up by Wang Anshi (1021–86), who proposed as part of his New Policies that examinations alone were not enough to select talent. His answer to the glut of graduates was to found new schools (shuyuan) for the selection of officials, with the ultimate goal of replacing the examinations altogether by selecting officials directly from the school's students. An alternative path to office was introduced: the Three Hall system. The government expanded the Taixue (National University) and ordered each circuit to grant land to schools and to hire supervising teachers for them. In 1076, a special examination for teachers was introduced. Implementation of the reforms was uneven and slow. Of the 320 prefectures, only 53 had prefectural schools with supervising teachers by 1078 and only a few were given the ordered allotment of land. Wang died and his reforms languished until the early 12th century when Emperor Huizong of Song injected more resources into the national education project. In 1102, Huizong and his chief councilor Cai Jing (1046-1126) decided to combine schools and examinations and make schools the focus of both education and recruitment. In 1103, the Taixue grew to 3,800 students with 200 in the upper hall, 600 in the lower hall, and 3,000 in the Biyong or outer hall. In 1104, students started being processed up the three-colleges (Three Hall) ranking system from the county school to the Taixue for direct appointment in the bureaucracy. In 1106, the "eight virtues" method of selection was introduced. The "eight virtues" method was to select and promote students based on eight varieties of virtuous conduct. By 1109, schools had received more than 100,000 jing of land (1.5 million acres), taken from the state granaries. Total student numbers were reported at 210,000 in 1104, 167,622 in 1109, and over 200,000 in 1116. At its height the Song education system included approximately 0.2% of its one hundred million people.
Boys entered  school at eight and  college at fifteen. Those whose talents could be developed were selected and gathered in the college, whereas the inferior ones were returned to the farm, for scholars and farmers did not exchange occupations. Having entered college, one would not work on the farm. Thus scholars and farmers were completely differentiated. As to support in college, there was no worry about sons of officials. But even sons of commoners, as soon as they entered college, were sure to be supported .The new education policies were sometimes criticized and the importance of schools attacked. Su Shi believed that educational institutions were nothing more than places for students to learn the necessary techniques to pass the imperial exams. Liu Ban (1023-1089) believed that an education at home was sufficient: "Education that scholars receive at home is sufficient for them to become talented people. Why do they need to turn to teachers in government schools to do so?" In 1078, a student of the Taixue, Yu Fan, submitted a memorial accusing the instructors of bias and improper teaching. In the following year, Emperor Shenzong of Song confirmed the accusations after a Censorate investigation. In 1112, a memorial criticized the prefectural and county schools for a variety of abuses. It accused the local supervisors of not understanding the goal of educating greater talent, wasting funds by buying excess food and drink, buying superfluous decorations, profiting by selling grain at market price, borrowing from students, and engaging in acts of violence towards officials. The "eight virtues" system received many complaints about its lack of academic rigor. In 1121, the local three-colleges system was dismantled and the local schools ordered to return land to the government.
The instructors in the Imperial University are not impartial. After they give regular examinations to students, they rely on self-serving assessment of student performance to decide who should be promoted to the next level. In addition, Your Majesty holds court conferences at the crack of dawn, but these instructors often arrive at the university after nine and leave at eleven o’clock. Every day Your Majesty handles numerous affairs of state while attending imperial seminars. It took only a few years for you to finish learning the Book of Odes. These instructors instead have already spent seven years on the Rites of Zhou, only to complete four volumes. Analects of Confucius and Mencius are the basis of morality and the behavior of the sages. Your Majesty designs the curriculum to focus on the great Classics, but now they have yet to be taught.The schools went into further decline starting from the Southern Song period (1127–1279) after the loss of the north to the Jin dynasty. The government was reluctant to fund them because they did not create immediate profits and as a result, cuts were made in teaching personnel, and the national university itself was reduced in size. The exception to this was in the 1140s when the court played an active role in reconstructing schools and made efforts to staff them with degree-holders. After this period, government involvement in state educational institutions ceased. According to contemporary complaints about the schools, they deteriorated in quality, were subjected to a variety of abuses and heterodox teaching, no teaching at all, and decline in revenue. Schools in imperial China never recovered from the decline starting from the Southern Song. For the rest of China's dynastic history, government funded academies functioned primarily as gateways to the examination system, and did not offer any real instruction to students. Functionally they were not schools but rather preparatory institutions for the examinations. Wang's goal of replacing the examinations was never realized. Although Wang's reforms fell short of their mark, they launched the first state led initiative to regulate the day to day education of its subjects through the appointment of teachers and funding of schools.
Primary education was relegated to private schools founded by kinship clusters during the Ming dynasty, although private teachers for individual households remained popular. Some schools were charity projects of the imperial government. The government also funded specialized schools for each of the Eight Banners to teach the Manchu language and Chinese. None of these institutions had a standardized curriculum or age of admission.
The problem of surplus graduates became especially acute starting from the 18th century. Due to the long period of peace established by the Qing dynasty, a large number of candidates were able to make significant progress in their studies and apply for the exams. Their papers were all of similar quality so that examiners found it difficult to differentiate them and make their selections. Officials began to think of new ways to eliminate candidates rather than how to select the best scholars. A complicated set of formal requirements for the examinations was created which undermined the whole system.
Inevitably a large number of candidates failed the exams, often repeatedly. During the Tang period, the ratio of success to failure in the palace exam was 1:100 or 2:100. In the Song dynasty, the ratio of success to failure for the metropolitan exam was about 1:50. In the Ming and Qing dynasties, success to failure for the provincial exam was about 1:100. For every shengyuan in the country, only one in three thousand would ever become a jinshi. While most candidates were men of certain means, those from poor families risked everything on passing the exams. Ambitious and talented candidates who suffered repeated failures felt the bite of indignation, and failure escalated from disappointment to desperation, and sometimes even revolt.
Huang Chao led a massive rebellion in the late Tang dynasty, after it had already been weakened by the An Lushan rebellion. He was born to a wealthy family in western Shandong. After repeated failures he created a secret society that engaged in illicit salt trading. Although Huang Chao's rebellion was ultimately defeated, it led to the final disintegration of the Tang dynasty. Among Huang Chao's cohort were other failed candidates such as Li Zhen, who targeted government officials, killed them and threw their bodies into the Yellow River. Zhang Yuanhao of the Northern Song defected to Western Xia after failing the examinations. He aided the Tanguts in setting up a Chinese-style court. Niu Jinxing of the late Ming was a general in Li Zicheng's rebel army. Having failed to become a jinshi, he targeted high officials and members of the royal family, butchering them as retribution. Hong Xiuquan led the mid-19th-century Taiping Rebellion against the Qing dynasty. After his fourth and final attempt at the shengyuan exam, he had a nervous breakdown, during which he had visions of a heaven where he was part of a celestial family. Influenced by the teachings of Christian missionaries, Hong announced to his family and followers that his visions had been of God, his father, and Jesus Christ, his brother. He created the Taiping Heavenly Kingdom and waged war on the Qing dynasty, devastating parts of southeast China which would not recover for decades.
Pu Songling (1640–1715) failed the examination multiple times. He immortalized the frustrations of candidates trapped in the relentless system in numerous stories that parodied the system.
Reformers charged that the set format of the "eight-legged essay" stifled original thought and satirists portrayed the rigidity of the system in novels such as The Scholars. In the twentieth century, the New Culture Movement portrayed the examination system as a cause for China's weakness in such stories as Lu Xun's "Kong Yiji". Some have suggested that limiting the topics prescribed in examination system removed the incentives for Chinese intellectuals to learn mathematics or to conduct experimentation, perhaps contributing to the Great Divergence, in which China's scientific and economic development fell behind Europe.
However, the political and ethical theories of Confucian classical curriculum have also been likened to the classical studies of humanism in European nations which proved instrumental in selecting an "all-rounded" top-level leadership. British and French civil service examinations adopted in the late 19th century were also heavily based on Greco-Roman classical subjects and general cultural studies, as well as assessing personal physique and character. US leaders included "virtue" such as reputation and support for the US constitution as a criterion for government service. These features have been compared to similar aspects of the earlier Chinese model. In the British civil service, just as it was in China, entrance to the civil service was usually based on a general education in ancient classics, which similarly gave bureaucrats greater prestige. The Cambridge-Oxford ideal of the civil service was identical to the Confucian ideal of a general education in world affairs through humanism. Well into the 20th century, classics, literature, history and language remained heavily favoured in British civil service examinations. In the period of 1925–1935, 67 percent of British civil service entrants consisted of such graduates.
In late imperial China, the examination system was the primary mechanism by which the central government captured and held the loyalty of local-level elites. Their loyalty, in turn, ensured the integration of the Chinese state, and countered tendencies toward regional autonomy and the breakup of the centralized system. The examination system distributed its prizes according to provincial and prefectural quotas, which meant that imperial officials were recruited from the whole country, in numbers roughly proportional to each province's population. Elite individuals all over China, even in the disadvantaged peripheral regions, had a chance at succeeding in the examinations and achieving the rewards and emoluments office brought.
The examination-based civil service thus promoted stability and social mobility. The Confucianism-based examinations meant that the local elites and ambitious would-be members of those elites across the whole of China were taught with similar values. Even though only a small fraction (about 5 percent) of those who attempted the examinations actually passed them and even fewer received titles, the hope of eventual success sustained their commitment. Those who failed to pass did not lose wealth or local social standing; as dedicated believers in Confucian orthodoxy, they served, without the benefit of state appointments, as civilian teachers, patrons of the arts, and managers of local projects, such as irrigation works, schools, or charitable foundations.
During the Tang dynasty, candidates were either recommended by their schools or had to register for exams at their home prefecture. By the Song dynasty, theoretically all adult Chinese men were eligible for the examinations. The only requirement was education.
In practice, a number of official and unofficial restrictions applied to who was able to take the imperial exams. The commoners were divided into four groups according to occupation: scholars, farmers, artisans, and merchants. Beneath the common people were the so-called "mean" people such as boat-people, beggars, sex-workers, entertainers, slaves, and low-level government employees. Among the forms of discrimination faced by the "mean" people were restriction from government office and the credential to take the imperial exam. Certain ethnic groups or castes such as the "degraded" Jin dynasty outcasts in Ningbo, around 3,000 people, were barred from taking the imperial exams as well. Women were excluded from taking the exams. Butchers and sorcerers were also excluded at times. Merchants were restricted from taking the exams until the Ming and Qing dynasties, although as early as 955, the scholar-officials themselves were involved in trading activities. During the Sui and Tang dynasties, artisans were also restricted from official service. During the Song dynasty, artisans, merchants, clerks, and Buddhist and Taoist priests were specifically excluded from the jinshi exam; and, in the Liao dynasty, physicians, diviners, butchers, and merchants were all prohibited from taking the examinations., citing Liao-shih. At times, quota systems were also used to restrict the number of candidates allowed to take or to pass the imperial civil service examinations, by region or by other criteria.
In one prefecture, within a few decades of the dynasty's founding, most of those who passed the jinshi examination came from families that had been members of the local elite for a generation or longer. Despite the six- or sevenfold increase in the numbers of men gaining jinshi, the sons of officials had better chances than most of getting these degrees...Aside from official restrictions, there was also the economic problem faced by men of poorer means. The route to a jinshi degree was long and the competition fierce. Men who achieved a jinshi degree in their twenties were considered extremely fortunate. Someone who obtained a jinshi degree in their thirties, the average age of jinshi candidates, was also considered on schedule. Both were expected to study continuously for years without interruption. Without the necessary financial support, studying for the exams would have been an impractical task. After completing their studies, candidates also had to pay for travel and lodging expenses, not to mention thank-you gifts for the examiners and tips for the staff. A jinshi candidate required someone in the bureaucracy to act as his patron to vouch for their integrity. Banquets and entertainment also had to be paid for. As a result of these expenses, the nurturing of a candidate was a common burden for the whole family.
Each candidate arrived at an examination compound with only a few amenities: a water pitcher, a chamber pot, bedding, food (prepared by the examinee), an inkstone, ink and brushes. Guards verified a student's identity and searched him for hidden texts such as cheat sheets. The facilities provided for the examinee consisted of an isolated room or cell with a makeshift bed, desk, and bench. Each examinee was assigned to a cell according to their number. Paper was provided by the examiners and stamped with an official seal. The examinees of the Ming and Qing periods could take up to three days and two nights writing "eight-legged essays"—literary compositions with eight distinct sections. Interruptions and outside communication were forbidden for the duration of the exam. If a candidate died, officials wrapped his body in a straw mat and tossed it over the high walls that ringed the compound.
At the end of the examination, answer sheets were processed by the sealing office. The Ming era The Book of Swindles (ca. 1617) contains an entire section of stories about "Corruption in Education," most of which involve swindlers exploiting exam-takers' desperate attempts to bribe the examiner. Exact quotes from the classics were required; misquoting even one character or writing it in the wrong form meant failure, so candidates went to great lengths to bring hidden copies of these texts with them, sometimes written on their underwear. The Minneapolis Institute of Arts holds an example of a Qing dynasty cheatsheet, a handkerchief with 10,000 characters of Confucian classics in microscopically small handwriting.
To prevent cheating, the sealing office erased any information about the candidate found on the paper and assigned a number to each candidate's papers. Persons in the copy office then recopied the entire text three times so that the examiners would not be able to identify the author. The first review was carried out by an examining official, and the papers were then handed over to a secondary examining official and to an examiner, either the chief examiner or one of several vice examiners. Judgments by the first and second examining official were checked again by a determining official, who fixed the final grade. Working with the team of examiners were a legion of gate supervisors, registrars, sealers, copyists and specialist assessors of literature.
Pu Songling, a Qing dynasty satirist, described the "seven transformations of the candidate":
When he first enters the examination compound and walks along, panting under his heavy load of luggage, he is just like a beggar. Next, while undergoing the personal body search and being scolded by the clerks and shouted at by the soldiers, he is just like a prisoner. When he finally enters his cell and, along with the other candidates, stretches his neck to peer out, he is just like the larva of a bee. When the examination is finished at last and he leaves, his mind in a haze and his legs tottering, he is just like a sick bird that has been released from a cage. While he is wondering when the results will be announced and waiting to learn whether he passed or failed, so nervous that he is startled even by the rustling of the trees and the grass and is unable to sit or stand still, his restlessness is like that of a monkey on a leash. When at last the results are announced and he has definitely failed, he loses his vitality like one dead, rolls over on his side, and lies there without moving, like a poisoned fly. Then, when he pulls himself together and stands up, he is provoked by every sight and sound, gradually flings away everything within his reach, and complains of the illiteracy of the examiners. When he calms down at last, he finds everything in the room broken. At this time he is like a pigeon smashing its own precious eggs. These are the seven transformations of a candidate.The middle of the stairs of the main hall of the imperial palace during the Tang and Song dynasties were carved with an image of the mythical turtle Ao (鼇). When the list of successful candidates of the palace examination was published, all the names of the graduates were read aloud in the presence of the emperor, and recorded in the archival documents of the dynasty. Graduates were given a green gown, a tablet as a symbol of status, and boots. The first ranked scholar received the title of Zhuàngyuán (狀元) and stood in the middle of the stairs on the carving of Ao. This gave rise to the phrase "to have seized Ao's head" (占鼇頭 zhàn ào tóu), or "to have alone seized Ao's head" (獨占鼇頭 dú zhàn àotóu) to describe a Zhuàngyuán, and more generally to refer to someone who excels in a certain field.
Once a scholar rides in a high carriage drawn by four horses, flagbearers in front, and a mounted escort forming the rear, people would gather on both sides of the road to watch and sigh. Ordinary men and stupid women rush forward in excitement and humble themselves by prostrating themselves in the dust stirred up by the carriage and the horses. This is the elation for a scholar when his ambition is fulfilled.Shengyuan degree holders were given some general tax exemptions. Metropolitan exam graduates were allowed to buy themselves free of exile in cases of crime and to decrease the number of blows with the stick for a fee. Other than the title of jinshi, graduates of the palace examination were also exempted from all taxes and corvee labour.
Graduates of the prefectural examination qualified for employment as teachers in local or family schools, as administrators of granaries or temples, and as subofficial local administrators. The top three graduates of the palace examinations were directly appointed to the Hanlin Academy. Lower ranked graduates could be appointed to offices like Hanlin bachelor, secretaries, messengers in the Ministry of Rites, case reviewers, erudites, prefectural judges, prefects or county magistrates (zhixian 知縣).
During the Tang dynasty, successful candidates reported to the Ministry of Personnel for placement examinations. Unassigned officials and honorary title holders were expected to take placement examinations at regular intervals. Non-assigned status could last a very long time especially when waiting for a substantive appointment. After being assigned to office, a junior official was given an annual merit rating. There was no specified term limit, but most junior officials served for at least three years or more in one post. Senior officials served indefinitely at the pleasure of the emperor.
In the Song dynasty, successful candidates were appointed to office almost immediately and waiting periods between appointments were not long. Between 60 and 80 percent of the civil service was composed of low-ranking officials. They all started their careers in counties not in their home prefecture. These assignments lasted three to four years before they were reassigned to another locality and position. Annual merit ratings were still taken but officials could request evaluation for reassignment. Officials who wished to escape harsh assignments often requested reassignment as a state supervisor of a Taoist temple or monastery. Senior officials in the capital also sometimes nominated themselves for the position of prefect in obscure prefectures.
In addition to an obvious thinning of the numbers as one progresses into the upper ranks, the numbers also reveal important divisions between groups in the middle ranges of the administrative class. The distinct bulge in the court official group (grades twenty-five through twenty-three), with a total of 1,091 officials, the largest of any group, reveals the real promotion barrier between grades twenty-three and twenty-two. The wide disparity between directors (grades nineteen through fifteen) with a total of 160 officials and vice directors (grades twenty-two through twenty) with 619 officials also reveals the importance and the difficulty of promotion above grade twenty. These patterns formed because as early as 1066 the state effectively placed quotas on the number of officials who could be appointed to each group. Promotion across these major boundaries into the above group thus became more difficult.Recruitment by examination during the Yuan dynasty constituted a very minor part of the Yuan administration. Hereditary Mongol nobility formed the elite nucleus of the government. Initially the Mongols drew administrators from their subjects. In 1261, Kublai Khan ordered the establishment of Mongolian schools to draw officials from. The School for the Sons of the State was established in 1271 to give two or three years of training for the sons of the Imperial Bodyguards so that they might become suitable for official recruitment.
Recruitment by examination flourished after 1384 in the Ming dynasty. Provincial graduates were sometimes appointed to low-ranking offices or entered the Guozijian for further training, after which they might be considered for better appointments. Before appointment to office, metropolitan graduates were assigned to observe the functions of an office for up to one year. The maximum tenure for an office was nine years, but triennial evaluations were also taken, at which point an official could be reassigned. Magistrates of districts submitted monthly evaluation reports to their prefects and the prefects submitted annual evaluations to provincial authorities. Every third year, provincial authorities submitted evaluations to the central government, at which point an "outer evaluation" was conducted, requiring local administration to send representatives to attend a grand audience at the capital. Officials at the capital conducted an evaluation every six years. Capital officials of rank 4 and above were exempted from regular evaluations. Irregular evaluations were conducted by censorial officials.
Graduates of the metropolitan examination during the Qing dynasty were assured influential posts in the officialdom. The Ministry of Personnel submitted a list of nominees to the emperor, who then decided all major appointments in the capital and in the provinces in consultation with the Grand Council. Appointments were generally on a three-year basis with an evaluation at the end and the option for renewal. Officials rank three and above were personally evaluated by the emperor. Due to a population boom in the early modern era, qualified men far exceeded vacancies in the bureaucracy so that many waited for years between active duty assignments. Purchase of office became a common practice during the 19th century since it was very hard for qualified men to be appointed to one of the very limited number of posts. Even receiving empty titles with no active assignment required a monetary contribution.
The Ministry of Rites, under the Department of State Affairs, was responsible for organizing and carrying out the imperial examinations.
The Hanlin Academy was an institution created during the reign of Emperor Xuanzong of Tang (r. 712–755). It was located directly in the palace compound and staffed by officials who drafted official documents such as edicts. These officials, commonly appointed from the top three ranks of the palace examination graduates, came to be known as academicians (xueshi) after 738, when a new building was constructed to provide them living quarters. The title "academician" was not only for the staff of the Hanlin Academy, but any special assignment to special posts. The number of academicians in the Hanlin Academy was fixed to six at a later date and they were given the task of doing paperwork and consulting the emperor. The Hanlin Academy drafted documents about the appointment and dismissal of high ministers, proclamation of amnesties, and imperial military commands. It also aided the emperor in reading documents.
The number of Hanlin academicians was reduced to two during the Song dynasty. During the Yuan dynasty, a Hanlin Academy just for Mongols was created to translate documents. More emphasis was put on the oversight of imperial publications such as dynastic histories.
In the Qing dynasty, the number of posts in the Hanlin Academy increased immensely and a Manchu official was installed at all times. The posts became purely honorary and the institution was reduced to just another stepping stone for persons seeking higher positions in the government. Lower officials in the Hanlin Academy often had other posts at the same time.
The Taixue (National University), was the highest educational institution in imperial China. During the reign of Emperor Wu of Han (r. 141-87 BC), Confucianism was adopted as the state doctrine. The Confucian scholar Dong Zhongshu suggested establishing a National University (Taixue) in the capital, Chang'an, so that erudites could teach the Classics. Teaching at the Taixue was a prestigious job because the emperor commonly picked from among them for appointment in high offices. At first the Taixue had only 50 students but increased to around 3,000 by the end of the millennium. Under the reign of Wang Mang (r. 9–23), two other educational institutions called the Biyong and Mingtang were established south of the city walls, each able to house 10,000 students. The professors and students were able to exercise some political power by criticizing their opponents such as governors and eunuchs. This eventually led to the arrest of more than 1,000 professors and students by the eunuchs.
After the collapse of the Han dynasty, the Taixue was reduced to just 19 teaching positions and 1,000 students but climbed back to 7,000 students under the Jin dynasty (266–420). After the nine rank system was introduced, a "Directorate of Education" (Guozijian) was created for persons rank five and above, effectively making it the educational institution for nobles, while the Taixue was relegated to teaching commoners. Over the next two centuries, the Guozijian became the primary educational institute in the Southern Dynasties. The Sixteen Kingdoms and Northern Dynasties also created their own schools but they were only available for sons and relatives of high officials. The Northern Wei dynasty founded the Primary School of Four Gates.
During the Sui dynasty, a Law School, Arithmetics School, and Calligraphy School were put under the administration of the Guozijian. These schools accepted the relatives of officials rank eight and below while the Taixue, Guozijian, and Four Gates School served higher ranks. By the start of the Tang dynasty (618-907), 300 students were enrolled in the Guozijian, 500 at the Taixue, 1,300 at the Four Gates School, 50 at the Law School, and a mere 30 at the Calligraphy and Arithmetics Schools. Emperor Gaozong of Tang (r. 649–683), founded a second Guozijian in Luoyang. The average age of admission was 14 to 19 but 18 to 25 for the Law School. Students of these institutions who applied for the state examinations had their names transmitted to the Ministry of Rites, which was also responsible for their appointment to a government post.
During the Song dynasty, Emperor Renzong of Song founded a new Taixue at Kaifeng with 200 students enrolled. Emperor Shenzong of Song (r. 1067–1085) raised the number of students to 2,400. He also implemented the "three-colleges law" (sanshefa三舍法) in 1079, which divided the Taixue into three colleges. Under the three-colleges law, students first attended the Outer College, then the Inner College, and finally the Superior College. One of the aims of the three-colleges was to provide a more balanced education for students and to de-emphasize Confucian learning. Students were taught in only one of the Confucian classics, depending on the college, as well as arithmetics and medicine. Students of the Outer College who passed a public and institutional examination were allowed to enter the Inner College. At the Inner College there were two exams over a two-year period on which the students were graded. Those who achieved the superior grade on both exams were directly appointed to office equal to that of a metropolitan exam graduate. Those who achieved an excellent grade on one exam but slightly worse on the other could still be considered for promotion, and having a good grade in one exam but mediocre in another still awarded merit equal to that of a provincial exam graduate.
Since the court ordered the erection of countrywide schools in the Qingli era, prefectures and counties had their own school campuses, fields and hostels. The Directorate of Education in the capital was also split to establish the Imperial University. Usually a few hundred students attend the Imperial University, while the number of students in the prefectural school have nearly reached a hundred. Such flourishing schools were reminiscent of the preceding Han and Tang dynasties. Nevertheless, our country does not have a selection mechanism through schools. I sincerely hope that those prefectural officials in service for over a year could nominate students of extraordinary talent and conduct for promotion to the Imperial University. When students are judged to be of insufficient merit, no nomination shall be made. After these students have registered at the Imperial University, the government should provide daily and monthly stipends, while closely observing and frequently evaluating their conduct and ability. Every year, the directorate supervisor and lecturers shall nominate ten candidates. After examination by the court, those selected candidates would be conferred the advanced scholar degree using the degree quota. For the scions of high officials as well as existing students, admissions should be sought in other ways.In 1104, the prefectural examinations were abolished in favor of the three-colleges system, which required each prefecture to send an annual quota of students to the Taixue. This drew criticism from some officials who claimed that the new system benefited the rich and young, and was less fair because the relatives of officials could enroll without being examined for their skills. In 1121, the local three-college system was abolished but retained at the national level. For a time, the national examination system was also abandoned in favor of directly appointing students of the Taixue to government posts. The Taixue itself did not survive the demise of the Song dynasty and ceased to exist afterwards, becoming a synonym for the Guozijian.
The Guozijian (Directorate of Education) was founded under Emperor Wu of Jin (r. 265–289) to educate the nobility. Under the Song dynasty, the Guozijian became the central administrative institution for all state schools throughout the empire. Among its duties were the maintenance of the buildings, the construction of new facilities, and the promotion of students. The Guozijian itself was equipped with a library and printing shop to create model printing blocks for distribution. The Guozijian was abolished in 1907.
The examinations were given a formal structure and tier system during the Song dynasty but the exact number of tiers, their titles, degrees awarded, and names differed depending on era and dynasty. Sometimes examinations of the same tier from different dynasties had different names. For example, the palace examination was called dianshi or yushi during the Song, but tingshi in the Ming and the metropolitan examination was shengshi in the Song and huishi in the Jin, Yuan, and Ming. In other cases, examinations of different tiers used the same name. Fushi in the Liao meant prefectural examination but provincial examination in the Jin. The lowest tier exam was usually held annually while higher tier exams held triennially. Tight quotas restricted the number of successful candidates at each level—for example, only three hundred students could pass the metropolitan examinations. Students often took the examinations several times before earning a degree.
By the Ming dynasty, the examinations and degrees formed a "ladder of success", with success generally being equated with being graduated as jinshi, a degree similar to a modern Doctor of Literature degree, or PhD. Modifications to the basic jinshi or other degree were made for higher-placing graduates, similar to the modern Summa cum laude. The examination process extended down to the county level, and included examinations at the provincial and national levels. The highest level tests would be at the imperial court or palace level, of which the jinshi was the highest regular level, although special purpose tests were occasionally offered, by imperial decree:
The classicist (mingjing) originated as a category for recruitment by local authorities to the appointment of state offices. The term was created during the Han dynasty under Emperor Wu of Han for candidates eligible for official appointment or for enrolment in the Taixue. Classicists were expected to be familiar with the Confucian canon and the Taoist text Laozi. The classicist category fell out of use under Cao Wei and the Jin dynasty but was revived during the Southern Dynasties for filling places in the Taixue.
During the Sui dynasty, examinations for classicists and cultivated talents were introduced. Unlike cultivated talents, classicists were only tested on the Confucian canon, which was considered an easy task at the time, so those who passed were awarded posts in the lower rungs of officialdom. Classicists were tested by being presented phrases from the classic texts. They then had to write the whole paragraph to complete the phrase. If the examinee was able to correctly answer five of ten questions, they passed. This was considered such an easy task that a 30-year-old candidate was said to be old for a classicist examinee, but young to be a jinshi. An oral version of the classicist examination known as moyi also existed but consisted of 100 questions rather than just ten. In contrast, the jinshi examination tested not only the Confucian classics, but also history, proficiency in compiling official documents, inscriptions, discursive treatises, memorials, and poems and rhapsodies. In 742, Laozi was replaced in the examination by the glossary Erya. In 1071, Emperor Shenzong of Song (r. 1067–1085) abolished the classicist as well as various other examinations on law and arithmetics.
The cultivated talent (xiucai) originated in the Han dynasty when Emperor Wu of Han declared that each province had to present one cultivated talent per year to be appointed in the government. During the Sui dynasty, examinations for "cultivated talents" were introduced. They were tested on matters of statecraft and the Confucian canon. This type of examination was very limited in its implementation and there were only 11 or 12 graduates throughout its entire history. During the Song dynasty, cultivated talent became a general title for graduates of the state examinations.
The strategic questions examination (cewen) was a question-and-answer type essay examination introduced during the Han dynasty. The purpose of the exam was to ensure examinees could apply Confucian doctrine to practical matters of statecraft. A question on a problematic political issue was asked and the examinee was expected to answer it according to his own opinion and how the issue could be resolved. The strategic questions examination became obsolete during the Ming dynasty due to the prevalence of the eight-legged essay.
During the reign of Wu Zetian the imperial government created military examinations for the selection of army officers as a response to the breakdown of garrison militias known as the Fubing system. The first formal military examinations (武舉 Wǔjǔ) were introduced in 702. The military examinations had the same general arrangement as the regular exams, with provincial, metropolitan and palace versions of the exams. Successful candidates were awarded military versions of Jinshi and Juren degrees: Wujinshi (武進士) and Wujuren (武舉人), and so on.
The military exam included both a written and physical portion. In theory, candidates were supposed to master not only the same Confucian texts as required by the civil exam, but also Chinese military texts such as The Art of War, in addition to martial skills such as archery and horsemanship. The district exam was conducted by the county magistrate and consisted of three sessions.
The first session tested mounted archery by having candidates shoot three arrows while riding a horse toward a target at a distance of 35 and 80 paces. The target was in the shape of a man 1.6 meters high. A perfect score was three hits, a good score two, and one hit earned a pass. Those who fell off their horse or failed to score even one hit were eliminated. The second session was held in a garden at the prefectural office. Candidates were ordered to shoot five arrows at a target at 50 paces. Again, five hits were graded excellent while one hit earned a pass. Next they had to bend a bow into the shape of a full moon. The bows were graded by strength into 72 kg, 60 kg, and 48 kg weapons. Bending a 72 kg bow was excellent while bending a 48 kg bow earned a pass. Then they were ordered to perform a number of exercises with a halberd without it touching the ground. The halberds were graded by weight from 72 kg to 48 kg, with the lowest grade weapon earning a pass. For the final portion of the second session, candidates were required to lift a stone 35 cm off the ground. Lifting a 180 kg stone earned an excellent grade, a 150 kg stone good, and a 120 kg stone passing.
The third session involved writing out by memory entire portions of the Seven Military Classics, but only three of the classics were ever used, those being The Methods of the Sima, the Wuzi, and The Art of War. Even just memorizing the reduced portion of the classics was too difficult for most military examinees, who resorted to cheating and bringing with them miniature books to copy, a behavior the examiners let slide owing to the greater weighting of the first two sessions. In some cases the examinees still made mistakes while copying the text word for word. The contents of the military exam were largely the same at the prefectural, provincial, metropolitan, and palace levels, with the only difference being tougher grading.
Military degrees were considered inferior to civil degrees and did not carry the same prestige. The names of civil jinshi were carved in marble whereas military jinshi were not. While the military and civil services were imagined in Chinese political philosophy as the two wheels of a chariot, in practice, the military examination degree was highly regarded by neither the army or the world at large. Soldiers preferred not having military exam graduates as commanders whose skills in test taking did not necessarily transfer to the army. Final decision for appointment in the military still came down to forces outside the examination system. For example, at the beginning of 755, An Lushan replaced 32 Han Chinese commanders with his own barbarian favorites without any repercussions. During the Qing dynasty, the pre-existing institutions of the Eight Banners and Green Standard Army had their own rules for promotion and left little room for military exam graduates. Some of the few military examination graduates who did achieve distinction include the Tang general Guo Ziyi, the father of the founder of the Song dynasty Zhao Hongyin, Ming generals Yu Dayou and Qi Jiguang, and Ming general turned traitor Wu Sangui. However these are but a minuscule number among those who passed the 282 military metropolitan exams held between their inception in 702 and abolishment in 1901. Even in desperate times of war, the majority of distinguished military figures in Chinese history have come from civil degree holders. The practices of the Ming and Qing military exams were incorporated into physical education in the Republic of China.
During the Qing dynasty, translation examinations were held for young men from the Eight Banners who held no military post. Manchus, Mongols and Chinese Bannermen were allowed to participate in the Manchu exam while the Mongolian exam was restricted to Mongol Bannermen. The examinees did not take the examinations expecting to become translators. The content of the exam consisted of material from the Manchu or Mongol versions of the Four Books and the Five Classics, while only a minor part of the exam consisted of translation from Chinese into Manchu or Mongolian. Three levels of the exam were implemented but there was no palace examination. The quota on the provincial level was 33 persons for the Manchu, and 9 for the Mongolian examination. The number of graduates declined to just 7 and 3 persons, respectively, in 1828, and 4 and 1 in 1837. In 1840 the Mongolian exam was abolished because there were only 6 candidates. Graduates of the metropolitan translation examination were all given the title of regular metropolitan translation graduate without further gradation or extraordinary designations. Excellent graduates of the Manchu exam were directly appointed secretaries in one of the Six Ministries while those of the Mongol one were commonly made officials in the Court of Colonial Affairs.
Besides the regular tests for the jinshi and other degrees, there were also occasionally special purpose examinations, by imperial decree (zhiju). Decree examinations could be for a number of purposes such as identifying talent for specific assignments, or to satisfy particular interest groups such as ethnic groups and the imperial clan. In the Tang dynasty, the emperor held on occasion irregular examinations for specialized topics. These were open to persons already employed by the government. During the Song dynasty, in 1061, Emperor Renzong of Song decreed special examinations for the purpose of finding men capable of "direct speech and full remonstrance" (zhiyan jijian): the testing procedure required the examinees to submit 50 previously prepared essays, 25 on particular contemporary problems, 25 on more general historical governmental themes. In the examination room, the examinees then had a day to write essays on six topics chosen by the test officials, and finally were required to write a 3,000 character essay on a complex policy problem, personally chosen by the emperor, Renzong. Among the few successful candidates were the Su brothers, Su Shi and Su Zhe (who had already attained their jinshi degrees, in 1057), with Su Shi scoring exceptionally high in the examinations, and subsequently having copies of his examination essays widely circulated.
All Chinese imperial examinations were written in Classical Chinese, also known as Literary Chinese, using the regular script (kaishu), which is today the most commonly seen calligraphic style in modern China. The importance of knowledge in Classical Chinese was retained in examination systems in other countries such as in Japan, Korea, and Vietnam, where candidates were required to be well-versed in the Confucian classics, to be able to compose essays and poetry in Classical Chinese, and to be able to write in regular script. Owing to the examination system, Classical Chinese became a basic educational standard throughout these countries. Readers of Classical Chinese did not need to learn spoken Chinese to understand or read the text because of its logographic nature. Texts written in Classical Chinese could be "read and understood by any sufficiently literate person, even if a given text was ultimately voiced in Japanese, Korean, or Vietnamese and not mutually intelligible in speech." This shared textual tradition and common understanding of the Confucian canon allowed these countries to communicate with each other through "brush talk" in the absence of a common language.
In both Korea and Japan, forms of writing were developed to assist readers in understanding Classical Chinese. The Korean Gugyeol and Japanese Kanbun Kundoku writing systems modified the Chinese text with markers and annotations to represent their respective languages' pronunciation and grammatical order. Chinese script was also adapted to write Korean and Japanese in their respective native word order. In Korea this was called the Idu script (official reading) and in Japan, Man'yōgana (ten thousand leaves). In Japan, Kanbun was used to write official documents from the 8th century until after World War 2. In Korea, the Idu script was used for writing official documents and the civil service examinations from their establishment in 958 to termination in 1894. Vietnamese examinations used Chữ Hán (Chinese script), also called Chữ nho (Confucian script), which is virtually indistinguishable from Classical Chinese in its written form, but uses Vietnamese pronunciations when read aloud. Chữ nho was made the official writing system of Đại Việt in 1174 and remained the writing system of the administration until Vietnam was taken over by France in the 19th century. Similar to Korea and Japan, Vietnam also adapted Chinese script to write the Vietnamese language in what is known as Chữ Nôm (southern characters).
Owing to the shared literary and philosophical traditions rooted in Confucian and Buddhist texts and the use of the same script, a large number of Chinese words were borrowed into Korean, Japanese, and Vietnamese (KJV). To prepare for the civil service exams, candidates mastered Chinese words through reciting the texts and composing prose and verses in Classical Chinese, so it is hardly surprising that they gradually added Chinese vocabulary to their native lexicon. In today’s terms, these borrowed words are referred to as Sino-Korean, Sino-Japanese, and Sino-Vietnamese words, where the prefix “Sino-” refers to China. However, they have long been integrated into the vocabulary of the KJV languages so that native speakers may be oblivious of their Sinitic origin.Some of the main outstanding questions regarding the imperial examinations are in regard to poetry. There is a long history of debate on the usefulness of the procedure of testing the ability of the candidates to write poetry. During the Tang dynasty, a poetry section was added to the examinations, requiring the examinee to compose a shi poem in the five-character, 12-line regulated verse form and a fu composition of 300 to 400 characters. The poetry requirement remained standard for many decades, despite some controversy, although briefly abolished for the examination year 833–834 (by order of Li Deyu). During the Song dynasty, in the late 1060s Wang Anshi removed the traditional poetry composition sections (regulated verse and fu), on the grounds of irrelevancy to the official functions of bureaucratic office: on the other side of the debate, Su Shi (Dongpo) pointed out that the selection of great ministers of the past had not been obstructed by the poetry requirements, that the study and practice of poetry encouraged careful writing, and that the evaluation and grading of poetry was more objective than for the prose essays, due to the strict and detailed rules for writing verse according to the formal requirements.
Starting from the Yuan dynasty, poetry was abolished as a subject in the examinations, being regarded as frivolous. This process was completed at the inception of the following Ming dynasty. It was revived in 1756 by the Qing dynasty.
The imperial examinations influenced traditional Chinese religion as well as contemporary literary tradition. The examination system ostensibly represented the Confucian system in its most rationalist expression and was designed to achieve a society ruled by men of merit, as determined by an objective measure of the candidates' knowledge and intelligence. In practice, the examinations also included various religious and superstitious beliefs that extend the examinations beyond Confucian idealism.  Traditional beliefs about fate, that cosmic forces predestine certain human affairs, and particularly that individual success or failure was subject to the will of Heaven and the influence and intervention by various deities, played into the interpretation of results when taking the tests. Zhong Kui, also known as Chung-kuei, was a deity associated with the examination system. The story is that he was a scholar who took the tests, and, despite his most excellent performance, he was unfairly deprived of the first-place prize by a corrupt system: in response, he killed himself, the act of suicide condemning him to be a ghost. Many people afraid of traveling on roads and paths that may be haunted by evil spirits have worshiped Zhong Kui as a protective deity. Also known as Kechang Yiwen Lu, the Strange Stories from the Examination Halls was a collection of stories popular among Confucian scholars of the Qing dynasty. The theme of many of the stories is that good deeds are rewarded by success in the examination halls, often by Heaven-inspired deities acting on karmic principles; and evil deeds result in failure, often under the influence of the ghosts of victims.
Some individuals were discriminated against because of their names, due to a naming taboo. For example, because the Tang dynasty poet Li He's father's name sounded like the jin, in jinshi, he was discouraged from taking the tests. The claim was that if Li He was called a jinshi, it would be against the rule of etiquette that a son not be called by his father's name.
Chinese legal institutions and the examination system began to heavily influence Japan during the Tang dynasty. In 728, Sugawara no Kiyotomo took part in the Tang examinations. Near the end of the Tang era, Japan implemented the examination system, which lasted for some 200 years during the Heian period (794-1185). According to the Records of Palace Examinations, four shinshi (jinshi) passed the exams in 916: Fujiwara Takaki, Oue Koretoki, Harubuchi Yoshiki, and Fujiwara Harufusa. Poets such as Sugawara no Michizane and Miyoshi Yoshimune mentioned their experiences with the examinations in their writing:
Recalling the hard days of preparing for exam, I feel the present happiness is greater than any other happiness; having passed the imperial exam, I feel my present standpoint is higher than any others.When I am alone, I cannot help thinking of successive failures in exams due to illness and inadequate talent, revealing the agony of those who failed.Like the Chinese examinations, the curriculum revolved around the Confucian canon. As Japanese examinations developed, they diverged in practice from the Chinese ones. The xiucai exams became more popular than the jinshi because it was simpler and considered more practical. The Japanese examinations were also never opened to the common folk to the same extent as in China during the Song dynasty. Due to aristocratic influence, by the 10th century, only students recommended based on their reputation and record of service could take the exams. While the exams were still held after the 11th century, they had lost all practical value, and any candidates who had been nominated by dignitaries passed unconditionally. The Japanese imperial examinations gradually died out afterwards. The examinations were revived in 1787 during the Edo period. The new examinations, called sodoku kugin, were more or less the same as the Chinese ones in terms of content (Confucianism and Neo-Confucianism), but did not confer official titles, only honorary titles. During the early Meiji era, Kanda Takahira wrote a letter advocating for the establishment of a Japanese recruitment system with the Chinese imperial examination system as a model. The proposal failed to gain support.
Heian Japan did have a three-step exam system, with testing on the bureau, ministry, and imperial levels. But due to the power of aristocratic lineages, exam success did not translate into recruitment and political success. Scholars did have authoritative status, which is even obvious in parodies castigating their stuffiness, presumption, and lackadaisical diction appearing in vernacular works such as Murasaki Shikibu’s The Tale of Genji (Genji monogatari 源氏物語, ca. -1014). But after the heyday of the State Academy during the eighth and ninth centuries, its social significance declined, to the point that it was not even rebuilt when it burned down in the twelfth century. With the famous exception of Sugawara no Michizane 菅原道真 (845– 903), who earned senior first rank posthumously after dying miserably in exile due to machinations of the ascendant Fujiwara clan, scholars were and remained typically of middle rank. The function of the Academy was taken over by clan schools (J. bessō 別曹), and Confucian learning became a hereditary profession, with members of the Nakahara and Kiyohara clans specializing in the Classics and members of the Sugawara, Ôe, and some branches of the Fujiwara clan focusing on the Letters track.Korea directly participated in the Chinese imperial examination system during the 9th century when as many as 88 Sillans received degrees after passing the Tang examinations.
The Korean examination system was established in 958 under the reign of Gwangjong of Goryeo. The examination system was spread to Goryeo in 957 by a visiting Hanlin scholar named Shuang Ji from Later Zhou. Gwangjong was highly pleased with Shuang Ji and requested that he remain at the Korean court permanently.
According to Xu Jing, writing during the Song dynasty, the Korean examinations were largely the same as the Chinese ones with some differences:
The Korean civil service recruitment system follows that of our country with some differences: the students take primary exams in the temple of King Wenxuan every year and the eligible ones are titled as gongshi; among them
about 350 will be selected after another examination one year later; those people then will take a higher exam held in Ying’en accommodation; about thirty to forty people will be admitted and classified to five grades, which is similar to metropolitan examination of our country; the final exam will be presided by the king himself who will test them on poetry, prose and argumentation essays. I think it is ridiculous that there is no question about current affairs in the exam. Moreover, there is one subject of diction in the exam, though seldom held, valuing literary grace and rhymes more than essence of classics, which might be inherited from the malpractices of the Tang Dynasty.Some Korean examination practices converged with the Chinese system. By the end of the Goryeo period, a military exam had been added, the triennial schedule observed, and the exam hierarchy organized into provincial, metropolitan, and palace levels, similar to the Chinese. Other practices, such as the inclusion of exams on Buddhism and the worship of Confucius, were not shared with China. Outside China, the examination system was most widely implemented in Korea, with enrollment rates surpassing even that of China. Any free man (not Nobi) was able to take the examinations. At the start of the Joseon period, 33 candidates were selected from every triennial examination, and the number increased to 50 later on. In comparison, China's selected candidates after each palace examination were no more than 40 to 300 from the Tang to Ming dynasties while encompassing a landmass six times larger than Korea. By the Joseon period, high offices were closed to aristocrats who had not passed the exams. Over the span of 600 years, the Joseon civil service selected more than 14,606 candidates in the highest level examinations on 744 occasions. The examination system continued until 1894 when it was abolished by the Gabo Reform. All Korean examinations were conducted using the Idu script from their establishment in 958 to their termination in 1894.
The Confucian examination system in Vietnam was established in 1075 under the Lý dynasty Emperor Lý Nhân Tông and lasted until the Nguyễn dynasty Emperor Khải Định (1919). During the Lý dynasty (1009–1225) the imperial examinations had very limited influence. Only four were held throughout the entire duration of the dynasty, producing a small handful of officials. Later during the Trần dynasty (1225–1400), Trần Thái Tông began to filter students through the Tiến sĩ (jinshi) examinations. From 1232 to 1314, ten Tiến sĩ examinations were held. In 1314, 50 Tiến sĩ were recruited in one exam. Throughout most of the Vietnamese examinations' history, there were only three levels to the Vietnamese system: interprovincial, pre-court, and court. A provincial examination was implemented by the Nguyễn dynasty (1802–1883) in 1807 and a metropolitan examination in 1825. Emperor Minh Mạng (r. 1820–1839) paid special attention to the examinations and even dined with newly recruited Tiến sĩ frequently. Although Confucian content took precedence in the examinations, material on Buddhism and Daoism were also included. In 1429, Lê Thái Tổ ordered famous Buddhist and Daoist monks to take the exams, and if they failed, they had to forfeit their religious life. Elephants were used to guard the examination halls until 1843 when the emperor said it was no longer necessary. During the 845 years of civil service examinations held in Vietnam, about 3,000 candidates passed the highest level exams and had their names carved on stelae in the Temple of Literature in Hanoi.
The imperial examination system was known to Europeans as early as 1570. It received great attention from the Jesuit Matteo Ricci (1552–1610), who viewed it and its Confucian appeal to rationalism favorably in comparison to religious reliance on "apocalypse." Knowledge of Confucianism and the examination system was disseminated broadly in Europe following the Latin translation of Ricci's journal in 1614. During the 18th century, the imperial examinations were often discussed in conjunction with Confucianism, which attracted great attention from contemporary European thinkers such as Gottfried Wilhelm Leibniz, Voltaire, Montesquieu, Baron d'Holbach, Johann Wolfgang von Goethe, and Friedrich Schiller. In France and Britain, Confucian ideology was used in attacking the privilege of the elite. Figures such as Voltaire claimed that the Chinese had "perfected moral science" and François Quesnay advocated an economic and political system modeled after that of the Chinese. According to Ferdinand Brunetière (1849-1906), followers of Physiocracy such as François Quesnay, whose theory of free trade was based on Chinese classical theory, were sinophiles bent on introducing "l'esprit chinois" to France. He also admits that French education was really based on Chinese literary examinations which were popularized in France by philosophers, especially Voltaire. Western perception of China in the 18th century admired the Chinese bureaucratic system as favourable over European governments for its seeming meritocracy. However those who admired China such as Christian Wolff were sometimes persecuted. In 1721 he gave a lecture at the University of Halle praising Confucianism, for which he was accused of atheism and forced to give up his position at the university.
Johann Heinrich Gottlob Justi based much of his inspiration for cameralism on contemporary accounts of the Chinese imperial bureaucracy. The growth of cameralist studies, which played an important role in Prussian civil service training, may be traced to Justi's admiration for the Imperial examinations of China. Justi, like other cameralists, also lauded many Chinese public policies including the examination system.
East Asia’s academies were associated with an examination system. What stunned early modern European missionaries in China was the connection between a state-run examination system and recruitment into civil service. The idea of a system that seemed to place merit over birth and allow for dramatic social mobility was most attractive for contemporary Europeans in the grip of the hazards of absolutist monarchies. Although access to the academies was often limited to children from families of a certain rank, and much modern scholarship has highlighted the limitations for social mobility in these systems, it is important to acknowledge the very existence of institutions that in their principles and ideological rhetoric rewarded moral and academic worth.The earliest evidence of examinations in Europe date to 1215 or 1219 in Bologna. These were chiefly oral in the form of a question or answer, disputation, determination, defense, or public lecture. The candidate gave a public lecture of two prepared passages assigned to him from the civil or canon law, and then doctors asked him questions, or expressed objections to answers. Evidence of written examinations do not appear until 1702 at Trinity College, Cambridge. According to Sir Michael Sadler, Europe may have had written examinations since 1518 but he admits the "evidence is not very clear." In Prussia, medication examinations began in 1725. The Mathematical Tripos, founded in 1747, is commonly believed to be the first honor examination, but James Bass Mullinger considered "the candidates not having really undergone any examination whatsoever" because the qualification for a degree was merely four years of residence. France adopted the examination system in 1791 as a result of the French Revolution but it collapsed after only ten years. Germany implemented the examination system around 1800.
Englishmen in the 18th century such as Eustace Budgell recommended imitating the Chinese examination system. Adam Smith recommended examinations to qualify for employment in 1776. In 1838, the Congregational church missionary Walter Henry Medhurst considered the Chinese exams to be "worthy of imitating." In 1806, the British established a Civil Service College near London for training of the East India Company's administrators in India. This was based on the recommendations of British East India Company officials serving in China and had seen the Imperial examinations. In 1829, the company introduced civil service examinations in India on a limited basis. This established the principle of qualification process for civil servants in England. In 1847 and 1856, Thomas Taylor Meadows strongly recommended the adoption of the Chinese principle of competitive examinations in Great Britain. Both Thomas Babington Macaulay, who was instrumental in passing the Saint Helena Act 1833, and Stafford Northcote, 1st Earl of Iddesleigh, who prepared the Northcote–Trevelyan Report that catalyzed the British civil service, were familiar with Chinese history and institutions. When the report was brought up in parliament in 1853, Lord Monteagle argued against the implementation of open examinations because it was a Chinese system and China was not an "enlightened country." Lord Stanley called the examinations the "Chinese Principle." The Earl of Granville did not deny this but argued in favor of the examination system, considering that the minority Manchus had been able to rule China with it for over 200 years. In 1854, Edwin Chadwick reported that some noblemen did not agree with the measures introduced because they were Chinese. The examination system was finally implemented in the British Indian Civil Service in 1855, prior to which admission into the civil service was purely a matter of patronage, and in England in 1870. Even as late as ten years after the competitive examination plan was passed, people still attacked it as an "adopted Chinese culture." Alexander Baillie-Cochrane, 1st Baron Lamington insisted that the English "did not know that it was necessary for them to take lessons from the Celestial Empire." In 1875, Archibald Sayce voiced concern over the prevalence of competitive examinations, which he described as "the invasion of this new Chinese culture."
After Great Britain's successful implementation of systematic, open, and competitive examinations in India in the 19th century, similar systems were instituted in the United Kingdom itself, and in other Western nations. Like the British, the development of the French and American civil service was influenced by the Chinese system. When Thomas Jenckes made a Report from the Joint Select Committee on Retrenchment in 1868, it contained a chapter on the civil service in China. In 1870, William Spear wrote a book called The Oldest and the Newest Empire-China and the United States, in which he urged the United States government to adopt the Chinese examination system. Like in Britain, many of the American elites scorned the plan to implement competitive examinations, which they considered foreign, Chinese, and "un-American." As a result, the civil services reform introduced into the House of Representatives in 1868 was not passed until 1883. The Civil Service Commission tried to combat such sentiments in its report:
...with no intention of commending either the religion or the imperialism of China, we could not see why the fact that the most enlightened and enduring government of the Eastern world had acquired an examination as to the merits of candidates for office, should any more deprive the American people of that advantage, if it might be an advantage, than the facts that Confucius had taught political morality, and the people of China had read books, used the compass, gunpowder, and the multiplication table, during centuries when this continent was a wilderness, should deprive our people of those conveniences.After the fall of the Qing in 1912, Dr. Sun Yat-sen, the leader of the newly risen Republic of China, developed similar procedures for the new political system through an institution called the Examination Yuan, one of the five branches of government, although this was quickly suspended due to the turmoil in China between the two world wars, such as the Warlord era and the Japanese invasion. The Kuomintang administration revived the Examination Yuan in 1947 after the defeat of Japan. This system continues into present times in Taiwan along with the government itself after loss of the mainland to the Chinese Communist Party.
The Civil Service in the People's Republic of China (PRC) since the economic reform era maintains a system of examinations for selection and promotion of civil servants.
In the Republic of China (ROC), the constitution specifies that a public servant cannot be employed without going through an examination. The employment is usually also lifelong (that is, until age about retirement).
Emperor Lizong of Song (26 January 1205 – 16 November 1264), personal name Zhao Yun, was the 14th emperor of the Song dynasty of China and the fifth emperor of the Southern Song dynasty. He reigned from 1224 to 1264.
His original name was Zhao Yuju but later changed his name to Zhao Guicheng and then finally changed his name to Zhao Yun being elevated to an imperial son. Although he was a descendant of the Song dynasty's founder Zhao Kuangyin (Emperor Taizu) through his son Zhao Dezhao and hence a member of the imperial clan, Zhao Yun was not in line to succeed to the throne as his family had no political status. Shi Miyuan (史彌遠), who was the chancellor for many years, collaborated with Empress Dowager Yang and when Emperor Ningzong eventually died in 1224, Shi, along with Empress Dowager Yang, supplanted the reigning crown prince Zhao Hong and replaced him with Zhao Yun as emperor, reigning with the era name Baoqing and the temple name Lizong. The 40-year-reign of Emperor Lizong did little to nothing to improve the Dynasty and instead sought pleasure even as the Mongols were terrorizing the borders. He died at age 59 in 1264 and was succeeded by his nephew Emperor Duzong.
Although related to the Song imperial family, Emperor Lizong was only distantly related as he was the 10th generation descendant of the founding Song Emperor, Emperor Taizu more than 250 years before. Emperor Lizong spent his childhood in obscurity living away from the imperial court in Shaoxing, Zhejiang as a minor official. One rainy day in his early teens, along with his younger brother Zhao Yurui, Lizong stood huddling under a bridge along with an official, Yu Tianxi, who was sent by Chancellor Shi Miyuan tasked to locate a suitable successor to the Song throne as the incumbent crown prince Zhao Xun has recently died at the age of 29. After discovering the true identity of this teenager, Yu quickly informed Shi Miyuan who agreed to groom Lizong to be replaced as the crown prince. It is said that Lizong was serious, studious, mostly silent, and wise before assuming the throne. When Ningzong died in 1224, Shi Miyuan told Lizong to enter the throne room and then ordered Zhao Hong, then the current crown prince into the room. Shi Miyuan subsequently declared Lizong emperor and according to history, he was wearing a white robe and the lights reflected his shadow on the throne. The result of Lizong becoming Emperor angered Zhao Hong who refused to bow until he was forced to by the palace commander. He was then sent away.
Lizong's long reign of 40 years did little to improve the predicament of the Song Empire in his time. The court of Emperor Lizong was dominated by consort clans, Yan and Jia, the eunuchs Dong Songchen and Lu Yunsheng, and his Co-Regent Empress Dowager Yang. The Emperor was uninterested in governmental affairs, and for the first decade of his rule he delegated matters into the hands of his ministers, notably Shi Miyuan, who was the de facto ruler in his absence. Many critics blamed Lizong's leadership for the eventual fall of the Song dynasty.
A year after Emperor Lizong took the throne, he honored Zhu Xi with the posthumous name Duke of Hui (徽國公). Lizong faced a minor rebellion led by the former crown prince Zhao Hong which was put down after two weeks; Zhao Hong himself was executed.
From the winter of 1230 to the autumn of 1231, the Mongols forcibly passed through the Song Dynasty. In the region centered on the three passes of Shukou, they entered into a series of battles with the Song army. This was the second and largest armed conflict between them before the Mongol conquest of Song officially began. This forced Lizong to ally with the Mongols as they might conquer the Song.
Wuzhun Shifan, a monk was summoned by Emperor Lizong in 1233. Emperor Lizong discussed with Wuzhun about Zen Buddhism, and Dharma.
After Shi Miyuan's death in 1233, Emperor Lizong assumed full authority briefly but again quickly abandoned the responsibility of ruling and delegated matters to his chancellor Ding Daquan in order to pursue personal enjoyment. It was said that the emperor often drank wine a lot, frequented brothels and invited prostitutes into the palace in his late years, which was vehemently opposed by his ministers.
Notable events during Emperor Lizong's reign included the demise of the Jurchen-led Jin dynasty in 1234, which was obliterated by the joint forces of the Mongol Empire and the Song Dynasty. The Jurchens had fought multiple wars with the Song Empire decades before it was conquered by the Mongols and although their ruler Emperor Aizong tried to make peace with the Song Dynasty to warn them that if the Mongols conquered the Jin Dynasty and they would attack the Song next, Emperor Lizong ignored the warning.

Once the Jin dynasty has fallen, the Song dynasty attempted to take back its northern territories originally occupied by the Jin and took back Luoyang and Kaifeng in July 1234.  However, in September 1234, the Mongols counterattacked with the siege of Luoyang. The Song army holding Luoyang was short of food supplies. Additionally, the Mongols diverted the water of Yellow River into the city causing great casualties among the Song army.The fall of Luoyang was simply a prologue of a series of upcoming battles which lasted decades. The fall of Luoyang also began the official start of the Mongol conquest of the Song dynasty. The Mongols blamed the Song for "breaking the alliance". However, it was more of an excuse for further Mongol expeditions.
Other notable events include the Song's ally Dali falling to the Mongols.
Emperor Lizong met Zhao Yifu (1189–1256), who was a minister and a defender of Jiangnan. Lizong started to Yifu, "You have had two years of great toil, minister." Yifu said "My memorials have all reflected Your Majesty's virtue. I have not had a hair of merit." Emperor Lizong replied "You discussed urgent matters like preventing rebellions, imperial succession, and the drought that hit Jiangnan and Hunan." Yifu then said "King Tang reproached himself over the six affairs. Your Majesty should determine whether in the present circumstances there is a 'six affairs' ." Yifu was named imperial reader-in-waiting the next day.
In 1241, Töregene Khatun had sent an envoy to make peace proposals and discuss with Emperor Lizong. However, the Song court arrested the envoy and imprisoned him in a fortress with his suite of seventy people. The envoy died, but his suite were detained until 1254. That year the Mongol army attacked to take Hejiu but failed. The Chinese freed the suite of the late envoy to show their desire for peace.
The New Khan Möngke concentrated all his attention on the conquest of the Song dynasty. Taking personal command late in the decade, he captured many of the fortified cities along the northern front. In October 1257 Möngke set out for South China, leaving his administration to his brother, Ariq Böke.
Wen Tianxiang and Lu Xiufu passed the imperial examination where Emperor Lizong personally gave Wen first rank.
In 1259, the Song government was forced to capitulate and cede all territories north of the Yangtze River to the Mongols. Despite this, Möngke Khan was killed earlier in the year in a siege with no designated heir setting the stage for the Division of the Mongol Empire. The invasions frightened Lizong to the extent that he attempted to move the Song capital but was stopped by his Empress Xie Daoqing; she thought that if they moved, it would create chaos among the people.
In 1260, Jia Sidao became chancellor who would eventually soon take control over the new emperor Zhao Qi after Lizong's death and expel his opponents like Wen Tianxiang and Li Fu.
In December 1261, Lizong arranged a marriage between his adopted son Zhao Qi and the 33-year-old Quan Jiu. Quan Jiu was a grandniece of Lizong's mother (who had by then died) making Quan a cousin of Zhao Qi. An account in Quan's official biography stated that Lizong ordered her to enter the palace where he greeted her, "Your father Zhaosun died in the service of his majesty in the Baoyou reign, the very thought of which makes me grieve." Quan replied "My father can be remembered, but even more should the plight of the masses of the Huai and Hu regions be remembered!" greatly impressing Lizong who said to his high-ranking officials "The words of this daughter of the Quan family are particularly grandiloquent. A betrothal should be arranged with the heir-apparent to permit continuation of the ritual line."
Internal-combustion rocket propulsion is mentioned in a reference to 1264, recording that the "ground-rat", a type of firework, had frightened the Empress-Mother Gongsheng at a feast held in her honor by her son Emperor Lizong.
Also in 1264, Emperor Lizong died sonless (his sons died prematurely) from an illness and was succeeded by his adopted son and biological nephew, Zhao Qi, known as Emperor Duzong after his death.
One of his daughters married a descendant of Zhu Xi.
Emperor Lizong was a skilled poet and calligrapher. The Metropolitan Museum reports, "Lizong was the finest calligrapher among the Song emperors who came after Gaozong (r. 1127–62)." Lizong developed his own unique calligraphic style which was easily recognizable as it showed sharp and rapid brush strokes. The Metropolitan Museum says, "Lizong developed his own unique manner, which was distinguished by angular brushstrokes with straight rapid brush movements—in contrast to the slower, more rounded brushstrokes of Gaozong—and by his preference for Tang, rather than Jin, dynasty models." Emperor Lizong also expresses his feelings in his poems as shown in his poem 'Quatrain on Late Spring' which reads: 
In this poem, it is clearly established that Emperor Lizong laments old age.
Consorts and Issue:
Adopted Issue:
The Huolongjing (traditional Chinese: 火龍經; simplified Chinese: 火龙经; pinyin: Huǒ Lóng Jīng; Wade-Giles: Huo Lung Ching; rendered in English as Fire Drake Manual or Fire Dragon Manual), also known as Huoqitu (“Firearm Illustrations”), is a Chinese military treatise compiled and edited by Jiao Yu and Liu Bowen of the early Ming dynasty (1368–1683) during the 14th-century. The Huolongjing is primarily based on the text known as Huolong Shenqi Tufa (Illustrations of Divine Fire Dragon Engines), which no longer exists.
The Huolongjing's intended function was to serve as a guide to "fire weapons" involving gunpowder during the 1280s to 1350s. The Huolongjing provides information on various gunpowder compositions and weapons. Some formulas mentioned are given names such as "divine gunpowder", "poison gunpowder", and "blinding and burning gunpowder". The weapons described include bombs, fire arrows, rockets, land mines, naval mines, fire lances, hand cannons, and cannons mounted on wheeled carriages.
Although the earliest edition of the Huolongjing was written by Jiao Yu sometime between 1360-1375, its preface was not provided until the Nanyang publication of 1412. The 1412 edition, known as Huolongjing Quanji (Complete Collection of the Fire Dragon Manual), remains largely unchanged from its predecessor with the exception of its preface, which provides an account of Jiao Yu's time in the Hongwu Emperor's army. In the preface Jiao Yu claims to describe gunpowder weapons that had seen use since 1355 during his involvement in the Red Turban Rebellion and revolt against the Yuan dynasty, while the oldest material found in his text dates to 1280.
A second and third volume to the Huolongjing known as Huolongjing Erji (Fire Dragon Manual Volume Two) and Huolongjing Sanji (Fire Dragon Manual Volume Three) were published in 1632 with content describing weapons such as the musket and breech-loading cannons. After the end of the Ming dynasty, the Qing dynasty outlawed reprinting of the Huolongjing for using expressions such as 'northern barbarians,' which offended the ruling Manchu elite.
Although its destructive force was widely recognized by the 11th century, gunpowder continued to be known as a "fire-drug" (huo yao) because of its original intended pharmaceutical properties. However soon after the chemical formula for gunpowder was recorded in the Wujing Zongyao of 1044, evidence of state interference in gunpowder affairs began appearing. Realizing the military applications of gunpowder, the Song court banned private transactions involving sulphur and saltpeter in 1067 despite the widespread use of saltpeter as a flavor enhancer, and moved to monopolize gunpowder production. In 1076 the Song prohibited the populaces of Hedong (Shanxi) and Hebei from selling sulphur and saltpetre to foreigners. In 1132 gunpowder was referred to specifically for its military values for the first time and was called "fire bomb medicine" rather than "fire medicine".
While Chinese gunpowder formulas by the late 12th century and at least 1230 were powerful enough for explosive detonations and bursting cast iron shells, gunpowder was made more potent by applying the enrichment of sulphur from pyrite extracts. Chinese gunpowder solutions reached maximum explosive potential in the 14th century and at least six formulas are considered to have been optimal for creating explosive gunpowder, with levels of nitrate ranging from 12% to 91%. Evidence of large scale explosive gunpowder weapons manufacturing began to appear. While engaged in war with the Mongols in 1259, the official Li Zengbo wrote in his  Ko Zhai Za Gao, Xu Gao Hou that the city of Qingzhou was manufacturing one to two thousand strong iron-cased bomb shells a month, and delivered them to Xiangyang and Yingzhou in loads of about ten to twenty thousand shells at a time.
The Huolongjing's primary contribution to gunpowder was in expanding its role as a chemical weapon. Jiao Yu proposed several gunpowder compositions in addition to the standard potassium nitrate (saltpetre), sulphur, and charcoal. Described are the military applications of "divine gunpowder", "poison gunpowder", and "blinding and burning gunpowder." Poisonous gunpowder for hand-thrown or trebuchet launched bombs was created using a mixture of tung oil, urine, sal ammoniac, feces, and scallion juice heated and coated upon tiny iron pellets and broken porcelain. According to Jiao Yu, "even birds flying in the air cannot escape the effects of the explosion".
Explosive devices include the "flying-sand divine bomb releasing ten thousand fires", which consisted of a tube of gunpowder placed in an earthenware pot filled with quicklime, resin, and alcoholic extracts of poisonous plants.
Jiao Yu called the earliest fire arrows shot from bows (not rocket launchers) "fiery pomegranate shot from a bow" because the lump of gunpowder–filled paper wrapped around the arrow below the metal arrowhead resembled the shape of a pomegranate. He advised that a piece of hemp cloth should be used to strengthen the wad of paper and sealed with molten pine resin. Although he described the fire arrow in great detail, it was mentioned by the much earlier Xia Shaozeng, when 20,000 fire arrows were handed over to the Jurchen conquerors of Kaifeng City in 1126. An even earlier text, the Wujing Zongyao (武经总要, "Collection of the Most Important Military Techniques"), written in 1044 by Song scholars Zeng Gongliang and Yang Weide, described the use of three spring or triple bow arcuballista that fired arrow bolts holding gunpowder. Although written in 1630 (second edition in 1664), the Wulixiaoshi of Fang Yizhi said that fire arrows were presented to Emperor Taizu of Song in 960. Even after the rocket was invented in China the fire arrow was never entirely phased out: it saw use in the Second Opium War when Chinese used fire arrows against the French in 1860.
By the time of Jiao Yu, the term "fire arrow" had taken on a new meaning and also referred to the earliest rockets found in China. The simple transition of this was to use a hollow tube instead of a bow or ballista firing gunpowder-impregnated fire arrows. The historian Joseph Needham wrote that this discovery came sometime before Jiao Yu during the late Southern Song Dynasty (1127–1279). From the section of the oldest passages in the Huolongjing, the text reads:
One uses a bamboo stick 4 ft 2 in long, with an iron (or steel) arrow–head 4.5 in long...behind the feathering there is an iron weight 0.4 in long. At the front end there is a carton tube bound on to the stick, where the 'rising gunpowder' is lit. When you want to fire it off, you use a frame shaped like a dragon, or else conveniently a tube of wood or bamboo to contain it.In the late 14th century, the rocket launching tube was combined with the fire lance. This involved three tubes attached to the same staff. As the first rocket tube was fired, a charge was ignited in the leading tube which expelled a blinding lachrymatory powder at the enemy, and finally the second rocket was fired. An illustration of this appears in the Huolongjing, and a description of its effectiveness in obfuscating the location of the rockets from the enemy is provided. The Huolongjing also describes and illustrates two kinds of mounted rocket launchers that fired multiple rockets. There was a cylindrical, basket-work rocket launcher called the "Mr. Facing-both-ways rocket arrow firing basket", as well as an oblong-section, rectangular, box rocket launcher known as the "divine rocket-arrow block". Rockets described in the Huolongjing were not all in the shape of standard fire arrows and some had artificial wings attached. An illustration shows that fins were used to increase aerodynamic stability for the flight path of the rocket, which according to Jiao Yu could rise hundreds of feet before landing at the designated enemy target.
The Huolongjing also describes and illustrates the oldest known multistage rocket; this was the "fire-dragon issuing from the water" (huo long chu shui), which was known to be used by the Chinese navy. It was a two-stage rocket that had carrier or booster rockets that would automatically ignite a number of smaller rocket arrows that were shot out of the front end of the missile, which was shaped like a dragon's head with an open mouth, before eventually burning out. This multistage rocket is considered by some historians to be the ancestor of modern cluster munitions. Needham says that the written material and illustration of this rocket come from the oldest stratum of the Huolongjing, which can be dated to about 1300-1350 from the book's part 1, chapter 3, page 23.
The fire lance or fire tube—a combination of a firearm and flamethrower—had been adapted and changed into several different forms by the time Jiao Yu edited the Huolongjing. The earliest depiction of a fire lance is dated c. 950, a Chinese painting on a silk banner found at the Buddhist site of Dunhuang. These early fire lances were made of bamboo tubes, but metal barrels had appeared during the 13th century, and shot gunpowder flames along with "coviative" projectiles such as small porcelain shards or metal scraps. The first metal barrels were not designed to withstand high-nitrate gunpowder and a bore-filling projectile; rather, they were designed for the low-nitrate flamethrower fire lance that shot small coviative missiles. This was called the "bandit-striking penetrating gun" (ji zei bian chong). Some of these low–nitrate gunpowder flamethrowers used poisonous mixtures such as arsenious oxide, and would blast a spray of porcelain shards as fragmentation.  Another fire lance described in the Huolongjing was called the 'lotus bunch' shot arrows accompanied by a fiery blast. In addition to fire lances, the Huolongjing also illustrates a tall, vertical, mobile shield used to hide and protect infantry, known as the "mysteriously moving phalanx-breaking fierce-flame sword-shield". This large, rectangular shield would have been mounted on wheels with five rows of six circular holes each where the fire lances could be placed. The shield itself would have been accompanied by swordsmen on either side to protect the gunmen.
In China, the first cannon-barrel design portrayed in artwork was a stone sculpture dated to 1128 found in Sichuan province. The oldest extant cannon containing an inscription is a bronze cannon of China inscribed with the date, "2nd year of the Dade era, Yuan Dynasty" (1298). The oldest confirmed extant cannon is the Heilongjiang hand cannon, dated to 1288 using contextual evidence. The History of Yuan records that in that year a rebellion of the Christian Mongol prince Nayan broke out and the Jurchen commander Li Ting who, along with a Korean brigade conscripted by Kublai Khan, suppressed Nayan's rebellion using hand cannons and portable bombards.
The predecessor of the metal barrel was made of bamboo, which was recorded in use by a Chinese garrison commander at Anlu, Hubei province, in the year 1132. One of the earliest references to the destructive force of a cannon in China was made by Zhang Xian in 1341, with his verse known as The Iron Cannon Affair. Zhang wrote that its cannonball could "pierce the heart or belly when it strikes a man or horse, and can even transfix several persons at once". Jiao Yu describes the cannon, called the "eruptor", as a cast bronze device which had an average length of 53 inches (130 cm). He wrote that some cannons were simply filled with about 100 lead balls, but others, called the "flying-cloud thunderclap eruptor" (飞云霹雳炮; feiyun pili pao) had large rounds that produced a bursting charge upon impact. The ammunition consisted of hollow cast iron shells packed with gunpowder to create an explosive effect. Also mentioned is a "poison-fog divine smoke eruptor," in which "blinding gunpowder" and "poisonous gunpowder" were packed into hollow shells used in burning the faces and eyes of enemies, along with choking them with a formidable spray of poisonous smoke. Cannons were mounted on frames or on wheeled carriages so that they could be rotated to change directions.
The Huolongjing also contains a hand held organ gun with up to ten barrels. For the "match-holding lance gun" (chi huo–sheng qiang), it described its arrangement as a match brought down to the touch hole of three gun barrels, one after the other. During the reign of the Yongle Emperor (1402–1424), the Shenjiying, a specialized military body, was in part a cavalry force that utilized tubes filled with inflammable materials holstered to their sides, and also a firearm infantry division that handled light artillery and their transportation, including the handling of gun carriages.
The first recorded use of land mines occurred in 1277 when officer Lou Qianxia of the late Song Dynasty, who is credited with their invention, used them to kill Mongol soldiers. Jiao Yu wrote that land mines were spherical, made of cast iron, and their fuses were ignited by the enemy movement disturbing a trigger mechanism. Although his book did not elaborate on the trigger mechanism, it does mention the use of steel wheels as the trigger mechanism. The earliest illustration and description of the "steel wheel" mechanism was the Binglu of 1606. According to it, the steel wheel trigger mechanism utilized a pin release, dropping weights, cords and axles that worked to rotate a spinning "steel wheel" that rotated against a piece of flint to provide sparks that ignited the mines' fuses underground.
The explosive mine is made of cast iron about the size of a rice-bowl, hollow inside with (black) powder rammed into it. A small bamboo tube is inserted and through this passes the fuse, while outside (the mine) a long fuse is led through fire-ducts. Pick a place where the enemy will have to pass through, dig pits and bury several dozen such mines in the ground. All the mines are connected by fuses through the gunpowder fire-ducts, and all originate from a steel wheel (gang lun). This must be well concealed from the enemy. On triggering the firing device the mines will explode, sending pieces of iron flying in all directions and shooting up flames towards the sky.For the use of naval mines, he wrote of slowly burning joss sticks that were disguised and timed to explode against enemy ships nearby:
The sea–mine called the 'submarine dragon–king' is made of wrought iron, and carried on a (submerged) wooden board, . The (mine) is enclosed in an ox-bladder. Its subtlety lies in the fact that a thin incense(–stick) is arranged (to float) above the mine in a container. The (burning) of this joss stick determines the time at which the fuse is ignited, but without air its glowing would of course go out, so the container is connected with the mine by a (long) piece of goat's intestine (through which passes the fuse). At the upper end the (joss stick in the container) is kept floating by (an arrangement of) goose and wild–duck feathers, so that it moves up and down with the ripples of the water. On a dark (night) the mine is sent downstream (towards the enemy's ships), and when the joss stick has burnt down to the fuse, there is a great explosion.In the later Tiangong Kaiwu (The Exploitation of the Works of Nature) treatise, written by Song Yingxing in 1637, the ox bladder described by Jiao Yu is replaced with a lacquer bag and a cord pulled from a hidden ambusher located on the nearby shore, which would release a flint steel–wheel firing mechanism to ignite the fuse of the naval mine.
Gunpowder warfare occurred in earnest during the Song dynasty. In China, gunpowder weapons underwent significant technological changes which resulted in a vast array of weapons that eventually led to the cannon. The cannon's first confirmed use occurred during the Mongol Yuan dynasty in a suppression of rebel forces by Yuan Jurchen forces armed with hand cannons. Cannon development continued into the Ming and saw greater proliferation during the Ming wars. Chinese cannon development reached internal maturity with the muzzle loading wrought iron "great general cannon" (大將軍炮), otherwise known by its heavier variant name "great divine cannon" (大神銃), which could weigh up to 600 kilograms and was capable of firing several iron balls and upward of a hundred iron shots at once. The lighter "great general cannon" weighed up to 360 kilograms and could fire a 4.8 kilogram lead ball. The great general and divine cannons were the last indigenous Chinese cannon designs prior to the incorporation of European models in the 16th century.
When the Portuguese reached China in the early 16th century, they were unimpressed with Chinese firearms compared with their own. With the progression of the earliest European arquebus to the matchlock and the wheellock, and the advent of the flintlock musket of the 17th century, they surpassed the level of earlier Chinese firearms. Illustrations of Ottoman and European riflemen with detailed illustrations of their weapons appeared in Zhao Shizhen's book Shenqipu of 1598, and Ottoman and European firearms were held in great esteem. However, by the 17th century Đại Việt had also been manufacturing muskets of their own, which the Ming considered to be superior to both European and Ottoman firearms, including Japanese imports as well. Vietnamese firearms were copied and disseminated throughout China in quick order.
The 16th-century breech-loading model entered China around 1517 when Fernão Pires de Andrade arrived in China. However, he and the Portuguese embassy were rejected as problems in Ming-Portuguese relations were exacerbated when the Malacca Sultanate, a tributary state of the Ming, was invaded in 1511 by the Portuguese under Afonso de Albuquerque, and in the process a large established Chinese merchant community was slaughtered. The Malacca Sultanate sent the Ming a plea for help but no relief expedition was sent. In 1521 the Portuguese were driven off from China by the Ming navy in a conflict known as the Battle of Tunmen.
An arrow strapped with gunpowder ready to be shot from a bow. The text reads: gong she huo zhe liu jian (bow firing a fiery pomegranate arrow).
Rocket arrows from the Huolongjing. The right arrow reads 'fire arrow' (huo jian), the middle is an 'dragon shaped arrow frame' (long xing jian jia), and the left is a 'complete fire arrow' (huo jian quan shi).
A 'divine fire arrow shield' (shen huo jian pai). Depiction of a fire arrow rocket launcher from the Huolongjing.
A 'watermelon bom (xi gua pao) as depicted in the Huolongjing. It contains 'fire rats,' mini rockets with hooks.
A 'fire brick' (huo zhuan) as depicted in the Huolongjing. It contains mini-rockets bearing sharp little spikes.
Depiction of a' wind-and-dust bom (feng chen pao) from the Huolongjing.
A 'rumbling thunder bom (hong lei pao) as depicted in the Huolongjing. The text describes ingredients including mini-rockets and caltrops with poisons.
'Dropping from heaven' (tian zhui pao) bombs as depicted in the Huolongjing.
'Bee swarm bombs' (qun feng pao) as depicted in the Huolongjing. Paper casing filled with gunpowder and shrapnel.
A 'divine fire meteor which goes against the wind' (zuan feng shen huo liu xing pao) bomb as depicted in the Huolongjing.
An illustration of a fragmentation bomb known as the 'divine bone dissolving fire oil bom (lan gu huo you shen pao) from the Huolongjing. The black dots represent iron pellets.
A 'flying-sand divine bomb releasing ten thousand fires' (wan huo fei sha shen pao) as depicted in the Huolongjing. A weak casing device possibly used in naval combat.
'Explosive bombs' (zha pao) from the Huolongjing. The device is operated by steel wheels contained in two boxes. When pressed, the wheel boxes are supposed to ignite a spark reaching the buried gunpowder packages, setting off the explosion.
The 'self-tripped trespass land mine' (zi fan pao) from the Huolongjing.
An 'explosive camp land mine' (di lei zha ying) from the Huolongjing. The mine is composed of eight explosive charges held erect by two disc shaped frames.
A 'pear-flower gun' (li hua qiang). A fire lance as depicted in the Huolongjing.
A 'fire gun' (huo qiang). A double barreled fire lance from the Huolongjing. Supposedly they fired in succession, and the second one is lit automatically after the first barrel finishes firing.
An 'awe-inspiring fierce-fire yaksha gun' (shen wei lie huo ye cha chong) as depicted in the Huolongjing.
A 'lotus bunch' (yi ba lian) as depicted in the Huolongjing. It is a bamboo tube firing darts along with flames.
A 'sky-filling spurting-tube' (man tian pen tong) as depicted in the Huolongjing. A bamboo tube filled with a mixture of gunpowder and porcelain fragments.
A 'bandit-striking penetrating gun' (ji zei bian chong) as depicted in the Huolongjing. The first known metal barreled fire lance, it throws low nitrate gunpowder flames along with coviative missiles.
A 'divine moving phalanx-breaking fierce-fire sword-shield' (shen xing po zhen meng huo dao pai) as depicted in the Huolongjing. A mobile shield fitted with fire lances used to break enemy formations.
Essentially a fire lance on a frame, the 'multiple bullets magazine eruptor' (bai zi lian zhu pao) shoots lead shots, which are loaded in a magazine and fed into the barrel when turned around on its axis.
A 'poison fog divine smoke eruptor' (du wu shen yan pao) as depicted in the Huolongjing. Small shells emitting poisonous smoke are fired.
A canister shot known as the 'flying-hidden-bomb cannon' (fei meng pao shi) from the Huolongjing. The poison canister is loaded into an iron barrel fitted to a wooden tiller.
An organ gun known as the 'mother of a hundred bullets gun' (zi mu bai dan chong) from the Huolongjing.
A bronze "thousand ball thunder cannon" (qian zi lei pao) from the Huolongjing.
An 'awe inspiring long range cannon' (wei yuan pao) from the Huolongjing.
The 'crouching tiger cannon' (hu dun pao) as depicted in the Huolongjing.
A 'seven star cannon' (qi xing chong) from the Huolongjing. It was a seven barreled organ gun with two auxiliary guns by its side on a two-wheeled carriage.
A 'barbarian attacking cannon' (gong rong pao) as depicted in the Huolongjing. Chains are attached to the cannon to adjust recoil. Not to be confused with the "Hongyipao".
Reconstruction of the "flying crow with magic fire" (shen huo fei ya).

Jiao Yu (Chinese: 焦玉; pinyin: Jiāo Yù; Wade–Giles: Chiao Yü) was a Chinese military general, philosopher, and writer of the Yuan dynasty and early Ming dynasty under Zhu Yuanzhang, who founded the dynasty and became known as the Hongwu Emperor. He was entrusted by Zhu as a leading artillery officer for the rebel army that overthrew the Mongol Yuan dynasty, and established the Ming Dynasty.
As a senior adviser and general, he was later appointed to the venerable and noble status of the Count of Dongning. He edited and wrote a famous military treatise that outlined the use of Chinese military technology during the mid 14th century based on his military campaign of 1355 AD. However, descriptions of some gunpowder weapons in his treatise derive from Song Dynasty (960–1279 AD) materials on battles against the Khitans (Liao dynasty), Jurchens (Jin dynasty), and Mongols. His Huolongjing, translated as the Fire Drake Manual, contains descriptions of fire arrows, fire lances, grenades, firearms, bombards, cannons, exploding cannonballs, land mines, naval mines, rockets, rocket launchers, two-stage rockets, and various gunpowder solutions including poisonous concoctions.
Jiao Yu was an aspiring scholar in his youth during the Yuan dynasty, but was not accepted into government service. Jiao Yu met an adept Daoist intellect living in the Tiantai Mountain known as Chichi Daoren (the "Knowing-when-to-stop Daoist"). Like Jiao Yu, Daoren accepted the teachings of Confucius and Mencius, but in military affairs Jiao was convinced that he had inherited the skill of the ancient Sun Tzu. After Jiao Yu became his protégé, Daoren urged Yu to join the cause of Zhu Yuanzhang's rebellion. Daoren had also shared with him various literary works on 'fire-weapons' and their recorded uses in battle. After joining his ranks, Jiao Yu became one of Zhu Yuanzhang's trusted confidants in the Red Turban Rebellion against the Yuan dynasty. Zhu was impressed with Jiao's knowledge of firearms which he had earlier acquired from Daoren, yet Zhu wanted to test their abilities. Zhu ordered his officer Xu Da to provide a demonstration of their destructive capabilities, and after the display Zhu Yuanzhang was most impressed with their power.
With the aid of Jiao's 'fire-weapons', Zhu's army, once stationed in Hezhou among a plethora of different rebel groups in surrounding towns, conquered Jingzhou and Xiangzhou in one expedition. In the second expedition the provinces of Jiang and Zhe, and in the third campaign the entire province of Fujian was taken, including its surrounding waterways. After this, Zhu's army captured the whole of Shandong in one campaign, strengthening his base while the authority of the Mongol regime at Beijing was collapsing all around. Zhu finally drove the Mongols north in 1367, establishing a new capital at Nanjing soon after while Beijing became the secondary capital.
After the successful rebellion and establishment of Zhu Yuanzhang as the Ming dynasty's new Hongwu Emperor, Jiao was charged with manufacturing firearms for the government. Jiao was eventually appointed as the head officer of the enormous Shen Zhi Ying Armory, where multitudes of manufactured guns and artillery were deposited for storage and safekeeping. Proper maintenance and safety measures for gunpowder arsenals were taken very seriously during Jiao's time due to the memory of previous disasters during the Song Dynasty, such as Prime Minister Zhao Nanchong's personal arsenal catching fire and exploding in 1260 AD, alongside the monumental disaster of the enormous Weiyang arsenal accidentally catching fire in 1280 AD and killing more than 100 people. With Zhu Yuanzhang in power over the government, he established various production facilities in the capital at Nanjing for the manufacture of gunpowder and fire-weapons, stored in various arsenals throughout the country. The Hongwu Emperor established a new Gunpowder Department in the central administration of the capital. Jiao Yu placed a lot of emphasis on the importance of fire-weapons, as he once wrote in a preface to his book, "the very existence or destruction of the Empire, and the lives of the whole armed forces depend on the exact timing of these weapons. This is what fire-weapons are all about."
Along with the scholar, general, and court adviser Liu Bowen (1311–1375), Jiao Yu became an editor of the 14th century military treatise known as the Huolongjing (Fire Drake Manual). The Nanyang publication of the book, known as the Huolongjing Quanzhi (Fire Drake Manual in One Complete Volume) featured a preface written by Jiao Yu much later in 1412 AD. Both publications falsely attributed the earliest passages of the book to the ancient Chinese Prime Minister Zhuge Liang (181-234 AD) of the Shu Han, even though gunpowder warfare did not exist in China until the advent of the gunpowder-fuse-ignited flamethrower (Pen Huo Qi) in the 10th century. The oldest passages found in the Huolongjing were collected from sources no earlier than circa 1270 AD.
Although Jiao Yu's biography does not appear in the History of Ming (1739), Yu was mentioned in Zhao Shizhen's Shenqipu (1598 AD), He Rubin's Binglu (1606 AD), and Jiao Xu's Zekelu (1643 AD). His text Huolongjing was reprinted in the 19th century, during the late Qing Dynasty.
The Huolongjing (Chinese: 火龍神器陣灋), compiled and edited by Jiao Yu and Liu Zhi, outlined the use of many different gunpowder weapons found in China during the 14th century. It provided information for:
Huolongchushui (fire dragon issuing from the water; Chinese: 火龙出水; pinyin: huolóngchushui; lit. fire dragon out of water) were the earliest form of multistage rockets and ballistic missiles used in post-classical China. The name of the weapon was used to strike fear into enemy troops. If the enemy was out of range, the fire dragon had a contingency: a magazine of three rocket driven arrows located within the mouth of the missile. It acted as one of the world's earliest multistage rockets and ballistic missiles, and was fired at enemy ships in naval battles.
The huolongchushui had a hollow bamboo tube with a carved wooden dragon head and tail about five feet long. The front and rear contained four rockets packed with gunpowder that propelled the dragon forwards. Fuses facing downwards out of the four rockets outside the dragon body were linked with the fuses of the rockets inside the dragon's belly. Just before the four rockets on the outside burnt out, it would automatically ignite fuses of arrow rockets hidden inside the rear of the dragon, which would then shoot out of its mouth propelled by the gunpowder to destroy the enemy. The huolongchushui would be used on both land and sea. The huolongchusui would operate on the principle of an early form of a two-stage rocket. The two stage rocket would have a booster rocket attached to it that would then burn out automatically issuing a batch of smaller rockets hidden inside the dragon's belly. The Huolongchushui may be considered the ancestor to the modern YingJi-62 ASCM.
An illustration of the huolongchushui is found in the 14th century Chinese military treatise Huolongjing by Jiao Yu and Liu Bowen during the early Ming dynasty. The illustration and description depicts the world earliest form of the multistage rocket and ballistic missile used by Ming Chinese army and navy.
Huolongchushui were the primary inspiration for the man-portable artillery depicted in Disney's Mulan.
Huolongchushui are discussed in Liu Cixin's 2010 science-fiction novel Death's End (on page 98 of 766 of the 2016 English translation by Ken Liu), as a comparison to a fictional multistage nuclear-propelled space probe.

Incendiary weapons, incendiary devices, incendiary munitions, or incendiary bombs are weapons designed to start fires or destroy sensitive equipment using fire (and sometimes used as anti-personnel weaponry), that use materials such as napalm, thermite, magnesium powder, chlorine trifluoride, or white phosphorus. Though colloquially often known as bombs, they are not explosives but in fact are designed to slow the process of chemical reactions and use ignition rather than detonation to start or maintain the reaction. Napalm for example, is petroleum especially thickened with certain chemicals into a 'gel'  to slow, but not stop, combustion, releasing energy over a longer time than an explosive device. In the case of napalm, the gel adheres to surfaces and resists suppression.
A range of early thermal weapons were utilized by ancient, medieval/post-classical and early modern armies, including hot pitch, oil, resin, animal fat and other similar compounds. Substances such as quicklime and sulfur could be toxic and blinding. Incendiary mixtures, such as the petroleum-based Greek fire, were launched by throwing machines or administered through a siphon. Sulfur- and oil-soaked materials were sometimes ignited and thrown at the enemy, or attached to spears, arrows or bolts, and fired by hand or machine. Some siege techniques—such as mining and boring—relied on combustibles and fire to complete the collapse of walls and structures.
Towards the latter part of the period, gunpowder was invented, which increased the sophistication of the weapons, starting with fire lances.
The first incendiary devices to be dropped during World War I fell on coastal towns in the east of England on the night of 18–19 January 1915. The small number of German bombs, also known as firebombs, were finned containers filled with kerosene and oil and wrapped with tar-covered rope. They were dropped from Zeppelin airships. On 8 September 1915, Zeppelin L-13 dropped a large number of firebombs, but even then the results were poor and they were generally ineffective in terms of the damage inflicted. They did have a considerable effect on the morale of the civilian population of the United Kingdom.
After further experiments with 5-litre barrels of benzol, in 1918, the B-1E Elektron fire bomb (German: Elektronbrandbombe) was developed by scientists and engineers at the Griesheim-Elektron chemical works. The bomb was ignited by a thermite charge, but the main incendiary effect was from the magnesium and aluminium alloy casing, which ignited at 650° Celsius, burned at 1,100 °C and emitted vapour that burned at 1,800 °C. A further advantage of the alloy casing was its lightness, being a quarter of the density of steel, which meant that each bomber could carry a considerable number. The German High Command devised an operation called "The Fire Plan" (German: Der Feuerplan), which involved the use of the whole German heavy bomber fleet, flying in waves over London and Paris and dropping all the incendiary bombs that they could carry, until they were either all shot down or the crews were too exhausted to fly. The hope was that the two capitals would be engulfed in an inextinguishable blaze, causing the Allies to sue for peace. Thousands of Elektron bombs were stockpiled at forward bomber bases and the operation was scheduled for August and again in early September 1918, but on both occasions, the order to take off was countermanded at the last moment, perhaps because of the fear of Allied reprisals against German cities. The Royal Air Force had already used their own "Baby" Incendiary Bomb (BIB) which also contained a thermite charge. A plan to fire bomb New York with new long range Zeppelins of the L70 class was proposed by the naval airship fleet commander Peter Strasser in July 1918, but it was vetoed by Admiral Reinhard Scheer.
Incendiary bombs were used extensively in World War II as an effective bombing weapon, often in a conjunction with high-explosive bombs. Probably the most famous incendiary attacks are the bombing of Dresden and the bombing of Tokyo on 10 March 1945. Many different configurations of incendiary bombs and a wide range of filling materials such as isobutyl methacrylate (IM) polymer, napalm, and similar jellied-petroleum formulas were used, many of them developed by the US Chemical Warfare Service. Different methods of delivery, e.g. small bombs, bomblet clusters and large bombs, were tested and implemented. For example, a large bomb casing was filled with small sticks of incendiary (bomblets); the casing was designed to open at altitude, scattering the bomblets in order to cover a wide area. An explosive charge would then ignite the incendiary material, often starting a raging fire. The fire would burn at extreme temperatures that could destroy most buildings made of wood or other combustible materials (buildings constructed of stone tend to resist incendiary destruction unless they are first blown open by high explosives).
The German Luftwaffe started the war using the 1918-designed one-kilogram magnesium alloy B-1E Elektronbrandbombe; later modifications included the addition of a small explosive charge intended to penetrate the roof of any building which it landed on. Racks holding 36 of these bombs were developed, four of which could, in turn, be fitted to an electrically triggered dispenser so that a single He 111 bomber could carry 1,152 incendiary bombs, or more usually a mixed load. Less successful was the Flammenbombe, a 250 kg or 500 kg high explosive bomb case filled with an inflammable oil mixture, which often failed to detonate and was withdrawn in January 1941.
In World War II, incendiaries were principally developed in order to destroy the many small, decentralised war industries located (often intentionally) throughout vast tracts of city land in an effort to escape destruction by conventionally aimed high-explosive bombs. Nevertheless, the civilian destruction caused by such weapons quickly earned them a reputation as terror weapons with the targeted populations. The Nazi regime began the campaign of incendiary bombings at the start of World War II with the bombing of Warsaw, and continued with the London Blitz and the bombing of Moscow, among other cities. Later, an extensive reprisal was enacted by the Allies in the strategic bombing campaign that led to the near-annihilation of many German cities. In the Pacific War, during the last seven months of strategic bombing by B-29 Superfortresses in the air war against Japan, a change to firebombing tactics resulted in the death of 500,000 Japanese and the homelessness of five million more. Sixty-seven Japanese cities lost significant areas to incendiary attacks. The most deadly single bombing raid in history was Operation Meetinghouse, an incendiary attack that killed some 100,000 Tokyo residents in one night.
The 4 lb (1.8 kg) incendiary bomb, developed by ICI, was the standard light incendiary bomb used by RAF Bomber Command in very large numbers, declining slightly in 1944 to 35.8 million bombs produced (the decline being due to more bombs arriving from the United States). It was the weapon of choice for the British "dehousing" plan. The bomb consisted of a hollow body made from aluminium-magnesium alloy with a cast iron/steel nose, and filled with thermite incendiary pellets. It was capable of burning for up to ten minutes. There was also a high explosive version and delayed high explosive versions (2–4 minutes) which were designed to kill rescuers and firefighters. It was normal for a proportion of high explosive bombs to be dropped during incendiary attacks in order to expose combustible material and to fill the streets with craters and rubble, hindering rescue services.
Towards the end of World War Two, the British introduced a much improved 30 lb (14 kg) incendiary bomb, whose fall was retarded by a small parachute and on impact sent out an extremely hot flame for 15 ft (4.6 m); This, the Incendiary Bomb, 30-lb., Type J, Mk I, burned for approximately two minutes. Articles in late 1944 claimed that the flame was so hot it could crumble a brick wall. For propaganda purposes the RAF dubbed the new incendiary bomb the Superflamer.
Around fifty-five million incendiary bombs were dropped on Germany by Avro Lancasters alone.
Many incendiary weapons developed and deployed during World War II were in the form of bombs and shells whose main incendiary component is white phosphorus (WP), and can be used in an offensive anti-personnel role against enemy troop concentrations, but WP is also used for signalling, smoke screens, and target-marking purposes. The U.S. Army and Marines used WP extensively in World War II and Korea for all three purposes, frequently using WP shells in large 4.2-inch chemical mortars. WP was widely credited by many Allied soldiers for breaking up numerous German infantry attacks and creating havoc among enemy troop concentrations during the latter part of World War II. In both World War II and Korea, WP was found particularly useful in overcoming enemy human wave attacks.
Modern incendiary bombs usually contain thermite, made from aluminium and ferric oxide. It takes very high temperatures to ignite, but when alight, it can burn through solid steel. In World War II, such devices were employed in incendiary grenades to burn through heavy armour plate, or as a quick welding mechanism to destroy artillery and other complex machined weapons.
A variety of pyrophoric materials can also be used: selected organometallic compounds, most often triethylaluminium, trimethylaluminium, and some other alkyl and aryl derivatives of aluminium, magnesium, boron, zinc, sodium, and lithium, can be used. Thickened triethylaluminium, a napalm-like substance that ignites in contact with air, is known as thickened pyrophoric agent, or TPA.
Napalm  was widely used by the United States during the Korean War, most notably during the battle "Outpost Harry" in South Korea during the night of June 10–11, 1953. Eighth Army chemical officer Donald Bode reported that on an "average good day" UN pilots used 70,000 US gallons (260,000 l) of napalm, with approximately 60,000 US gallons (230,000 l) of this thrown by US forces. Winston Churchill, among others, criticized American use of napalm in Korea, calling it "very cruel", as the US/UN forces, he said, were "splashing it all over the civilian population", "tortur great masses of people". The American official who took this statement declined to publicize it.
During the Vietnam War, the U.S. Air Force developed the CBU-55, a cluster bomb incendiary fuelled by propane, a weapon that was used only once in warfare. Napalm however, became an intrinsic element of U.S. military action during the Vietnam War as forces made increasing use of it for its tactical and psychological effects. Reportedly about 388,000 tons of U.S. napalm bombs were dropped in the region between 1963 and 1973, compared to 32,357 tons used over three years in the Korean War, and 16,500 tons dropped on Japan in 1945.
Napalm proper is no longer used by the United States, although the kerosene-fuelled Mark 77 MOD 5 Firebomb is currently in use. The United States has confirmed the use of Mark 77s in Operation Iraqi Freedom in 2003.
Signatory states are bound by Protocol III of the UN Convention on Conventional Weapons which governs the use of incendiary weapons:
Protocol III states though that incendiary weapons do not include:


A siege is a military blockade of a city, or fortress, with the intent of conquering by attrition, or a well-prepared assault. This derives from Latin: sedere, lit. 'to sit'. Siege warfare is a form of constant, low-intensity conflict characterized by one party holding a strong, static, defensive position. Consequently, an opportunity for negotiation between combatants is common, as proximity and fluctuating advantage can encourage diplomacy. The art of conducting and resisting sieges is called siege warfare, siegecraft, or poliorcetics.
A siege occurs when an attacker encounters a city or fortress that cannot be easily taken by a quick assault, and which refuses to surrender. Sieges involve surrounding the target to block the provision of supplies and the reinforcement or escape of troops (a tactic known as "investment"). This is typically coupled with attempts to reduce the fortifications by means of siege engines, artillery bombardment, mining (also known as sapping), or the use of deception or treachery to bypass defenses.
Failing a military outcome, sieges can often be decided by starvation, thirst, or disease, which can afflict either the attacker or defender. This form of siege, though, can take many months or even years, depending upon the size of the stores of food the fortified position holds.
The attacking force can circumvallate the besieged place, which is to build a line of earth-works, consisting of a rampart and trench, surrounding it. During the process of circumvallation, the attacking force can be set upon by another force, an ally of the besieged place, due to the lengthy amount of time required to force it to capitulate. A defensive ring of forts outside the ring of circumvallated forts, called contravallation, is also sometimes used to defend the attackers from outside.
Ancient cities in the Middle East show archaeological evidence of fortified city walls. During the Warring States era of ancient China, there is both textual and archaeological evidence of prolonged sieges and siege machinery used against the defenders of city walls. Siege machinery was also a tradition of the ancient Greco-Roman world. During the Renaissance and the early modern period, siege warfare dominated the conduct of war in Europe. Leonardo da Vinci gained as much of his renown from the design of fortifications as from his artwork.
Medieval campaigns were generally designed around a succession of sieges. In the Napoleonic era, increasing use of ever more powerful cannons reduced the value of fortifications. In the 20th century, the significance of the classical siege declined. With the advent of mobile warfare, a single fortified stronghold is no longer as decisive as it once was. While traditional sieges do still occur, they are not as common as they once were due to changes in modes of battle, principally the ease by which huge volumes of destructive power can be directed onto a static target. Modern sieges are more commonly the result of smaller hostage, militant, or extreme resisting arrest situations.
The Assyrians deployed large labour forces to build new palaces, temples, and defensive walls. Some settlements in the Indus Valley civilization were also fortified. By about 3500 BC, hundreds of small farming villages dotted the Indus River floodplain. Many of these settlements had fortifications and planned streets.
The stone and mud brick houses of Kot Diji were clustered behind massive stone flood dikes and defensive walls, for neighbouring communities quarrelled constantly about the control of prime agricultural land. Mundigak (c. 2500 BC) in present-day south-east Afghanistan has defensive walls and square bastions of sun-dried bricks.
City walls and fortifications were essential for the defence of the first cities in the ancient Near East. The walls were built of mudbricks, stone, wood, or a combination of these materials, depending on local availability. They may also have served the dual purpose of showing potential enemies the might of the kingdom. The great walls surrounding the Sumerian city of Uruk gained a widespread reputation. The walls were 9.5 km (5.9 mi) in length, and up to 12 m (39 ft) in height.
Later, the walls of Babylon, reinforced by towers, moats, and ditches, gained a similar reputation. In Anatolia, the Hittites built massive stone walls around their cities atop hillsides, taking advantage of the terrain. In Shang Dynasty China, at the site of Ao, large walls were erected in the 15th century BC that had dimensions of 20 m (66 ft) in width at the base and enclosed an area of some 2,100 yards (1,900 m) squared. The ancient Chinese capital for the State of Zhao, Handan, founded in 386 BC, also had walls that were 20 m (66 ft) wide at the base; they were 15 m (49 ft) tall, with two separate sides of its rectangular enclosure at a length of 1,530 yd (1,400 m).
The cities of the Indus Valley Civilization showed less effort in constructing defences, as did the Minoan civilization on Crete. These civilizations probably relied more on the defence of their outer borders or sea shores. Unlike the ancient Minoan civilization, the Mycenaean Greeks emphasized the need for fortifications alongside natural defences of mountainous terrain, such as the massive Cyclopean walls built at Mycenae and other adjacent Late Bronze Age (c. 1600–1100 BC) centers of central and southern Greece.
Although there are depictions of sieges from the ancient Near East in historical sources and in art, there are very few examples of siege systems that have been found archaeologically. Of the few examples, several are noteworthy:
The earliest representations of siege warfare have been dated to the Protodynastic Period of Egypt, c. 3000 BC. These show the symbolic destruction of city walls by divine animals using hoes.
The first siege equipment is known from Egyptian tomb reliefs of the 24th century BC, showing Egyptian soldiers storming Canaanite town walls on wheeled siege ladders. Later Egyptian temple reliefs of the 13th century BC portray the violent siege of Dapur, a Syrian city, with soldiers climbing scale ladders supported by archers.
Assyrian palace reliefs of the 9th to 7th centuries BC display sieges of several Near Eastern cities. Though a simple battering ram had come into use in the previous millennium, the Assyrians improved siege warfare and used huge wooden tower-shaped battering rams with archers positioned on top.
In ancient China, sieges of city walls (along with naval battles) were portrayed on bronze 'hu' vessels, like those found in Chengdu, Sichuan in 1965, which have been dated to the Warring States period (5th to 3rd centuries BC).
An attacker's first act in a siege might be a surprise attack, attempting to overwhelm the defenders before they were ready or were even aware there was a threat. This was how William de Forz captured Fotheringhay Castle in 1221.
The most common practice of siege warfare was to lay siege and just wait for the surrender of the enemies inside or, quite commonly, to coerce someone inside to betray the fortification. During the medieval period, negotiations would frequently take place during the early part of the siege. An attacker – aware of a prolonged siege's great cost in time, money, and lives – might offer generous terms to a defender who surrendered quickly. The defending troops would be allowed to march away unharmed, often retaining their weapons. However, a garrison commander who was thought to have surrendered too quickly might face execution by his own side for treason.
As a siege progressed, the surrounding army would build earthworks (a line of circumvallation) to completely encircle their target, preventing food, water, and other supplies from reaching the besieged city. If sufficiently desperate as the siege progressed, defenders and civilians might have been reduced to eating anything vaguely edible – horses, family pets, the leather from shoes, and even each other.
The Hittite siege of a rebellious Anatolian vassal in the 14th century BC ended when the queen mother came out of the city and begged for mercy on behalf of her people. The Hittite campaign against the kingdom of Mitanni in the 14th century BC bypassed the fortified city of Carchemish. If the main objective of a campaign was not the conquest of a particular city, it could simply be passed by. When the main objective of the campaign had been fulfilled, the Hittite army returned to Carchemish and the city fell after an eight-day siege.
Disease was another effective siege weapon, although the attackers were often as vulnerable as the defenders. In some instances, catapults or similar weapons were used to fling diseased animals over city walls in an early example of biological warfare. If all else failed, a besieger could claim the booty of his conquest undamaged, and retain his men and equipment intact, for the price of a well-placed bribe to a disgruntled gatekeeper. The Assyrian siege of Jerusalem in the 8th century BC came to an end when the Israelites bought them off with gifts and tribute, according to the Assyrian account, or when the Assyrian camp was struck by mass death, according to the Biblical account. Due to logistics, long-lasting sieges involving a minor force could seldom be maintained. A besieging army, encamped in possibly squalid field conditions and dependent on the countryside and its own supply lines for food, could very well be threatened with the disease and starvation intended for the besieged.
To end a siege more rapidly, various methods were developed in ancient and medieval times to counter fortifications, and a large variety of siege engines was developed for use by besieging armies. Ladders could be used to escalade over the defenses. Battering rams and siege hooks could also be used to force through gates or walls, while catapults, ballistae, trebuchets, mangonels, and onagers could be used to launch projectiles to break down a city's fortifications and kill its defenders. A siege tower, a substantial structure built to equal or greater height than the fortification's walls, could allow the attackers to fire down upon the defenders and also advance troops to the wall with less danger than using ladders.
In addition to launching projectiles at the fortifications or defenders, it was also quite common to attempt to undermine the fortifications, causing them to collapse. This could be accomplished by digging a tunnel beneath the foundations of the walls, and then deliberately collapsing or exploding the tunnel. This process is known as mining. The defenders could dig counter-tunnels to cut into the attackers' works and collapse them prematurely.
Fire was often used as a weapon when dealing with wooden fortifications. The Byzantine Empire used Greek fire, which contained additives that made it hard to extinguish. Combined with a primitive flamethrower, it proved an effective offensive and defensive weapon.
The universal method for defending against siege is the use of fortifications, principally walls and ditches, to supplement natural features. A sufficient supply of food and water was also important to defeat the simplest method of siege warfare: starvation. On occasion, the defenders would drive 'surplus' civilians out to reduce the demands on stored food and water.
During the Warring States period in China (481–221 BC), warfare lost its honorable, gentlemen's duty that was found in the previous era of the Spring and Autumn period, and became more practical, competitive, cut-throat, and efficient for gaining victory. The Chinese invention of the hand-held, trigger-mechanism crossbow during this period revolutionized warfare, giving greater emphasis to infantry and cavalry and less to traditional chariot warfare.
The philosophically pacifist Mohists (followers of the philosopher Mozi) of the 5th century BC believed in aiding the defensive warfare of smaller Chinese states against the hostile offensive warfare of larger domineering states. The Mohists were renowned in the smaller states (and the enemies of the larger states) for the inventions of siege machinery to scale or destroy walls. These included traction trebuchet catapults, eight-foot-high ballistas, a wheeled siege ramp with grappling hooks known as the Cloud Bridge (the protractible, folded ramp slinging forward by means of a counterweight with rope and pulley), and wheeled 'hook-carts' used to latch large iron hooks onto the tops of walls to pull them down.
When enemies attempted to dig tunnels under walls for mining or entry into the city, the defenders used large bellows (the type the Chinese commonly used in heating up a blast furnace for smelting cast iron) to pump smoke into the tunnels in order to suffocate the intruders.
Advances in the prosecution of sieges in ancient and medieval times naturally encouraged the development of a variety of defensive countermeasures. In particular, medieval fortifications became progressively stronger—for example, the advent of the concentric castle from the period of the Crusades—and more dangerous to attackers—witness the increasing use of machicolations and murder-holes, as well the preparation of hot or incendiary substances. Arrowslits (also called arrow loops or loopholes), sally ports (airlock-like doors) for sallies and deep water wells were also integral means of resisting siege at this time. Particular attention would be paid to defending entrances, with gates protected by drawbridges, portcullises, and barbicans. Moats and other water defenses, whether natural or augmented, were also vital to defenders.
In the European Middle Ages, virtually all large cities had city walls—Dubrovnik in Dalmatia is a well-preserved example—and more important cities had citadels, forts, or castles. Great effort was expended to ensure a good water supply inside the city in case of siege. In some cases, long tunnels were constructed to carry water into the city. Complex systems of tunnels were used for storage and communications in medieval cities like Tábor in Bohemia, similar to those used much later in Vietnam during the Vietnam War.
Until the invention of gunpowder-based weapons (and the resulting higher-velocity projectiles), the balance of power and logistics definitely favored the defender. With the invention of gunpowder, cannon and mortars and howitzers (in modern times), the traditional methods of defense became less effective against a determined siege.
Although there are numerous ancient accounts of cities being sacked, few contain any clues to how this was achieved. Some popular tales existed on how the cunning heroes succeeded in their sieges. The best-known is the Trojan Horse of the Trojan War, and a similar story tells how the Canaanite city of Joppa was conquered by the Egyptians in the 15th century BC. The Biblical Book of Joshua contains the story of the miraculous Battle of Jericho.
A more detailed historical account from the 8th century BC, called the Piankhi stela, records how the Nubians laid siege to and conquered several Egyptian cities by using battering rams, archers, and slingers and building causeways across moats.
During the Peloponnesian War, one hundred sieges were attempted and fifty-eight ended with the surrender of the besieged area.
Alexander the Great's army successfully besieged many powerful cities during his conquests. Two of his most impressive achievements in siegecraft took place in the siege of Tyre and the siege of the Sogdian Rock. His engineers built a causeway that was originally 60 m (200 ft) wide and reached the range of his torsion-powered artillery, while his soldiers pushed siege towers housing stone throwers and light catapults to bombard the city walls.
Most conquerors before him had found Tyre, a Phoenician island-city about 1 km from the mainland, impregnable. The Macedonians built a mole, a raised spit of earth across the water, by piling stones up on a natural land bridge that extended underwater to the island, and although the Tyrians rallied by sending a fire ship to destroy the towers, and captured the mole in a swarming frenzy, the city eventually fell to the Macedonians after a seven-month siege. In complete contrast to Tyre, Sogdian Rock was captured by stealthy attack. Alexander used commando-like tactics to scale the cliffs and capture the high ground, and the demoralized defenders surrendered.
The importance of siege warfare in the ancient period should not be underestimated. One of the contributing causes of Hannibal's inability to defeat Rome was his lack of siege engines, thus, while he was able to defeat Roman armies in the field, he was unable to capture Rome itself. The legionary armies of the Roman Republic and Empire are noted as being particularly skilled and determined in siege warfare. An astonishing number and variety of sieges, for example, formed the core of Julius Caesar's mid-1st-century BC conquest of Gaul (modern France).
In his Commentarii de Bello Gallico (Commentaries on the Gallic War), Caesar describes how, at the Battle of Alesia, the Roman legions created two huge fortified walls around the city. The inner circumvallation, 16 km (10 mi), held in Vercingetorix's forces, while the outer contravallation kept relief from reaching them. The Romans held the ground in between the two walls. The besieged Gauls, facing starvation, eventually surrendered after their relief force met defeat against Caesar's auxiliary cavalry.
The Sicarii Zealots who defended Masada in AD 73 were defeated by the Roman legions, who built a ramp 100 m high up to the fortress's west wall.
During the Roman-Persian Wars, siege warfare was extensively being used by both sides.
The early Muslims, led by the Islamic prophet Muhammad, made extensive use of sieges during military campaigns. The first use was during the Invasion of Banu Qaynuqa. According to Islamic tradition, the invasion of Banu Qaynuqa occurred in 624 AD. The Banu Qaynuqa were a Jewish tribe expelled by Muhammad for allegedly breaking the treaty known as the Constitution of Medina: 209  by pinning the clothes of a Muslim woman, which led to her being stripped naked. A Muslim killed a Jew in retaliation, and the Jews in turn killed the Muslim man. This escalated to a chain of revenge killings, and enmity grew between Muslims and the Banu Qaynuqa, leading to the siege of their fortress.: 122  The tribe eventually surrendered to Muhammad, who initially wanted to kill the members of Banu Qaynuqa but ultimately yielded to Abdullah ibn Ubayy's insistence and agreed to expel the Qaynuqa.
The second siege was during the Invasion of Banu Nadir. According to The Sealed Nectar, the siege did not last long; the Banu Nadir Jews willingly offered to comply with the Muhammad's order and leave Madinah. Their caravan counted 600 loaded camels, including their chiefs, Huyai bin Akhtab, and Salam bin Abi Al-Huqaiq, who left for Khaibar, whereas another party shifted to Syria. Two of them embraced Islam, Yameen bin ‘Amr and Abu Sa‘d bin Wahab, and so they retained their personal wealth. Muhammad seized their weapons, land, houses, and wealth. Amongst the other booty he managed to capture, there were 50 armours, 50 helmets, and 340 swords. This booty was exclusively Muhammad's because no fighting was involved in capturing it. He divided the booty at his own discretion among the early Emigrants and two poor Helpers, Abu Dujana and Suhail bin Haneef.
Other examples include the Invasion of Banu Qurayza in February–March 627 and the siege of Ta'if in January 630.
In the Middle Ages, the Mongol Empire's campaign against China (then comprising the Western Xia Dynasty, Jin Dynasty, and Southern Song dynasty) by Genghis Khan until Kublai Khan, who eventually established the Yuan Dynasty in 1271, was very effective, allowing the Mongols to sweep through large areas. Even if they could not enter some of the more well-fortified cities, they used innovative battle tactics to grab hold of the land and the people:
Another Mongol tactic was to use catapults to launch corpses of plague victims into besieged cities. The disease-carrying fleas from the bodies would then infest the city, and the plague would spread, allowing the city to be easily captured, although this transmission mechanism was not known at the time. In 1346, the bodies of Mongol warriors of the Golden Horde who had died of plague were thrown over the walls of the besieged Crimean city of Kaffa (now Feodosiya). It has been speculated that this operation may have been responsible for the advent of the Black Death in Europe. The Black Death is estimated to have killed 30%–60% of Europe's population.
On the first night while laying siege to a city, the leader of the Mongol forces would lead from a white tent: if the city surrendered, all would be spared. On the second day, he would use a red tent: if the city surrendered, the men would all be killed, but the rest would be spared. On the third day, he would use a black tent: no quarter would be given.
However, the Chinese were not completely defenseless, and from AD 1234 until 1279, the Southern Song Chinese held out against the enormous barrage of Mongol attacks. Much of this success in defense lay in the world's first use of gunpowder (i.e. with early flamethrowers, grenades, firearms, cannons, and land mines) to fight back against the Khitans, the Tanguts, the Jurchens, and then the Mongols.
The Chinese of the Song period also discovered the explosive potential of packing hollowed cannonball shells with gunpowder. Written later around 1350 in the Huo Long Jing, this manuscript of Jiao Yu recorded an earlier Song-era cast-iron cannon known as the 'flying-cloud thunderclap eruptor' (fei yun pi-li pao). The manuscript stated that (Wade–Giles spelling):
The shells (phao) are made of cast iron, as large as a bowl and shaped like a ball. Inside they contain half a pound of 'magic' gunpowder (shen huo). They are sent flying towards the enemy camp from an eruptor (mu phao); and when they get there a sound like a thunder-clap is heard, and flashes of light appear. If ten of these shells are fired successfully into the enemy camp, the whole place will be set ablaze...
During the Ming Dynasty (AD 1368–1644), the Chinese were very concerned with city planning in regards to gunpowder warfare. The site for constructing the walls and the thickness of the walls in Beijing's Forbidden City were favoured by the Chinese Yongle Emperor (r. 1402–1424) because they were in pristine position to resist cannon volley and were built thick enough to withstand attacks from cannon fire.
For more, see Technology of the Song dynasty.
The introduction of gunpowder and the use of cannons brought about a new age in siege warfare. Cannons were first used in Song dynasty China during the early 13th century, but did not become significant weapons for another 150 years or so. In early decades, cannons could do little against strong castles and fortresses, providing little more than smoke and fire. By the 16th century, however, they were an essential and regularized part of any campaigning army, or castle's defences.
The greatest advantage of cannons over other siege weapons was the ability to fire a heavier projectile, farther, faster, and more often than previous weapons. They could also fire projectiles in a straight line, so that they could destroy the bases of high walls. Thus, 'old fashioned' walls – that is, high and, relatively, thin – were excellent targets, and, over time, easily demolished. In 1453, the Theodosian Walls of Constantinople, the capital of the Roman Empire, were broken through in just six weeks by the 62 cannons of Mehmed II's army, although in the end the conquest was a long and extremely difficult siege with heavy Ottoman casualties due to the repeated attempts at taking the city by assault.
However, new fortifications, designed to withstand gunpowder weapons, were soon constructed throughout Europe. During the Renaissance and the early modern period, siege warfare continued to dominate the conduct of the European wars.
Once siege guns were developed, the techniques for assaulting a town or fortress became well known and ritualized. The attacking army would surround a town. Then the town would be asked to surrender. If they did not comply, the besieging army would surround the town with temporary fortifications to stop sallies from the stronghold or relief getting in. The attackers would next build a length of trenches parallel to the defences (these are known as the "First parallel") and just out of range of the defending artillery. They would dig a trench (known as a Forward) towards the town in a zigzag pattern so that it could not be enfiladed by defending fire. Once they were within artillery range, they would dig another parallel (the Second Parallel) trench and fortify it with gun emplacements. This technique is commonly called entrenchment.
If necessary, using the first artillery fire for cover, the forces conducting the siege would repeat the process until they placed their guns close enough to be laid (aimed) accurately to make a breach in the fortifications. In order to allow the forlorn hope and support troops to get close enough to exploit the breach, more zigzag trenches could be dug even closer to the walls, with more parallel trenches to protect and conceal the attacking troops. After each step in the process, the besiegers would ask the besieged to surrender. If the forlorn hope stormed the breach successfully, the defenders could expect no mercy.
The castles that in earlier years had been formidable obstacles were easily breached by the new weapons. For example, in Spain, the newly equipped army of Ferdinand and Isabella was able to conquer Moorish strongholds in Granada in 1482–1492 that had held out for centuries before the invention of cannons.
In the early 15th century, Italian architect Leon Battista Alberti wrote a treatise entitled De Re aedificatoria, which theorized methods of building fortifications capable of withstanding the new guns. He proposed that walls be "built in uneven lines, like the teeth of a saw". He proposed star-shaped fortresses with low, thick walls.
However, few rulers paid any attention to his theories. A few towns in Italy began building in the new style late in the 1480s, but it was only with the French invasion of the Italian peninsula in 1494–1495 that the new fortifications were built on a large scale. Charles VIII invaded Italy with an army of 18,000 men and a horse-drawn siege-train. As a result, he could defeat virtually any city or state, no matter how well defended. In a panic, military strategy was completely rethought throughout the Italian states of the time, with a strong emphasis on the new fortifications that could withstand a modern siege.
The most effective way to protect walls against cannonfire proved to be depth (increasing the width of the defences) and angles (ensuring that attackers could only fire on walls at an oblique angle, not square on). Initially, walls were lowered and backed, in front and behind, with earth. Towers were reformed into triangular bastions. This design matured into the trace italienne. Star-shaped fortresses surrounding towns and even cities with outlying defences proved very difficult to capture, even for a well-equipped army. Fortresses built in this style throughout the 16th century did not become fully obsolete until the 19th century, and were still in use throughout World War I (though modified for 20th-century warfare). During World War II, trace italienne fortresses could still present a formidable challenge, for example, in the last days of World War II, during the Battle in Berlin, that saw some of the heaviest urban fighting of the war, the Soviets did not attempt to storm the Spandau Citadel (built between 1559 and 1594), but chose to invest it and negotiate its surrender.
However, the cost of building such vast modern fortifications was incredibly high, and was often too much for individual cities to undertake. Many were bankrupted in the process of building them; others, such as Siena, spent so much money on fortifications that they were unable to maintain their armies properly, and so lost their wars anyway. Nonetheless, innumerable large and impressive fortresses were built throughout northern Italy in the first decades of the 16th century to resist repeated French invasions that became known as the Italian Wars. Many stand to this day.
In the 1530s and '40s, the new style of fortification began to spread out of Italy into the rest of Europe, particularly to France, the Netherlands, and Spain. Italian engineers were in enormous demand throughout Europe, especially in war-torn areas such as the Netherlands, which became dotted by towns encircled in modern fortifications. The densely populated areas of Northern Italy and the United Provinces (the Netherlands) were infamous for their high degree of fortification of cities. It made campaigns in these areas very hard to successfully conduct, considering even minor cities had to be captured by siege within the span of the campaigning season. In the Dutch case, the possibility of flooding large parts of the land provided an additional obstacle to besiegers, for example at the siege of Leiden. For many years, defensive and offensive tactics were well balanced, leading to protracted and costly wars such as Europe had never known, involving more and more planning and government involvement. The new fortresses ensured that war rarely extended beyond a series of sieges. Because the new fortresses could easily hold 10,000 men, an attacking army could not ignore a powerfully fortified position without serious risk of counterattack. As a result, virtually all towns had to be taken, and that was usually a long, drawn-out affair, potentially lasting from several months to years, while the members of the town were starved to death. Most battles in this period were between besieging armies and relief columns sent to rescue the besieged.
At the end of the 17th century, two influential military engineers, the French Marshal Vauban and the Dutch military engineer Menno van Coehoorn, developed modern fortification to its pinnacle, refining siege warfare without fundamentally altering it: ditches would be dug; walls would be protected by glacis; and bastions would enfilade an attacker. Both engineers developed their ideas independently, but came to similar general rules regarding defensive construction and offensive action against fortifications. Both were skilled in conducting sieges and defences themselves. Before Vauban and Van Coehoorn, sieges had been somewhat slapdash operations. Vauban and Van Coehoorn refined besieging to a science with a methodical process that, if uninterrupted, would break even the strongest fortifications. Examples of their styles of fortifications are Arras (Vauban) and the no-longer-existent fortress of Bergen op Zoom (Van Coehoorn). The main differences between the two lay in the difference in terrain on which Vauban and Van Coehoorn constructed their defences: Vauban in the sometimes more hilly and mountainous terrain of France, Van Coehoorn in the flat and floodable lowlands of the Netherlands.
Planning and maintaining a siege is just as difficult as fending one off. A besieging army must be prepared to repel both sorties from the besieged area and also any attack that may try to relieve the defenders. It was thus usual to construct lines of trenches and defenses facing in both directions. The outermost lines, known as the lines of contravallation, would surround the entire besieging army and protect it from attackers.
This would be the first construction effort of a besieging army, built soon after a fortress or city had been invested. A line of circumvallation would also be constructed, facing in towards the besieged area, to protect against sorties by the defenders and to prevent the besieged from escaping. The next line, which Vauban usually placed at about 600 meters from the target, would contain the main batteries of heavy cannons so that they could hit the target without being vulnerable themselves. Once this line was established, work crews would move forward, creating another line at 250 meters. This line contained smaller guns. The final line would be constructed only 30 to 60 meters from the fortress. This line would contain the mortars and would act as a staging area for attack parties once the walls were breached. Van Coehoorn developed a small and easily movable mortar named the coehorn, variations of which were used in sieges until the 19th century. It would also be from this line that miners working to undermine the fortress would operate.
The trenches connecting the various lines of the besiegers could not be built perpendicular to the walls of the fortress, as the defenders would have a clear line of fire along the whole trench. Thus, these lines (known as saps) needed to be sharply jagged.
Another element of a fortress was the citadel. Usually, a citadel was a "mini fortress" within the larger fortress, sometimes designed as a reduit, but more often as a means of protecting the garrison from potential revolt in the city. The citadel was used in wartime and peacetime to keep the residents of the city in line.
As in ages past, most sieges were decided with very little fighting between the opposing armies. An attacker's army was poorly served, incurring the high casualties that a direct assault on a fortress would entail. Usually, they would wait until supplies inside the fortifications were exhausted or disease had weakened the defenders to the point that they were willing to surrender. At the same time, diseases, especially typhus, were a constant danger to the encamped armies outside the fortress, and often forced a premature retreat. Sieges were often won by the army that lasted the longest.
An important element of strategy for the besieging army was whether or not to allow the encamped city to surrender. Usually, it was preferable to graciously allow a surrender, both to save on casualties, and to set an example for future defending cities. A city that was allowed to surrender with minimal loss of life was much better off than a city that held out for a long time and was brutally butchered at the end. Moreover, if an attacking army had a reputation of killing and pillaging regardless of a surrender, then other cities' defensive efforts would be redoubled. Usually, a city would surrender (with no honour lost) when its inner lines of defence were reached by the attacker. In case of refusal, however, the inner lines would have to be stormed by the attacker and the attacking troops would be seen to be justified in sacking the city.
Siege warfare dominated in Western Europe for most of the 17th and 18th centuries. An entire campaign, or longer, could be used in a single siege (for example, Ostend in 1601–1604; La Rochelle in 1627–1628). This resulted in extremely prolonged conflicts. The balance was that, while siege warfare was extremely expensive and very slow, it was very successful—or, at least, more so than encounters in the field. Battles arose through clashes between besiegers and relieving armies, but the principle was a slow, grinding victory by the greater economic power. The relatively rare attempts at forcing pitched battles (Gustavus Adolphus in 1630; the French against the Dutch in 1672 or 1688) were almost always expensive failures.
The exception to this rule were the English. During the English Civil War, anything which tended to prolong the struggle, or seemed like want of energy and avoidance of a decision, was bitterly resented by the men of both sides. In France and Germany, the prolongation of a war meant continued employment for the soldiers, but in England, both sides were looking to end the war quickly. Even when in the end the New Model Army—a regular professional army—developed the original decision-compelling spirit permeated the whole organisation, as was seen when pitched against regular professional continental troops the Battle of the Dunes during the Interregnum.
Experienced commanders on both sides in the English Civil War recommended the abandonment of garrisoned fortifications for two primary reasons. The first, as for example proposed by the Royalist Sir Richard Willis to King Charles, was that by abandoning the garrisoning of all but the most strategic locations in one's own territory, far more troops would be available for the field armies, and it was the field armies which would decide the conflict. The other argument was that by slighting potential strong points in one's own territory, an enemy expeditionary force, or local enemy rising, would find it more difficult to consolidate territorial gains against an inevitable counterattack. Sir John Meldrum put forward just such an argument to the Parliamentary Committee of Both Kingdoms, to justify his slighting of Gainsborough in Lincolnshire.
Sixty years later, during the War of the Spanish Succession, the Duke of Marlborough preferred to engage the enemy in pitched battles, rather than engage in siege warfare, although he was very proficient in both types of warfare.
On 15 April 1746, the day before the Battle of Culloden, at Dunrobin Castle, a party of William Sutherland's militia conducted the last siege fought on the mainland of Great Britain against Jacobite members of Clan MacLeod.
In the French Revolutionary and Napoleonic Wars, new techniques stressed the division of armies into all-arms corps that would march separately and only come together on the battlefield. The less-concentrated army could now live off the country and move more rapidly over a larger number of roads.
Fortresses commanding lines of communication could be bypassed and would no longer stop an invasion. Since armies could not live off the land indefinitely, Napoleon Bonaparte always sought a quick end to any conflict by pitched battle. This military revolution was described and codified by Clausewitz.
Advances in artillery made previously impregnable defences useless. For example, the walls of Vienna that had held off the Turks in the mid-17th century were no obstacle to Napoleon in the early 19th.
Where sieges occurred (such as the siege of Delhi and the siege of Cawnpore during the Indian Rebellion of 1857), the attackers were usually able to defeat the defences within a matter of days or weeks, rather than weeks or months as previously. The great Swedish white-elephant fortress of Karlsborg was built in the tradition of Vauban and intended as a reserve capital for Sweden, but it was obsolete before it was completed in 1869.
Railways, when they were introduced, made possible the movement and supply of larger armies than those that fought in the Napoleonic Wars. It also reintroduced siege warfare, as armies seeking to use railway lines in enemy territory were forced to capture fortresses which blocked these lines.
During the Franco-Prussian War, the battlefield front-lines moved rapidly through France. However, the Prussian and other German armies were delayed for months at the siege of Metz and the siege of Paris, due to the greatly increased firepower of the defending infantry, and the principle of detached or semi-detached forts with heavy-caliber artillery. This resulted in the later construction of fortress works across Europe, such as the massive fortifications at Verdun. It also led to the introduction of tactics which sought to induce surrender by bombarding the civilian population within a fortress, rather than the defending works themselves.
The siege of Sevastopol during the Crimean War and the siege of Petersburg (1864–1865) during the American Civil War showed that modern citadels, when improved by improvised defences, could still resist an enemy for many months. The Siege of Plevna during the Russo-Turkish War (1877–1878) proved that hastily constructed field defences could resist attacks prepared without proper resources, and were a portent of the trench warfare of World War I.
Advances in firearms technology without the necessary advances in battlefield communications gradually led to the defence again gaining the ascendancy. An example of siege during this time, prolonged during 337 days due to the isolation of the surrounded troops, was the siege of Baler, in which a reduced group of Spanish soldiers was besieged in a small church by the Philippine rebels in the course of the Philippine Revolution and the Spanish–American War, until months after the Treaty of Paris, the end of the conflict.
Furthermore, the development of steamships availed greater speed to blockade runners, ships with the purpose of bringing cargo, e.g. food, to cities under blockade, as with Charleston, South Carolina during the American Civil War.
Mainly as a result of the increasing firepower (such as machine guns) available to defensive forces, First World War trench warfare briefly revived a form of siege warfare. Although siege warfare had moved out from an urban setting because city walls had become ineffective against modern weapons, trench warfare was nonetheless able to use many of the techniques of siege warfare in its prosecution (sapping, mining, barrage and, of course, attrition), but on a much larger scale and on a greatly extended front.
More traditional sieges of fortifications took place in addition to trench sieges. The siege of Tsingtao was one of the first major sieges of the war, but the inability for significant resupply of the German garrison made it a relatively one-sided battle. The Germans and the crew of an Austro-Hungarian protected cruiser put up a hopeless defence and, after holding out for more than a week, surrendered to the Japanese, forcing the German East Asia Squadron to steam towards South America for a new coal source.
The other major siege outside Europe during the First World War was in Mesopotamia, at the siege of Kut. After a failed attempt to move on Baghdad, stopped by the Ottomans at the bloody Battle of Ctesiphon, the British and their large contingent of Indian sepoy soldiers were forced to retreat to Kut, where the Ottomans under German General Baron Colmar von der Goltz laid siege. The British attempts to resupply the force via the Tigris river failed, and rationing was complicated by the refusal of many Indian troops to eat cattle products. By the time the garrison fell on 29 April 1916, starvation was rampant. Conditions did not improve greatly under Turkish imprisonment. Along with the battles of Tanga, Sandfontein, Gallipoli, and Namakura, it would be one of Britain's numerous embarrassing colonial defeats of the war.
The largest sieges of the war, however, took place in Europe. The initial German advance into Belgium produced four major sieges: the Battle of Liège, the Battle of Namur, the siege of Maubeuge, and the siege of Antwerp. All four would prove crushing German victories, at Liège and Namur against the Belgians, at Maubeuge against the French and at Antwerp against a combined Anglo-Belgian force. The weapon that made these victories possible were the German Big Berthas and the Skoda 305 mm Model 1911 siege mortars, one of the best siege mortars of the war, on loan from Austria-Hungary. These huge guns were the decisive weapon of siege warfare in the 20th century, taking part at Przemyśl, the Belgian sieges, on the Italian Front and Serbian Front, and even being reused in World War II.
At the second siege of Przemyśl, the Austro-Hungarian garrison showed an excellent knowledge of siege warfare, not only waiting for relief, but sending sorties into Russian lines and employing an active defence that resulted in the capture of the Russian General Lavr Kornilov. Despite its excellent performance, the garrison's food supply had been requisitioned for earlier offensives, a relief expedition was stalled by the weather, ethnic rivalries flared up between the defending soldiers, and a breakout attempt failed. When the commander of the garrison Hermann Kusmanek finally surrendered, his troops were eating their horses and the first attempt of large-scale air supply had failed. It was one of the few great victories obtained by either side during the war; 110,000 Austro-Hungarian prisoners were marched back to Russia. Use of aircraft for siege running, bringing supplies to areas under siege, would nevertheless prove useful in many sieges to come.
The largest siege of the war, and arguably the roughest, most gruesome battle in history, was the Battle of Verdun. Whether the battle can be considered true siege warfare is debatable. Under the theories of Erich von Falkenhayn, it is more distinguishable as purely attrition with a coincidental presence of fortifications on the battlefield. When considering the plans of Crown Prince Wilhelm, purely concerned with taking the citadel and not with French casualty figures, it can be considered a true siege. The main fortifications were Fort Douaumont, Fort Vaux, and the fortified city of Verdun itself. The Germans, through the use of huge artillery bombardments, flamethrowers, and infiltration tactics, were able to capture both Vaux and Douaumont, but were never able to take the city, and eventually lost most of their gains. It was a battle that, despite the French ability to fend off the Germans, neither side won. The German losses were not worth the potential capture of the city, and the French casualties were not worth holding the symbol of her defence.
The development of the armoured tank and improved infantry tactics at the end of World War I swung the pendulum back in favour of manoeuvre, and with the advent of Blitzkrieg in 1939, the end of traditional siege warfare was at hand. The Maginot Line would be the prime example of the failure of immobile, post–World War I fortifications. Although sieges would continue, it would be in a totally different style and on a reduced scale.
The Blitzkrieg of the Second World War truly showed that fixed fortifications are easily defeated by manoeuvre instead of frontal assault or long sieges. The great Maginot Line was bypassed, and battles that would have taken weeks of siege could now be avoided with the careful application of air power (such as the German paratrooper capture of Fort Eben-Emael, Belgium, early in World War II).
The most important siege was the siege of Leningrad, that lasted over 29 months, about half of the duration of the entire Second World War. The siege of Leningrad resulted in the deaths of some one million of the city's inhabitants. Along with the Battle of Stalingrad, the siege of Leningrad on the Eastern Front was the deadliest siege of a city in history. In the west, apart from the Battle of the Atlantic, the sieges were not on the same scale as those on the European Eastern front; however, there were several notable or critical sieges: the island of Malta, for which the population won the George Cross and Tobruk. In the South-East Asian Theatre, there was the siege of Singapore, and in the Burma Campaign, sieges of Myitkyina, the Admin Box, Imphal, and  Kohima, which was the high-water mark for the Japanese advance into India.
The siege of Sevastopol saw the use of the heaviest and most powerful individual siege engines ever to be used: the German 800mm railway gun and the 600mm siege mortar. Though a single shell could have disastrous local effect, the guns were susceptible to air attack in addition to being slow to move.
Throughout the war both the Western Allies and the Germans tried to supply forces besieged behind enemy lines with ad-hoc airbridges. Sometimes these attempts failed, as happened to the besieged German Sixth Army the siege of Stalingrad, and sometimes they succeeded as happened during the Battle of the Admin Box (5 – 23 February 1944) and, during the short Siege of Bastogne (December 1944).
The logistics of strategic airbridge operations were developed by the Americans flying military transport aircraft from India to China over the Hump (1942–1945), to resupply the Chinese war effort of Chiang Kai-shek, and to the USAAF XX Bomber Command (during Operation Matterhorn).
Tactical airbridge methods were developed and, as planned, used extensively for supplying the Chindits during Operation Thursday (February – May 1944). The Chindits a specially trained division of the British and Indian armies were flown deep behind Japanese front lines in the South-East Asian theatre to jungle clearings in Burma where they set up fortified airheads from which they sailed out to attack Japanese lines of communications, while defending the bases from Japanese counterattacks. The bases were re-supplied by air with casualties flown out by returning aircraft. When the Japanese attacked in strength the Chindits abandoned the bases and either moved to new bases, or back to Allied lines.
Several times during the Cold War the western powers had to use their airbridge expertise.
In both Vietnamese cases, the Viet Minh and NLF were able to cut off the opposing army by capturing the surrounding rugged terrain. At Dien Bien Phu, the French were unable to use air power to overcome the siege and were defeated. However, at Khe Sanh, a mere 14 years later, advances in air power—and a reduction in Vietnamese anti-aircraft capability—allowed the United States to withstand the siege. The resistance of US forces was assisted by the PAVN and PLAF forces' decision to use the Khe Sanh siege as a strategic distraction to allow their mobile warfare offensive, the first Tet Offensive, to unfold securely.
The siege of Khe Sanh displays typical features of modern sieges, as the defender has greater capacity to withstand the siege, the attacker's main aim is to bottle operational forces or create a strategic distraction, rather than take the siege to a conclusion.
In neighbouring Cambodia, at that time known as the Khmer Republic, the Khmer Rouge used siege tactics to cut off supplies from Phnom Penh to other government-held enclaves in an attempt to break the will of the government to continue fighting.
In 1972, during the Easter offensive, the siege of An Lộc Vietnam occurred. ARVN troops and U.S. advisers and air power successfully defeated communist forces. The Battle of An Lộc pitted some 6,350 ARVN men against a force three times that size. During the peak of the battle, ARVN had access to only one 105 mm howitzer to provide close support, while the enemy attack was backed by an entire artillery division. ARVN had no tanks, the NVA communist forces had two armoured regiments. ARVN prevailed after over two months of continuous fighting. As General Paul Vanuxem, a French veteran of the Indochina War, wrote in 1972 after visiting the liberated city of An Lộc: "An Lộc was the Verdun of Vietnam, where Vietnam received as in baptism the supreme consecration of her will."
During the Yugoslav Wars in the 1990s, Republika Srpska forces besieged Sarajevo, the capital of Bosnia-Herzegovina. The siege lasted from 1992 until 1996.
Numerous sieges haven taken place during the Syrian Civil War, such as the Siege of Homs, Siege of Kobanî, Siege of Deir ez-Zor (2014–2017) and Siege of al-Fu'ah and Kafriya.
Multiple sieges took place in the 2022 Russian invasion of Ukraine, most notably the Siege of Mariupol.
Siege tactics continue to be employed in police conflicts. This has been due to a number of factors, primarily risk to life, whether that of the police, the besieged, bystanders, or hostages. Police make use of trained negotiators, psychologists, and, if necessary, force, generally being able to rely on the support of their nation's armed forces if required.
One of the complications facing police in a siege involving hostages is Stockholm syndrome, where sometimes hostages can develop a sympathetic rapport with their captors. If this helps keep them safe from harm, this is considered to be a good thing, but there have been cases where hostages have tried to shield the captors during an assault or refused to cooperate with the authorities in bringing prosecutions.
The 1993 police siege on the Branch Davidian church in Waco, Texas, lasted 51 days, an atypically long police siege. Unlike traditional military sieges, police sieges tend to last for hours or days, rather than weeks, months, or years.
In Britain, if the siege involves perpetrators who are considered by the British Government to be terrorists, and if an assault is to take place, the civilian authorities hand command and control over to the military. The threat of such an action ended the Balcombe Street siege in 1975, but the Iranian Embassy siege in 1980 ended in a military assault and the deaths of all but one of the hostage-takers.
Historiography
Konrad Kyeser (26 August 1366 – after 1405) was a German military engineer and the author of Bellifortis (c. 1405), a book on military technology that was popular throughout the 15th century. Originally conceived for King Wenceslaus, Kyeser dedicated the finished work to Rupert of Germany.
A native of Eichstätt, Kyeser was trained as a physician and lived at the court in Padua before he joined the crusade against the Turks which ended in disaster at the Battle of Nicopolis in 1396. Kyeser lived in exile in a mountain village in Bohemia during the reign of Sigismund in 1402 to 1403.
During his time in exile, Kyeser began to write his book on the military arts. The book is the most prominent illustrated treatise on military engineering of the Late Middle Ages. There are at least twelve surviving 15th-century manuscripts which copy, excerpt, or amplify the work. One of these is the Thott Fechtbuch of Hans Talhoffer (1459).
The book is divided into ten chapters, though there are also issues with the full content divided into 7 chapters. The topics of the chapters include cars, siege engines, hydraulic engines, elevators, firearms, defensive arms, "wondrous secrets", fireworks for warfare, fireworks for pleasure, and auxiliary tools.
The diving suit presented in the book has precedents reaching back to the 12th century and to Roger Bacon. The book also has the earliest known depiction of a chastity belt. Kyeser counts the artes magicae among the mechanical arts, and his work contains various applications of magic in warfare.
The original codex is kept in the Göttingen University library (Cod. Ms. philos. 63). Other editions include the following:
Kyeser appears as an NPC in the 2018 video game Kingdom Come: Deliverance, and is voiced by Brian Blessed.
Bellifortis ("Strong in War", "War Fortifications") is the first fully illustrated manual of military technology written by Konrad Kyeser and dating from the start of the 15th century. It summarises material from classical writers on military technology, like Vegetius' De Re Militari and Frontinus' anecdotal Strategemata, emphasising poliorcetics, or the art of siege warfare, but treating magic as a supplement to the military arts; it is "saturated with astrology", remarked Lynn White, Jr. in a review of the first facsimile edition.
Konrad Kyeser wrote his treatise between 1402 and 1405 when he was exiled from Prague to his hometown of Eichstätt. Many of the illustrations for the book were made by German illuminators who were sent to Eichstätt after their own ousting from the Prague scriptorium. The work, which was not printed until 1967, survived in a single original presentation manuscript on parchment at University of Göttingen, bearing the date 1405, and in numerous copies, excerpts and amplifications, both of the text and of the illustrations, made in German lands.
Bellifortis was written in Latin and contained many elaborate illustrations of war weaponry. The manual discusses machines and technology that were old and new. It described weapons such as trebuchets, battering rams, movable portable bridges, cannons, rockets, chariots, ships, mills, scaling ladders, incendiary devices, crossbows, and instruments of torture. The portrait of the author is called by its modern editor the first realistic portrait of an author since Antiquity.
Kyeser’s viewpoint was that warfare in the broadest sense was most effective if looked at from all angles, which included astrology and sorcery. His manual presented the technology of the art of war through the association of education and Latin letters. The book was of a large expensive format. It had elaborate illustrations and lavish drawings of a large number of war devices and machines. The treatise was designed more for a prince or king than for an engineer. Kyeser believed his war manual would make other armies run in all directions.
His treatise often made reference to antiquity, especially the war tactics of Alexander the Great. He writes that Alexander had many war technical abilities. In one illustration he shows Alexander with a giant spearhead-like artifact in his hands with the mysterious letters: MEUFATON. In another illustration Alexander is shown as the supposed inventor of a very large war carriage. Kyeser writes that Alexander was not only a great inventor of war devices but was able to use them himself. Alexander is portrayed with magical abilities.
Konrad dedicated his finished treatise to the weak Ruprecht III in a bitter response to his exile. He emphasizes in the dedication the relationship of technical knowledge to technical skills. He writes of the German soldiers, "Just as the sky shines with stars, Germany shines forth with liberal disciplines, is embellished with mechanics, and adorned with diverse arts."
At the end of the treatise, Kyeser gives a markedly unusual appearance of himself. He portrays himself as a dying worried person. He even provides his own epitaph, "May my soul be joined to your very high one."
The Bellifortis is survived in 45 manuscripts and was either copied completely or partly, and sometimes amplified, in several later manuscripts. The most famous are the Thott manuscript of Hans Talhoffer of the 15th century, but there are editions of Vegetius' De Re Militari from 1535 in Latin and 1536 in French, that contain pictures clearly copied from the Bellifortis (with more up-to-date clothing for the soldiers), to augment the original text-only treatise by Vegetius. In the Renaissance in Italy Bellifortis was well-known and widespread. The result of new research shows that also Leonardo da Vinci knew the work of Kyeser and that several of Leonardos technical illustrations are based on the Bellifortis.
 Media related to Bellifortis at Wikimedia Commons
Leonhard Fronsperger (c. 1520–1575) was a Bavarian  German soldier and author.  He received the citizenship of Ulm in 1548 and served in the imperial army  during 1553–1573, under  Charles V, Ferdinand I and Maximilian II.
He retired and received a pension from Maximilian in 1566. He became a military expert in the service of the city of Ulm, and died in an accident while on an inspection in 1575.
His major work is the Kriegsbuch, published in three parts in 1573,   a sweeping overview over the military theory and technology of his time. He also wrote an early treatise on the economic theory of self-interest (Von dem Lob deß Eigen Nutzen, 1564), anticipating the gist of the Mandeville's paradox.
Conrad Haas (1509–1576) was an Austrian or a Transylvanian Saxon military engineer from Transylvania.
He was a pioneer of rocket propulsion. His designs include a three-stage rocket and a manned rocket.
Haas was perhaps born in Dornbach (now part of Hernals, Vienna). 
He held the post of the Zeugwart (arsenal master) of the Imperial Habsburg army under Ferdinand I. 
In 1551, Stephen Báthory, the grand prince of Transylvania invited Haas to Nagyszeben (German: Hermannstadt), Eastern Hungarian Kingdom (now Sibiu, Romania), where he acted as weapons engineer and also he started to teach at Klausenburg (now Cluj-Napoca).
He wrote a German-language treatise on rocket technology, involving the combination of fireworks and weapons technologies. This manuscript was discovered in 1961, in the Sibiu public records (Sibiu public records Varia II 374).
His work also dealt with the theory of motion of multi-stage rockets, different fuel mixtures using liquid fuel, and introduced delta-shape fins and bell-shaped nozzles.
In the last paragraph of his chapter on the military use of rockets, he wrote (translated):
"But my advice is for more peace and no war, leaving the rifles calmly in storage, so the bullet is not fired, the gunpowder is not burned or wet, so the prince keeps his money, the arsenal master his life; that is the advice Conrad Haas gives."Johann Schmidlap, a German fireworks maker, is believed to have experimented with staging in 1590, using a design he called "step rockets." Before the discovery of Haas' manuscript, the first description of the three-stage rocket was credited to the artillery specialist Casimir Siemienowicz, from the Polish–Lithuanian Commonwealth, in his 1650 work, Artis Magnae Artilleriae Pars Prima ("Great Art of Artillery, Part One").
Rocket artillery is artillery that uses rocket explosives as the projectile. The use of rocket artillery dates back to medieval China where devices such as fire arrows were used (albeit mostly as a psychological weapon). Fire arrows were also used in multiple launch systems and transported via carts. First true rocket artillery was developed in India by the Kingdom of Mysore. In the late nineteenth century, due to improvements in the power and range of conventional artillery, the use of early military rockets declined; they were finally used on a small scale by both sides during the American Civil War. Modern rocket artillery was first employed during World War II, in the form of the German Nebelwerfer family of rocket ordnance designs, Soviet Katyusha-series and numerous other systems employed on a smaller scale by the Western allies and Japan. In modern use, the rockets are often guided by an internal guiding system or GPS in order to maintain accuracy.
The use of rockets as some form of artillery dates back to medieval China where devices such as fire arrows were used (albeit mostly as a psychological weapon). Fire arrows were also used in multiple launch systems and transported via carts. Devices such as the Korean hwacha were able to fire hundreds of fire arrows simultaneously. The use of medieval rocket artillery was picked up by the invading Mongols and spread to the Ottoman Turks who in turn used them on the European battlefield.
The use of war-rockets is well documented in Medieval Europe. In 1408 Duke John the Fearless of Burgundy used 300 incendiary rockets in the Battle of Othée. The city dwellers coped with this tactic by covering their roofs with dirt.
The earliest successful utilization of metal-cylinder rocket artillery is associated with Kingdom of Mysore, South India. Tipu Sultan's father Hyder Ali successfully established the powerful Sultanate of Mysore and introduced the first iron-cased metal-cylinder rocket. The Mysorean rockets of this period were innovative, chiefly because of the use of iron tubes that tightly packed the gunpowder propellant; this enabled higher thrust and longer range for the missile (up to 2 km range). 
Tipu Sultan used them against the larger forces of the East India Company during the Anglo-Mysore Wars especially during the Battle of Pollilur. Another battle where these missiles were deployed was the Battle of Sultanpet Tope, where Colonel Arthur Wellesley, later famous as the First Duke of Wellington. Wellesley was almost defeated by Tipu's Diwan Purnaiah.
Although the rockets were quite primitive, they had a demoralizing effect on the enemy due to the noise and bursting light. The rockets could be of various sizes but usually consisted of a tube of soft hammered iron about 8 inches (20 cm) long and 1.5 to 3 inches (3.8 to 7.6 cm) in diameter, closed at one end and strapped to a shaft of bamboo about 4 ft (1 m) long. The iron tube acted as a combustion chamber and contained well-packed black powder propellant. A rocket carrying about one pound (~500 gm) of powder could travel almost 1,000 yards (~900 m). 
According to Stephen Oliver Fought and John F. Guilmartin, Jr. in Encyclopædia Britannica (2008):
Hyder Ali, prince of Mysore, developed war rockets with an important change: the use of metal cylinders to contain the combustion powder. Although the hammered soft iron he used was crude, the bursting strength of the container of black powder was much higher than the earlier paper construction. Thus a greater internal pressure was possible, with a resultant greater thrust of the propulsive jet. The rocket body was lashed with leather thongs to a long bamboo stick. The range was perhaps up to three-quarters of a mile (more than a kilometre). Although individually these rockets were not accurate, dispersion error became less important when large numbers were fired rapidly in mass attacks. They were particularly effective against cavalry and were hurled into the air, after lighting, or skimmed along the hard dry ground. Hyder Ali's son, Tipu Sultan, continued to develop and expand the use of rocket weapons, reportedly increasing the number of rocket troops from 1,200 to a corps of 5,000. In battles at Seringapatam in 1792 and 1799 these rockets were used with minimal effect against the British.The Indian Tipu Sultan's rocket experiences, including Munro's book of 1789, eventually led to the Royal Arsenal beginning a military rocket R&amp;D program in 1801. Several rocket cases were collected from Mysore and sent to Britain for analysis. The development was chiefly the work of Col. (later Sir) William Congreve, son of the Comptroller of the Royal Arsenal, Woolwich, London, who set on a vigorous research and development programme at the Arsenal's laboratory; after development work was complete, the rockets were manufactured in quantity further north, near Waltham Abbey, Essex. He was told that "the British at Seringapatam had suffered more from the rockets than from the shells or any other weapon used by the enemy". "In at least one instance", an eyewitness told Congreve, "a single rocket had killed three men and badly wounded others".
It has been suggested that Congreve may have adapted iron-cased gunpowder rockets for use by the British military from prototypes created by the Irish nationalist Robert Emmet during Emmet's Rebellion in 1803. But this seems far less likely given the fact that the British had been exposed to Indian rockets since 1780 at the latest, and that a vast quantity of unused rockets and their construction equipment fell into British hands at the end of the Anglo-Mysore Wars in 1799, at least 4 years before Emmet's rockets.
Congreve introduced a standardised formula for the making of gunpowder at Woolwich and introduced mechanical grinding mills to produce powder of uniform size and consistency. Machines were also employed to ensure the packing of the powder was perfectly uniform. His rockets were more elongated had a much larger payload and were mounted on sticks; this allowed them to be launched from the sea at a greater range. He also introduced shot into the payload that added shrapnel damage to the incendiary capability of the rocket. By 1805 he was able to introduce a comprehensive weapons system to the British Army.
The rocket had a "cylindro-conoidal" warhead and was launched in pairs from half troughs on simple metal A-frames. The original rocket design had the guide pole side-mounted on the warhead, this was improved in 1815 with a base plate with a threaded hole. They could be fired up to two miles, the range being set by the degree of elevation of the launching frame, although at any range they were fairly inaccurate and had a tendency for premature explosion. They were as much a psychological weapon as a physical one, and they were rarely or never used except alongside other types of artillery. Congreve designed several different warhead sizes from 3 to 24 pounds (1.4 to 10.9 kg). The 24 pounds (11 kg) type with a 15 foot (4.6 m) guide pole was the most widely used variant. Different warheads were used, including explosive, shrapnel and incendiary. They were manufactured at a special facility near the Waltham Abbey Royal Gunpowder Mills beside the River Lea in Essex.
These rockets were used during the Napoleonic Wars against the city of Boulogne, and during the naval bombardment of Copenhagen, where over 25,000 rockets were launched causing severe incendiary damage to the city. The rockets were also adapted for the purpose of flares for signalling and battlefield illumination. Henry Trengrouse utilized the rocket in his life-saving apparatus, in which the rocket was launched at a shipwreck with an attached line to help rescue the victims.
The Congreve rockets are also famous for inspiring the lawyer Francis Scott Key to pen the words the "rockets' red glare" in what became the US National Anthem during the War of 1812.
After the rockets were successfully used during Napoleon's defeat at the Battle of Waterloo, various countries were quick to adopt the weapon and establish special rocket brigades. The British created the British Army Rocket Brigade in 1818, followed by the Austrian Army and the Russian Army.
One persistent problem with the rockets was their lack of aerodynamic stability. The British engineer William Hale designed a rocket with a combination of tail fins and directed nozzles for the exhaust. This imparted a spin to the rocket during flight, which stabilized its trajectory and greatly improved its accuracy, although it did sacrifice somewhat of the maximum range. Hale rockets were enthusiastically adopted by the United States, and during the Mexican War in 1846 a volunteer brigade of rocketeers was pivotal in the surrender of Mexican forces at the Siege of Veracruz.
By the late nineteenth century, due to improvements in the power and range of conventional artillery, the use of military rockets declined; they were finally used on a small scale by both sides during the American Civil War.
Modern rocket artillery was first employed during World War II, in the form of the German Nebelwerfer family of rocket ordnance designs, and Soviet Katyusha-series. The Soviet Katyushas, nicknamed by German troops Stalin's Organ because of their visual resemblance to a church musical organ and alluding to the sound of the weapon's rockets, were mounted on trucks or light tanks, while the early German Nebelwerfer ordnance pieces were mounted on a small wheeled carriage which was light enough to be moved by several men and could easily be deployed nearly anywhere, while also being towed by most vehicles. The Germans also had self-propelled rocket artillery in the form of the Panzerwerfer and Wurfrahmen 40 which equipped half-track armoured fighting vehicles. An oddity in the subject of rocket artillery during this time was the German "Sturmtiger", a vehicle based on the Tiger I heavy tank chassis that was armed with a 380 mm rocket mortar.
The Western Allies of World War II employed little rocket artillery. During later periods of the war, British and Canadian troops used the Land Mattress, a towed rocket launcher. The United States Army built and deployed a small number of turret-mounted T34 Calliope and T40 Whizbang rocket artillery tanks (converted from M4 Sherman medium tanks) in France and Italy. In 1945, the British Army also fitted some M4 Shermans with two 60 lb RP3 rockets, the same as used on ground attack aircraft and known as "Tulip".
In the Pacific, however, the US Navy made heavy use of rocket artillery on their LSM(R) transports, adding to the already intense bombardment by the guns of heavy warships to soften up Japanese-held islands before the US Marines would land. On Iwo Jima, the Marines made use of rocket artillery trucks in a similar fashion as the Soviet Katyusha, but on a smaller scale.
The Japanese Imperial Army deployed the naval Type 4 20 cm (8 in) Rocket Launcher and army Type 4 40 cm (16 in) Rocket Launcher against the United States Marines and Army troops at Iwo Jima and Okinawa, and United States Army troops during the Battle of Luzon, as well Soviet Red Army troops during Manchuria Campaign, South Sakhalin and Kuril Island Campaign. Their deployment was limited relative to other mortar types and the projectiles on the 40 cm launcher were so large and heavy that they had to be loaded using small hand-operated cranes, but they were extremely accurate and had a pronounced psychological effect on opposing troops, who called them "Screaming Mimis", a nickname originally applied to the German Nebelwerfer tube-launched rocket mortar series in the European Theater of Operations. They were often used at night to conceal their launching sites and increase their disruptiveness and psychological effectiveness. The Japanese 20 cm rockets were launched from tubes or launching troughs, while the larger rockets were launched from steel ramps reinforced with wooden monopods.
The Japanese also deployed a limited number of 447mm rocket launchers, termed 45 cm Rocket Mortars by United States personnel who test-fired them at the close of the war. Their projectiles consisted of a 1,500 lb cylinder filled with propellant and ballistite sticks detonated by black powder, which produced a blast crater approximately the size of an American 1,000 lb bomb. In effect, this made the 447mm projectile a type of surface-to-surface barrel bomb. While these latter weapons were captured at Luzon and proved effective in subsequent testing, it is not clear that they were ever used against American troops, in contrast to the more common 20 and 40 cm types, which clearly contributed to the 37,870 American casualties sustained at Luzon.
Israel fitted some of their Sherman tanks with different rocket artillery. An unconventional Sherman conversion was the turretless Kilshon ("Trident") that launched an AGM-45 Shrike anti-radiation missile.
The Soviet Union continued its development of the Katyusha during the Cold War, and also exported them widely.
Modern rocket artillery such as the US M270 Multiple Launch Rocket System is highly mobile and are used in similar fashion to other self-propelled artillery. Global Positioning and Inertial Navigation terminal guidance systems have been introduced.
During the Kargil war of 1999, Indian army pressed into service the Pinaka MBRL against Pakistani forces. The system was under development and still was able to successfully perform after which the Indian Army showed interest in inducting the system into service.
Kazimierz Siemienowicz (Latin: Casimirus Siemienowicz, Lithuanian: Kazimieras Simonavičius; born c. 1600 – c. 1651) was a general of artillery, gunsmith, military engineer, and one of pioneers of rocketry. Born in the Raseiniai region of the Grand Duchy of Lithuania, he served in the armies of the Polish–Lithuanian Commonwealth and of Frederick Henry, Prince of Orange, the ruler of the Netherlands. No portrait or detailed biography of him has survived and much of his life is a subject of dispute.
After contributing his expertise to several battles, Siemienowicz published Artis Magnae Artilleriae in 1650. This treatise, which discusses rocketry and pyrotechnics, remained a standard work in those fields for two centuries.
Lithuanian literature asserts that he was born near Raseiniai in Samogitia. The family, who was relatively poor, bore the Ostoja Coat of Arms with military service traditions in the Grand Duchy. In a book dedication, he refers to himself as an "Eques Lithuanus" (Lithuanian nobleman). Siemenowicz was educated in the Academy of Vilnius.
The Polish school describes his identity simply as member of the szlachta (i.e., nobility in the Commonwealth) from Grand Duchy. Some sources use the term "Polish," others describe him as "Lithuanian". Those terms should be understood in proper context: "Polish" means "of the Polish–Lithuanian Commonwealth"; "Lithuanian" from the Grand Duchy of Lithuania, a federal part of the Commonwealth. Polish historian professor Tadeusz Marian Nowak described Siemienowicz as a Polonized Lithuanian nobleman. Polish historians for the most part accept that he used the Ostoja Coat of Arms and that he was an alumnus of the Academy of Vilnius.
The Belarusian school asserts that he was born in the vicinity of Dubrowna in the Viciebsk land, to a family of minor Ruthenian princes (knyaz) of Siemienowicz, who possessed the small tracts of land in that part of the Belarusian Dnieper-land (Падняпроўе) in the 14th–17th centuries. Some examples of lexicography used by K. Siemienowicz support this interpretation.
There are no records of families with the surname Siemienowicz having the right to bear the Ostoja coat of arms and it is possible that Siemienowicz acquired the right to use the image of Ostoja in his book to facilitate its circulation.
As Siemienowicz wrote, he was fascinated by artillery since childhood, and he studied many sciences to increase his knowledge (mathematics, mechanics, hydraulics, architecture, optics, tactics). In 1632–1634 he took part in the Smolensk War, in the Siege of Belaya under Mikołaj Abramowicz (who in 1640 became the first Lithuanian General of Artillery). It is possible that in 1644 he took part in the Battle of Ochmatów.
He spent some time in the Netherlands, where he was sent by the King Władysław IV Vasa to serve in the army of Duke Frederick Henry of Orange during the war with Spain; he participated in the Siege of Hulst in 1645. In 1646 he returned to Poland when Władysław created the Polish artillery corps and gathered specialists from Europe, planning a war with Ottoman Empire. He served as an engineering expert in the fields of artillery and rocketry in the royal artillery forces. From 1648 he served as Second in Command of the Polish Royal Artillery.
In late 1648 the newly elected king John II Casimir Vasa, who had no plans for the war with Ottomans, advised him to return to the Netherlands and publish his studies there. There are rumors that in 1649 Siemienowicz became embroiled in a conflict with General of the Artillery Krzysztof Arciszewski over a bureaucratic matter; around 1649 he decided to leave the Commonwealth and work on his book in Amsterdam.

Siemienowicz considered the use of poison gases dishonorable. In his work, he wrote:"and most of all, they shall not construct any poisoned globes, nor other sorts of pyrobolic inventions, in which he shall introduce no poison whatsoever, besides which, they shall never employ them for the ruin and destruction of men, because the first inventors of our art thought such actions as unjust among themselves as unworthy of a man of heart and a real soldier.In a historically early instance of biowarfare, Siemienowicz sponsored the firing of artillery containing the saliva of rabid dogs during a 1650 battle. While the success of this experiment is unknown, it demonstrated an educated guess about the disease's communicability that was not confirmed until the 18th century. It was popular in ancient warfare to catapult a deadly disease by using an infected cadaver or its parts to the enemy. One of the most notable examples was Genghis Khan's war against besieged Chinese cities, where he catapulted dead bodies infected with plague into cities.
In 1650 Siemienowicz published a notable work, Artis Magnae Artilleriae pars prima (Great Art of Artillery, the First Part). Its name implies a second part, and it is rumored that he wrote its manuscript before his death. It is also rumored that he was killed by members of the metallurgy/gunsmith/pyrotechnics guilds, who were opposed to him publishing a book about their secrets, and that they hid or destroyed the manuscript of the second part. Guilds aggressively protecting their production secrets was widespread in these times, as we can see from James Stirling having to flee Venice in 1725 for fear of being assassinated after finding out a trade secret of the glassmakers of Venice. Siemienowicz disparaged what he saw as a culture of secrecy based on "canting Alchymists of the times Past...they dealed in nothing but Smoke, yet arrogantly took upon them to be Professors of so noble and excellent an art as Chymistry."
Artis Magnae Artilleriae pars prima was first printed in Amsterdam in 1650, was translated to French in 1651, German in 1676, English and Dutch in 1729, and Polish in 1963.
In the first part of his work he wrote that the second one would contain the "universal pyrotechnic invention, containing all of our current knowledge." According to his short description, this invention was supposed to greatly ease all measurements and calculations.
For over two centuries this work was used in Europe as a basic artillery manual / handbook. Its pyrotechnic formulations were used for over a century. The book provided the standard designs for creating rockets, fireballs, and other pyrotechnic devices. It discussed for the first time the idea of applying a reactive technique to artillery (rocket artillery). It contains a large chapter on caliber, construction, production and properties of rockets for both military and civil purposes, including multistage rockets, batteries of rockets, and rockets with delta wing stabilisers (instead of the common guiding rods). It was the first book in the world to systematically present knowledge about the development of multistage rockets and rocket artillery.

Amsterdam (/ˈæmstərdæm/ AM-stər-dam, UK also /ˌæmstərˈdæm/ AM-stər-DAM, Dutch:  (listen), lit. The Dam on the River Amstel) is the capital and most populous city of the Netherlands; with a population of 907,976 within the city proper, 1,558,755 in the urban area and 2,480,394 in the metropolitan area. Found within the Dutch province of North Holland, Amsterdam is colloquially referred to as the "Venice of the North", due to the large number of canals which form a UNESCO World Heritage Site.
Amsterdam was founded at the Amstel, that was dammed to control flooding; the city's name derives from the Amstel dam. Originating as a small fishing village in the late 12th century, Amsterdam became one of the most important ports in the world during the Dutch Golden Age of the 17th century, and became the leading centre for the finance and trade sectors. In the 19th and 20th centuries, the city expanded and many new neighborhoods and suburbs were planned and built. The 17th-century canals of Amsterdam and the 19–20th century Defence Line of Amsterdam are on the UNESCO World Heritage List. Sloten, annexed in 1921 by the municipality of Amsterdam, is the oldest part of the city, dating to the 9th century.
Amsterdam's main attractions include its historic canals, the Rijksmuseum, the Van Gogh Museum, the Stedelijk Museum, Hermitage Amsterdam, the Concertgebouw, the Anne Frank House, the Scheepvaartmuseum, the Amsterdam Museum, the Heineken Experience, the Royal Palace of Amsterdam, Natura Artis Magistra, Hortus Botanicus Amsterdam, NEMO, the red-light district and many cannabis coffee shops. It drew more than 5 million international visitors in 2014. The city is also well known for its nightlife and festival activity; with several of its nightclubs (Melkweg, Paradiso) among the world's most famous. Primarily known for its artistic heritage, elaborate canal system and narrow houses with gabled façades; well-preserved legacies of the city's 17th-century Golden Age. These characteristics are arguably responsible for attracting millions of Amsterdam's visitors annually. Cycling is key to the city's character, and there are numerous biking paths and lanes spread throughout the entire city.
The Amsterdam Stock Exchange is considered the oldest "modern" securities market stock exchange in the world. As the commercial capital of the Netherlands and one of the top financial centres in Europe, Amsterdam is considered an alpha world city by the Globalization and World Cities (GaWC) study group. The city is also the cultural capital of the Netherlands. Many large Dutch institutions have their headquarters in the city, including: the Philips conglomerate, AkzoNobel, Booking.com, TomTom, and ING. Moreover, many of the world's largest companies are based in Amsterdam or have established their European headquarters in the city, such as leading technology companies Uber, Netflix and Tesla. In 2022, Amsterdam was ranked the ninth best city in the world to live in by the Economist Intelligence Unit (EIU) and 12th globally on quality of living for environment and infrastructure by Mercer. The city was ranked 4th place globally as top tech hub in the Savills Tech Cities 2019 report (2nd in Europe), and 3rd in innovation by Australian innovation agency 2thinknow in their Innovation Cities Index 2009. The Port of Amsterdam is the fifth largest in Europe. The KLM hub and Amsterdam's main airport, Schiphol, is the Netherlands' busiest airport as well as the third busiest in Europe and 11th busiest airport in the world. The Dutch capital is considered one of the most multicultural cities in the world, with at least 177 nationalities represented.
A few of Amsterdam's notable residents throughout its history include: painters Rembrandt and Van Gogh, the diarist Anne Frank, and philosopher Baruch Spinoza.
Due to its geographical location in what used to be wet peatland, the founding of Amsterdam is of a younger age than the founding of other urban centers in the Low Countries. However, in and around the area of what later became Amsterdam, farmers settled as early as three millennia ago. They lived along the prehistoric IJ river and upstream of its tributary Amstel. The prehistoric IJ was a shallow and quiet stream in peatland behind beach ridges. This secluded area was able to grow into an important local settlement center, especially in the late Bronze Age, the Iron Age and the Roman Age. Neolithic and Roman artefacts have been found in the prehistoric Amstel bedding under Amsterdam's Damrak and Rokin, such as shards of Bell Beaker culture pottery (2200-2000 BC) and a granite grinding stone (2700-2750 BC). But the location of these artefacts around the river banks of the Amstel probably point to a presence of a modest semi-permanent or seasonal settlement of the previous mentioned local farmers. A permanent settlement would not have been possible, since the river mouth and the banks of the Amstel in this period in time were too wet for permanent habitation.
The origins of Amsterdam is linked to the development of the peatland called Amestelle, meaning 'watery area', from Aa(m) 'river' + stelle 'site at a shoreline', 'river bank'. In this area, land reclamation started as early as the late 10th century. Amestelle was located along a side arm of the IJ. This side arm took the name from the eponymous land: Amstel. Amestelle was inhabited by farmers, who lived more inland and more upstream, where the land was not as wet as at the banks of the downstream river mouth. These farmers were starting the reclamation around upstream Ouderkerk aan de Amstel, and later at the other side of the river at Amstelveen. The Van Amstel family, known in documents by this name since 1019, held the stewardship in this northwestern nook of the ecclesiastical district of the bishop of Utrecht. The family later served also under the count of Holland.
A major turning point in the development of the Amstel river mouth was the All Saint's Flood of 1170. In an extremely short period of time, the shallow river IJ turned into a wide estuary, which from then on offered the Amstel an open connection to the Zuiderzee, IJssel and waterways further afield. This made the water flow of the Amstel more active, so excess water could be drained better. With drier banks, the downstream Amstel mouth became attractive for permanent habitation. Moreover, the river had grown from an insignificant peat stream into a junction of international waterways. A settlement was built here immediately after the landscape change of 1170, and right from the start of its foundation it focused on traffic, production and trade; not on farming, as opposed to how communities had lived further upstream for the past 200 years and northward for thousands of years. The construction of a dam at the mouth of the Amstel, eponymously named Dam, is historically estimated to have occurred between 1264 and 1275. The settlement first appeared in a document concerning a road toll granted by the count of Holland Floris V to the residents apud Amestelledamme 'at the dam in the Amstel' or 'at the dam of Amstelland'. This allowed the inhabitants of the village to travel freely through the County of Holland, paying no tolls at bridges, locks and dams. By 1327, the name had developed into Aemsterdam.
Amsterdam was granted city rights in either 1300 or 1306. From the 14th century on, Amsterdam flourished, largely from trade with the Hanseatic League. In 1345, an alleged Eucharistic miracle in Kalverstraat rendered the city an important place of pilgrimage until the adoption of the Protestant faith. The Miracle devotion went underground but was kept alive. In the 19th century, especially after the jubilee of 1845, the devotion was revitalised and became an important national point of reference for Dutch Catholics. The Stille Omgang—a silent walk or procession in civil attire—is the expression of the pilgrimage within the Protestant Netherlands since the late 19th century. In the heyday of the Silent Walk, up to 90,000 pilgrims came to Amsterdam. In the 21st century, this has reduced to about 5,000.
In the 16th century, the Dutch rebelled against Philip II of Spain and his successors. The main reasons for the uprising were the imposition of new taxes, the tenth penny, and the religious persecution of Protestants by the newly introduced Inquisition. The revolt escalated into the Eighty Years' War, which ultimately led to Dutch independence. Strongly pushed by Dutch Revolt leader William the Silent, the Dutch Republic became known for its relative religious tolerance. Jews from the Iberian Peninsula, Huguenots from France, prosperous merchants and printers from Flanders, and economic and religious refugees from the Spanish-controlled parts of the Low Countries found safety in Amsterdam. The influx of Flemish printers and the city's intellectual tolerance made Amsterdam a centre for the European free press.
The 17th century is considered Amsterdam's Golden Age, during which it became the wealthiest city in the western world. Ships sailed from Amsterdam to the Baltic Sea, North America, and Africa, as well as present-day Indonesia, India, Sri Lanka, and Brazil, forming the basis of a worldwide trading network. Amsterdam's merchants had the largest share in both the Dutch East India Company and the Dutch West India Company. These companies acquired overseas possessions that later became Dutch colonies.
Amsterdam was Europe's most important point for the shipment of goods and was the leading financial centre of the western world. In 1602, the Amsterdam office of the international trading Dutch East India Company became the world's first stock exchange by trading in its own shares. The Bank of Amsterdam started operations in 1609, acting as a full-service bank for Dutch merchant bankers and as a reserve bank.
Amsterdam's prosperity declined during the 18th and early 19th centuries. The wars of the Dutch Republic with England and France took their toll on the city. During the Napoleonic Wars, Amsterdam's significance reached its lowest point, with Holland being absorbed into the French Empire. However, the later establishment of the United Kingdom of the Netherlands in 1815 marked a turning point.
The end of the 19th century is sometimes called Amsterdam's second Golden Age. New museums, a railway station, and the Concertgebouw were built; in this same time, the Industrial Revolution reached the city. The Amsterdam–Rhine Canal was dug to give Amsterdam a direct connection to the Rhine, and the North Sea Canal was dug to give the port a shorter connection to the North Sea. Both projects dramatically improved commerce with the rest of Europe and the world. In 1906, Joseph Conrad gave a brief description of Amsterdam as seen from the seaside, in The Mirror of the Sea.
Shortly before the First World War, the city started to expand again, and new suburbs were built. Even though the Netherlands remained neutral in this war, Amsterdam suffered a food shortage, and heating fuel became scarce. The shortages sparked riots in which several people were killed. These riots are known as the Aardappeloproer (Potato rebellion). People started looting stores and warehouses in order to get supplies, mainly food.
On 1 January 1921, after a flood in 1916, the depleted municipalities of Durgerdam, Holysloot, Zunderdorp and Schellingwoude, all lying north of Amsterdam, were, at their own request, annexed to the city. Between the wars, the city continued to expand, most notably to the west of the Jordaan district in the Frederik Hendrikbuurt and surrounding neighbourhoods.
Nazi Germany invaded the Netherlands on 10 May 1940 and took control of the country. Some Amsterdam citizens sheltered Jews, thereby exposing themselves and their families to a high risk of being imprisoned or sent to concentration camps. More than 100,000 Dutch Jews were deported to Nazi concentration camps, of whom some 60,000 lived in Amsterdam. In response, the Dutch Communist Party organized the February strike attended by 300,000 people to protest against the raids. Perhaps the most famous deportee was the young Jewish girl Anne Frank, who died in the Bergen-Belsen concentration camp. At the end of the Second World War, communication with the rest of the country broke down, and food and fuel became scarce. Many citizens traveled to the countryside to forage. Dogs, cats, raw sugar beets, and tulip bulbs—cooked to a pulp—were consumed to stay alive. Many trees in Amsterdam were cut down for fuel, and wood was taken from the houses, apartments and other buildings of deported Jews.
Many new suburbs, such as Osdorp, Slotervaart, Slotermeer and Geuzenveld, were built in the years after the Second World War.
These suburbs contained many public parks and wide-open spaces, and the new buildings provided improved housing conditions with larger and brighter rooms, gardens, and balconies. Because of the war and other events of the 20th century, almost the entire city centre had fallen into disrepair. As society was changing, politicians and other influential figures made plans to redesign large parts of it. There was an increasing demand for office buildings, and also for new roads, as the automobile became available to most people. A metro started operating in 1977 between the new suburb of Bijlmermeer in the city's Zuidoost (southeast) exclave and the centre of Amsterdam. Further plans were to build a new highway above the metro to connect Amsterdam Centraal and the city centre with other parts of the city.
The required large-scale demolitions began in Amsterdam's former Jewish neighborhood. Smaller streets, such as the Jodenbreestraat and Weesperstraat, were widened and almost all houses and buildings were demolished. At the peak of the demolition, the Nieuwmarktrellen (Nieuwmarkt Riots) broke out; the rioters expressed their fury about the demolition caused by the restructuring of the city.
As a result, the demolition was stopped and the highway into the city's centre was never fully built; only the metro was completed. Only a few streets remained widened. The new city hall was built on the almost completely demolished Waterlooplein. Meanwhile, large private organizations, such as Stadsherstel Amsterdam, were founded to restore the entire city centre. Although the success of this struggle is visible today, efforts for further restoration are still ongoing. The entire city centre has reattained its former splendour and, as a whole, is now a protected area. Many of its buildings have become monuments, and in July 2010 the Grachtengordel (the three concentric canals: Herengracht, Keizersgracht, and Prinsengracht) was added to the UNESCO World Heritage List.
In the 21st century, the Amsterdam city centre has attracted large numbers of tourists: between 2012 and 2015, the annual number of visitors rose from 10 to 17 million. Real estate prices have surged, and local shops are making way for tourist-oriented ones, making the centre unaffordable for the city's inhabitants. These developments have evoked comparisons with Venice, a city thought to be overwhelmed by the tourist influx.
Construction of a new metro line connecting the part of the city north of the IJ to its southern part was started in 2003. The project was controversial because its cost had exceeded its budget by a factor of three by 2008, because of fears of damage to buildings in the centre, and because construction had to be halted and restarted multiple times. The new metro line was completed in 2018.
Since 2014, renewed focus has been given to urban regeneration and renewal, especially in areas directly bordering the city centre, such as Frederik Hendrikbuurt. This urban renewal and expansion of the traditional centre of the city—with the construction on artificial islands of the new eastern IJburg neighbourhood—is part of the Structural Vision Amsterdam 2040 initiative.
Amsterdam is located in the Western Netherlands, in the province of North Holland, the capital of which is not Amsterdam, but rather Haarlem. The river Amstel ends in the city centre and connects to a large number of canals that eventually terminate in the IJ. Amsterdam is about 2 metres (6.6 feet) below sea level. The surrounding land is flat as it is formed of large polders. A man-made forest, Amsterdamse Bos, is in the southwest. Amsterdam is connected to the North Sea through the long North Sea Canal.
Amsterdam is intensely urbanised, as is the Amsterdam metropolitan area surrounding the city. Comprising 219.4 km2 (84.7 sq mi) of land, the city proper has 4,457 inhabitants per km2 and 2,275 houses per km2. Parks and nature reserves make up 12% of Amsterdam's land area.
Amsterdam has more than 100 km (60 mi) of canals, most of which are navigable by boat. The city's three main canals are the Prinsengracht, Herengracht and Keizersgracht.
In the Middle Ages, Amsterdam was surrounded by a moat, called the Singel, which now forms the innermost ring in the city, and gives the city centre a horseshoe shape. The city is also served by a seaport. It has been compared with Venice, due to its division into about 90 islands, which are linked by more than 1,200 bridges.
Amsterdam has an oceanic climate (Köppen Cfb) strongly influenced by its proximity to the North Sea to the west, with prevailing westerly winds.
Amsterdam, as well as most of the North Holland province, lies in USDA Hardiness zone 8b. Frosts mainly occur during spells of easterly or northeasterly winds from the inner European continent. Even then, because Amsterdam is surrounded on three sides by large bodies of water, as well as having a significant heat-island effect, nights rarely fall below −5 °C (23 °F), while it could easily be −12 °C (10 °F) in Hilversum, 25 km (16 mi) southeast.
Summers are moderately warm with a number of hot and humid days with occasional rain every month. The average daily high in August is 22.1 °C (72 °F), and 30 °C (86 °F) or higher is only measured on average on 2.5 days, placing Amsterdam in AHS Heat Zone 2. The record extremes range from −19.7 °C (−3.5 °F) to 36.3 °C (97.3 °F). 
Days with more than 1 mm (0.04 in) of precipitation are common, on average 133 days per year.
Amsterdam's average annual precipitation is 838 mm (33 in). A large part of this precipitation falls as light rain or brief showers. Cloudy and damp days are common during the cooler months of October through March.
In 1300, Amsterdam's population was around 1,000 people. While many towns in Holland experienced population decline during the 15th and 16th centuries, Amsterdam's population grew, mainly due to the rise of the profitable Baltic maritime trade after the Burgundian victory in the Dutch–Hanseatic War. Still, the population of Amsterdam was only modest compared to the towns and cities of Flanders and Brabant, which comprised the most urbanised area of the Low Countries.
This changed when, during the Dutch Revolt, many people from the Southern Netherlands fled to the North, especially after Antwerp fell to Spanish forces in 1585. Jewish people from Spain, Portugal and Eastern Europe similarly settled in Amsterdam, as did Germans and Scandinavians. In thirty years, Amsterdam's population more than doubled between 1585 and 1610. By 1600, its population was around 50,000. During the 1660s, Amsterdam's population reached 200,000. The city's growth levelled off and the population stabilised around 240,000 for most of the 18th century.
In 1750, Amsterdam was the fourth largest city in Western Europe, behind London (676,000), Paris (560,000) and Naples (324,000). This was all the more remarkable as Amsterdam was neither the capital city nor the seat of government of the Dutch Republic, which itself was a much smaller state than England, France or the Ottoman Empire. In contrast to those other metropolises, Amsterdam was also surrounded by large towns such as Leiden (about 67,000), Rotterdam (45,000), Haarlem (38,000) and Utrecht (30,000).
The city's population declined in the early 19th century, dipping under 200,000 in 1820. By the second half of the 19th century, industrialisation spurred renewed growth. Amsterdam's population hit an all-time high of 872,000 in 1959, before declining in the following decades due to government-sponsored suburbanisation to so-called groeikernen (growth centres) such as Purmerend and Almere. Between 1970 and 1980, Amsterdam experienced its sharp population decline, peaking at a net loss of 25,000 people in 1973. By 1985 the city had only 675,570 residents. This was soon followed by reurbanisation and gentrification, leading to renewed population growth in the 2010s. Also in the 2010s, much of Amsterdam's population growth was due to immigration to the city. Amsterdam's population failed to beat the expectations of 873,000 in 2019.
In the 16th and 17th century, non-Dutch immigrants to Amsterdam were mostly Huguenots, Flemings, Sephardi Jews and Westphalians. Huguenots came after the Edict of Fontainebleau in 1685, while the Flemish Protestants came during the Eighty Years' War. The Westphalians came to Amsterdam mostly for economic reasons – their influx continued through the 18th and 19th centuries. Before the Second World War, 10% of the city population was Jewish. Just twenty percent of them survived the Shoah.
The first mass immigration in the 20th century was by people from Indonesia, who came to Amsterdam after the independence of the Dutch East Indies in the 1940s and 1950s. In the 1960s guest workers from Turkey, Morocco, Italy, and Spain emigrated to Amsterdam. After the independence of Suriname in 1975, a large wave of Surinamese settled in Amsterdam, mostly in the Bijlmer area. Other immigrants, including refugees asylum seekers and illegal immigrants, came from Europe, America, Asia and Africa. In the 1970s and 1980s, many 'old' Amsterdammers moved to 'new' cities like Almere and Purmerend, prompted by the third planological bill of the Dutch Government. This bill promoted suburbanisation and arranged for new developments in so-called "groeikernen", literally cores of growth. Young professionals and artists moved into neighborhoods De Pijp and the Jordaan abandoned by these Amsterdammers. The non-Western immigrants settled mostly in the social housing projects in Amsterdam-West and the Bijlmer. Today, people of non-Western origin make up approximately one-fifth of the population of Amsterdam, and more than 30% of the city's children. Ethnic Dutch (as defined by the Dutch census) now make up a minority of the total population, although by far the largest one. Only one in three inhabitants under 15 is an autochthon, or a person who has two parents of Dutch origin. Segregation along ethnic lines is clearly visible, with people of non-Western origin, considered a separate group by Statistics Netherlands, concentrating in specific neighbourhoods especially in Nieuw-West, Zeeburg, Bijlmer and in certain areas of Amsterdam-Noord.
In 2000, Christians formed the largest religious group in the city (28% of the population). The next largest religion was Islam (8%), most of whose followers were Sunni. In 2015, Christians formed the largest religious group in the city (28% of the population). The next largest religion was Islam (7.1%), most of whose followers were Sunni.
Religion in Amsterdam (2015)
In 1578, the largely Catholic city of Amsterdam joined the revolt against Spanish rule, late in comparison to other major northern Dutch cities. Catholic priests were driven out of the city. Following the Dutch takeover, all churches were converted to Protestant worship. Calvinism was declared the main religion; although Catholicism was not forbidden and priests allowed to serve, the Catholic hierarchy was prohibited. This led to the establishment of schuilkerken, covert religious buildings that were hidden in pre-existing buildings. Catholics, some Jewish and dissenting Protestants worshiped in such buildings. A large influx of foreigners of many religions came to 17th-century Amsterdam, in particular Sefardic Jews from Spain and Portugal, Huguenots from France, Lutherans, Mennonites, as well as Protestants from across the Netherlands. This led to the establishment of many non-Dutch-speaking churches. In 1603, the Jewish received permission to practice their religion in the city. In 1639, the first synagogue was consecrated. The Jews came to call the town 'Jerusalem of the West'.
As they became established in the city, other Christian denominations used converted Catholic chapels to conduct their own services. The oldest English-language church congregation in the world outside the United Kingdom is found at the Begijnhof. Regular services there are still offered in English under the auspices of the Church of Scotland. Being Calvinists, the Huguenots soon integrated into the Dutch Reformed Church, though often retaining their own congregations. Some, commonly referred by the moniker 'Walloon', are recognizable today as they offer occasional services in French.
In the second half of the 17th century, Amsterdam experienced an influx of Ashkenazim, Jews from Central and Eastern Europe. Jews often fled the pogroms in those areas. The first Ashkenazis who arrived in Amsterdam were refugees from the Khmelnytsky Uprising occurring in Ukraine and the Thirty Years' War, which devastated much of Central Europe. They not only founded their own synagogues, but had a strong influence on the 'Amsterdam dialect' adding a large Yiddish local vocabulary. Despite an absence of an official Jewish ghetto, most Jews preferred to live in the eastern part, which used to be the center of medieval Amsterdam. The main street of this Jewish neighbourhood was Jodenbreestraat. The neighbourhood comprised the Waterlooplein and the Nieuwmarkt. Buildings in this neighbourhood fell into disrepair after the Second World War a large section of the neighbourhood was demolished during the construction of the metro system. This led to riots, and as a result the original plans for large-scale reconstruction were abandoned by the government. The neighbourhood was rebuilt with smaller-scale residence buildings on the basis of its original layout.
Catholic churches in Amsterdam have been constructed since the restoration of the episcopal hierarchy in 1853. One of the principal architects behind the city's Catholic churches, Cuypers, was also responsible for the Amsterdam Centraal station and the Rijksmuseum.
In 1924, the Catholic Church hosted the International Eucharistic Congress in Amsterdam; numerous Catholic prelates visited the city, where festivities were held in churches and stadiums. Catholic processions on the public streets, however, were still forbidden under law at the time. Only in the 20th century was Amsterdam's relation to Catholicism normalised, but despite its far larger population size, the episcopal see of the city was placed in the provincial town of Haarlem.
Historically, Amsterdam has been predominantly Christian, in 1900 Christians formed the largest religious group in the city (70% of the population), Dutch Reformed Church formed 45% of the city population, while the Catholic Church formed 25% of the city population.
In recent times, religious demographics in Amsterdam have been changed by immigration from former colonies. Hinduism has been introduced from the Hindu diaspora from Suriname and several distinct branches of Islam have been brought from various parts of the world. Islam is now the largest non-Christian religion in Amsterdam. The large community of Ghanaian immigrants have established African churches, often in parking garages in the Bijlmer area.
Amsterdam experienced an influx of religions and cultures after the Second World War. With 180 different nationalities, Amsterdam is home to one of the widest varieties of nationalities of any city in the world. The proportion of the population of immigrant origin in the city proper is about 50% and 88% of the population are Dutch citizens.
Amsterdam has been one of the municipalities in the Netherlands which provided immigrants with extensive and free Dutch-language courses, which have benefited many immigrants.
Amsterdam fans out south from the Amsterdam Centraal station and Damrak, the main street off the station. The oldest area of the town is known as De Wallen (English: "The Quays"). It lies to the east of Damrak and contains the city's famous red-light district. To the south of De Wallen is the old Jewish quarter of Waterlooplein.
The medieval and colonial age canals of Amsterdam, known as grachten, embraces the heart of the city where homes have interesting gables. Beyond the Grachtengordel are the former working-class areas of Jordaan and de Pijp. The Museumplein with the city's major museums, the Vondelpark, a 19th-century park named after the Dutch writer Joost van den Vondel, as well as the Plantage neighbourhood, with the zoo, are also located outside the Grachtengordel.
Several parts of the city and the surrounding urban area are polders. This can be recognised by the suffix -meer which means lake, as in Aalsmeer, Bijlmermeer, Haarlemmermeer and Watergraafsmeer.
The Amsterdam canal system is the result of conscious city planning. In the early 17th century, when immigration was at a peak, a comprehensive plan was developed that was based on four concentric half-circles of canals with their ends emerging at the IJ bay. Known as the Grachtengordel, three of the canals were mostly for residential development: the Herengracht (where "Heren" refers to Heren Regeerders van de stad Amsterdam, ruling lords of Amsterdam, whilst gracht means canal, so that the name can be roughly translated as "Canal of the Lords"), Keizersgracht (Emperor's Canal) and Prinsengracht (Prince's Canal). The fourth and outermost canal is the Singelgracht, which is often not mentioned on maps because it is a collective name for all canals in the outer ring. The Singelgracht should not be confused with the oldest and innermost canal, the Singel.
The canals served for defense, water management and transport. The defenses took the form of a moat and earthen dikes, with gates at transit points, but otherwise no masonry superstructures. The original plans have been lost, so historians, such as Ed Taverne, need to speculate on the original intentions: it is thought that the considerations of the layout were purely practical and defensive rather than ornamental.
Construction started in 1613 and proceeded from west to east, across the breadth of the layout, like a gigantic windshield wiper as the historian Geert Mak calls it – and not from the centre outwards, as a popular myth has it. The canal construction in the southern sector was completed by 1656. Subsequently, the construction of residential buildings proceeded slowly. The eastern part of the concentric canal plan, covering the area between the Amstel river and the IJ bay, has never been implemented. In the following centuries, the land was used for parks, senior citizens' homes, theatres, other public facilities, and waterways without much planning. Over the years, several canals have been filled in, becoming streets or squares, such as the Nieuwezijds Voorburgwal and the Spui.
After the development of Amsterdam's canals in the 17th century, the city did not grow beyond its borders for two centuries. During the 19th century, Samuel Sarphati devised a plan based on the grandeur of Paris and London at that time. The plan envisaged the construction of new houses, public buildings and streets just outside the Grachtengordel. The main aim of the plan, however, was to improve public health. Although the plan did not expand the city, it did produce some of the largest public buildings to date, like the Paleis voor Volksvlijt.
Following Sarphati, civil engineers Jacobus van Niftrik and Jan Kalff designed an entire ring of 19th-century neighbourhoods surrounding the city's centre, with the city preserving the ownership of all land outside the 17th-century limit, thus firmly controlling development. Most of these neighbourhoods became home to the working class.
In response to overcrowding, two plans were designed at the beginning of the 20th century which were very different from anything Amsterdam had ever seen before: Plan Zuid (designed by the architect Berlage) and West. These plans involved the development of new neighbourhoods consisting of housing blocks for all social classes.
After the Second World War, large new neighbourhoods were built in the western, southeastern, and northern parts of the city. These new neighbourhoods were built to relieve the city's shortage of living space and give people affordable houses with modern conveniences. The neighbourhoods consisted mainly of large housing blocks located among green spaces, connected to wide roads, making the neighbourhoods easily accessible by motor car. The western suburbs which were built in that period are collectively called the Westelijke Tuinsteden. The area to the southeast of the city built during the same period is known as the Bijlmer.
Amsterdam has a rich architectural history. The oldest building in Amsterdam is the Oude Kerk (English: Old Church), at the heart of the Wallen, consecrated in 1306. The oldest wooden building is Het Houten Huys at the Begijnhof. It was constructed around 1425 and is one of only two existing wooden buildings. It is also one of the few examples of Gothic architecture in Amsterdam. The oldest stone building of the Netherlands, The Moriaan is built in 's-Hertogenbosch.
In the 16th century, wooden buildings were razed and replaced with brick ones. During this period, many buildings were constructed in the architectural style of the Renaissance. Buildings of this period are very recognisable with their stepped gable façades, which is the common Dutch Renaissance style. Amsterdam quickly developed its own Renaissance architecture. These buildings were built according to the principles of the architect Hendrick de Keyser. One of the most striking buildings designed by Hendrick de Keyser is the Westerkerk. In the 17th century baroque architecture became very popular, as it was elsewhere in Europe. This roughly coincided with Amsterdam's Golden Age. The leading architects of this style in Amsterdam were Jacob van Campen, Philips Vingboons and Daniel Stalpaert.
Philip Vingboons designed splendid merchants' houses throughout the city. A famous building in baroque style in Amsterdam is the Royal Palace on Dam Square. Throughout the 18th century, Amsterdam was heavily influenced by French culture. This is reflected in the architecture of that period. Around 1815, architects broke with the baroque style and started building in different neo-styles. Most Gothic style buildings date from that era and are therefore said to be built in a neo-gothic style. At the end of the 19th century, the Jugendstil or Art Nouveau style became popular and many new buildings were constructed in this architectural style. Since Amsterdam expanded rapidly during this period, new buildings adjacent to the city centre were also built in this style. The houses in the vicinity of the Museum Square in Amsterdam Oud-Zuid are an example of Jugendstil. The last style that was popular in Amsterdam before the modern era was Art Deco. Amsterdam had its own version of the style, which was called the Amsterdamse School. Whole districts were built this style, such as the Rivierenbuurt. A notable feature of the façades of buildings designed in Amsterdamse School is that they are highly decorated and ornate, with oddly shaped windows and doors.
The old city centre is the focal point of all the architectural styles before the end of the 19th century.
Jugendstil and Georgian are mostly found outside the city's centre in the neighbourhoods built in the early
20th century, although there are also some striking examples of these styles in the city centre.
Most historic buildings in the city centre and nearby are houses, such as the famous merchants' houses lining the canals.
Amsterdam has many parks, open spaces, and squares throughout the city. The Vondelpark, the largest park in the city, is located in the Oud-Zuid neighbourhood and is named after the 17th-century Amsterdam author Joost van den Vondel. Yearly, the park has around 10 million visitors. In the park is an open-air theatre, a playground and several horeca facilities. In the Zuid borough, is the Beatrixpark, named after Queen Beatrix. Between Amsterdam and Amstelveen is the Amsterdamse Bos ("Amsterdam Forest"), the largest recreational area in Amsterdam. Annually, almost 4.5 million people visit the park, which has a size of 1.000 hectares and is approximately three times the size of Central Park. The Amstelpark in the Zuid borough houses the Rieker windmill, which dates to 1636. Other parks include the Sarphatipark in the De Pijp neighbourhood, the Oosterpark in the Oost borough and the Westerpark in the Westerpark neighbourhood. The city has three beaches: Nemo Beach, Citybeach "Het stenen hoofd" (Silodam) and Blijburg, all located in the Centrum borough.
The city has many open squares (plein in Dutch). The namesake of the city as the site of the original dam, Dam Square, is the main city square and has the Royal Palace and National Monument. Museumplein hosts various museums, including the Rijksmuseum, Van Gogh Museum, and Stedelijk Museum. Other squares include Rembrandtplein, Muntplein, Nieuwmarkt, Leidseplein, Spui and Waterlooplein. Also, near to Amsterdam is the Nekkeveld estate conservation project.
Amsterdam is the financial and business capital of the Netherlands.
According to the 2007 European Cities Monitor (ECM) – an annual location survey of Europe's leading companies carried out by global real estate consultant Cushman &amp; Wakefield – Amsterdam is one of the top European cities in which to locate an international business, ranking fifth in the survey. with the survey determining London, Paris, Frankfurt and Barcelona as the four European cities surpassing Amsterdam in this regard.
A substantial number of large corporations and banks' headquarters are located in the Amsterdam area, including: AkzoNobel, Heineken International, ING Group, ABN AMRO, TomTom, Delta Lloyd Group, Booking.com and Philips. 
Although many small offices remain along the historic canals, centrally based companies have increasingly relocated outside Amsterdam's city centre. Consequently, the Zuidas (English: South Axis) has become the new financial and legal hub of Amsterdam, with the country's five largest law firms and several subsidiaries of large consulting firms, such as Boston Consulting Group and Accenture, as well as the World Trade Centre (Amsterdam) located in the Zuidas district. In addition to the Zuidas, there are three smaller financial districts in Amsterdam:
The adjoining municipality of Amstelveen is the location of KPMG International's global headquarters. Other non-Dutch companies have chosen to settle in communities surrounding Amsterdam since they allow freehold property ownership, whereas Amsterdam retains ground rent.
The Amsterdam Stock Exchange (AEX), now part of Euronext, is the world's oldest stock exchange and, due to Brexit, has overtaken LSE as the largest bourse in Europe. It is near Dam Square in the city centre.
The Port of Amsterdam is the fourth-largest port in Europe, the 38th largest port in the world and the second-largest port in the Netherlands by metric tons of cargo. In 2014, the Port of Amsterdam had a cargo throughput of 97,4 million tons of cargo, which was mostly bulk cargo.
Amsterdam has the biggest cruise port in the Netherlands with more than 150 cruise ships every year.
In 2019, the new lock in IJmuiden opened; since then, the port has been able to grow to 125 million tonnes in capacity.
Amsterdam is one of the most popular tourist destinations in Europe, receiving more than 5.34 million international visitors annually, this is excluding the 16 million day-trippers visiting the city every year. The number of visitors has been growing steadily over the past decade. This can be attributed to an increasing number of European visitors. Two-thirds of the hotels are located in the city's centre. Hotels with 4 or 5 stars contribute 42% of the total beds available and 41% of the overnight stays in Amsterdam. The room occupation rate was 85% in 2017, up from 78% in 2006. The majority of tourists (74%) originate from Europe. The largest group of non-European visitors come from the United States, accounting for 14% of the total. Certain years have a theme in Amsterdam to attract extra tourists. For example, the year 2006 was designated "Rembrandt 400", to celebrate the 400th birthday of Rembrandt van Rijn. Some hotels offer special arrangements or activities during these years. The average number of guests per year staying at the four campsites around the city range from 12,000 to 65,000.

De Wallen, also known as Walletjes or Rosse Buurt, is a designated area for legalised prostitution and is Amsterdam's largest and best-known red-light district. This neighbourhood has become a famous attraction for tourists. It consists of a network of canals, streets, and alleys containing several hundred small, one-room apartments rented by sex workers who offer their services from behind a window or glass door, typically illuminated with red lights. In recent years, the city government has been closing and repurposing the famous red-light district windows in an effort to clean up the area and reduce the amount of party and sex tourism.
Shops in Amsterdam range from large high-end department stores such as De Bijenkorf founded in 1870 to small speciality shops. Amsterdam's high-end shops are found in the streets P.C. Hooftstraat and Cornelis Schuytstraat, which are located in the vicinity of the Vondelpark. One of Amsterdam's busiest high streets is the narrow, medieval Kalverstraat in the heart of the city. Other shopping areas include the Negen Straatjes and Haarlemmerdijk and Haarlemmerstraat. Negen Straatjes are nine narrow streets within the Grachtengordel, the concentric canal system of Amsterdam. The Negen Straatjes differ from other shopping districts with the presence of a large diversity of privately owned shops. The Haarlemmerstraat and Haarlemmerdijk were voted best shopping street in the Netherlands in 2011. These streets have as the Negen Straatjes a large diversity of privately owned shops. However, as the Negen Straatjes are dominated by fashion stores, the Haarlemmerstraat and Haarlemmerdijk offer a wide variety of stores, just to name some specialities: candy and other food-related stores, lingerie, sneakers, wedding clothing, interior shops, books, Italian deli's, racing and mountain bikes, skatewear, etc.
The city also features a large number of open-air markets such as the Albert Cuyp Market, Westerstraat-markt, Ten Katemarkt, and Dappermarkt. Some of these markets are held daily, like the Albert Cuypmarkt and the Dappermarkt. Others, like the Westerstraatmarkt, are held every week.
Several fashion brands and designers are based in Amsterdam. Fashion designers include Iris van Herpen, Mart Visser, Viktor &amp; Rolf, Marlies Dekkers and Frans Molenaar. Fashion models like Yfke Sturm, Doutzen Kroes and Kim Noorda started their careers in Amsterdam. Amsterdam has its garment centre in the World Fashion Center. Fashion photographers Inez van Lamsweerde and Vinoodh Matadin were born in Amsterdam.
During the later part of the 16th century, Amsterdam's Rederijkerskamer (Chamber of rhetoric) organised contests between different Chambers in the reading of poetry and drama. In 1637, Schouwburg, the first theatre in Amsterdam was built, opening on 3 January 1638. The first ballet performances in the Netherlands were given in Schouwburg in 1642 with the Ballet of the Five Senses. In the 18th century, French theatre became popular. While Amsterdam was under the influence of German music in the 19th century there were few national opera productions; the Hollandse Opera of Amsterdam was built in 1888 for the specific purpose of promoting Dutch opera. In the 19th century, popular culture was centred on the Nes area in Amsterdam (mainly vaudeville and music-hall). An improved metronome was invented in 1812 by Dietrich Nikolaus Winkel. The Rijksmuseum (1885) and Stedelijk Museum (1895) were built and opened. In 1888, the Concertgebouworkest orchestra was established. With the 20th century came cinema, radio and television. Though most studios are located in Hilversum and Aalsmeer, Amsterdam's influence on programming is very strong. Many people who work in the television industry live in Amsterdam. Also, the headquarters of the Dutch SBS Broadcasting Group is located in Amsterdam.
The most important museums of Amsterdam are located on the Museumplein (Museum Square), located at the southwestern side of the Rijksmuseum. It was created in the last quarter of the 19th century on the grounds of the former World's fair. The northeastern part of the square is bordered by the large Rijksmuseum. In front of the Rijksmuseum on the square itself is a long, rectangular pond. This is transformed into an ice rink in winter. The northwestern part of the square is bordered by the Van Gogh Museum, House of Bols Cocktail &amp; Genever Experience and Coster Diamonds. The southwestern border of the Museum Square is the Van Baerlestraat, which is a major thoroughfare in this part of Amsterdam. The Concertgebouw is located across this street from the square. To the southeast of the square are several large houses, one of which contains the American consulate. A parking garage can be found underneath the square, as well as a supermarket. The Museumplein is covered almost entirely with a lawn, except for the northeastern part of the square which is covered with gravel. The current appearance of the square was realised in 1999, when the square was remodelled. The square itself is the most prominent site in Amsterdam for festivals and outdoor concerts, especially in the summer. Plans were made in 2008 to remodel the square again because many inhabitants of Amsterdam are not happy with its current appearance.
The Rijksmuseum possesses the largest and most important collection of classical Dutch art.
It opened in 1885. Its collection consists of nearly one million objects. The artist most associated with Amsterdam is Rembrandt, whose work, and the work of his pupils, is displayed in the Rijksmuseum. Rembrandt's masterpiece The Night Watch is one of the top pieces of art of the museum. It also houses paintings from artists like Bartholomeus van der Helst, Johannes Vermeer, Frans Hals, Ferdinand Bol, Albert Cuyp, Jacob van Ruisdael and Paulus Potter. Aside from paintings, the collection consists of a large variety of decorative art. This ranges from Delftware to giant doll-houses from the 17th century. The architect of the gothic revival building was P.J.H. Cuypers. The museum underwent a 10-year, 375 million euro renovation starting in 2003. The full collection was reopened to the public on 13 April 2013 and the Rijksmuseum has remained the most visited museum in Amsterdam with 2.2 million visitors in 2016 and 2.16 million in 2017.
Van Gogh lived in Amsterdam for a short while and there is a museum dedicated to his work. The museum is housed in one of the few modern buildings in this area of Amsterdam. The building was designed by Gerrit Rietveld. This building is where the permanent collection is displayed. A new building was added to the museum in 1999. This building, known as the performance wing, was designed by Japanese architect Kisho Kurokawa. Its purpose is to house temporary exhibitions of the museum. Some of Van Gogh's most famous paintings, like The Potato Eaters and Sunflowers, are in the collection. The Van Gogh museum is the second most visited museum in Amsterdam, not far behind the Rijksmuseum in terms of the number of visits, being approximately 2.1 million in 2016, for example.
Next to the Van Gogh museum stands the Stedelijk Museum. This is Amsterdam's most important museum of modern art. The museum is as old as the square it borders and was opened in 1895. The permanent collection consists of works of art from artists like Piet Mondrian, Karel Appel, and Kazimir Malevich. After renovations lasting several years, the museum opened in September 2012 with a new composite extension that has been called 'The Bathtu due to its resemblance to one.
Amsterdam contains many other museums throughout the city. They range from small museums such as the Verzetsmuseum (Resistance Museum), the Anne Frank House, and the Rembrandt House Museum, to the very large, like the Tropenmuseum (Museum of the Tropics), Amsterdam Museum (formerly known as Amsterdam Historical Museum), Hermitage Amsterdam (a dependency of the Hermitage Museum in Saint Petersburg) and the Joods Historisch Museum (Jewish Historical Museum). The modern-styled Nemo is dedicated to child-friendly science exhibitions.
Amsterdam's musical culture includes a large collection of songs that treat the city nostalgically and lovingly. The 1949 song "Aan de Amsterdamse grachten" ("On the canals of Amsterdam") was performed and recorded by many artists, including John Kraaijkamp Sr.; the best-known version is probably that by Wim Sonneveld (1962). In the 1950s Johnny Jordaan rose to fame with "Geef mij maar Amsterdam" ("I prefer Amsterdam"), which praises the city above all others (explicitly Paris); Jordaan sang especially about his own neighbourhood, the Jordaan ("Bij ons in de Jordaan"). Colleagues and contemporaries of Johnny include Tante Leen and Manke Nelis. Another notable Amsterdam song is "Amsterdam" by Jacques Brel (1964). A 2011 poll by Amsterdam newspaper Het Parool that Trio Bier's "Oude Wolf" was voted "Amsterdams lijflied". Notable Amsterdam bands from the modern era include the Osdorp Posse and The Ex.
AFAS Live (formerly known as the Heineken Music Hall) is a concert hall located near the Johan Cruyff Arena (known as the Amsterdam Arena until 2018). Its main purpose is to serve as a podium for pop concerts for big audiences. Many famous international artists have performed there. Two other notable venues, Paradiso and the Melkweg are located near the Leidseplein. Both focus on broad programming, ranging from indie rock to hip hop, R&amp;B, and other popular genres. Other more subcultural music venues are OCCII, OT301, De Nieuwe Anita, Winston Kingdom, and Zaal 100. Jazz has a strong following in Amsterdam, with the Bimhuis being the premier venue. In 2012, Ziggo Dome was opened, also near Amsterdam Arena, a state-of-the-art indoor music arena.
AFAS Live is also host to many electronic dance music festivals, alongside many other venues. Armin van Buuren and Tiesto, some of the worlds leading Trance DJ's hail from the Netherlands and frequently perform in Amsterdam. Each year in October, the city hosts the Amsterdam Dance Event (ADE) which is one of the leading electronic music conferences and one of the biggest club festivals for electronic music in the world, attracting over 350,000 visitors each year. Another popular dance festival is 5daysoff, which takes place in the venues Paradiso and Melkweg. In the summertime, there are several big outdoor dance parties in or nearby Amsterdam, such as Awakenings, Dance Valley, Mystery Land, Loveland, A Day at the Park, Welcome to the Future, and Valtifest.
Amsterdam has a world-class symphony orchestra, the Royal Concertgebouw Orchestra. Their home is the Concertgebouw, which is across the Van Baerlestraat from the Museum Square. It is considered by critics to be a concert hall with some of the best acoustics in the world. The building contains three halls, Grote Zaal, Kleine Zaal, and Spiegelzaal. Some nine hundred concerts and other events per year take place in the Concertgebouw, for a public of over 700,000, making it one of the most-visited concert halls in the world. The opera house of Amsterdam is located adjacent to the city hall. Therefore, the two buildings combined are often called the Stopera, (a word originally coined by protesters against it very construction: Stop the Opera). This huge modern complex, opened in 1986, lies in the former Jewish neighbourhood at Waterlooplein next to the river Amstel. The Stopera is the home base of Dutch National Opera, Dutch National Ballet and the Holland Symfonia. Muziekgebouw aan 't IJ is a concert hall, which is located in the IJ near the central station. Its concerts perform mostly modern classical music. Located adjacent to it, is the Bimhuis, a concert hall for improvised and Jazz music.
Amsterdam has three main theatre buildings.
The Stadsschouwburg at the Leidseplein is the home base of Toneelgroep Amsterdam. The current building dates from 1894. Most plays are performed in the Grote Zaal (Great Hall). The normal program of events encompasses all sorts of theatrical forms. In 2009, the new hall of the Stadsschouwburg Amsterdam, Toneelgroep Amsterdam and Melkweg opened, and the renovation of the front end of the theatre was ready.
The Dutch National Opera and Ballet (formerly known as Het Muziektheater), dating from 1986, is the principal opera house and home to Dutch National Opera and Dutch National Ballet. Royal Theatre Carré was built as a permanent circus theatre in 1887 and is currently mainly used for musicals, cabaret performances, and pop concerts.
The recently re-opened DeLaMar Theater houses more commercial plays and musicals. A new theatre has also moved into the Amsterdam scene in 2014, joining other established venues: Theater Amsterdam is located in the west part of Amsterdam, on the Danzigerkade. It is housed in a modern building with a panoramic view over the harbour. The theatre is the first-ever purpose-built venue to showcase a single play entitled ANNE, the play based on Anne Frank's life.
On the east side of town, there is a small theatre in a converted bathhouse, the Badhuistheater. The theatre often has English programming.
The Netherlands has a tradition of cabaret or kleinkunst, which combines music, storytelling, commentary, theatre and comedy. Cabaret dates back to the 1930s and artists like Wim Kan, Wim Sonneveld and Toon Hermans were pioneers of this form of art in the Netherlands. In Amsterdam is the Kleinkunstacademie (English: Cabaret Academy) and Nederlied Kleinkunstkoor (English: Cabaret Choir). Contemporary popular artists are Youp van 't Hek, Freek de Jonge, Herman Finkers, Hans Teeuwen, Theo Maassen, Herman van Veen, Najib Amhali, Raoul Heertje, Jörgen Raymann, Brigitte Kaandorp and Comedytrain. The English spoken comedy scene was established with the founding of Boom Chicago in 1993. They have their own theatre at Leidseplein.
Amsterdam is famous for its vibrant and diverse nightlife. Amsterdam has many cafés (bars). They range from large and modern to small and cosy. The typical Bruine Kroeg (brown café) breathe a more old fashioned atmosphere with dimmed lights, candles, and somewhat older clientele. These brown cafés mostly offer a wide range of local and international artisanal beers. Most cafés have terraces in summertime. A common sight on the Leidseplein during summer is a square full of terraces packed with people drinking beer or wine. Many restaurants can be found in Amsterdam as well. Since Amsterdam is a multicultural city, a lot of different ethnic restaurants can be found. Restaurants range from being rather luxurious and expensive to being ordinary and affordable. Amsterdam also possesses many discothèques. The two main nightlife areas for tourists are the Leidseplein and the Rembrandtplein. The Paradiso, Melkweg and Sugar Factory are cultural centres, which turn into discothèques on some nights. Examples of discothèques near the Rembrandtplein are the Escape, Air, John Doe and Club Abe. Also noteworthy are Panama, Hotel Arena (East), TrouwAmsterdam and Studio 80. In recent years '24-hour' clubs opened their doors, most notably Radion De School, Shelter and Marktkantine. Bimhuis located near the Central Station, with its rich programming hosting the best in the field is considered one of the best jazz clubs in the world. The Reguliersdwarsstraat is the main street for the LGBT community and nightlife.
In 2008, there were 140 festivals and events in Amsterdam. During the same year, Amsterdam was designated as the World Book Capital for one year by UNESCO.
Famous festivals and events in Amsterdam include: Koningsdag (which was named Koninginnedag until the crowning of King Willem-Alexander in 2013) (King's Day – Queen's Day); the Holland Festival for the performing arts; the yearly Prinsengrachtconcert (classical concerto on the Prinsen canal) in August; the 'Stille Omgang' (a silent Roman Catholic evening procession held every March); Amsterdam Gay Pride; The Cannabis Cup; and the Uitmarkt. On Koningsdag—that is held each year on 27 April—hundreds of thousands of people travel to Amsterdam to celebrate with the city's residents. The entire city becomes overcrowded with people buying products from the freemarket, or visiting one of the many music concerts.
The yearly Holland Festival attracts international artists and visitors from all over Europe. Amsterdam Gay Pride is a yearly local LGBT parade of boats in Amsterdam's canals, held on the first Saturday in August. The annual Uitmarkt is a three-day cultural event at the start of the cultural season in late August. It offers previews of many different artists, such as musicians and poets, who perform on podia.
Amsterdam is home of the Eredivisie football club AFC Ajax. The stadium Johan Cruyff Arena is the home of Ajax. It is located in the south-east of the city next to the new Amsterdam Bijlmer ArenA railway station. Before moving to their current location in 1996, Ajax played their regular matches in the now demolished De Meer Stadion in the eastern part of the city or in the Olympic Stadium.
In 1928, Amsterdam hosted the Summer Olympics. The Olympic Stadium built for the occasion has been completely restored and is now used for cultural and sporting events, such as the Amsterdam Marathon. In 1920, Amsterdam assisted in hosting some of the sailing events for the Summer Olympics held in neighbouring Antwerp, Belgium by hosting events at Buiten IJ.
The city holds the Dam to Dam Run, a 16-kilometre (10 mi) race from Amsterdam to Zaandam, as well as the Amsterdam Marathon. The ice hockey team Amstel Tijgers play in the Jaap Eden ice rink. The team competes in the Dutch ice hockey premier league. Speed skating championships have been held on the 400-meter lane of this ice rink.
Amsterdam holds two American football franchises: the Amsterdam Crusaders and the Amsterdam Panthers. The Amsterdam Pirates baseball team competes in the Dutch Major League. There are three field hockey teams: Amsterdam, Pinoké and Hurley, who play their matches around the Wagener Stadium in the nearby city of Amstelveen. The basketball team MyGuide Amsterdam competes in the Dutch premier division and play their games in the Sporthallen Zuid.
There is one rugby club in Amsterdam, which also hosts sports training classes such as RTC (Rugby Talenten Centrum or Rugby Talent Centre) and the National Rugby stadium.
Since 1999, the city of Amsterdam honours the best sportsmen and women at the Amsterdam Sports Awards. Boxer Raymond Joval and field hockey midfielder Carole Thate were the first to receive the awards, in 1999.
Amsterdam hosted the World Gymnaestrada in 1991 and will do so again in 2023.
The city of Amsterdam is a municipality under the Dutch Municipalities Act. It is governed by a directly elected municipal council, a municipal executive board and a mayor. Since 1981, the municipality of Amsterdam has gradually been divided into semi-autonomous boroughs, called stadsdelen or 'districts'. Over time, a total of 15 boroughs were created. In May 2010, under a major reform, the number of Amsterdam boroughs was reduced to eight: Amsterdam-Centrum covering the city centre including the canal belt, Amsterdam-Noord consisting of the neighbourhoods north of the IJ lake, Amsterdam-Oost in the east, Amsterdam-Zuid in the south, Amsterdam-West in the west, Amsterdam Nieuw-West in the far west, Amsterdam Zuidoost in the southeast, and Westpoort covering the Port of Amsterdam area.
As with all Dutch municipalities, Amsterdam is governed by a directly elected municipal council, a municipal executive board and a government appointed mayor (burgemeester). The mayor is a member of the municipal executive board, but also has individual responsibilities in maintaining public order. On 27 June 2018, Femke Halsema (former member of House of Representatives for GroenLinks from 1998 to 2011) was appointed as the first woman to be Mayor of Amsterdam by the King's Commissioner of North Holland for a six-year term after being nominated by the Amsterdam municipal council and began serving a six-year term on 12 July 2018. She replaces Eberhard van der Laan (Labour Party) who was the Mayor of Amsterdam from 2010 until his death in October 2017. After the 2014 municipal council elections, a governing majority of D66, VVD and SP was formed – the first coalition without the Labour Party since World War II. Next to the Mayor, the municipal executive board consists of eight wethouders ('alderpersons') appointed by the municipal council: four D66 alderpersons, two VVD alderpersons and two SP alderpersons.
On 18 September 2017, it was announced by Eberhard van der Laan in an open letter to Amsterdam citizens that Kajsa Ollongren would take up his office as acting Mayor of Amsterdam with immediate effect due to ill health. Ollongren was succeeded as acting Mayor by Eric van der Burg on 26 October 2017 and by Jozias van Aartsen on 4 December 2017.
Unlike most other Dutch municipalities, Amsterdam is subdivided into eight boroughs, called stadsdelen or 'districts', and the urban area of Weesp, a system that was implemented gradually in the 1980s to improve local governance. The boroughs are responsible for many activities that had previously been run by the central city. In 2010, the number of Amsterdam boroughs reached fifteen. Fourteen of those had their own district council (deelraad), elected by a popular vote. The fifteenth, Westpoort, covers the harbour of Amsterdam and had very few residents. Therefore, it was governed by the central municipal council.
Under the borough system, municipal decisions are made at borough level, except for those affairs pertaining to the whole city such as major infrastructure projects, which are the jurisdiction of the central municipal authorities. In 2010, the borough system was restructured, in which many smaller boroughs merged into larger boroughs. In 2014, under a reform of the Dutch Municipalities Act, the Amsterdam boroughs lost much of their autonomous status, as their district councils were abolished.
The municipal council of Amsterdam voted to maintain the borough system by replacing the district councils with smaller, but still directly elected district committees (bestuurscommissies). Under a municipal ordinance, the new district committees were granted responsibilities through delegation of regulatory and executive powers by the central municipal council.
"Amsterdam" is usually understood to refer to the municipality of Amsterdam. Colloquially, some areas within the municipality, such as the town of Durgerdam, may not be considered part of Amsterdam.
Statistics Netherlands uses three other definitions of Amsterdam: metropolitan agglomeration Amsterdam (Grootstedelijke Agglomeratie Amsterdam, not to be confused with Grootstedelijk Gebied Amsterdam, a synonym of Groot Amsterdam), Greater Amsterdam (Groot Amsterdam, a COROP region) and the urban region Amsterdam (Stadsgewest Amsterdam). The Amsterdam Department for Research and Statistics uses a fourth conurbation, namely the Stadsregio Amsterdam ('City Region of Amsterdam'). The city region is similar to Greater Amsterdam but includes the municipalities of Zaanstad and Wormerland. It excludes Graft-De Rijp.
The smallest of these areas is the municipality of Amsterdam with a population of 802,938 in 2013. The conurbation had a population of 1,096,042 in 2013. It includes the municipalities of Zaanstad, Wormerland, Oostzaan, Diemen and Amstelveen only, as well as the municipality of Amsterdam. Greater Amsterdam includes 15 municipalities, and had a population of 1,293,208 in 2013. Though much larger in area, the population of this area is only slightly larger, because the definition excludes the relatively populous municipality of Zaanstad. The largest area by population, the Amsterdam Metropolitan Area (Dutch: Metropoolregio Amsterdam), has a population of 2,33 million. It includes for instance Zaanstad, Wormerland, Muiden, Abcoude, Haarlem, Almere and Lelystad but excludes Graft-De Rijp. Amsterdam is part of the conglomerate metropolitan area Randstad, with a total population of 6,659,300 inhabitants.
Of these various metropolitan area configurations, only the Stadsregio Amsterdam (City Region of Amsterdam) has a formal governmental status. Its responsibilities include regional spatial planning and the metropolitan public transport concessions.
Under the Dutch Constitution, Amsterdam is the capital of the Netherlands. Since the 1983 constitutional revision, the constitution mentions "Amsterdam" and "capital" in chapter 2, article 32: The king's confirmation by oath and his coronation take place in "the capital Amsterdam" ("de hoofdstad Amsterdam"). Previous versions of the constitution only mentioned "the city of Amsterdam" ("de stad Amsterdam"). For a royal investiture, therefore, the States General of the Netherlands (the Dutch Parliament) meets for a ceremonial joint session in Amsterdam. The ceremony traditionally takes place at the Nieuwe Kerk on Dam Square, immediately after the former monarch has signed the act of abdication at the nearby Royal Palace of Amsterdam. Normally, however, the Parliament sits in The Hague, the city which has historically been the seat of the Dutch government, the Dutch monarchy, and the Dutch supreme court. Foreign embassies are also located in The Hague.
The coat of arms of Amsterdam is composed of several historical elements. First and centre are three St Andrew's crosses, aligned in a vertical band on the city's shield (although Amsterdam's patron saint was Saint Nicholas). These St Andrew's crosses can also be found on the city shields of neighbours Amstelveen and Ouder-Amstel. This part of the coat of arms is the basis of the flag of Amsterdam, flown by the city government, but also as civil ensign for ships registered in Amsterdam. Second is the Imperial Crown of Austria. In 1489, out of gratitude for services and loans, Maximilian I awarded Amsterdam the right to adorn its coat of arms with the king's crown. Then, in 1508, this was replaced with Maximilian's imperial crown when he was crowned Holy Roman Emperor. In the early years of the 17th century, Maximilian's crown in Amsterdam's coat of arms was again replaced, this time with the crown of Emperor Rudolph II, a crown that became the Imperial Crown of Austria. The lions date from the late 16th century, when city and province became part of the Republic of the Seven United Netherlands. Last came the city's official motto: Heldhaftig, Vastberaden, Barmhartig ("Heroic, Determined, Merciful"), bestowed on the city in 1947 by Queen Wilhelmina, in recognition of the city's bravery during the Second World War.
Currently, there are sixteen tram routes and five metro routes. All are operated by municipal public transport operator Gemeentelijk Vervoerbedrijf (GVB), which also runs the city bus network.
Four fare-free GVB ferries carry pedestrians and cyclists across the IJ lake to the borough of Amsterdam-Noord, and two fare-charging ferries run east and west along the harbour. There are also privately operated water taxis, a water bus, a boat sharing operation, electric rental boats and canal cruises, that transport people along Amsterdam's waterways.
Regional buses, and some suburban buses, are operated by Connexxion and EBS. International coach services are provided by Eurolines from Amsterdam Amstel railway station, IDBUS from Amsterdam Sloterdijk railway station, and Megabus from the Zuiderzeeweg in the east of the city.
In order to facilitate easier transport to the centre of Amsterdam, the city has various P+R Locations where people can park their car at an affordable price and transfer to one of the numerous public transport lines.
Amsterdam was intended in 1932 to be the hub, a kind of Kilometre Zero, of the highway system of the Netherlands, with freeways numbered One to Eight planned to originate from the city. The outbreak of the Second World War and shifting priorities led to the current situation, where only roads A1, A2, and A4 originate from Amsterdam according to the original plan. The A3 to Rotterdam was cancelled in 1970 in order to conserve the Groene Hart. Road A8, leading north to Zaandam and the A10 Ringroad were opened between 1968 and 1974. Besides the A1, A2, A4 and A8, several freeways, such as the A7 and A6, carry traffic mainly bound for Amsterdam.
The A10 ringroad surrounding the city connects Amsterdam with the Dutch national network of freeways. Interchanges on the A10 allow cars to enter the city by transferring to one of the 18 city roads, numbered S101 through to S118. These city roads are regional roads without grade separation, and sometimes without a central reservation. Most are accessible by cyclists. The S100 Centrumring is a smaller ringroad circumnavigating the city's centre.
In the city centre, driving a car is discouraged. Parking fees are expensive, and many streets are closed to cars or are one-way. The local government sponsors carsharing and carpooling initiatives such as Autodelen and Meerijden.nu. The local government has also started removing parking spaces in the city, with the goal of removing 10,000 spaces (roughly 1,500 per year) by 2025
Amsterdam is served by ten stations of the Nederlandse Spoorwegen (Dutch Railways). Five are intercity stops: Sloterdijk, Zuid, Amstel, Bijlmer ArenA and Amsterdam Centraal. The stations for local services are: Lelylaan, RAI, Holendrecht, Muiderpoort and Science Park. Amsterdam Centraal is also an international railway station. From the station there are regular services to destinations such as Austria, Belarus, Belgium, Czechia, Denmark, France, Germany, Hungary, Poland, Russia, Switzerland and the United Kingdom. Among these trains are international trains of the Nederlandse Spoorwegen (Amsterdam-Berlin), the Eurostar (Amsterdam-Brussels-London), Thalys (Amsterdam-Brussels-Paris/Lille), and Intercity-Express (Amsterdam–Cologne–Frankfurt).
Amsterdam Airport Schiphol is less than 20 minutes by train from Amsterdam Centraal station and is served by domestic and international intercity trains, such as Thalys, Eurostar and Intercity Brussel. Schiphol is the largest airport in the Netherlands, the third-largest in Europe, and the 14th-largest in the world in terms of passengers. It handles over 68 million passengers per year and is the home base of four airlines, KLM, Transavia, Martinair and Arkefly. As of 2014, Schiphol was the fifth busiest airport in the world measured by international passenger numbers. This airport is 4 meters below sea level. Although Schiphol is internationally known as Amsterdam Schiphol Airport it actually lies in the neighbouring municipality of Haarlemmermeer, southwest of the city.
Amsterdam is one of the most bicycle-friendly large cities in the world and is a centre of bicycle culture with good facilities for cyclists such as bike paths and bike racks, and several guarded bike storage garages (fietsenstalling) which can be used.
According to the most recent figures published by Central Bureau of Statistics (CBS), in 2015 the 442.693 households (850.000 residents) in Amsterdam together owned 847.000 bicycles – 1.91 bicycle per household. Theft is widespread—in 2011, about 83,000 bicycles were stolen in Amsterdam. Bicycles are used by all socio-economic groups because of their convenience, Amsterdam's small size, the 400 kilometres (249 miles) of bike paths, the flat terrain, and the inconvenience of driving an automobile.
Amsterdam has two universities: the University of Amsterdam (Universiteit van Amsterdam, UvA), and the Vrije Universiteit Amsterdam (VU). Other institutions for higher education include an art school – Gerrit Rietveld Academie, a university of applied sciences – the Hogeschool van Amsterdam, and the Amsterdamse Hogeschool voor de Kunsten. Amsterdam's International Institute of Social History is one of the world's largest documentary and research institutions concerning social history, and especially the history of the labour movement. Amsterdam's Hortus Botanicus, founded in the early 17th century, is one of the oldest botanical gardens in the world, with many old and rare specimens, among them the coffee plant that served as the parent for the entire coffee culture in Central and South America.
There are over 200 primary schools in Amsterdam. Some of these primary schools base their teachings on particular pedagogic theories like the various Montessori schools. The biggest Montessori high school in Amsterdam is the Montessori Lyceum Amsterdam. Many schools, however, are based on religion. This used to be primarily Roman Catholicism and various Protestant denominations, but with the influx of Muslim immigrants, there has been a rise in the number of Islamic schools. Jewish schools can be found in the southern suburbs of Amsterdam.
Amsterdam is noted for having five independent grammar schools (Dutch: gymnasia), the Vossius Gymnasium, Barlaeus Gymnasium, St. Ignatius Gymnasium, Het 4e Gymnasium and the Cygnus Gymnasium where a classical curriculum including Latin and classical Greek is taught. Though believed until recently by many to be an anachronistic and elitist concept that would soon die out, the gymnasia have recently experienced a revival, leading to the formation of a fourth and fifth grammar school in which the three aforementioned schools participate. Most secondary schools in Amsterdam offer a variety of different levels of education in the same school. The city also has various colleges ranging from art and design to politics and economics which are mostly also available for students coming from other countries.
Schools for foreign nationals in Amsterdam include the Amsterdam International Community School, British School of Amsterdam, Albert Einstein International School Amsterdam, Lycée Vincent van Gogh La Haye-Amsterdam primary campus (French school), International School of Amsterdam, and the Japanese School of Amsterdam.
Amsterdam is a prominent centre for national and international media. Some locally based newspapers include Het Parool, a national daily paper; De Telegraaf, the largest Dutch daily newspaper; the daily newspapers Trouw, de Volkskrant and NRC; De Groene Amsterdammer, a weekly newspaper; the free newspapers Metro and The Holland Times (printed in English).
Amsterdam is home to the second-largest Dutch commercial TV group SBS Broadcasting Group, consisting of TV-stations SBS 6, Net 5 and Veronica. However, Amsterdam is not considered 'the media city of the Netherlands'. The town of Hilversum, 30 kilometres (19 miles) south-east of Amsterdam, has been crowned with this unofficial title. Hilversum is the principal centre for radio and television broadcasting in the Netherlands. Radio Netherlands, heard worldwide via shortwave radio since the 1920s, is also based there. Hilversum is home to an extensive complex of audio and television studios belonging to the national broadcast production company NOS, as well as to the studios and offices of all the Dutch public broadcasting organisations and many commercial TV production companies.
In 2012, the music video of Far East Movement, 'Live My Life', was filmed in various parts of Amsterdam.
Also, several movies were filmed in Amsterdam, such as James Bond's Diamonds Are Forever, Ocean's Twelve, Girl with a Pearl Earring and The Hitman's Bodyguard. Amsterdam is also featured in John Green's book The Fault in Our Stars, which has been made into a film as well that partly takes place in Amsterdam.
From the late 1960s onwards many buildings in Amsterdam have been squatted both for housing and for using as social centres. A number of these squats have legalised and become well known, such as OCCII, OT301, Paradiso and Vrankrijk.

Mysorean rockets were an Indian military weapon, the iron-cased rockets were successfully deployed for military use. The Mysorean army, under Hyder Ali and his son Tipu Sultan, used the rockets effectively against the British East India Company during the 1780s and 1790s. Their conflicts with the company exposed the British to this technology further, which was then used to advance European rocketry with the development of the Congreve rocket in 1805.
There was a regular rocket corps in the Mysore Army, beginning with about 1,200 men in Hyder Ali's time.  During the Second Anglo-Mysore War, Colonel William Baillie's ammunition stores are thought to have been detonated by a hit from one of Hyder Ali's rockets at the Battle of Pollilur in 1780, which contributed to British defeat in the battle.
Hyder Ali and his son Tipu Sultan deployed them against the larger British East India Company forces during the Anglo-Mysore Wars. These missiles were fitted with swords and traveled hundreds of metres through the air before coming down with edges facing the enemy. The British took an interest in the technology and developed it further during the 19th century. Due to the use of iron tubes for holding the propellant, higher thrust and longer range for the missile (up to 2 km range) could be achieved. Rockets also existed in Europe, but they were not iron-cased and their range was far less than their South Asian counterparts. These hammered soft iron rockets were crude, but the bursting strength of the container of black powder was much higher than the earlier paper construction, and a greater internal pressure was possible. These rockets were used with considerable effect against the British East India Company in battles at Srirangapatam in 1792 and 1799.
By the order of Tipu Sultan, his general Mir Zain-ul-'Abidin Shushtari compiled a military manual called Fathul Mujahidin in which 200 rocket men were assigned to each Mysorean cushoon (brigade). Mysore had 16 to 24 cushoons of infantry.  The rocket men were trained to launch their rockets at an angle calculated from the diameter of the cylinder and the distance to the target. In addition, wheeled rocket launchers were used in war that were capable of launching five to ten rockets almost simultaneously.
Rockets could be of various sizes but usually consisted of a tube of soft hammered iron about 8 inches (20 cm) long and 1.5 to 3 inches (3.8 to 7.6 cm) in diameter, closed at one end and strapped to a shaft of bamboo about 4 ft (1 m) long. The iron tube acted as a combustion chamber and contained well-packed black powder propellant. A rocket carrying about one pound (~500 g) of powder could travel almost 1,000 yards (~900 m). In contrast, rockets in Europe could not take large chamber pressures, not being iron cased, and were consequently not capable of reaching such distances.
The entire road alongside Jumma Masjid near City Market and Taramandalpet, Bangalore was the hub of Tipu's rocket project where he had set up a laboratory.
Two rocket units were fielded by Tipu Sultan in 1792 during the Third Anglo-Mysore War, one of 120 men and the other of 131 men. Lt. Col. Knox was attacked by rockets near Srirangapatna on the night of 6 February 1792 while advancing towards the Kaveri River from the north. The Rocket Corps ultimately reached a strength of about 5,000 in Tipu Sultan's army. Mysore rockets were also used for ceremonial purposes. The Jacobin Club of Mysore sent a delegation to Tipu Sultan, and 500 rockets were launched as part of the gun salute.
Rockets were again used on several occasions during the Fourth Anglo-Mysore War. One of these involved Colonel Arthur Wellesley, later famous as the First Duke of Wellington. Wellesley was almost defeated by Tipu's Diwan Purnaiah at the Battle of Sultanpet Tope.
Wellesley launched a fresh attack with a larger force the following day, and took the whole position without losing a single man. Rocketeers worked their way around to the rear of the British encampment on 22 April 1799, 12 days before the main battle, and fired a large number of rockets at the same moment to signal the beginning of an assault by 6,000 Indian infantry and a corps of Frenchmen, all directed by Mir Golam Hussain and Mohomed Hulleen Mir Mirans. The rockets had a range of about 1,000 yards. Some burst in the air like shells, while others (called ground rockets) would rise again on striking the ground and bound along in a serpentine motion until their force was spent. A young English officer named Bayly observed: "So pestered were we with the rocket boys that there was no moving without danger from the destructive missiles". He continued:
The rockets and musketry from 20,000 of the enemy were incessant. No hail could be thicker. Every illumination of blue lights was accompanied by a shower of rockets, some of which entered the head of the column, passing through to the rear, causing death, wounds, and dreadful lacerations from the long bamboos of twenty or thirty feet, which are invariably attached to them.A British shot struck a magazine of rockets within Tipu Sultan's fort during the decisive British attack on Srirangapattana on 2 May 1799, causing it to explode and send a towering cloud of black smoke with cascades of exploding white light rising up from the battlements. Baird led the final attack on the fort on the afternoon of 4 May and was again met by "furious musket and rocket fire", but this did not help much; the fort was taken in about an hour's time. Perhaps within another hour Tipu had been shot (the precise time of his death is not known), and the war was effectively over.
After the fall of Srirangapattana, 600 launchers, 700 serviceable rockets, and 9,000 empty rockets were found. Some of the rockets had pierced cylinders, to allow them to act like incendiaries, while some had iron points or steel blades bound to the bamboo. By attaching these blades to rockets they became very unstable towards the end of their flight causing the blades to spin around like flying scythes, cutting down soldiers in their path.
These experiences eventually led the Royal Woolwich Arsenal to start a military rocket research and development program in 1801, based on the Mysorean technology. Several rocket cases were collected from Mysore and sent to Britain for analysis. Their first demonstration of solid-fuel rockets came in 1805 and was followed by publication of A Concise Account of the Origin and Progress of the Rocket System in 1807 by William Congreve, son of the arsenal's commandant. Congreve rockets were systematically used by the British during the Napoleonic Wars and the War of 1812. They were also used in the 1814 Battle of Baltimore, and are mentioned in "The Star-Spangled Banner", the national anthem of the United States: And the rockets' red glare, the bombs bursting in air.
In 2002, a cache of metallic shells was unearthed during restoration of an old well in Nagara, 60 kilometres from Shivamogga. About one hundred of these rusted cylindrical shells were stored in Shivappa Nayaka Palace Government Museum identified only as 'shells' and without being registered in museum catalog. In 2010 these shells were identified to have a possible link to Tipu's rockets. And only in 2013 these shells were recognized for their significance.
In April 2017, 102 unused rockets of varying sizes were found in Shimoga district.
In July 2018, another 500 rockets (or 1,000, according to one source) were found in an abandoned well in the same area, confirming it as a major repository and fort under the Tipu Sultan.
As of November 2019, more than 3,000 such rockets have been recovered during debris clearances undertaken in Nagara.

The Kingdom of Mysore  was a realm in southern India, traditionally believed to have been founded in 1399 in the vicinity of the modern city of Mysore. From 1799 until 1950, it was a princely state, until 1947 in a subsidiary alliance with British India. The British took Direct Control over the Princely State in 1831. It then became Mysore State (later enlarged and renamed to Karnataka) with its ruler remaining as Rajapramukh until 1956, when he became the first Governor of the reformed state.
The kingdom, which was founded and ruled for most part by the Hindu Wodeyar family, initially served as a vassal state of the Vijayanagara Empire. The 17th century saw a steady expansion of its territory and during the rule of Narasaraja Wodeyar I and Chikka Devaraja Wodeyar, the kingdom annexed large expanses of what is now southern Karnataka and parts of Tamil Nadu to become a powerful state in the southern Deccan. During a brief Muslim rule, the kingdom shifted to a Sultanate style of administration.
During this time, it came into conflict with the Marathas, the Nizam of Hyderabad, the Kingdom of Travancore and the British, which culminated in the four Anglo-Mysore Wars. Success in the First Anglo-Mysore war and stalemate in the Second was followed by defeats in the Third and the Fourth. Following Tipu's death in the fourth war in the Siege of Seringapatam (1799), large parts of his kingdom were annexed by the British, which signalled the end of a period of Mysorean hegemony over South India. The British restored the Wodeyars to their throne by way of a subsidiary alliance and the diminished Mysore was transformed into a princely state. The Wodeyars continued to rule the state until Indian independence in 1947, when Mysore acceded to the Union of India.
Even as a princely state, Mysore came to be counted among the more developed and urbanised regions of India. This period (1799–1947) also saw Mysore emerge as one of the important centres of art and culture in India. The Mysore kings were not only accomplished exponents of the fine arts and men of letters, they were enthusiastic patrons as well, and their legacies continue to influence rocket science, music, and art even today.
Sources for the history of the kingdom include numerous extant lithic and copper plate inscriptions, records from the Mysore palace and contemporary literary sources in Kannada, Persian and other languages. According to traditional accounts, the kingdom originated as a small state based in the modern city of Mysore and was founded by two brothers, Yaduraya (also known as Vijaya) and Krishnaraya. Their origins are mired in legend and are still a matter of debate; while some historians posit a northern origin at Dwarka, others locate it in Karnataka. Yaduraya is said to have married Chikkadevarasi, the local princess and assumed the feudal title "Wodeyar" (Kannada: ಒಡೆಯರ್, romanized: Oḍeyar, lit. 'lord'), which the ensuing dynasty retained. The first unambiguous mention of the Wodeyar family is in 16th century Kannada literature from the reign of the Vijayanagara king Achyuta Deva Raya (1529–1542); the earliest available inscription, issued by the Wodeyars themselves, dates to the rule of the petty chief Timmaraja II in 1551.
The kings who followed ruled as vassals of the Vijayanagara empire until the decline of the latter in 1565. By this time, the kingdom had expanded to thirty-three villages protected by a force of 300 soldiers. King Timmaraja II conquered some surrounding chiefdoms, and King Bola Chamaraja IV (lit, "Bald"), the first ruler of any political significance among them, withheld tribute to the nominal Vijayanagara monarch Aravidu Ramaraya. After the death of Aravidu Aliya Rama Raya, the Wodeyars began to assert themselves further and King Raja Wodeyar I wrested control of Srirangapatna from the Vijayanagara governor (Mahamandaleshvara) Aravidu Tirumalla – a development which elicited, if only ex post facto, the tacit approval of Venkatapati Raya, the incumbent king of the diminished Vijayanagar empire ruling from Chandragiri. Raja Wodeyar I's reign also saw territorial expansion with the annexation of Channapatna to the north from Jaggadeva Raya – a development which made Mysore a regional political factor to reckon with.
Consequently, by 1612–13, the Wodeyars exercised a great deal of autonomy and even though they acknowledged the nominal overlordship of the Aravidu dynasty, tributes and transfers of revenue to Chandragiri stopped. This was in marked contrast to other major chiefs Nayaks of Tamil country who continued to pay off Chandragiri emperors well into the 1630s. Chamaraja VI and Kanthirava Narasaraja I attempted to expand further northward but were thwarted by the Bijapur Sultanate and its Maratha subordinates, though the Bijapur armies under Ranadullah Khan were effectively repelled in their 1638 siege of Srirangapatna. Expansionist ambitions then turned southward into Tamil country where Narasaraja Wodeyar acquired Satyamangalam (in modern northern Erode district) while his successor Dodda Devaraja Wodeyar expanded further to capture western Tamil regions of Erode and Dharmapuri, after successfully repulsing the chiefs of Madurai. The invasion of the Keladi Nayakas of Malnad was also dealt with successfully. This period was followed by one of complex geo-political changes, when in the 1670s, the Marathas and the Mughals pressed into the Deccan.
Chikka Devaraja (r. 1672–1704), the most notable of Mysore's early kings, who ruled during much of this period, managed to not only survive the exigencies but further expanded territory. He achieved this by forging strategic alliances with the Marathas and the Mughals. The kingdom soon grew to include Salem and Bangalore to the east, Hassan to the west, Chikkamagaluru and Tumkur to the north and the rest of Coimbatore to the south. Despite this expansion, the kingdom, which now accounted for a fair share of land in the southern Indian heartland, extending from the Western Ghats to the western boundaries of the Coromandel plain, remained landlocked without direct coastal access. Chikka Devaraja's attempts to remedy this brought Mysore into conflict with the Nayaka chiefs of Ikkeri and the kings (Rajas) of Kodagu (modern Coorg); who between them controlled the Kanara coast (coastal areas of modern Karnataka) and the intervening hill region respectively. The conflict brought mixed results with Mysore annexing Periyapatna but suffering a reversal at Palupare.
Nevertheless, from around 1704, when the kingdom passed on to "Muteking" (Mukarasu) Kanthirava Narasaraja II, the survival and expansion of the kingdom was achieved by playing a delicate game of alliance, negotiation, subordination on occasion, and annexation of territory in all directions. According to historians Sanjay Subrahmanyam and Sethu Madhava Rao, Mysore was now formally a tributary of the Mughal empire. Mughul records claim a regular tribute (peshkash) was paid by Mysore. However, historian Suryanath U. Kamath feels the Mughals may have considered Mysore an ally, a situation brought about by Mughal–Maratha competition for supremacy in southern India. By the 1720s, with the Mughal empire in decline, further complications arose with the Mughal residents at both Arcot and Sira claiming tribute. The years that followed saw Krishnaraja Wodeyar I tread cautiously on the matter while keeping the Kodagu chiefs and the Marathas at bay. He was followed by Chamaraja Wodeyar VII during whose reign power fell into the hands of prime minister (Dalwai or Dalavoy) Nanjarajiah (or Nanjaraja) and chief minister (Sarvadhikari) Devarajiah (or Devaraja), the influential brothers from Kalale town near Nanjangud who would rule for the next three decades with the Wodeyars relegated to being the titular heads. The latter part of the rule of Krishnaraja II saw the Deccan Sultanates being eclipsed by the Mughals and in the confusion that ensued, Haider Ali, a captain in the army, rose to prominence. His victory against the Marathas at Bangalore in 1758, resulting in the annexation of their territory, made him an iconic figure. In honour of his achievements, the king gave him the title "Nawab Haider Ali Khan Bahadur".
Haider Ali has earned an important place in the history of Karnataka for his fighting skills and administrative acumen. The rise of Haidar came at a time of important political developments in the sub-continent. While the European powers were busy transforming themselves from trading companies to political powers, the Nizam as the subedar of the Mughals pursued his ambitions in the Deccan, and the Marathas, following their defeat at Panipat, sought safe havens in the south. The period also saw the French vie with the British for control of the Carnatic—a contest in which the British would eventually prevail as British commander Sir Eyre Coote decisively defeated the French under the Comte de Lally at the Battle of Wandiwash in 1760, a watershed in Indian history as it cemented British supremacy in South Asia. Though the Wodeyars remained the nominal heads of Mysore during this period, real power lay in the hands of Haider Ali and his son Tipu.
By 1761, Maratha power had diminished and by 1763, Haider Ali had captured the Keladi kingdom, defeated the rulers of Bilgi, Bednur and Gutti, invaded the Malabar in the south and conquered the Zamorin's capital Calicut with ease in 1766 and extended the Mysore kingdom up to Dharwad and Bellary in the north. Mysore was now a major political power in the subcontinent and Haider's meteoric rise from relative obscurity and his defiance formed one of the last remaining challenges to complete British hegemony over the Indian subcontinent—a challenge which would take them more than three decades to overcome.
In a bid to stem Haidar's rise, the British formed an alliance with the Marathas and the Nizam of Golconda, culminating in the First Anglo-Mysore War in 1767. Despite numerical superiority Haider Ali suffered defeats at the battles of Chengham and Tiruvannamalai. The British ignored his overtures for peace until Haider Ali had strategically moved his armies to within five miles of Madras (modern Chennai) and was able to successfully sue for peace. In 1770, when the Maratha armies of Madhavrao Peshwa invaded Mysore (three wars were fought between 1764 and 1772 by Madhavrao against Haider, in which Haider lost), Haider expected British support as per the 1769 treaty but they betrayed him by staying out of the conflict. The British betrayal and Haider's subsequent defeat reinforced Haider's deep distrust of the British—a sentiment that would be shared by his son and one which would inform Anglo-Mysore rivalries of the next three decades. In 1777, 
Haider Ali recovered the previously lost territories of Coorg and Malabar from the Marathas.
Haider Ali's army advanced towards the Marathas and fought them at the Battle of Saunshi and came out victorious during the same year.
By 1779, Haider Ali had captured parts of modern Tamil Nadu and Kerala in the south, extending the Kingdom's area to about 80,000 mi2 (205,000 km2). In 1780, he befriended the French and made peace with the Marathas and the Nizam. However, Haider Ali was betrayed by the Marathas and the Nizam, who made treaties with the British as well. In July 1779,Haider Ali headed an army of 80,000, mostly cavalry, descending through the passes of the Ghats amid burning villages, before laying siege to British forts in northern Arcot starting the Second Anglo-Mysore War. Haider Ali had some initial successes against the British notably at Pollilur, the worst defeat the British suffered in India until Chillianwala, and Arcot, until the arrival of Sir Eyre Coote, when the fortunes of the British began to change. On 1 June 1781 Coote struck the first heavy blow against Haider Ali in the decisive Battle of Porto Novo. The battle was won by Coote against odds of five to one, and is regarded as one of the greatest feats of the British in India. It was followed up by another hard-fought battle at Pollilur (the scene of an earlier triumph of Haider Ali over a British force) on 27 August, in which the British won another success, and by the rout of the Mysore troops at Sholinghur a month later. Haider Ali died on 7 December 1782, even as fighting continued with the British. He was succeeded by his son Tipu Sultan who continued hostilities against the British by recapturing Baidanur and Mangalore.
By 1783 neither the British nor Mysore were able to obtain a clear overall victory. The French withdrew their support of Mysore following the peace settlement in Europe. Undaunted, Tipu, popularly known as the "Tiger of Mysore", continued the war against the British but lost some regions in modern coastal Karnataka to them. The Maratha–Mysore War occurred between 1785 and 1787 and consisted of a series of conflicts between the Sultanate of Mysore and the Maratha Empire. Following Tipu Sultan's victory against the Marathas at the siege of Bahadur Benda, a peace agreement was signed between the two kingdoms with mutual gains and losses. Similarly, the treaty of Mangalore was signed in 1784 bringing hostilities with the British to a temporary and uneasy halt and restoring the others' lands to the status quo ante bellum. The treaty is an important document in the history of India, because it was the last occasion when an Indian power dictated terms to the British, who were made to play the role of humble supplicants for peace. A start of fresh hostilities between the British and French in Europe would have been sufficient reason for Tipu to abrogate his treaty and further his ambition of striking at the British. His attempts to lure the Nizam, the Marathas, the French and the Sultan of Turkey failed to bring direct military aid.
Tipu's successful attacks in 1790 on the Kingdom of Travancore, a British ally, was an effective victory for him, however it resulted in greater hostilities with the British which resulted in the Third Anglo-Mysore War. In the beginning, the British made gains, taking the Coimbatore district, but Tipu's counterattack reversed many of these gains. By 1792, with aid from the Marathas who attacked from the north-west and the Nizam who moved in from the north-east, the British under Lord Cornwallis successfully besieged Srirangapatna, resulting in Tipu's defeat and the Treaty of Srirangapatna. Half of Mysore was distributed among the allies, and two of his sons were held to ransom. A humiliated but indomitable Tipu went about re-building his economic and military power. He attempted to covertly win over support from Revolutionary France, the Amir of Afghanistan, the Ottoman Empire and Arabia. However, these attempts to involve the French soon became known to the British, who were at the time fighting the French in Egypt, were backed by the Marathas and the Nizam. In 1799, Tipu died defending Srirangapatna in the Fourth Anglo-Mysore War, heralding the end of the Kingdom's independence. Modern Indian historians consider Tipu Sultan an inveterate enemy of the British, an able administrator and an innovator.
Following Tipu's fall, a part of the kingdom of Mysore was annexed and divided between the Madras Presidency and the Nizam. The remaining territory was transformed into a Princely State; the five-year-old scion of the Wodeyar family, Krishnaraja III, was installed on the throne with chief minister (Diwan) Purnaiah, who had earlier served under Tipu, handling the reins as regent and Lt. Col. Barry Close taking charge as the British Resident. The British then took control of Mysore's foreign policy and also exacted an annual tribute and a subsidy for maintaining a standing British army at Mysore. As Diwan, Purnaiah distinguished himself with his progressive and innovative administration until he retired from service in 1811 (and died shortly thereafter) following the 16th birthday of the boy king.
The years that followed witnessed cordial relations between Mysore and the British until things began to sour in the 1820s. Even though the Governor of Madras, Thomas Munro, determined after a personal investigation in 1825 that there was no substance to the allegations of financial impropriety made by A. H. Cole, the incumbent Resident of Mysore, the Nagar revolt (a civil insurrection) which broke out towards the end of the decade changed things considerably. In 1831, close on the heels of the insurrection and citing mal-administration, the British took direct control of the princely state. For the next fifty years, Mysore passed under the rule of successive British Commissioners; Sir Mark Cubbon, renowned for his statesmanship, served from 1834 until 1861 and put into place an efficient and successful administrative system which left Mysore a well-developed state.
In 1876–77, however, towards the end of the period of direct British rule, Mysore was struck by a devastating famine with estimated mortality figures ranging between 700,000 and 1,100,000, or nearly a fifth of the population. Shortly thereafter, Maharaja Chamaraja X, educated in the British system, took over the rule of Mysore in 1881, following the success of a lobby set up by the Wodeyar dynasty that was in favour of rendition. Accordingly, a resident British officer was appointed at the Mysore court and a Diwan to handle the Maharaja's administration. From then onwards, until Indian independence in 1947, Mysore remained a Princely State within the British Indian Empire, with the Wodeyars continuing their rule.
After the demise of Maharaja Chamaraja X, Krishnaraja IV, still a boy of eleven, ascended the throne in 1895. His mother Maharani Kemparajammanniyavaru ruled as regent until Krishnaraja took over the reins on 8 February 1902. Under his rule, with Sir M. Vishweshwariah as his Diwan, the Maharaja set about transforming Mysore into a progressive and modern state, particularly in industry, education, agriculture and art. Such were the strides that Mysore made that Mahatma Gandhi called the Maharaja a "saintly king" (Rajarishi). Paul Brunton, the British philosopher and orientalist, John Gunther, the American author, and British statesman Lord Samuel praised the ruler's efforts. Much of the pioneering work in educational infrastructure that took place during this period would serve Karnataka invaluably in the coming decades. The Maharaja was an accomplished musician, and like his predecessors, avidly patronised the development of the fine arts. He was followed by his nephew Jayachamarajendra whose rule continued for some years after he signed the instrument of accession and Mysore joined the Indian Union on 9 August 1947. Jayachamarajendra continued to rule as Rajapramukh of Mysore until 1956, when as a result of the States Reorganisation Act, 1956, his position was converted into Governor of Mysore State. From 1963 until 1966 he was the first Governor of Madras State.
There are no records relating to the administration of the Mysore territory during the Vijayanagara Empire's reign (1399–1565). Signs of a well-organised and independent administration appear from the time of Raja Wodeyar I who is believed to have been sympathetic towards peasants (raiyats) who were exempted from any increases in taxation during his time. The first sign that the kingdom had established itself in the area was the issuing of gold coins (Kanthirayi phanam) resembling those of the erstwhile Vijayanagara Empire during Narasaraja Wodeyar's rule.
The rule of Chikka Devaraja saw several reforms were effected. Internal administration was remodelled to suit the kingdom's growing needs and became more efficient. A postal system came into being. Far reaching financial reforms were also introduced. A number of petty taxes were imposed in place of direct taxes, as a result of which the peasants were compelled to pay more by way of land tax. The king is said to have taken a personal interest in the regular collection of revenues the treasury burgeoned to 90,000,000 Pagoda (a unit of currency) – earning him the epithet "Nine crore Narayana" (Navakoti Narayana). In 1700, he sent an embassy to Aurangazes court who bestowed upon him the title Jug Deo Raja and awarded permission to sit on the ivory throne. Following this, he founded the district offices (Attara Kacheri), the central secretariat comprising eighteen departments, and his administration was modelled on Mughal lines.
During Haider Ali's rule, the kingdom was divided into five provinces (Asofis) of unequal size, comprising 171 taluks (Paraganas) in total. When Tipu Sultan became the de facto ruler, the kingdom, which encompassed 160,000 km2 (61,776 sq mi) (62,000 mi2), was divided into 37 provinces and a total of 124 taluks (Amil). Each province had a governor (Asof), and one deputy governor. Each taluk had a headman called Amildar and a group of villages were in charge of a Patel. The central administration comprised six departments headed by ministers, each aided by an advisory council of up to four members.
When the princely state came under direct British rule in 1831, early commissioners Lushington, Briggs and Morrison were followed by Mark Cubbon, who took charge in 1834. He made Bangalore the capital and divided the princely state into four divisions, each under a British superintendent. The state was further divided into 120 taluks with 85 taluk courts, with all lower level administration in the Kannada language. The office of the commissioner had eight departments; revenue, post, police, cavalry, public works, medical, animal husbandry, judiciary and education. The judiciary was hierarchical with the commissioners' court at the apex, followed by the Huzur Adalat, four superintending courts and eight Sadar Munsiff courts at the lowest level. Lewin Bowring became the chief commissioner in 1862 and held the position until 1870. During his tenure, the property "Registration Act", the "Indian Penal code" and "Code of Criminal Procedure" came into effect and the judiciary was separated from the executive branch of the administration. The state was divided into eight districts – Bangalore, Chitraldroog, Hassan, Kadur, Kolar, Mysore, Shimoga, and Tumkur.
After rendition, C. V. Rungacharlu, was made the Diwan. Under him, the first Representative Assembly of British India, with 144 members, was formed in 1881. He was followed by K. Seshadri Iyer in 1883 during whose tenure gold mining at the Kolar Gold Fields began, the Shivanasamudra hydroelectric project was initiated in 1899 (the first such major attempt in India) and electricity and drinking water (the latter through pipes) was supplied to Bangalore. Seshadri Iyer was followed by P. N. Krishnamurti, who founded The Secretariat Manual to maintain records and the Co-operative Department in 1905, V. P. Madhava Rao who focussed on conservation of forests and T. Ananda Rao, who finalised the Kannambadi Dam project.
Sir Mokshagundam Visvesvaraya, popularly known as the "Maker of Modern Mysore", holds a key place in the history of Karnataka. An engineer by education, he became the Diwan in 1909. Under his tenure, membership of the Mysore Legislative Assembly was increased from 18 to 24, and it was given the power to discuss the state budget. The Mysore Economic Conference was expanded into three committees; industry and commerce, education, and agriculture, with publications in English and Kannada. Important projects commissioned during his time included the construction of the Kannambadi Dam, the founding of the Mysore Iron Works at Bhadravathi, founding of the Mysore University in 1916, the University Visvesvaraya College of Engineering in Bangalore, establishment of the Mysore state railway department and numerous industries in Mysore. In 1955, he was awarded the Bharat Ratna, India's highest civilian honour.
Sir Mirza Ismail took office as Diwan in 1926 and built on the foundation laid by his predecessor. Amongst his contributions were the expansion of the Bhadravathi Iron Works, the founding of a cement and paper factory in Bhadravathi and the launch of Hindustan Aeronautics Limited. A man with a penchant for gardens, he founded the Brindavan Gardens (Krishnaraja Sagar) and built the Kaveri River high-level canal to irrigate 120,000 acres (490 km2) in modern Mandya district.
In 1939 Mandya District was carved out of Mysore District, bringing the number of districts in the state to nine.
The vast majority of the people lived in villages and agriculture was their main occupation. The economy of the kingdom was based on agriculture. Grains, pulses, vegetables and flowers were cultivated. Commercial crops included sugarcane and cotton. The agrarian population consisted of landlords (vokkaliga, zamindar, heggadde) who tilled the land by employing a number of landless labourers, usually paying them in grain. Minor cultivators were also willing to hire themselves out as labourers if the need arose. It was due to the availability of these landless labourers that kings and landlords were able to execute major projects such as palaces, temples, mosques, anicuts (dams) and tanks. Because land was abundant and the population relatively sparse, no rent was charged on land ownership. Instead, landowners paid tax for cultivation, which amounted to up to one-half of all harvested produce.
The Kingdom of Mysore reached a peak in economic power under Hyder Ali and Tipu Sultan, in the post-Mughal era of the mid-late 18th century.
Tipu Sultan is credited with founding state trading depots in various locations of his kingdom. In addition, he founded depots in foreign locations such as Karachi, Jeddah and Muscat, where Mysore products were sold. During Tipu's rule French technology was used for the first time in carpentry and smithing, Chinese technology was used for sugar production, and technology from Bengal helped improve the sericulture industry. State factories were established in Kanakapura and Taramandelpeth for producing cannons and gunpowder respectively. The state held the monopoly in the production of essentials such as sugar, salt, iron, pepper, cardamom, betel nut, tobacco and sandalwood, as well as the extraction of incense oil from sandalwood and the mining of silver, gold and precious stones. Sandalwood was exported to China and the Persian Gulf countries and sericulture was developed in twenty-one centers within the kingdom.
The Mysore silk industry was initiated during the rule of Tipu Sultan. Later the industry was hit by a global depression and competition from imported silk and rayon. In the second half of the 20th century, it however revived and the Mysore State became the top multivoltine silk producer in India.
This system changed under the subsidiary alliance with the British, when tax payments were made in cash and were used for the maintenance of the army, police and other civil and public establishments. A portion of the tax was transferred to England as the "Indian tribute". Unhappy with the loss of their traditional revenue system and the problems they faced, peasants rose in rebellion in many parts of south India. After 1800, the Cornwallis land reforms came into effect. Reade, Munro, Graham and Thackeray were some administrators who improved the economic conditions of the masses. However, the homespun textile industry suffered while most of India was under British rule, with the exception of the producers of the finest cloth and the coarse cloth which was popular with the rural masses. This was due to the manufacturing mills of Manchester, Liverpool and Scotland being more than a match for the traditional handweaving industry, especially in spinning and weaving.
The economic revolution in England and the tariff policies of the British also caused massive de-industrialization in other sectors throughout British India and Mysore. For example, the gunny bag weaving business had been a monopoly of the Goniga people, which they lost when the British began ruling the area. The import of a chemical substitute for saltpetre (potassium nitrate) affected the Uppar community, the traditional makers of saltpetre for use in gunpowder. The import of kerosene affected the Ganiga community which supplied oils. Foreign enamel and crockery industries affected the native pottery business, and mill-made blankets replaced the country-made blankets called kambli. This economic fallout led to the formation of community-based social welfare organisations to help those within the community to cope better with their new economic situation, including youth hostels for students seeking education and shelter. However, the British economic policies created a class structure consisting of a newly established middle class comprising various blue and white-collared occupational groups, including agents, brokers, lawyers, teachers, civil servants and physicians. Due to a more flexible caste hierarchy, the middle class contained a heterogeneous mix of people from different castes.
The early kings of the Wodeyar dynasty worshipped the Hindu god Shiva. The later kings, starting from the 17th century, took to Vaishnavism, the worship of the Hindu god Vishnu. According to musicologist Meera Rajaram Pranesh, King Raja Wodeyar I was a devotee of the god Vishnu, King Dodda Devaraja was honoured with the title "Protector of Brahmins" (Deva Brahmana Paripalaka) for his support to Brahmins, and Maharaja Krishnaraja III was devoted to the goddess Chamundeshwari (a form of Hindu goddess Durga). Wilks ("History of Mysore", 1800) wrote about a Jangama (Veerashaiva saint-devotee of Shiva) uprising, related to excessive taxation, which was put down firmly by Chikka Devaraja. Historian D.R. Nagaraj claims that four hundred Jangamas were murdered in the process but clarifies that Veerashiava literature itself is silent about the issue. Historian Suryanath Kamath claims King Chikka Devaraja was a Srivaishnava (follower of Sri Vaishnavism, a sect of Vaishnavism) but was not anti-Veerashaiva. Historian Aiyangar concurs that some of the kings including the celebrated Narasaraja I and Chikka Devaraja were Vaishnavas, but suggests this may not have been the case with all Wodeyar rulers. The rise of the modern day Mysore city as a centre of south Indian culture has been traced from the period of their sovereignty. Raja Wodeyar I initiated the celebration of the Dasara festival in Mysore, a proud tradition of the erstwhile Vijayanagara royal family.
Jainism, though in decline during the late medieval period, also enjoyed the patronage of the Mysore kings, who made munificent endowments to the Jain monastic order at the town of Shravanabelagola. Records indicate that some Wodeyar kings not only presided over the Mahamastakabhisheka ceremony, an important Jain religious event at Shravanabelagola, but also personally offered prayers (puja) during the years 1659, 1677, 1800, 1825, 1910, 1925, 1940, and 1953.
The contact between South India and Islam goes back to the 7th century, when trade between Hindu kingdoms and Islamic caliphates thrived. These Muslim traders settled on the Malabar Coast and married local Hindu women, and their descendants came to be known as Mappillas. By the 14th century, Muslims had become a significant minority in the south, though the advent of Portuguese missionaries checked their growth. Haider Ali, though a devout Muslim, did not allow his faith to interfere with the administration of the predominantly Hindu kingdom. Historians are, however, divided on the intentions of Haider Ali's son, Tipu Sultan. It has been claimed that Tipu raised Hindus to prominent positions in his administration, made generous grants to Hindu temples and brahmins, and generally respected other faiths, and that any religious conversions that Tipu undertook were as punishment to those who rebelled against his authority. However, this has been countered by other historians who claim that Tipu Sultan treated the non-Muslims of Mysore far better than those of the Malabar, Raichur and Kodagu regions. They opine that Tipu was responsible for mass conversions of Christians and Hindus in these regions, either by force or by offering them tax incentives and revenue benefits to convert.
Prior to the 18th century, the society of the kingdom followed age-old and deeply established norms of social interaction between people. Accounts by contemporaneous travellers indicate the widespread practice of the Hindu caste system and of animal sacrifices during the nine-day celebrations (called Mahanavami). Later, fundamental changes occurred due to the struggle between native and foreign powers. Though wars between the Hindu kingdoms and the Sultanates continued, the battles between native rulers (including Muslims) and the newly arrived British took centre stage. The spread of English education, the introduction of the printing press and the criticism of the prevailing social system by Christian missionaries helped make the society more open and flexible. The rise of modern nationalism throughout India also affected Mysore.
With the advent of British power, English education gained prominence in addition to traditional education in local languages. These changes were orchestrated by Lord Elphinstone, the governor of the Madras Presidency. His plan became the constitution of the central collegiate institution or University Board in 1841. Accordingly, a high school department of the university was established. For imparting education in the interior regions, schools were raised in principal towns which eventually were elevated to college level, with each college becoming central to many local schools (zilla schools). The earliest English-medium schools appeared in 1833 in Mysore and spread across the region. In 1858, the department of education was founded in Mysore and by 1881, there were an estimated 2,087 English-medium schools in the state of Mysore. Higher education became available with the formation of Bangalore Central College in Bangalore (1870), Maharaja's College (1879), Maharani's College (1901) and the Mysore University (1916) in Mysore and the St. Agnes College in Mangalore (1921).
Social reforms aimed at removing practices such as sati and social discrimination based upon untouchability, as well as demands for the emancipation of the lower classes, swept across India and influenced Mysore territory. In 1894, the kingdom passed laws to abolish the marriage of girls below the age of eight. Remarriage of widowed women and marriage of destitute women was encouraged, and in 1923, some women were granted the permission to exercise their franchise in elections. There were, however, uprisings against British authority in the Mysore territory, notably the Kodagu uprising in 1835 (after the British dethroned the local ruler Chikkaviraraja) and the Kanara uprising of 1837. The era of printing heralded by Christian missionaries, notably Hermann Mögling, resulted in the founding of printing presses across the kingdom. The publication of ancient and contemporary Kannada books (such as the Pampa Bharata and the Jaimini Bharata), a Kannada-language Bible, a bilingual dictionary and a Kannada newspaper called Kannada Samachara began in the early 19th century. Aluru Venkata Rao published a consolidated Kannada history glorifying the achievements of Kannadigas in his book Karnataka Gatha Vaibhava.
Classical English and Sanskrit drama, and native Yakshagana musical theatre influenced the Kannada stage and produced famous dramatists like Gubbi Veeranna. The public began to enjoy Carnatic music through its broadcast via public address systems set up on the palace grounds. Mysore paintings, which were inspired by the Bengal Renaissance, were created by artists such as Sundarayya, Ala Singarayya, and B. Venkatappa.
The era of the Kingdom of Mysore is considered a golden age in the development of Kannada literature. Not only was the Mysore court adorned by famous Brahmin and Veerashaiva writers and composers, the kings themselves were accomplished in the fine arts and made important contributions. While conventional literature in philosophy and religion remained popular, writings in new genres such as chronicle, biography, history, encyclopaedia, novel, drama, and musical treatise became popular. A native form of folk literature with dramatic representation called Yakshagana gained popularity. A remarkable development of the later period was the influence of English literature and classical Sanskrit literature on Kannada.
Govinda Vaidya, a native of Srirangapatna, wrote Kanthirava Narasaraja Vijaya, a eulogy of his patron King Narasaraja I. Written in sangatya metre (a composition meant to be rendered to the accompaniment of a musical instrument), the book describes the king's court, popular music and the types of musical compositions of the age in twenty-six chapters. King Chikka Devaraja was the earliest composer of the dynasty. To him is ascribed the famous treatise on music called Geetha Gopala. Though inspired by Jayadeva's Sanskrit writing Geetha Govinda, it had an originality of its own and was written in saptapadi metre. Contemporary poets who left their mark on the entire Kannada-speaking region include the brahmin poet Lakshmisa and the itinerant Veerashaiva poet Sarvajna. Female poets also played a role in literary developments, with Cheluvambe (the queen of Krishnaraja Wodeyar I), Helavanakatte Giriyamma, Sri Rangamma (1685) and Sanchi Honnamma (Hadibadeya Dharma, late 17th century) writing notable works.
A polyglot, King Narasaraja II authored fourteen Yakshaganas in various languages, though all are written in Kannada script. Maharaja Krishnaraja III was a prolific writer in Kannada for which he earned the honorific Abhinava Bhoja (a comparison to the medieval King Bhoja). Over forty writings are attributed to him, of which the musical treatise Sri Tatwanidhi and a poetical romance called Saugandika Parinaya written in two versions, a sangatya and a drama, are most well known. Under the patronage of the Maharaja, Kannada literature began its slow and gradual change towards modernity. Kempu Narayana's Mudramanjusha ("The Seal Casket", 1823) is the earliest work that has touches of modern prose. However, the turning point came with the historically important Adbhuta Ramayana (1895) and Ramaswamedham (1898) by Muddanna, whom the Kannada scholar Narasimha Murthy considers "a Janus like figure" of modern Kannada literature. Muddanna has deftly handled an ancient epic from an entirely modern viewpoint.
Basavappa Shastry, a native of Mysore and a luminary in the court of Maharaja Krishnaraja III and Maharaja Chamaraja X, is known as the "Father of Kannada theatre" (Kannada Nataka Pitamaha). He authored dramas in Kannada and translated William Shakespeare's "Othello" to Shurasena Charite. His well-known translations from Sanskrit to Kannada are many and include Kalidasa and Abhignyana Shakuntala.
Under Maharaja Krishnaraja III and his successors – Chamaraja X, Krishnaraja IV and the last ruler, Jayachamaraja, the Mysore court came to be the largest and most renowned patron of music. While the Tanjore and Travancore courts also extended great patronage and emphasised preservation of the art, the unique combination of royal patronage of individual musicians, founding of music schools to kindle public interest and a patronage of European music publishers and producers set Mysore apart. Maharaja Krishnaraja III, himself a musician and musicologist of merit, composed a number of javalis (light lyrics) and devotional songs in Kannada under the title Anubhava pancharatna. His compositions bear the pen name (mudra) "Chamundi'" or '"Chamundeshwari'", in honour of the Wodeyar family deity.
Under Krishnaraja IV, art received further patronage. A distinct school of music which gave importance to raga and bhava evolved. The Royal School of Music founded at the palace helped institutionalise teaching of the art. Carnatic compositions were printed and the European staff notation came to be employed by royal musicians. Western music was also encouraged – Margaret Cousins' piano concerto with the Palace Orchestra marked the celebrations of Beethoven's centenary in Bangalore. Maharaja Jayachamaraja, also a renowned composer of Carnatic kritis (a musical composition), sponsored a series of recordings of Russian composer Nikolas Medtner and others. The court ensured that Carnatic music also kept up with the times. Gramophone recordings of the palace band were made and sold commercially. Attention was paid to "technology of the concert". Lavish sums were spent on acquiring various instruments including the unconventional horn violin, theremin and calliaphone, a mechanical music player.
The Mysore court was home to several renowned experts (vidwan) of the time. Veena Sheshanna, a court musician during the rule of Maharaja Chamaraja X, is considered one of the greatest exponents of the veena. His achievements in classical music won Mysore a premier place in the art of instrumental Carnatic music and he was given the honorific Vainika Shikhamani by Maharaja Krishnaraja Wodeyar IV. Mysore Vasudevacharya was a noted musician and composer in Sanskrit and Telugu from Mysore. He holds the unique distinction of being patronised by four generations of Mysore kings and rulers and for being court musician to three of them. H.L. Muthiah Bhagavatar was another musician-composer who adorned the Mysore court. Considered one of the most important composers of the post-Tyagaraja period, he is credited with about 400 compositions in Sanskrit, Kannada, Telugu and Tamil under the pen name "Harikesha". Among violinists, T. Chowdiah emerged as one of the most accomplished exponents of the time. He is known to have mastered the seven-stringed violin. Chowdiah was appointed court musician by Maharaja Krishnaraja Wodeyar IV in 1939 and received such titles as "Sangeeta Ratna" and "Sangeeta Kalanidhi". He is credited with compositions in Kannada, Telugu and Sanskrit under the pen name "Trimakuta".
The architectural style of courtly and royal structures in the kingdom underwent profound changes during British rule – a mingling of European traditions with native elements. The Hindu temples in the kingdom were built in typical South Indian Dravidian style – a modest version of the Vijayanagara building idiom. When in power, Tipu Sultan constructed a palace and a mosque in Srirangapatna, his capital. However, it is the city of Mysore that is best known for its royal palaces, earning it the nickname "City of Palaces". The city's main palace, the Mysore Palace, is also known as the Amba Vilas Palace. The original complex was destroyed by fire and a new palace was commissioned by the Queen-Regent and designed by the English architect Henry Irwin in 1897. The overall design is a combination of Hindu, Islamic, Indo-Saracenic and Moorish styles, which for the first time in India, used cast iron columns and roof frames. The striking feature of the exterior is the granite columns that support cusped arches on the portico, a tall tower whose finial is a gilded dome with an umbrella (chattri) on it, and groups of other domes around it. The interior is richly decorated with marbled walls and a teakwood ceiling on which are sculptures of Hindu deities. The Durbar hall leads to an inner private hall through silver doors. This opulent room has floor planels that are inlaid with semi-precious stones, and a stained glass roof supported centrally by columns and arches. The marriage hall (Kalyana mantapa) in the palace complex is noted for its stained glass octagonal dome with peacock motifs.
The Lalitha Mahal Palace was built in 1921 by E. W. Fritchley under the commission of Maharaja Krishnaraja IV. The architectural style is called "Renaissance" and exhibits concepts from English manor houses and Italian palazzos. The central dome is believed to be modelled on St. Paul's Cathedral in London. Other important features are the Italian marble staircase, the polished wooden flooring in the banquet and dance halls, and the Belgian cut glass lamps. The Jaganmohan Palace was commissioned in 1861 and was completed in 1910. The three-storeyed building with attractive domes, finials and cupolas was the venue of many a royal celebration. It is now called the Chamarajendra Art Gallery and houses a rich collection of artefacts.
The Mysore University campus, also called "Manasa Gangotri", is home to several architecturally interesting buildings. Some of them are in European style and were completed in the late 19th century. They include the Jayalakshmi Vilas mansion, the Crawford Hall, the Oriental Research Institute (built between 1887 and 1891) with its Ionic and Corinthian columns, and the district offices (Athara Kutchery, 1887). The Athara Kutchery, which initially served as the office of the British commissioner, has an octagonal dome and a finial that adds to its beauty. The maharaja's summer palace, built in 1880, is called the Lokaranjan Mahal, and initially served as a school for royalty. The Rajendra Vilas Palace, built in the Indo-British style atop the Chamundi Hill, was commissioned in 1922 and completed in 1938 by Maharaja Krishnaraja IV. Other royal mansions built by the Mysore rulers were the Chittaranjan Mahal in Mysore and the Bangalore Palace in Bangalore, a structure built on the lines of England's Windsor Castle. The Central Food Technical Research Institute (Cheluvamba Mansion), built in baroque European renaissance style, was once the residence of princess Cheluvambaamani Avaru, a sister of Maharaja Krishnaraja IV. Its extensive pilaster work and mosaic flooring are noteworthy.
Most famous among the many temples built by the Wodeyars is the Chamundeshwari Temple atop the Chamundi Hill. The earliest structure here was consecrated in the 12th century and was later patronised by the Mysore rulers. Maharaja Krishnaraja III added a Dravidian-style gopuram in 1827. The temple has silver-plated doors with images of deities. Other images include those of the Hindu god Ganesha and of Maharaja Krishnaraja III with his three queens. Surrounding the main palace in Mysore and inside the fort are a group of temples, built in various periods. The Prasanna Krishnaswamy Temple (1829), the Lakshmiramana Swamy Temple whose earliest structures date to 1499, the Trinesvara Swamy Temple (late 16th century), the Shweta Varaha Swamy Temple built by Purnaiah with a touch of Hoysala style of architecture, the Prasanna Venkataramana Swami Temple (1836) notable for 12 murals of the Wodeyar rulers. Well-known temples outside Mysore city are the yali ("mythical beast") pillared Venkataramana temple built in the late 17th century in the Bangalore fort, and the Ranganatha temple in Srirangapatna.
Tipu Sultan built a wooden colonnaded palace called the Dariya Daulat Palace (lit, "garden of the wealth of the sea") in Srirangapatna in 1784. Built in the Indo-Saracenic style, the palace is known for its intricate woodwork consisting of ornamental arches, striped columns and floral designs, and paintings. The west wall of the palace is covered with murals depicting Tipu Sultan's victory over Colonel Baillie's army at Pollilur, near Kanchipuram in 1780. One mural shows Tipu enjoying the fragrance of a bouquet of flowers while the battle is in progress. In that painting, the French soldiers' moustaches distinguish them from the cleanshaven British soldiers. Also in Srirangapatna is the Gumbaz mausoleum, built by Tipu Sultan in 1784. It houses the graves of Tipu and Haider Ali. The granite base is capped with a dome built of brick and pilaster.
Mysore Palace
The Gopura (tower) of the Chamundeshwari Temple on the Chamundi Hills. The temple is dedicated to Mysore's patron deity.
The Jaganmohan Palace at Mysore – now an art gallery which is home to some of Raja Ravi Varma's masterpieces
Tipu Sultan's tomb at Srirangapatna
Lalitha Mahal at Mysore, now a five-star hotel, plays host to visiting dignitaries and VIPs.
The first iron-cased and metal-cylinder rocket artillery were developed by Tipu Sultan and his father Hyder Ali, in the 1780s. He successfully used these metal-cylinder rockets against the larger forces of the British East India Company during the Anglo-Mysore Wars. The Mysore rockets of this period were much more advanced than what the British had seen, chiefly because of the use of iron tubes for holding the propellant; this enabled higher thrust and longer range for the missile (up to 2 km (1 mi) range). After Tipu's eventual defeat in the Fourth Anglo-Mysore War and the capture of the Mysore iron rockets, they were influential in British rocket development, inspiring the Congreve rocket, which was soon put into use in the Napoleonic Wars.
According to Stephen Oliver Fought and John F. Guilmartin, Jr. in Encyclopædia Britannica (2008): 
Hyder Ali, prince of Mysore, developed war rockets with an important change: the use of metal cylinders to contain the combustion powder. Although the hammered soft iron he used was crude, the bursting strength of the container of black powder was much higher than the earlier paper construction. Thus a greater internal pressure was possible, with a resultant greater thrust of the propulsive jet. The rocket body was lashed with leather thongs to a long bamboo stick. Range was perhaps up to three-quarters of a mile (more than a kilometre). Although individually these rockets were not accurate, dispersion error became less important when large numbers were fired rapidly in mass attacks. They were particularly effective against cavalry and were hurled into the air, after lighting, or skimmed along the hard dry ground. Tipu Sultan, continued to develop and expand the use of rocket weapons, reportedly increasing the number of rocket troops from 1,200 to a corps of 5,000. In battles at Seringapatam in 1792 and 1799 these rockets were used with considerable effect against the British."

Coordinates: .mw-parser-output .geo-default,.mw-parser-output .geo-dms,.mw-parser-output .geo-dec{display:inline}.mw-parser-output .geo-nondefault,.mw-parser-output .geo-multi-punct{display:none}.mw-parser-output .longitude,.mw-parser-output .latitude{white-space:nowrap}12°18′N 76°39′E﻿ / ﻿12.30°N 76.65°E﻿ / 12.30; 76.65

India, officially the Republic of India (Hindi: Bhārat Gaṇarājya), is a country in South Asia. It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand, Myanmar and Indonesia.
Modern humans arrived on the Indian subcontinent from Africa no later than 55,000 years ago.
Their long occupation, initially in varying forms of isolation as hunter-gatherers, has made the region highly diverse, second only to Africa in human genetic diversity. Settled life emerged on the subcontinent in the western margins of the Indus river basin 9,000 years ago, evolving gradually into the Indus Valley civilisation of the third millennium BCE.
By 1200 BCE, an archaic form of Sanskrit, an Indo-European language, had diffused into India from the northwest, unfolding as the language of the Rigveda, and recording the dawning of Hinduism in India. The Dravidian languages of India were supplanted in the northern and western regions.
By 400 BCE, stratification and exclusion by caste had emerged within Hinduism,
and Buddhism and Jainism had arisen, proclaiming social orders unlinked to heredity.
Early political consolidations gave rise to the loose-knit Maurya and Gupta Empires based in the Ganges Basin.
Their collective era was suffused with wide-ranging creativity, but also marked by the declining status of women, and the incorporation of untouchability into an organised system of belief. In South India, the Middle kingdoms exported Dravidian-languages scripts and religious cultures to the kingdoms of Southeast Asia.
In the early medieval era, Christianity, Islam, Judaism, and Zoroastrianism became established on India's southern and western coasts.
Muslim armies from Central Asia intermittently overran India's northern plains,
eventually founding the Delhi Sultanate, and drawing northern India into the cosmopolitan networks of medieval Islam.
In the 15th century, the Vijayanagara Empire created a long-lasting composite Hindu culture in south India.
In the Punjab, Sikhism emerged, rejecting institutionalised religion.
The Mughal Empire, in 1526, ushered in two centuries of relative peace,
leaving a legacy of luminous architecture.
Gradually expanding rule of the British East India Company followed, turning India into a colonial economy, but also consolidating its sovereignty. British Crown rule began in 1858. The rights promised to Indians were granted slowly, but technological changes were introduced, and ideas of education, modernity and the public life took root. A pioneering and influential nationalist movement emerged, which was noted for nonviolent resistance and became the major factor in ending British rule. In 1947 the British Indian Empire was partitioned into two independent dominions, a Hindu-majority Dominion of India and a Muslim-majority Dominion of Pakistan, amid large-scale loss of life and an unprecedented migration.
India has been a federal republic since 1950, governed in a democratic parliamentary system. It is a pluralistic, multilingual and multi-ethnic society. India's population grew from 361 million in 1951 to 1.211 billion in 2011.
During the same time, its nominal per capita income increased from US$64 annually to US$1,498, and its literacy rate from 16.6% to 74%. From being a comparatively destitute country in 1951,
India has become a fast-growing major economy and a hub for information technology services, with an expanding middle class. It has a space programme which includes several planned or completed extraterrestrial missions. Indian movies, music, and spiritual teachings play an increasing role in global culture.
India has substantially reduced its rate of poverty, though at the cost of increasing economic inequality.
India is a nuclear-weapon state, which ranks high in military expenditure. It has disputes over Kashmir with its neighbours, Pakistan and China, unresolved since the mid-20th century.
Among the socio-economic challenges India faces are gender inequality, child malnutrition,
and rising levels of air pollution.
India's land is megadiverse, with four biodiversity hotspots. Its forest cover comprises 21.7% of its area. India's wildlife, which has traditionally been viewed with tolerance in India's culture, is supported among these forests, and elsewhere, in protected habitats.
According to the Oxford English Dictionary (third edition 2009), the name "India" is derived from the Classical Latin India, a reference to South Asia and an uncertain region to its east; and in turn derived successively from: Hellenistic Greek India ( Ἰνδία); ancient Greek Indos ( Ἰνδός); Old Persian Hindush, an eastern province of the Achaemenid empire; and ultimately its cognate, the Sanskrit Sindhu, or "river," specifically the Indus River and, by implication, its well-settled southern basin. The ancient Greeks referred to the Indians as Indoi (Ἰνδοί), which translates as "The people of the Indus".
The term Bharat (Bhārat; pronounced  (listen)), mentioned in both Indian epic poetry and the Constitution of India, is used in its variations by many Indian languages. A modern rendering of the historical name Bharatavarsha, which applied originally to North India, Bharat gained increased currency from the mid-19th century as a native name for India.
Hindustan ( (listen)) is a Middle Persian name for India, introduced during the Mughal Empire and used widely since. Its meaning has varied, referring to a region encompassing present-day northern India and Pakistan or to India in its near entirety.
By 55,000 years ago, the first modern humans, or Homo sapiens, had arrived on the Indian subcontinent from Africa, where they had earlier evolved. The earliest known modern human remains in South Asia date to about 30,000 years ago. After 6500 BCE, evidence for domestication of food crops and animals, construction of permanent structures, and storage of agricultural surplus appeared in Mehrgarh and other sites in what is now Balochistan, Pakistan. These gradually developed into the Indus Valley civilisation, the first urban culture in South Asia, which flourished during 2500–1900 BCE in what is now Pakistan and western India. Centred around cities such as Mohenjo-daro, Harappa, Dholavira, and Kalibangan, and relying on varied forms of subsistence, the civilisation engaged robustly in crafts production and wide-ranging trade.
During the period 2000–500 BCE, many regions of the subcontinent transitioned from the Chalcolithic cultures to the Iron Age ones. The Vedas, the oldest scriptures associated with Hinduism, were composed during this period, and historians have analysed these to posit a Vedic culture in the Punjab region and the upper Gangetic Plain. Most historians also consider this period to have encompassed several waves of Indo-Aryan migration into the subcontinent from the north-west. The caste system, which created a hierarchy of priests, warriors, and free peasants, but which excluded indigenous peoples by labelling their occupations impure, arose during this period. On the Deccan Plateau, archaeological evidence from this period suggests the existence of a chiefdom stage of political organisation. In South India, a progression to sedentary life is indicated by the large number of megalithic monuments dating from this period, as well as by nearby traces of agriculture, irrigation tanks, and craft traditions.
In the late Vedic period, around the 6th century BCE, the small states and chiefdoms of the Ganges Plain and the north-western regions had consolidated into 16 major oligarchies and monarchies that were known as the mahajanapadas. The emerging urbanisation gave rise to non-Vedic religious movements, two of which became independent religions. Jainism came into prominence during the life of its exemplar, Mahavira. Buddhism, based on the teachings of Gautama Buddha, attracted followers from all social classes excepting the middle class; chronicling the life of the Buddha was central to the beginnings of recorded history in India. In an age of increasing urban wealth, both religions held up renunciation as an ideal, and both established long-lasting monastic traditions. Politically, by the 3rd century BCE, the kingdom of Magadha had annexed or reduced other states to emerge as the Mauryan Empire. The empire was once thought to have controlled most of the subcontinent except the far south, but its core regions are now thought to have been separated by large autonomous areas. The Mauryan kings are known as much for their empire-building and determined management of public life as for Ashoka's renunciation of militarism and far-flung advocacy of the Buddhist dhamma.
The Sangam literature of the Tamil language reveals that, between 200 BCE and 200 CE, the southern peninsula was ruled by the Cheras, the Cholas, and the Pandyas, dynasties that traded extensively with the Roman Empire and with West and South-East Asia. In North India, Hinduism asserted patriarchal control within the family, leading to increased subordination of women. By the 4th and 5th centuries, the Gupta Empire had created a complex system of administration and taxation in the greater Ganges Plain; this system became a model for later Indian kingdoms. Under the Guptas, a renewed Hinduism based on devotion, rather than the management of ritual, began to assert itself. This renewal was reflected in a flowering of sculpture and architecture, which found patrons among an urban elite. Classical Sanskrit literature flowered as well, and Indian science, astronomy, medicine, and mathematics made significant advances.
The Indian early medieval age, from 600 to 1200 CE, is defined by regional kingdoms and cultural diversity. When Harsha of Kannauj, who ruled much of the Indo-Gangetic Plain from 606 to 647 CE, attempted to expand southwards, he was defeated by the Chalukya ruler of the Deccan. When his successor attempted to expand eastwards, he was defeated by the Pala king of Bengal. When the Chalukyas attempted to expand southwards, they were defeated by the Pallavas from farther south, who in turn were opposed by the Pandyas and the Cholas from still farther south. No ruler of this period was able to create an empire and consistently control lands much beyond their core region. During this time, pastoral peoples, whose land had been cleared to make way for the growing agricultural economy, were accommodated within caste society, as were new non-traditional ruling classes. The caste system consequently began to show regional differences.
In the 6th and 7th centuries, the first devotional hymns were created in the Tamil language. They were imitated all over India and led to both the resurgence of Hinduism and the development of all modern languages of the subcontinent. Indian royalty, big and small, and the temples they patronised drew citizens in great numbers to the capital cities, which became economic hubs as well. Temple towns of various sizes began to appear everywhere as India underwent another urbanisation. By the 8th and 9th centuries, the effects were felt in South-East Asia, as South Indian culture and political systems were exported to lands that became part of modern-day Myanmar, Thailand, Laos, Brunei, Cambodia, Vietnam, Philippines, Malaysia, and Indonesia. Indian merchants, scholars, and sometimes armies were involved in this transmission; South-East Asians took the initiative as well, with many sojourning in Indian seminaries and translating Buddhist and Hindu texts into their languages.
After the 10th century, Muslim Central Asian nomadic clans, using swift-horse cavalry and raising vast armies united by ethnicity and religion, repeatedly overran South Asia's north-western plains, leading eventually to the establishment of the Islamic Delhi Sultanate in 1206. The sultanate was to control much of North India and to make many forays into South India. Although at first disruptive for the Indian elites, the sultanate largely left its vast non-Muslim subject population to its own laws and customs. By repeatedly repulsing Mongol raiders in the 13th century, the sultanate saved India from the devastation visited on West and Central Asia, setting the scene for centuries of migration of fleeing soldiers, learned men, mystics, traders, artists, and artisans from that region into the subcontinent, thereby creating a syncretic Indo-Islamic culture in the north. The sultanate's raiding and weakening of the regional kingdoms of South India paved the way for the indigenous Vijayanagara Empire. Embracing a strong Shaivite tradition and building upon the military technology of the sultanate, the empire came to control much of peninsular India, and was to influence South Indian society for long afterwards.
In the early 16th century, northern India, then under mainly Muslim rulers, fell again to the superior mobility and firepower of a new generation of Central Asian warriors. The resulting Mughal Empire did not stamp out the local societies it came to rule. Instead, it balanced and pacified them through new administrative practices and diverse and inclusive ruling elites, leading to more systematic, centralised, and uniform rule. Eschewing tribal bonds and Islamic identity, especially under Akbar, the Mughals united their far-flung realms through loyalty, expressed through a Persianised culture, to an emperor who had near-divine status. The Mughal state's economic policies, deriving most revenues from agriculture and mandating that taxes be paid in the well-regulated silver currency, caused peasants and artisans to enter larger markets. The relative peace maintained by the empire during much of the 17th century was a factor in India's economic expansion, resulting in greater patronage of painting, literary forms, textiles, and architecture. Newly coherent social groups in northern and western India, such as the Marathas, the Rajputs, and the Sikhs, gained military and governing ambitions during Mughal rule, which, through collaboration or adversity, gave them both recognition and military experience. Expanding commerce during Mughal rule gave rise to new Indian commercial and political elites along the coasts of southern and eastern India. As the empire disintegrated, many among these elites were able to seek and control their own affairs.
By the early 18th century, with the lines between commercial and political dominance being increasingly blurred, a number of European trading companies, including the English East India Company, had established coastal outposts. The East India Company's control of the seas, greater resources, and more advanced military training and technology led it to increasingly assert its military strength and caused it to become attractive to a portion of the Indian elite; these factors were crucial in allowing the company to gain control over the Bengal region by 1765 and sideline the other European companies. Its further access to the riches of Bengal and the subsequent increased strength and size of its army enabled it to annexe or subdue most of India by the 1820s. India was then no longer exporting manufactured goods as it long had, but was instead supplying the British Empire with raw materials. Many historians consider this to be the onset of India's colonial period. By this time, with its economic power severely curtailed by the British parliament and having effectively been made an arm of British administration, the company began more consciously to enter non-economic arenas, including education, social reform and culture.
Historians consider India's modern age to have begun sometime between 1848 and 1885. The appointment in 1848 of Lord Dalhousie as Governor General of the East India Company set the stage for changes essential to a modern state. These included the consolidation and demarcation of sovereignty, the surveillance of the population, and the education of citizens. Technological changes—among them, railways, canals, and the telegraph—were introduced not long after their introduction in Europe. However, disaffection with the company also grew during this time and set off the Indian Rebellion of 1857. Fed by diverse resentments and perceptions, including invasive British-style social reforms, harsh land taxes, and summary treatment of some rich landowners and princes, the rebellion rocked many regions of northern and central India and shook the foundations of Company rule. Although the rebellion was suppressed by 1858, it led to the dissolution of the East India Company and the direct administration of India by the British government. Proclaiming a unitary state and a gradual but limited British-style parliamentary system, the new rulers also protected princes and landed gentry as a feudal safeguard against future unrest. In the decades following, public life gradually emerged all over India, leading eventually to the founding of the Indian National Congress in 1885.
The rush of technology and the commercialisation of agriculture in the second half of the 19th century was marked by economic setbacks and many small farmers became dependent on the whims of far-away markets. There was an increase in the number of large-scale famines, and, despite the risks of infrastructure development borne by Indian taxpayers, little industrial employment was generated for Indians. There were also salutary effects: commercial cropping, especially in the newly canalled Punjab, led to increased food production for internal consumption. The railway network provided critical famine relief, notably reduced the cost of moving goods, and helped nascent Indian-owned industry.
After World War I, in which approximately one million Indians served, a new period began. It was marked by British reforms but also repressive legislation, by more strident Indian calls for self-rule, and by the beginnings of a nonviolent movement of non-co-operation, of which Mohandas Karamchand Gandhi would become the leader and enduring symbol. During the 1930s, slow legislative reform was enacted by the British; the Indian National Congress won victories in the resulting elections. The next decade was beset with crises: Indian participation in World War II, the Congress's final push for non-co-operation, and an upsurge of Muslim nationalism. All were capped by the advent of independence in 1947, but tempered by the partition of India into two states: India and Pakistan.
Vital to India's self-image as an independent nation was its constitution, completed in 1950, which put in place a secular and democratic republic. It has remained a democracy with civil liberties, an active supreme court, and a largely independent press. Economic liberalisation, which began in the 1990s, has created a large urban middle class, transformed India into one of the world's fastest-growing economies, and increased its geopolitical clout. Indian movies, music, and spiritual teachings play an increasing role in global culture. Yet, India is also shaped by seemingly unyielding poverty, both rural and urban; by religious and caste-related violence; by Maoist-inspired Naxalite insurgencies; and by separatism in Jammu and Kashmir and in Northeast India. It has unresolved territorial disputes with China and with Pakistan. India's sustained democratic freedoms are unique among the world's newer nations; however, in spite of its recent economic successes, freedom from want for its disadvantaged population remains a goal yet to be achieved.
India accounts for the bulk of the Indian subcontinent, lying atop the Indian tectonic plate, a part of the Indo-Australian Plate. India's defining geological processes began 75 million years ago when the Indian Plate, then part of the southern supercontinent Gondwana, began a north-eastward drift caused by seafloor spreading to its south-west, and later, south and south-east. Simultaneously, the vast Tethyan oceanic crust, to its northeast, began to subduct under the Eurasian Plate. These dual processes, driven by convection in the Earth's mantle, both created the Indian Ocean and caused the Indian continental crust eventually to under-thrust Eurasia and to uplift the Himalayas. Immediately south of the emerging Himalayas, plate movement created a vast crescent-shaped trough that rapidly filled with river-borne sediment and now constitutes the Indo-Gangetic Plain.   The original Indian plate makes its first appearance above the sediment in the ancient Aravalli range, which extends from the Delhi Ridge in a southwesterly direction. To the west lies the Thar desert, the eastern spread of which is checked by the Aravallis.
The remaining Indian Plate survives as peninsular India, the oldest and geologically most stable part of India. It extends as far north as the Satpura and Vindhya ranges in central India. These parallel chains run from the Arabian Sea coast in Gujarat in the west to the coal-rich Chota Nagpur Plateau in Jharkhand in the east. To the south, the remaining peninsular landmass, the Deccan Plateau, is flanked on the west and east by coastal ranges known as the Western and Eastern Ghats; the plateau contains the country's oldest rock formations, some over one billion years old. Constituted in such fashion, India lies to the north of the equator between 6° 44′ and 35° 30′ north latitude and 68° 7′ and 97° 25′ east longitude.
India's coastline measures 7,517 kilometres (4,700 mi) in length; of this distance, 5,423 kilometres (3,400 mi) belong to peninsular India and 2,094 kilometres (1,300 mi) to the Andaman, Nicobar, and Lakshadweep island chains. According to the Indian naval hydrographic charts, the mainland coastline consists of the following: 43% sandy beaches; 11% rocky shores, including cliffs; and 46% mudflats or marshy shores.
Major Himalayan-origin rivers that substantially flow through India include the Ganges and the Brahmaputra, both of which drain into the Bay of Bengal. Important tributaries of the Ganges include the Yamuna and the Kosi; the latter's extremely low gradient, caused by long-term silt deposition, leads to severe floods and course changes. Major peninsular rivers, whose steeper gradients prevent their waters from flooding, include the Godavari, the Mahanadi, the Kaveri, and the Krishna, which also drain into the Bay of Bengal; and the Narmada and the Tapti, which drain into the Arabian Sea. Coastal features include the marshy Rann of Kutch of western India and the alluvial Sundarbans delta of eastern India; the latter is shared with Bangladesh. India has two archipelagos: the Lakshadweep, coral atolls off India's south-western coast; and the Andaman and Nicobar Islands, a volcanic chain in the Andaman Sea.
Indian climate is strongly influenced by the Himalayas and the Thar Desert, both of which drive the economically and culturally pivotal summer and winter monsoons. The Himalayas prevent cold Central Asian katabatic winds from blowing in, keeping the bulk of the Indian subcontinent warmer than most locations at similar latitudes. The Thar Desert plays a crucial role in attracting the moisture-laden south-west summer monsoon winds that, between June and October, provide the majority of India's rainfall. Four major climatic groupings predominate in India: tropical wet, tropical dry, subtropical humid, and montane.
Temperatures in India have risen by 0.7 °C (1.3 °F) between 1901 and 2018. Climate change in India is often thought to be the cause. The retreat of Himalayan glaciers has adversely affected the flow rate of the major Himalayan rivers, including the Ganges and the Brahmaputra. According to some current projections, the number and severity of droughts in India will have markedly increased by the end of the present century.
India is a megadiverse country, a term employed for 17 countries which display high biological diversity and contain many species exclusively indigenous, or endemic, to them. India is a habitat for 8.6% of all mammal species, 13.7% of bird species, 7.9% of reptile species, 6% of amphibian species, 12.2% of fish species, and 6.0% of all flowering plant species. Fully a third of Indian plant species are endemic. India also contains four of the world's 34 biodiversity hotspots, or regions that display significant habitat loss in the presence of high endemism.
According to official statistics, India's forest cover is 713,789 km2 (275,595 sq mi), which is 21.71% of the country's total land area. It can be subdivided further into broad categories of canopy density, or the proportion of the area of a forest covered by its tree canopy. Very dense forest, whose canopy density is greater than 70%, occupies 3.02% of India's land area. It predominates in the tropical moist forest of the Andaman Islands, the Western Ghats, and Northeast India. Moderately dense forest, whose canopy density is between 40% and 70%, occupies 9.39% of India's land area. It predominates in the temperate coniferous forest of the Himalayas, the moist deciduous sal forest of eastern India, and the dry deciduous teak forest of central and southern India. Open forest, whose canopy density is between 10% and 40%, occupies 9.26% of India's land area. India has two natural zones of thorn forest, one in the Deccan Plateau, immediately east of the Western Ghats, and the other in the western part of the Indo-Gangetic plain, now turned into rich agricultural land by irrigation, its features no longer visible.
Among the Indian subcontinent's notable indigenous trees are the astringent Azadirachta indica, or neem, which is widely used in rural Indian herbal medicine, and the luxuriant Ficus religiosa, or peepul, which is displayed on the ancient seals of Mohenjo-daro, and under which the Buddha is recorded in the Pali canon to have sought enlightenment.
Many Indian species have descended from those of Gondwana, the southern supercontinent from which India separated more than 100 million years ago. India's subsequent collision with Eurasia set off a mass exchange of species. However, volcanism and climatic changes later caused the extinction of many endemic Indian forms. Still later, mammals entered India from Asia through two zoogeographical passes flanking the Himalayas. This had the effect of lowering endemism among India's mammals, which stands at 12.6%, contrasting with 45.8% among reptiles and 55.8% among amphibians.} Among endemics are the vulnerable hooded leaf monkey and the threatened Beddome's toad of the Western Ghats.
India contains 172 IUCN-designated threatened animal species, or 2.9% of endangered forms. These include the endangered Bengal tiger and the Ganges river dolphin. Critically endangered species include: the gharial, a crocodilian; the great Indian bustard; and the Indian white-rumped vulture, which has become nearly extinct by having ingested the carrion of diclofenac-treated cattle. Before they were extensively utilized for agriculture and cleared for human settlement, the thorn forests of Punjab were mingled at intervals with open grasslands that were grazed by large herds of blackbuck preyed on by the Asiatic cheetah; the blackbuck, no longer extant in Punjab, is now severely endangered in India, and the cheetah is extinct. The pervasive and ecologically devastating human encroachment of recent decades has critically endangered Indian wildlife. In response, the system of national parks and protected areas, first established in 1935, was expanded substantially. In 1972, India enacted the Wildlife Protection Act and Project Tiger to safeguard crucial wilderness; the Forest Conservation Act was enacted in 1980 and amendments added in 1988. India hosts more than five hundred wildlife sanctuaries and thirteen biosphere reserves, four of which are part of the World Network of Biosphere Reserves; twenty-five wetlands are registered under the Ramsar Convention.
India is the world's most populous democracy. A parliamentary republic with a multi-party system, it has eight recognised national parties, including the Indian National Congress and the Bharatiya Janata Party (BJP), and more than 40 regional parties. The Congress is considered centre-left in Indian political culture, and the BJP right-wing. For most of the period between 1950—when India first became a republic—and the late 1980s, the Congress held a majority in the parliament. Since then, however, it has increasingly shared the political stage with the BJP, as well as with powerful regional parties which have often forced the creation of multi-party coalition governments at the centre.
In the Republic of India's first three general elections, in 1951, 1957, and 1962, the Jawaharlal Nehru-led Congress won easy victories. On Nehru's death in 1964, Lal Bahadur Shastri briefly became prime minister; he was succeeded, after his own unexpected death in 1966, by Nehru's daughter Indira Gandhi, who went on to lead the Congress to election victories in 1967 and 1971. Following public discontent with the state of emergency she declared in 1975, the Congress was voted out of power in 1977; the then-new Janata Party, which had opposed the emergency, was voted in. Its government lasted just over two years. Voted back into power in 1980, the Congress saw a change in leadership in 1984, when Indira Gandhi was assassinated; she was succeeded by her son Rajiv Gandhi, who won an easy victory in the general elections later that year. The Congress was voted out again in 1989 when a National Front coalition, led by the newly formed Janata Dal in alliance with the Left Front, won the elections; that government too proved relatively short-lived, lasting just under two years. Elections were held again in 1991; no party won an absolute majority. The Congress, as the largest single party, was able to form a minority government led by P. V. Narasimha Rao.
A two-year period of political turmoil followed the general election of 1996. Several short-lived alliances shared power at the centre. The BJP formed a government briefly in 1996; it was followed by two comparatively long-lasting United Front coalitions, which depended on external support. In 1998, the BJP was able to form a successful coalition, the National Democratic Alliance (NDA). Led by Atal Bihari Vajpayee, the NDA became the first non-Congress, coalition government to complete a five-year term. Again in the 2004 Indian general elections, no party won an absolute majority, but the Congress emerged as the largest single party, forming another successful coalition: the United Progressive Alliance (UPA). It had the support of left-leaning parties and MPs who opposed the BJP. The UPA returned to power in the 2009 general election with increased numbers, and it no longer required external support from India's communist parties. That year, Manmohan Singh became the first prime minister since Jawaharlal Nehru in 1957 and 1962 to be re-elected to a consecutive five-year term. In the 2014 general election, the BJP became the first political party since 1984 to win a majority and govern without the support of other parties. The incumbent prime minister is Narendra Modi, a former chief minister of Gujarat. On 20 July 2017, Ram Nath Kovind was elected India's 14th president and took the oath of office on 25 July 2017.
India is a federation with a parliamentary system governed under the Constitution of India—the country's supreme legal document. It is a constitutional republic and representative democracy, in which "majority rule is tempered by minority rights protected by law". Federalism in India defines the power distribution between the union and the states. The Constitution of India, which came into effect on 26 January 1950, originally stated India to be a "sovereign, democratic republic;" this characterisation was amended in 1971 to "a sovereign, socialist, secular, democratic republic". India's form of government, traditionally described as "quasi-federal" with a strong centre and weak states, has grown increasingly federal since the late 1990s as a result of political, economic, and social changes.
The Government of India comprises three branches:
India is a federal union comprising 28 states and 8 union territories. All states, as well as the union territories of Jammu and Kashmir, Puducherry and the National Capital Territory of Delhi, have elected legislatures and governments following the Westminster system of governance. The remaining five union territories are directly ruled by the central government through appointed administrators. In 1956, under the States Reorganisation Act, states were reorganised on a linguistic basis. There are over a quarter of a million local government bodies at city, town, block, district and village levels.

In the 1950s, India strongly supported decolonisation in Africa and Asia and played a leading role in the Non-Aligned Movement. After initially cordial relations with neighbouring China, India went to war with China in 1962, and was widely thought to have been humiliated. India has had tense relations with neighbouring Pakistan; the two nations have gone to war four times: in 1947, 1965, 1971, and 1999. Three of these wars were fought over the disputed territory of Kashmir, while the fourth, the 1971 war, followed from India's support for the independence of Bangladesh. In the late 1980s, the Indian military twice intervened abroad at the invitation of the host country: a peace-keeping operation in Sri Lanka between 1987 and 1990; and an armed intervention to prevent a 1988 coup d'état attempt in the Maldives. After the 1965 war with Pakistan, India began to pursue close military and economic ties with the Soviet Union; by the late 1960s, the Soviet Union was its largest arms supplier.
Aside from ongoing its special relationship with Russia, India has wide-ranging defence relations with Israel and France. In recent years, it has played key roles in the South Asian Association for Regional Cooperation and the World Trade Organization. The nation has provided 100,000 military and police personnel to serve in 35 UN peacekeeping operations across four continents. It participates in the East Asia Summit, the G8+5, and other multilateral forums. India has close economic ties with countries in South America, Asia, and Africa; it pursues a "Look East" policy that seeks to strengthen partnerships with the ASEAN nations, Japan, and South Korea that revolve around many issues, but especially those involving economic investment and regional security.
China's nuclear test of 1964, as well as its repeated threats to intervene in support of Pakistan in the 1965 war, convinced India to develop nuclear weapons. India conducted its first nuclear weapons test in 1974 and carried out additional underground testing in 1998. Despite criticism and military sanctions, India has signed neither the Comprehensive Nuclear-Test-Ban Treaty nor the Nuclear Non-Proliferation Treaty, considering both to be flawed and discriminatory. India maintains a "no first use" nuclear policy and is developing a nuclear triad capability as a part of its "Minimum Credible Deterrence" doctrine. It is developing a ballistic missile defence shield and, a fifth-generation fighter jet. Other indigenous military projects involve the design and implementation of Vikrant-class aircraft carriers and Arihant-class nuclear submarines.
Since the end of the Cold War, India has increased its economic, strategic, and military co-operation with the United States and the European Union. In 2008, a civilian nuclear agreement was signed between India and the United States. Although India possessed nuclear weapons at the time and was not a party to the Nuclear Non-Proliferation Treaty, it received waivers from the International Atomic Energy Agency and the Nuclear Suppliers Group, ending earlier restrictions on India's nuclear technology and commerce. As a consequence, India became the sixth de facto nuclear weapons state. India subsequently signed co-operation agreements involving civilian nuclear energy with Russia, France, the United Kingdom, and Canada.
The President of India is the supreme commander of the nation's armed forces; with 1.45 million active troops, they compose the world's second-largest military. It comprises the Indian Army, the Indian Navy, the Indian Air Force, and the Indian Coast Guard. The official Indian defence budget for 2011 was US$36.03 billion, or 1.83% of GDP. Defence expenditure was pegged at US$70.12 billion for fiscal year 2022–23 and, increased 9.8% than previous fiscal year. India is the world's second largest arms importer; between 2016 and 2020, it accounted for 9.5% of the total global arms imports. Much of the military expenditure was focused on defence against Pakistan and countering growing Chinese influence in the Indian Ocean. In May 2017, the Indian Space Research Organisation launched the South Asia Satellite, a gift from India to its neighbouring SAARC countries. In October 2018, India signed a US$5.43 billion (over ₹400 billion) agreement with Russia to procure four S-400 Triumf surface-to-air missile defence systems, Russia's most advanced long-range missile defence system.
According to the International Monetary Fund (IMF), the Indian economy in 2021 was nominally worth $3.04 trillion; it is the sixth-largest economy by market exchange rates, and is around $10.219 trillion, the third-largest by purchasing power parity (PPP). With its average annual GDP growth rate of 5.8% over the past two decades, and reaching 6.1% during 2011–2012, India is one of the world's fastest-growing economies. However, the country ranks 139th in the world in nominal GDP per capita and 118th in GDP per capita at PPP. Until 1991, all Indian governments followed protectionist policies that were influenced by socialist economics. Widespread state intervention and regulation largely walled the economy off from the outside world. An acute balance of payments crisis in 1991 forced the nation to liberalise its economy; since then it has moved slowly towards a free-market system by emphasising both foreign trade and direct investment inflows. India has been a member of World Trade Organization since 1 January 1995.
The 522-million-worker Indian labour force is the world's second-largest, as of 2017. The service sector makes up 55.6% of GDP, the industrial sector 26.3% and the agricultural sector 18.1%. India's foreign exchange remittances of US$87 billion in 2021, highest in the world, were contributed to its economy by 32 million Indians working in foreign countries. Major agricultural products include: rice, wheat, oilseed, cotton, jute, tea, sugarcane, and potatoes. Major industries include: textiles, telecommunications, chemicals, pharmaceuticals, biotechnology, food processing, steel, transport equipment, cement, mining, petroleum, machinery, and software. In 2006, the share of external trade in India's GDP stood at 24%, up from 6% in 1985. In 2008, India's share of world trade was 1.68%; In 2011, India was the world's tenth-largest importer and the nineteenth-largest exporter. Major exports include: petroleum products, textile goods, jewellery, software, engineering goods, chemicals, and manufactured leather goods. Major imports include: crude oil, machinery, gems, fertiliser, and chemicals. Between 2001 and 2011, the contribution of petrochemical and engineering goods to total exports grew from 14% to 42%. India was the world's second largest textile exporter after China in the 2013 calendar year.
Averaging an economic growth rate of 7.5% for several years prior to 2007, India has more than doubled its hourly wage rates during the first decade of the 21st century. Some 431 million Indians have left poverty since 1985; India's middle classes are projected to number around 580 million by 2030. Though ranking 51st in global competitiveness, as of 2010, India ranks 17th in financial market sophistication, 24th in the banking sector, 44th in business sophistication, and 39th in innovation, ahead of several advanced economies. With seven of the world's top 15 information technology outsourcing companies based in India, as of 2009, the country is viewed as the second-most favourable outsourcing destination after the United States. India was ranked 46th in the Global Innovation Index in 2021, it has increased its ranking considerably since 2015, where it was 81st. India's consumer market, the world's eleventh-largest, is expected to become fifth-largest by 2030.
Driven by growth, India's nominal GDP per capita increased steadily from US$308 in 1991, when economic liberalisation began, to US$1,380 in 2010, to an estimated US$1,730 in 2016. It is expected to grow to US$2,313 by 2022. However, it has remained lower than those of other Asian developing countries such as Indonesia, Malaysia, Philippines, Sri Lanka, and Thailand, and is expected to remain so in the near future.
According to a 2011 PricewaterhouseCoopers (PwC) report, India's GDP at purchasing power parity could overtake that of the United States by 2045. During the next four decades, Indian GDP is expected to grow at an annualised average of 8%, making it potentially the world's fastest-growing major economy until 2050. The report highlights key growth factors: a young and rapidly growing working-age population; growth in the manufacturing sector because of rising education and engineering skill levels; and sustained growth of the consumer market driven by a rapidly growing middle-class. The World Bank cautions that, for India to achieve its economic potential, it must continue to focus on public sector reform, transport infrastructure, agricultural and rural development, removal of labour regulations, education, energy security, and public health and nutrition.
According to the Worldwide Cost of Living Report 2017 released by the Economist Intelligence Unit (EIU) which was created by comparing more than 400 individual prices across 160 products and services, four of the cheapest cities were in India: Bangalore (3rd), Mumbai (5th), Chennai (5th) and New Delhi (8th).
India's telecommunication industry is the second-largest in the world with over 1.2 billion subscribers. It contributes 6.5% to India's GDP. After the third quarter of 2017, India surpassed the US to become the second largest smartphone market in the world after China.
The Indian automotive industry, the world's second-fastest growing, increased domestic sales by 26% during 2009–2010, and exports by 36% during 2008–2009. At the end of 2011, the Indian IT industry employed 2.8 million professionals, generated revenues close to US$100 billion equalling 7.5% of Indian GDP, and contributed 26% of India's merchandise exports.
The pharmaceutical industry in India emerged as a global player. As of 2021, with 3000 pharmaceutical companies and 10,500 manufacturing units India is the world's third-largest pharmaceutical producer, largest producer of generic medicines and supply up to 50%—60% of global vaccines demand, these all contribute up to US$24.44 billions in exports and India's local pharmacutical market is estimated up to US$42 billion. India is among the top 12 biotech destinations in the world. The Indian biotech industry grew by 15.1% in 2012–2013, increasing its revenues from ₹204.4 billion (Indian rupees) to ₹235.24 billion (US$3.94 billion at June 2013 exchange rates).
India's capacity to generate electrical power is 300 gigawatts, of which 42 gigawatts is renewable. The country's usage of coal is a major cause of greenhouse gas emissions by India but its renewable energy is competing strongly. India emits about 7% of global greenhouse gas emissions. This equates to about 2.5 tons of carbon dioxide per person per year, which is half the world average. Increasing access to electricity and clean cooking with liquefied petroleum gas have been priorities for energy in India.
Despite economic growth during recent decades, India continues to face socio-economic challenges. In 2006, India contained the largest number of people living below the World Bank's international poverty line of US$1.25 per day. The proportion decreased from 60% in 1981 to 42% in 2005. Under the World Bank's later revised poverty line, it was 21% in 2011. 30.7% of India's children under the age of five are underweight. According to a Food and Agriculture Organization report in 2015, 15% of the population is undernourished. The Mid-Day Meal Scheme attempts to lower these rates.
A 2018 Walk Free Foundation report estimated that nearly 8 million people in India were living in different forms of modern slavery, such as bonded labour, child labour, human trafficking, and forced begging, among others. According to the 2011 census, there were 10.1 million child labourers in the country, a decline of 2.6 million from 12.6 million in 2001.
Since 1991, economic inequality between India's states has consistently grown: the per-capita net state domestic product of the richest states in 2007 was 3.2 times that of the poorest. Corruption in India is perceived to have decreased. According to the Corruption Perceptions Index, India ranked 78th out of 180 countries in 2018 with a score of 41 out of 100, an improvement from 85th in 2014.
With 1,210,193,422 residents reported in the 2011 provisional census report, India is the world's second-most populous country. Its population grew by 17.64% from 2001 to 2011, compared to 21.54% growth in the previous decade (1991–2001). The human sex ratio, according to the 2011 census, is 940 females per 1,000 males. The median age was 28.7 as of 2020. The first post-colonial census, conducted in 1951, counted 361 million people. Medical advances made in the last 50 years as well as increased agricultural productivity brought about by the "Green Revolution" have caused India's population to grow rapidly.
The average life expectancy in India is at 70 years—71.5 years for women, 68.7 years for men. There are around 93 physicians per 100,000 people. Migration from rural to urban areas has been an important dynamic in India's recent history. The number of people living in urban areas grew by 31.2% between 1991 and 2001. Yet, in 2001, over 70% still lived in rural areas. The level of urbanisation increased further from 27.81% in the 2001 Census to 31.16% in the 2011 Census. The slowing down of the overall population growth rate was due to the sharp decline in the growth rate in rural areas since 1991. According to the 2011 census, there are 53 million-plus urban agglomerations in India; among them Mumbai, Delhi, Kolkata, Chennai, Bangalore, Hyderabad and Ahmedabad, in decreasing order by population. The literacy rate in 2011 was 74.04%: 65.46% among females and 82.14% among males. The rural-urban literacy gap, which was 21.2 percentage points in 2001, dropped to 16.1 percentage points in 2011. The improvement in the rural literacy rate is twice that of urban areas. Kerala is the most literate state with 93.91% literacy; while Bihar the least with 63.82%.
India is home to two major language families: Indo-Aryan (spoken by about 74% of the population) and Dravidian (spoken by 24% of the population). Other languages spoken in India come from the Austroasiatic and Sino-Tibetan language families. India has no national language. Hindi, with the largest number of speakers, is the official language of the government. English is used extensively in business and administration and has the status of a "subsidiary official language"; it is important in education, especially as a medium of higher education. Each state and union territory has one or more official languages, and the constitution recognises in particular 22 "scheduled languages".
The 2011 census reported the religion in India with the largest number of followers was Hinduism (79.80% of the population), followed by Islam (14.23%); the remaining were Christianity (2.30%), Sikhism (1.72%), Buddhism (0.70%), Jainism (0.36%) and others (0.9%). India has the third-largest Muslim population—the largest for a non-Muslim majority country.
Indian cultural history spans more than 4,500 years. During the Vedic period (c. 1700 BCE – c. 500 BCE), the foundations of Hindu philosophy, mythology, theology and literature were laid, and many beliefs and practices which still exist today, such as dhárma, kárma, yóga, and mokṣa, were established. India is notable for its religious diversity, with Hinduism, Buddhism, Sikhism, Islam, Christianity, and Jainism among the nation's major religions. The predominant religion, Hinduism, has been shaped by various historical schools of thought, including those of the Upanishads, the Yoga Sutras, the Bhakti movement, and by Buddhist philosophy.
India has a very ancient tradition of art, which has exchanged many influences with the rest of Eurasia, especially in the first millennium, when Buddhist art spread with Indian religions to Central, East and South-East Asia, the last also greatly influenced by Hindu art.  Thousands of seals from the Indus Valley Civilization of the third millennium BCE have been found, usually carved with animals, but a few with human figures. The "Pashupati" seal, excavated in Mohenjo-daro, Pakistan, in 1928–29, is the best known.  After this there is a long period with virtually nothing surviving. Almost all surviving ancient Indian art thereafter is in various forms of religious sculpture in durable materials, or coins. There was probably originally far more in wood, which is lost. In north India Mauryan art is the first imperial movement. In the first millennium CE, Buddhist art spread with Indian religions to Central, East and South-East Asia, the last also greatly influenced by Hindu art. Over the following centuries a distinctly Indian style of sculpting the human figure developed, with less interest in articulating precise anatomy than ancient Greek sculpture but showing smoothly-flowing forms expressing prana ("breath" or life-force). This is often complicated by the need to give figures multiple arms or heads, or represent different genders on the left and right of figures, as with the Ardhanarishvara form of Shiva and Parvati.
Most of the earliest large sculpture is Buddhist, either excavated from Buddhist stupas such as Sanchi, Sarnath and Amaravati, or is rock-cut reliefs at sites such as Ajanta, Karla and Ellora. Hindu and Jain sites appear rather later. In spite of this complex mixture of religious traditions, generally, the prevailing artistic style at any time and place has been shared by the major religious groups, and sculptors probably usually served all communities. Gupta art, at its peak c. 300 CE – c. 500 CE, is often regarded as a classical period whose influence lingered for many centuries after; it saw a new dominance of Hindu sculpture, as at the Elephanta Caves. Across the north, this became rather stiff and formulaic after c. 800 CE, though rich with finely carved detail in the surrounds of statues. But in the South, under the Pallava and Chola dynasties, sculpture in both stone and bronze had a sustained period of great achievement; the large bronzes with Shiva as Nataraja have become an iconic symbol of India.
Ancient painting has only survived at a few sites, of which the crowded scenes of court life in the Ajanta Caves are by far the most important, but it was evidently highly developed, and is mentioned as a courtly accomplishment in Gupta times. Painted manuscripts of religious texts survive from Eastern India about the 10th century onwards, most of the earliest being Buddhist and later Jain. No doubt the style of these was used in larger paintings. The Persian-derived Deccan painting, starting just before the Mughal miniature, between them give the first large body of secular painting, with an emphasis on portraits, and the recording of princely pleasures and wars. The style spread to Hindu courts, especially among the Rajputs, and developed a variety of styles, with the smaller courts often the most innovative, with figures such as Nihâl Chand and Nainsukh. As a market developed among European residents, it was supplied by Company painting by Indian artists with considerable Western influence. In the 19th century, cheap Kalighat paintings of gods and everyday life, done on paper, were urban folk art from Calcutta, which later saw the Bengal School of Art, reflecting the art colleges founded by the British, the first movement in modern Indian painting.
Bhutesvara Yakshis, Buddhist reliefs from Mathura, 2nd century CE
Gupta terracotta relief, Krishna Killing the Horse Demon Keshi, 5th century
Elephanta Caves, triple-bust (trimurti) of Shiva, 18 feet (5.5 m) tall, c. 550
Chola bronze of Shiva as Nataraja ("Lord of Dance"), Tamil Nadu, 10th or 11th century.
Jahangir Receives Prince Khurram at Ajmer on His Return from the Mewar Campaign, Balchand,  c. 1635
Krishna Fluting to the Milkmaids, Kangra painting, 1775–1785
Much of Indian architecture, including the Taj Mahal, other works of Indo-Islamic Mughal architecture,  and South Indian architecture, blends ancient local traditions with imported styles. Vernacular architecture is also regional in its flavours. Vastu shastra, literally "science of construction" or "architecture" and ascribed to Mamuni Mayan, explores how the laws of nature affect human dwellings; it employs precise geometry and directional alignments to reflect perceived cosmic constructs. As applied in Hindu temple architecture, it is influenced by the Shilpa Shastras, a series of foundational texts whose basic mythological form is the Vastu-Purusha mandala, a square that embodied the "absolute". The Taj Mahal, built in Agra between 1631 and 1648 by orders of Mughal emperor Shah Jahan in memory of his wife, has been described in the UNESCO World Heritage List as "the jewel of Muslim art in India and one of the universally admired masterpieces of the world's heritage". Indo-Saracenic Revival architecture, developed by the British in the late 19th century, drew on Indo-Islamic architecture.
The earliest literature in India, composed between 1500 BCE and 1200 CE, was in the Sanskrit language. Major works of Sanskrit literature include the Rigveda (c. 1500 BCE – c. 1200 BCE), the epics: Mahābhārata ( c. 400 BCE – c. 400 CE) and the Ramayana ( c. 300 BCE and later); Abhijñānaśākuntalam (The Recognition of Śakuntalā, and other dramas of Kālidāsa ( c. 5th century CE) and Mahākāvya poetry. In Tamil literature, the Sangam literature (c. 600 BCE – c. 300 BCE) consisting of 2,381 poems, composed by 473 poets, is the earliest work. From the 14th to the 18th centuries, India's literary traditions went through a period of drastic change because of the emergence of devotional poets like Kabīr, Tulsīdās, and Guru Nānak. This period was characterised by a varied and wide spectrum of thought and expression; as a consequence, medieval Indian literary works differed significantly from classical traditions. In the 19th century, Indian writers took a new interest in social questions and psychological descriptions. In the 20th century, Indian literature was influenced by the works of the Bengali poet, author and philosopher Rabindranath Tagore, who was a recipient of the Nobel Prize in Literature.
Indian music ranges over various traditions and regional styles. Classical music encompasses two genres and their various folk offshoots: the northern Hindustani and southern Carnatic schools. Regionalised popular forms include filmi and folk music; the syncretic tradition of the bauls is a well-known form of the latter. Indian dance also features diverse folk and classical forms. Among the better-known folk dances are: the bhangra of Punjab, the bihu of Assam, the Jhumair and chhau of Jharkhand, Odisha and West Bengal, garba and dandiya of Gujarat, ghoomar of Rajasthan, and the lavani of Maharashtra. Eight dance forms, many with narrative forms and mythological elements, have been accorded classical dance status by India's National Academy of Music, Dance, and Drama. These are: bharatanatyam of the state of Tamil Nadu, kathak of Uttar Pradesh, kathakali and mohiniyattam of Kerala, kuchipudi of Andhra Pradesh, manipuri of Manipur, odissi of Odisha, and the sattriya of Assam.
Theatre in India melds music, dance, and improvised or written dialogue. Often based on Hindu mythology, but also borrowing from medieval romances or social and political events, Indian theatre includes: the bhavai of Gujarat, the jatra of West Bengal, the nautanki and ramlila of North India, tamasha of Maharashtra, burrakatha of Andhra Pradesh and Telangana, terukkuttu of Tamil Nadu, and the yakshagana of Karnataka. India has a theatre training institute the National School of Drama (NSD) that is situated at New Delhi It is an autonomous organisation under the Ministry of culture, Government of India.
The Indian film industry produces the world's most-watched cinema. Established regional cinematic traditions exist in the Assamese, Bengali, Bhojpuri, Hindi, Kannada, Malayalam, Punjabi, Gujarati, Marathi, Odia, Tamil, and Telugu languages. The Hindi language film industry (Bollywood) is the largest sector representing 43% of box office revenue, followed by the South Indian Telugu and Tamil film industries which represent 36% combined.
Television broadcasting began in India in 1959 as a state-run medium of communication and expanded slowly for more than two decades. The state monopoly on television broadcast ended in the 1990s. Since then, satellite channels have increasingly shaped the popular culture of Indian society. Today, television is the most penetrative media in India; industry estimates indicate that as of 2012 there are over 554 million TV consumers, 462 million with satellite or cable connections compared to other forms of mass media such as the press (350 million), radio (156 million) or internet (37 million).
Traditional Indian society is sometimes defined by social hierarchy. The Indian caste system embodies much of the social stratification and many of the social restrictions found on the Indian subcontinent. Social classes are defined by thousands of endogamous hereditary groups, often termed as jātis, or "castes". India declared untouchability to be illegal in 1947 and has since enacted other anti-discriminatory laws and social welfare initiatives.
Family values are important in the Indian tradition, and multi-generational patrilineal joint families have been the norm in India, though nuclear families are becoming common in urban areas. An overwhelming majority of Indians, with their consent, have their marriages arranged by their parents or other family elders. Marriage is thought to be for life, and the divorce rate is extremely low, with less than one in a thousand marriages ending in divorce. Child marriages are common, especially in rural areas; many women wed before reaching 18, which is their legal marriageable age. Female infanticide in India, and lately female foeticide, have created skewed gender ratios; the number of missing women in the country quadrupled from 15 million to 63 million in the 50-year period ending in 2014, faster than the population growth during the same period, and constituting 20 percent of India's female electorate. Accord to an Indian government study, an additional 21 million girls are unwanted and do not receive adequate care. Despite a government ban on sex-selective foeticide, the practice remains commonplace in India, the result of a preference for boys in a patriarchal society. The payment of dowry, although illegal, remains widespread across class lines. Deaths resulting from dowry, mostly from bride burning, are on the rise, despite stringent anti-dowry laws.
Many Indian festivals are religious in origin. The best known include: Diwali, Ganesh Chaturthi, Thai Pongal, Holi, Durga Puja, Eid ul-Fitr, Bakr-Id, Christmas, and Vaisakhi.
In the 2011 census, about 73% of the population was literate, with 81% for men and 65% for women. This compares to 1981 when the respective rates were 41%, 53% and 29%. In 1951 the rates were 18%, 27% and 9%. In 1921 the rates 7%, 12% and 2%. In 1891 they were 5%, 9% and 1%, According to Latika Chaudhary, in 1911 there were under three primary schools for every ten villages. Statistically, more caste and religious diversity reduced private spending. Primary schools taught literacy, so local diversity limited its growth.
The education system of India is the world's second-largest. India has over 900 universities, 40,000 colleges and 1.5 million schools. In India's higher education system, a significant number of seats are reserved under affirmative action policies for the historically disadvantaged. In recent decades India's improved education system is often cited as one of the main contributors to its economic development.
From ancient times until the advent of the modern, the most widely worn traditional dress in India was draped. For women it took the form of a sari, a single piece of cloth many yards long. The sari was traditionally wrapped around the lower body and the shoulder. In its modern form, it is combined with an underskirt, or Indian petticoat, and tucked in the waist band for more secure fastening. It is also commonly worn with an Indian blouse, or choli, which serves as the primary upper-body garment, the sari's end—passing over the shoulder—serving to cover the midriff and obscure the upper body's contours. For men, a similar but shorter length of cloth, the dhoti, has served as a lower-body garment.
The use of stitched clothes became widespread after Muslim rule was established at first by the Delhi sultanate (ca 1300 CE) and then continued by the Mughal Empire (ca 1525 CE). Among the garments introduced during this time and still commonly worn are: the shalwars and pyjamas, both styles of trousers, and the tunics kurta and kameez. In southern India, the traditional draped garments were to see much longer continuous use.
Shalwars are atypically wide at the waist but narrow to a cuffed bottom. They are held up by a drawstring, which causes them to become pleated around the waist. The pants can be wide and baggy, or they can be cut quite narrow, on the bias, in which case they are called churidars. When they are ordinarily wide at the waist and their bottoms are hemmed but not cuffed, they are called pyjamas. The kameez is a long shirt or tunic, its side seams left open below the waist-line.  The kurta is traditionally collarless and made of cotton or silk; it is worn plain or with embroidered decoration, such as chikan; and typically falls to either just above or just below the wearer's knees.
In the last 50 years, fashions have changed a great deal in India. Increasingly, in urban northern India, the sari is no longer the apparel of everyday wear, though they remain popular on formal occasions. The traditional shalwar kameez is rarely worn by younger urban women, who favour churidars or jeans.  In white-collar office settings, ubiquitous air conditioning allows men to wear sports jackets year-round. For weddings and formal occasions, men in the middle- and upper classes often wear bandgala, or short Nehru jackets, with pants, with the groom and his groomsmen sporting sherwanis and churidars. The dhoti, once the universal garment of Hindu males, the wearing of which in the homespun and handwoven khadi allowed Gandhi to bring Indian nationalism to the millions,
is seldom seen in the cities.
The foundation of a typical Indian meal is a cereal cooked in a plain fashion and complemented with flavourful savoury dishes. The cooked cereal could be steamed rice; chapati, a thin unleavened bread made from wheat flour, or occasionally cornmeal, and griddle-cooked dry; the idli, a steamed breakfast cake, or dosa, a griddled pancake, both leavened and made from a batter of rice- and gram meal.  The savoury dishes might include lentils, pulses and vegetables commonly spiced with ginger and garlic, but also with a combination of spices that may include coriander, cumin, turmeric, cinnamon, cardamon and others as informed by culinary conventions. They might also include poultry, fish, or meat dishes. In some instances, the ingredients might be mixed during the process of cooking.
A platter, or thali, used for eating usually has a central place reserved for the cooked cereal, and peripheral ones for the flavourful accompaniments, which are often served in small bowls. The cereal and its accompaniments are eaten simultaneously rather than a piecemeal manner.  This is accomplished by mixing—for example of rice and lentils—or folding, wrapping, scooping or dipping—such as chapati and cooked vegetables or lentils.
India has distinctive vegetarian cuisines, each a feature of the geographical and cultural histories of its adherents. The appearance of ahimsa, or the avoidance of violence toward all forms of life in many religious orders early in Indian history, especially Upanishadic Hinduism, Buddhism and Jainism, is thought to have contributed to the predominance of vegetarianism among a large segment of India's Hindu population, especially in southern India, Gujarat, the Hindi-speaking belt of north-central India, as well as among Jains.  Although meat is eaten widely in India,  the proportional consumption of meat in the overall diet is low. Unlike China, which has increased its per capita meat consumption substantially in its years of increased economic growth, in India the strong dietary traditions have contributed to dairy, rather than meat, becoming the preferred form of animal protein consumption.
The most significant import of cooking techniques into India during the last millennium occurred during the Mughal Empire. Dishes such as the pilaf, developed in the Abbasid caliphate, and cooking techniques such as the marinating of meat in yogurt, spread into northern India from regions to its northwest.  To the simple yogurt marinade of Persia, onions, garlic, almonds, and spices began to be added in India. Rice was partially cooked and layered alternately with the sauteed meat, the pot sealed tightly, and slow cooked according to another Persian cooking technique, to produce what has today become the Indian biryani, a feature of festive dining in many parts of India. In the food served in Indian restaurants worldwide the diversity of Indian food has been partially concealed by the dominance of Punjabi cuisine. The popularity of tandoori chicken—cooked in the tandoor oven, which had traditionally been used for baking bread in the rural Punjab and the Delhi region, especially among Muslims, but which is originally from Central Asia—dates to the 1950s, and was caused in large part by an entrepreneurial response among people from the Punjab who had been displaced by the 1947 partition of India.
Several traditional indigenous sports such as kabaddi, kho kho, pehlwani and gilli-danda, and also martial arts, such as Kalarippayattu and marma adi remain popular. Chess is commonly held to have originated in India as chaturaṅga; There has been a rise in the number of Indian grandmasters. Viswanathan Anand became the undisputed Chess World Champion in 2007 and held the status until 2013. Parcheesi is derived from Pachisi another traditional Indian pastime, which in early modern times was played on a giant marble court by Mughal emperor Akbar the Great.
Cricket is the most popular sport in India. Major domestic competitions include the Indian Premier League, which is the most-watched cricket league in the world and ranks sixth among all sports leagues. Other professional leagues include the Indian Super League (football) and the pro Kabaddi league.
India has won two ODI Cricket world cups, the 1983 edition and the 2011 edition and has eight field hockey gold medals in the summer olympics
The improved results garnered by the Indian Davis Cup team and other Indian tennis players in the early 2010s have made tennis increasingly popular in the country. India has a comparatively strong presence in shooting sports, and has won several medals at the Olympics, the World Shooting Championships, and the Commonwealth Games. Other sports in which Indians have succeeded internationally include badminton (Saina Nehwal and P. V. Sindhu are two of the top-ranked female badminton players in the world), boxing, and wrestling. Football is popular in West Bengal, Goa, Tamil Nadu, Kerala, and the north-eastern states.
India has hosted or co-hosted several international sporting events: the 1951 and 1982 Asian Games; the 1987, 1996, and 2011 Cricket World Cup tournaments; the 2003 Afro-Asian Games; the 2006 ICC Champions Trophy; the 2009 World Badminton Championships; the 2010 Hockey World Cup; the 2010 Commonwealth Games; and the 2017 FIFA U-17 World Cup. Major international sporting events held annually in India include the Maharashtra Open, the Mumbai Marathon, the Delhi Half Marathon, and the Indian Masters. The first Formula 1 Indian Grand Prix featured in late 2011 but has been discontinued from the F1 season calendar since 2014. India has traditionally been the dominant country at the South Asian Games. An example of this dominance is the basketball competition where the Indian team won three out of four tournaments to date.
Overview
Etymology
History
Geography
Biodiversity
Politics
Foreign relations and military
Economy
Demographics
Art
Culture
Government
General information
Coordinates: 21°N 78°E﻿ / ﻿21°N 78°E﻿ / 21; 78

Hyder Ali, Haidarālī (c. 1720 – 7 December 1782) was the Sultan and de facto ruler of the Kingdom of Mysore in southern India. Born as Hyder Ali, he distinguished himself as a soldier, eventually drawing the attention of Mysore's rulers. Rising to the post of Dalavayi (commander-in-chief) to Krishnaraja Wodeyar II, he came to dominate the titular monarch and the Mysore government. He became the de facto ruler of Mysore as Sarvadhikari (Chief Minister) by 1761. During intermittent conflicts against the East India Company during the First and Second Anglo–Mysore Wars, Hyder Ali was the military leader.
Though illiterate, Hyder Ali concluded an alliance with the French, and used the services of French workmen in raising his artillery and arsenal. His rule of Mysore was characterised by frequent warfare with his neighbours and rebellion within his territories. This was not unusual for the time as much of the Indian subcontinent was then in turmoil. He left his eldest son, Tipu Sultan, an extensive kingdom bordered by the Krishna River in the north, the Eastern Ghats in the east and the Arabian Sea in the west.
The exact date of Hyder Ali's birth is not known with certainty. He was born to Fath Muhammad.Ibrahim Saheb was the Maternal Uncle of Hyder Ali's. Various historical sources provide dates ranging between 1717 and 1722 for his birth. There are also some variations in reports of his ancestry. According to some accounts, his grandfather was descended from a line tracing their lineage back to Baghdad, while another traces his lineage instead to the area of present-day Afghanistan In a third account, written by one of his French military officers, Hyder himself claimed descent from the Bani Hashim clan of the Quraysh. His father, Fath Muhammad, was born in Kolar, and served as a commander of 50 men in the bamboo rocket artillery (mainly used for signalling) in the army of the Nawab of Carnatic. Fath Muhammad eventually entered the service of the Wodeyar Rajas of the Kingdom of Mysore, where he rose to become a powerful military commander. The Wodeyars awarded him Budikote as a jagir (land grant), where he then served as Naik (Lord).
Hyder Ali was born in Budikote, Kolar district; he was Fath Muhammad's fifth child, and the second by his third wife. His early years are not well documented; he entered military service along with his brother Shahbaz after their father died in combat. After serving for a number of years under the rulers of Arcot, they came to Srirangapatna, where Hyder's uncle served. He introduced them to Devaraja, the dalwai (chief minister, military leader, and virtual ruler) of Krishnaraja Wodeyar II, and his brother Nanjaraja, who also held important ministerial posts. hyder and his brother were both given commands in the Mysorean army; Hyder served under Shahbaz, commanding 100 cavalry and 2,000 infantry.
In 1748, Qamar-ud-din Khan, Asaf Jah I, the longtime Nizam of Hyderabad, died. The struggle to succeed him is known as the Second Carnatic War, and pitted Asaf Jah's son Nasir Jung against a nephew, Muzaffar Jung.
Both sides were supported by other local leaders, and French and British forces were also involved.
Devaraja had started vesting more military authority in his brother, and in 1749 Nanjaraja marched the Mysorean army in support of Nasir Jung. The army went to Devanhalli, where the Mysoreans participated in the Siege of Devanahalli Fort.
The fort was held by Muzaffar Jung's forces and the siege was conducted by the Marquis de Bussy. During the successful eight-month siege, Hyder Ali and his brother distinguished themselves, and were rewarded by the dalwai with enlarged commands.
Although Hyder Ali was from Mysore his early loyalties were to the "Nizam of Hyderabad", though whom Hyder Ali and his companions became Sepoys in the Deccan with partial investiture from the "Great Moghul" of that period.
By 1755 Hyder Ali commanded 3,000 infantry and 1,500 cavalry, and was reported to be enriching himself on campaigns by plunder. In that year he was also appointed Faujdar (military commander) of Dindigul. In this position he first retained French advisers to organise and train his artillery companies. He is also known to have personally served alongside de Bussy, and is believed to have met both Muzaffar Jung and Chanda Shahib.
In these early wars he also came to dislike and mistrust Muhammed Ali Khan Wallajah, the Nawab of the Carnatic. In fact Muhammed Ali Khan Wallajah and the Mysorean leaders were long at odds with each other, seeking territorial gains at the other's expense. Muhammad Ali Khan Wallajah had by then formed an alliance with the British, and he was accused by Hyder Ali in later years of effectively preventing him from making any sort of long-lasting alliances or agreements with the British.
Throughout the Carnatic Wars, Hyder Ali and his Mysore battalions served alongside French commanders such as Joseph Francois Dupleix, Count de Lally and de Bussy, he also assisted Chanda Sahib on various occasions. Hyder Ali supported the claims of Muzaffar Jung and later sided with Salabat Jung.
Early in his career, Hyder Ali retained as one of his chief financial assistants a Brahmin named Khande Rao. Hyder Ali, who was illiterate, was reported to be blessed with a prodigious memory and numerical acumen.
Hyder Ali could rival or outperform expert accountants with his great arithmetic skills and worked to develop a system, with Rao, that included checks and balances so sophisticated that all manner of income, including plunder of physical goods of all types, could be accounted for with little possibility for fraud or embezzlement.
This financial management may have played a role in Hyder Ali's rise in power.
In 1757 Hyder Ali was called to Srirangapatam to support Devaraja against threats from Hyderabad and the Marathas. Upon his arrival he found the Mysorean army in disarray and near mutiny over pay. While Devaraja bought his way out of the threats to Seringapatam, Hyder Ali arranged for the army to be paid and arrested the ringleaders of the mutiny.
In 1757, to resist the invasion of the Zamorin of Calicut, the Palakkad Raja sought the help of Hyder Ali. Hyder Ali then led campaigns against the Zamorin of Calicut in the Malabar Coast of India. In 1766, Hyder Ali defeated the Zamorin of Kozhikode – an East India Company ally at the time – and absorbed Kozhikode into his state. The smaller princely states in northern and north-central parts of modern-day state of Kerala (Malabar region) including Kolathunadu, Kottayam, Kadathanadu, Kozhikode, Tanur, Valluvanad, and Palakkad were unified under the rulers of Mysore and were made a part of the larger Kingdom of Mysore. For his role in these activities Hyder Ali was rewarded by Devaraja with the jaghir (regional governorship) of Bangalore.
In 1758 Hyder Ali successfully forced the Marathas to lift a siege of Bangalore. Hyder Ali's forces entered the city, thus capturing it.
By 1759 Hyder Ali was in command of the entire Mysorean army.
The young King of Mysore Krishnaraja Wodeyar II rewarded Hyder Ali's performance by granting him the title of Fath Hyder Bahadur or Nawab Hyder Ali Khan. Hyder Ali is also known to be the first ruler of Mysore to be granted the title of Nawab, thus it can be said that he was briefly the "Nawab of Mysore" by 1759.
Because of the ongoing conflicts with the Marathas the Mysorean treasury was virtually bankrupted, prompting the queen mother to force into exile Nanjaraj, who had assumed the position of dalwai upon his brother's death in 1758. Hyder Ali was a beneficiary of this action, rising in influence in the court.
In 1760 the queen mother conspired with Khande Rao, who had gone into the raja's service, to oust Hyder Ali. He was precipitously forced out of Seringapatam, leaving his family, including his son Tipu Sultan, under house arrest.
The sudden departure left Hyder Ali with few resources. He may have been fortuitously aided at this time by the faraway Third Battle of Panipat, in which the Marathas suffered a major defeat, Jan 1761. Because of this loss, the Marathas withdrew forces from Mysore and Hyder Ali's brother-in-law Makdum Ali chased them into Bidnur and Sunda.
Hyder Ali soon consolidated his strength by placing Mirza Sahib as the commander of Sira, Ibrahim Ali Khan in Bangalore and Amin Sahib his cousin in Basnagar. Soon afterward Hyder Ali marched alongside Makdum Ali's forces, which numbered about 6,000, along with the 3,000 men from his garrison at Bangalore, toward Seringapatam.
They clashed with Khande Rao's forces before reaching the capital. Khande Rao, with 11,000 men, won the battle, and Hyder Ali was forced to apply to the exiled Nanjaraj for support. Nanjaraj gave him command of his army, and the title of Dalwai.
With this force Hyder Ali again moved out against Khande Rao. The two armies faced each other again, but a deception by Hyder Ali convinced Khande Rao to flee instead of engaging in battle. Hyder Ali sent letters appearing to be from Nanjaraj to some of Khande Rao's commanders, confirming their agreement to hand Khande Rao over to Hyder Ali. Fearing a conspiracy, Khande Rao fled into Seringapatam.
After a minor battle against the now-leaderless army, Hyder Ali took over most of its remnants and surrounded Seringapatam. The ensuing negotiations left Hyder Ali in nearly complete military control of Mysore. Concessions that he extracted included the surrender of Khande Rao, who Hyder Ali imprisoned in Bangalore.
Hyder Ali assumed control of Mysore in 1761 after overthrowing the prime minster and making the king, Krishnaraja Wodeyar II, a prisoner in his own palace.
As Hyder Ali's control progressed, the Mughal Emperor Shah Alam II became the pensioner of the East India Company by the year 1765.
Hyder Ali formally styled himself Sultan Hyder Ali Khan in his correspondence with the Mughal Emperor Shah Alam II. Hyder Ali retained his title during the first Anglo-Mysore War that raged in 1766, and onwards.
He was very cautious in his diplomacy with the Nizam of Hyderabad, who was, according to an official Mughal firman, the sovereign of all Muslim-ruled territories in southern India.
The English and the Marathas continued to refer to Hyder Ali and later his son Tipu Sultan as "Nabobs".
Seal of Mysore.
The flag of the Sultanate of Mysore at the entrance into the fort of Bangalore.
Over the next few years Hyder expanded his territories to the north. Two key acquisitions were Sira, taken from the Marathas, and the kingdom of Bednore, where as a casus belli he agreed to support a claimant to its throne against usurpers. In 1763 he took its capital, Ikkeri, which included a large treasury. He renamed the capital Haidernagar, and began styling himself Hyder Ali Khan Bahadur, a title that had been bestowed on him by Salabat Jung as reward for his taking of Sira. He moved most of his family to Ikkeri, a natural fortress, in the hopes that it would "serve him for a safe refuge". He assumed the trappings of the ruler of Bednore, began issuing coins, and established a system of weights and measures. He made sure his son Tipu received a quality education, "employing learned tutors" and "appointing a suitable hand of attendants" to see to his upbringing. He cultivated a suspicion of foreigners, specifically refusing to allow the East India Company to have a resident at his court. His security, however, was not assured in Bednore: a bout of illness and a widespread conspiracy against him convinced him that it would not make an ideal capital for his domain, and he returned to Mysore.
The taking of Bednore included several ports on the Malabar coast, including Mangalore. Hyder used these ports to establish a small navy. The documentary record on the navy is fragmentary; Portuguese records indicate that the fleet was launched sometime between 1763 and 1765. It was apparently officered by Europeans, and its first admiral was an Englishman; by 1768 its admiral was a Mysorean cavalry officer named Ali Bey (or Lutf Ali Beg), apparently chosen by Hyder because he did not trust the European captains.
Hyder had amicable relations with the Christian population in Mangalore, which had long been under Portuguese influence and had a sizeable Roman Catholic population, and with Christians in general. He had a very close friendship with two Goan Catholic clergymen, Bishop Noronha and Fr. Joachim Miranda, and allowed a Protestant missionary to live at his court. Hyder's army also included Catholic soldiers, and he allowed Christians to build a church at Seringapatam, where French generals used to offer prayers and priests used to visit. Mangalorean historian A. L. P. D'Souza mentions that Hyder also had Christians in his administration. Pursuant to treaties concluded with the Portuguese, he also allowed Portuguese priests to settle disputes among Christians. However, many Mangaloreans (not just Christians) disliked him for the heavy tax burden he imposed on them.
The Maratha Confederacy had just been routed at the Third Battle of Panipat by Ahmad Shah Durrani and the Mughals had been restored in the year 1761.
The Maratha Empire was most vulnerable and feeble to any attack and the Peshwa's power had been almost eliminated in all of India.
At this point in his life Hyder Ali decided to go to war with the Marathas and put an end to the threat they posed to his power.
He therefore attacked the Maratha aligned Rani of Bednore. She had appealed to the Nawab of Savanur for assistance when Hyder invaded. Hyder consequently threatened the Nawab, attempting to extort tribute from him. Failing in this, he overran that territory, reaching as far as Dharwad, north of the Tungabhadra River.
Since Savanur was a tributary of the Marathas, the Peshwa Madhavrao I countered with a strong force, and defeated Hyder near Rattihalli. Following the victory the Marathas restored their power under the reign of Madhavrao Peshwa. The Maratha victory forced Hyder to retreat; he had to abandon Bednore, although he was able to remove its treasures to Seringapatam. Hyder paid 35 lakhs rupees in tribute to end the war, and returned most of his gains, although he did retain Sira.
In 1766 Hyder Ali returned to the Malabar, this time at the invitation of the raja of Cannanore, who sought independence from the Zamorin, the ruler of Calicut who held sway over Cannanore. Hyder also claimed a debt of tribute from the Zamorin, who had supported Hyder's opponents in earlier campaigns. After a difficult campaign, Hyder reached Calicut, where the Zamorin, after promising to make payment, failed to deliver. Hyder placed the Zamorin under house arrest and had his finance minister tortured. Fearing similar treatment, the Zamorin set fire to his palace and perished in the flames, ending Eradi dynastic rule of Calicut. After establishing control of Calicut, Hyder departed, but was forced to return several months later when the Nairs rebelled against the rule of his lieutenant, Reza Sahib. Hyder's response was harsh: after putting down the rebellion, many rebels were executed, and thousands of others were forcibly relocated to the Mysorean highlands.
Mysore's titular ruler Krishnaraja died in April 1766, while Hyder was in Malabar. Hyder had left orders that Krishnaraja's son Nanjaraja Wodeyar be invested should that happen, and he only later came to formally pay his respects to the new rajah. He took advantage of this opportunity to engage in a sort of house cleaning: the raja's palace was plundered, and its staff reduced to the point where virtually everyone employed there was also a spy for Hyder Ali.
After the Battle of Buxar the British led by Hector Monro decided to support the Maratha Confederacy against the Shah Alam II, the Nawabs and Mysore.
As the power struggle between Mysore and the Peshwa continued it soon began to involve the British and other European trading companies.
Being himself a former ally of the French, Hyder Ali expected the support of the British against the Marathas, but such support never materialized.
In 1766 Mysore began to become drawn into territorial and diplomatic disputes between the Nizam of Hyderabad and the East India Company, which had by then become the dominant European power on the Eastern Indian coast. The Nizam, seeking to deflect the Company from their attempts to gain control of the Northern Circars, made overtures to Hyder Ali to launch an invasion of the Carnatic. Company representatives also appealed to Hyder Ali, but he rebuffed to them. The Nizam then ostensibly struck a deal with the Company administration in the Madras Presidency for their support, but apparently did so with the expectation that when Hyder Ali was prepared for war, the deal with the British would be broken. This diplomatic manoeuvring resulted in the start of the First Anglo-Mysore War in August 1767 when a company outpost at Changama was attacked by a combined Mysore-Hyderabad army under Hyder Ali's command. Despite significantly outnumbering the British force (British estimates place the allied army size at 70,000 to the British 7,000), the allies were repulsed with heavy losses. Hyder Ali moved on to capture Kaveripattinam after two days of siege, while the British commander at Changama, Colonel Joseph Smith, eventually retreated to Tiruvannamalai for supplies and reinforcements. There Hyder Ali was decisively repulsed on 26 September 1767. With the onset of the monsoon season, Hyder Ali opted to continue campaigning rather than adopting the usual practice of suspending operations because of the difficult conditions the weather created for armies. After over-running a few lesser outposts, he besieged Ambur in November 1767, forcing the British to resume campaigning. The British garrison commander refused large bribes offered by Hyder Ali in exchange for surrender, and the arrival of a relief column in early December forced Hyder Ali to lift the siege. He retreated northward, covering the movements of the Nizam's forces, but was disheartened when an entire corps of European cavalry deserted to the British. The failures of this campaign, combined with successful British advances in the Northern Circars and secret negotiations between the British and the Nizam Asaf Jah II, led to a split between Hyder Ali and the Nizam. The latter withdrew back to Hyderabad and eventually negotiated a new treaty with the British company in 1768. Hyder Ali, apparently seeking an end to the conflict, made peace overtures to the British, but was rebuffed.
In early 1768, the Bombay Presidency in Bombay organised an expedition to Mysore's Malabar coast territories. Hyder Ali's fleet, which the British reported as numbering about ten ships, deserted en masse, apparently because the captains were unhappy with the oustering of their British admirals and some even demanded the return of Ali Raja Kunhi Amsa II, but Hyder Ali chose a cavalry commander Lutf Ali Beg as fleet commander. Owing to a British bluff, Lutf Ali Beg also withdrew much of the Mangalore garrison to move on what he perceived to be the British target, Onore. The British consequently captured Mangalore with minimal opposition in February. This activity, combined with the loss of the Nizam as an ally, prompted Hyder Ali to withdraw from the Carnatic, and move with speed to Malabar. Dispatching his son Tipu with an advance force, Hyder Ali followed, and eventually re-took Mangalore and the other ports held by the over-extended British forces. He also levied additional taxes as punishment against local malabari Nair chieftains which were then stripped of rights and authority.
After his reconquest, Hyder Ali learned that the Mangalorean Catholics had helped the British in their conquest of Mangalore, behaviour he considered treasonous. He summoned a Portuguese officer and several Christian priests from Mangalore to suggest an appropriate punishment to impose on the Mangalorean Catholics for their treachery. The Portuguese officer suggested the death penalty for those Catholics who helped the British as a typical punishment for the betrayal of one's sovereign in Catholic nations. But Hyder Ali exhibited a diplomatic stance and instead imprisoned those Christians who were condemned for treachery. He afterwards opened negotiations with the Portuguese, and reached an agreement with them that removed suspicion from the clergy and other Christians. The Mangalorean Catholic community flourished during the rest of Hyder Ali's reign.
During Hyder Ali's absence from the Carnatic, the British recovered many places that Hyder Ali had taken and only weakly garrisoned, and advanced as far south as Dindigul. They also convinced the Marathas to enter the conflict, and a large force of theirs, under the command of Morari Rao, joined with Colonel Smith at Ooscota in early August 1768. This army then began preparations to besiege Bangalore, but Hyder Ali returned to Bangalore from Malabar on 9 August, in time to harass the allies before the siege could begin. On 22 August, Hyder Ali and his Mysore forces attacked the Maratha camp during the Battle of Ooscota, but was repulsed when faced with the large Maratha reinforcements. Hyder Ali was then foiled in an attempt to prevent the arrival of a second British column at the allied camp; the strength of these combined forces convinced him to retreat from Bangalore toward Gurramkonda, where he was reinforced by his brother in law. He also attempted diplomatic measures to prevent a siege of Bangalore, offering to pay ten lakhs rupees and grant other land concessions in exchange for peace. The Company administration countered with a list of demands that included payments of tribute to the Nizam and several land concessions to the East India Company. Hyder Ali specifically refused to deal with Muhammed Ali Khan Wallajah, his nemesis in the Carnatic. The negotiations failed to reach common ground.
On 3 October, Hyder Ali, while moving his army from Guuramkonda back toward Bangalore, surprised a small garrison of Muhammed Ali Khan Wallajah's men at a rock fort call Mulwagal, near Ooscota. British reinforcements were sent, and Colonel Wood was able to recover the lower fort but not the upper. The next day he went out with a few companies of men to investigate movements that might have been cover for enemy reinforcements. This small force, numbering four companies, was surrounded by Hyder Ali's entire army in the Battle of Mulwagal. A strategem by another officer, Colonel Brooks, prevented the loss of this detachment; Colonel Brooks and another two companies dragged two cannons to the top of a nearby rise, and Brooks called out "Smith! Smith!" while firing the cannons. Both sides interpreted this to mean that Colonel Smith was arriving in force, and Hyder's troops began to retreat. This enabled Colonel Wood to join with Brooks and other reinforcements from Mulwagal before Hyder Ali realised his tactical error. Hyder Ali renewed his attack, but was eventually repulsed with heavy losses: he was estimated to lose 1,000 men while the British lost about 200. The severity of the conflict convinced Colonel Smith that he would be unable to effectively besiege Bangalore without first inflicting a major defeat on Hyder Ali in open battle. Company officials blamed Smith for the failure to decisively defeat Hyder Ali, and recalled him to Madras. Hyder Ali took the opportunity to besiege Hosur, and Colonel Wood marched in relief of the town. As Wood approached, Hyder Ali raised the siege, sneaked around Wood's column, and attacked his baggage train in a battle near Bagalur. Hyder Ali successfully captured supplies and arms, and drove Wood in disgrace toward Venkatagiri. Wood was consequently recalled and replaced by Colonel Lang.
Hyder Ali then raised additional forces in Mysore and went on the offensive. In November 1768 he split his army into two, and crossed the ghats into the Carnatic, regaining control of many minor posts held by the British. En route to Erode Hyder Ali overwhelmed one contingent of British, who were sent as prisoners to Seringapatam when it was established that one of its officers was serving in violation of a parole agreement. After rapidly establishing control over much of the southern Carnatic, his march approached Madras. This prompted the British to send an envoy to discuss peace; because of Hyder Ali's insistence that the Nawab of the Carnatic be excluded from the negotiations, they went nowhere. Hyder Ali then surprised Company authorities by taking a picked force of 6,000 cavalry and a small number of infantry, and made in three days a forced march of 130 miles (210 km) to the gates of Madras.
This show of force compelled the Company to negotiate further. Hyder Ali, who was seeking diplomatic leverage against the Marathas, wanted an alliance of mutual defence and offence. The Company refused to accede to an offensive military treaty; the treaty signed at Madras on 29 March 1769, restored the status quo ante bellum, except for Mysore's acquisition of Karur, and also included language that each side would help the other defend its territory. In summarising Hyder Ali's conduct of the war, biographer Lewin Bowring notes that he "evinced high qualities as a tactician and the sagacity of a born diplomatist."
When Hyder took over the Malabar territories, he took advantage of the coastal access to develop relations with trading partners overseas. To this end he established port tariffs that were biased against European traders and preferential for Mysorean and Arab traders. Beginning in 1770 he sent ambassadors to Abu Hilal Ahmad bin Said in Muscat and Karim Khan in Shiraz, then the capital of Persia, seeking military and economic alliances.
In a 1774 embassy to Karim Khan, the ruler of Persia, he sought to establish a trading post on the Persian Gulf. Karim responded by offering Bandar Abbas, but nothing further seems to have passed between them on the subject. Karim Khan later did send 1,000 troops to Mysore in 1776 in response to another embassy in 1775.
Nursullah Khan, Hyder's ambassador, had more success in Muscat, where a trading house was established in 1776.
During the final years of his reign Hyder Ali also planned to send an embassy to the Ottoman Sultan Mustafa III, but it was his son Tipu Sultan who succeeded in making direct contact with Istanbul.
Hyder, believing he would be supported by the British in conflict with the Marathas, began demanding tribute payments from smaller states on the frontiers between Maratha and Mysore territories, and refused to pay tributes demanded by the Marathas. The Marathas responded in November 1770 with an invasion by an army of 35,000 men. Pursuant to their treaty, Hyder requested British assistance. The Company refused, and Hyder retreated, slashing and burning as he went to deny the bounty of the land to the Marathas. The Marathas captured much of north-eastern Mysore, and consolidated their gains during the monsoon season. Hyder offered to pay some of the tribute demanded, but his offer was rejected as insufficient, and the Marathas renewed the offensive after the monsoons. They advanced to the vicinity of Seringapatam, and then feinted a withdrawal to the north. When Hyder followed, they turned in force, and claimed to inflict serious casualties on Hyder's army, and captured most of its baggage. They then fruitlessly besieged Seringapatam for five weeks, before abandoning the effort and instead took Bangalore. Hyder again appealed to the British for help, but their pre-conditions and proposed terms were unacceptable to him, and an attempt by Hyder to get them to go on the offensive scuttled the negotiations. In 1772 Hyder finally sued for peace. He agreed to pay 3.6 million rupees in tribute arrears, and 1.4 million rupees in annual tribute, and ceded territory all the way to Bangalore. Upon his return to Seringapatam after the peace was concluded, Hyder learned that Nanjaraja, the titular ruler of Mysore, had been engaged in secret communications with the Marathas. Hyder ordered Nanjaraja strangled, and placed his brother Chamaraja on the throne.
The peace with the Marathas was short-lived. The Peshwa Madhavrao I died late in 1772, beginning a struggle for his succession. In 1773, Hyder used this opportunity to send Tipu with an army to recover territories lost to the Marathas to the north, while he descended into Coorg, which provided a more secure route to the Malabar territories he wanted to recover from the Marathas. A claimant to the Coorg throne had asked for Hyder's assistance in 1770 when he was pre-occupied with the Marathas. He quickly captured Coorg's capital, Merkara, imprisoning Raja Vira Rajendra. He installed a Brahmin as Governor to collect revenues before continuing to Malabar, where by the end of 1774 he had recovered all his lost territory. The Coorgs rose in rebellion against his Governor, upon which Hyder returned to Coorg, crushed the rebellion, and hanged most of the ring-leaders. This did not stop the restive Coorgs from becoming a continuing problem for Hyder, and for, Tipu after his death.
In 1776 the young Raja Chamaraja Wodeyar VIII died. To choose a successor, Hyder had all of the children of the royal family brought together, and watched them play. A child, also named Chamaraja Wodeyar IX, chose to play with a jewelled dagger, and was supposedly selected on that basis as the new Raja of Mysore.,
By March 1775, the leadership situation at Poona, the Maratha capital, had stabilised, and the Marathas joined an alliance with the Nizam of Hyderabad to oppose Hyder. The Maratha army was routed by one of Hyder's Generals in 1776, and Hyder either bribed or sufficiently threatened the Nizam's military leaders so that they withdrew from the campaign. This only temporarily halted the conflict, which was fought with renewed vigor until 1779. Hyder successfully extended his domain to the Krishna River after a lengthy siege of Dharwad. In a controversial action, Hyder in 1779, dealt harshly with Madakari Nayaka, the ruler of Chitradurga. Madakari had supported Hyder in earlier conflicts, but in 1777 had changed allegiance to the Marathas. After seizing Chitradurga, Hyder sent Madakari Nayaka to Seringapatam as a prisoner, where he died. Hyder further sent 20,000 of Madakari's followers to Seringapatam, where the boys among them were allegedly forcibly converted to Islam and formed into so-called chela battalions in the Mysorean army.
During the lengthy conflict with the Marathas, Hyder had several times requested the assistance of the East India Company, and it had each time been refused, in part due to the influence at Madras, of Hyder's enemy, the Nawab of Arcot. The British had also angered the Marathas by repudiating treaties, with whom they were at war for much of the 1770s, and they had also upset the Nizam of Hyderabad Asaf Jah II over their occupation of Guntur.
In 1771, Maratha envoys had approached Hyder with a proposal to ally against the Company, with the goal of wresting control of eastern India from their rule. Since Hyder was at the time still attempting alliance with the British, he informed them of this offer, noting that he thought the Marathas would gain too much power and even threaten his own position under those circumstances. The Marathas, still at war with the British, renewed an offer of alliance in 1779. In this case, the alliance was to include the Nizam. His decision to join this alliance was prompted by two British actions. The first was the British capture by capitulation of the west-coast port of Mahé, part of a concerted effort by the British to capture all French outposts following the 1778 French entry into the American Revolutionary War. Hyder received much of his French-supplied equipment through this French-controlled port, and had provided troops for its defence. Furthermore, the action had provoked the Nairs on the Malabar coast to rise in rebellion again, although Hyder had quickly put this down. The second offence was the movement of British troops through territory under his control (and also other territory controlled by the Nizam) from Madras to Guntur. There was a skirmish in the hills, and the British detachment ended up retreating to Madras.
Hyder Ali began rebuilding his navy in 1778. Employing Joze Azelars, a Dutchman, he had built eight ketches with masts and 40 cannons and eight smaller dhows. When the war broke out in 1779, Azelars noted that the Brahmans and their allies made every possible effort to halt progress of the newly rebuilt navy based at Bhatkal.
The alliance planned to make virtually simultaneous attacks on British holdings all throughout India, while the Marathas agreed to honour Hyder's claims to territories he currently held north of the Tungabhadra River and reduced the amount of tribute he was required to pay under earlier agreements. Hyder expected to receive assistance from the French, especially in the Carnatic, the territory he sought to conquer. However, diplomatic actions by Governor Warren Hastings and the Company successfully convinced both the Nizam and the Marathas not to take up arms, and Hyder ended up fighting the war on his own.
He successfully gained alliances with Ali Raja Bibi Junumabe II of Cannanore Arakkal Kingdom and the Muslim Mappila community and later even met with Muslim Malays from Malacca, who were in Dutch service.
A British illustration of Sayed Sahib leading Hyder Ali's forces during the Siege of Cuddalore.
The Battle of Pollilur, where the forces of Hyder Ali effectively used Mysorean rockets and Rocket artillery against closely massed East India Company troops.
Pierre André de Suffren ally of Hyder Ali.
Marquis de Bussy-Castelnau ally of Hyder Ali.
French Admiral Suffren (with the support of Hyder Ali) comes to the aid of Reynier van Vlissingen's Dutch forces against the Admiral Edward Hughes.
The army Hyder assembled was one of the largest seen in southern India, estimated to number 83,000. Carefully co-ordinating the actions of his subordinate commanders, he swept down the Eastern Ghats onto the coastal plain in July 1780, laying waste the countryside. Due to Hyder's secrecy and poor British intelligence, officials in Madras were unaware of his movements until the fires of burning villages just 9 miles (14 km) away were seen in Madras. Hyder himself organised the Siege of Arcot, while detaching his son Karim Khan Sahib to take Porto Novo. The movement in August of Sir Hector Munro with a force of over 5,000 from Madras to Kanchipuram (Conjeevaram) prompted Hyder to lift the siege of Arcot and move to confront him. Word then arrived that Munro was awaiting the arrival of reinforcements from Guntur under Colonel William Baillie, so he sent a detachment under Tipu to intercept them, and eventually followed in strength himself, when Munro sent a force from his army to meet Baillie. Tipu and Hyder surrounded Baillie's force, and compelled the surrender of about 3,000 men in the Battle of Pollilur on 10 September; it was the first effective use of rocket artillery and made a strong impression upon the British. Hyder then renewed the siege of Arcot, which fell in November.
Shortly after the outbreak of hostilities, Governor Hastings had sent General Sir Eyre Coote south from Bengal to take charge of British forces opposing Hyder. He arrived at Madras in November to take command from Munro. Coote marched into the Carnatic, and eventually occupied Cuddalore. After being re-supplied there, he besieged Chidambram, where an assault on the fort was repulsed.
Hyder had in the mean-time descended into Tanjore, with severe consequences. After extracting the allegiance of the Maratha king Thuljaji, Hyder plundered the country, destroying cattle and crops. The economic output of Tanjore is estimated to have fallen by 90% between 1780 and 1782. Hyder's ravages were followed by alleged expeditions of plunder launched by the Kallars. The economic devastation wrought by these attacks was so severe that Tanjore's economy did not recover until the start of the 19th century; the era is referred to in local folklore as the Hyderakalam.
With General Coote at Cuddalore, Hyder then made a forced march to interpose his army between Chidambram and Cuddalore, cutting Coote's supply line. Coote marched to face him, and won a decisive victory in the Battle of Porto Novo on 1 July 1781; Coote estimated that Hyder lost 10,000 men in the battle. Hyder then dispatched Tipu in an attempt to prevent the junction of Coote's army with reinforcements from Bengal. This failed, and in late August the two armies met again at Pollilur, chosen by Hyder as a place to make a stand, because it was the site of his victory over Baillie the previous year. Hyder was defeated this time, although the battle was not decisive. While Coote re-grouped and searched for provisions, Hyder took the opportunity to besiege Vellore. Madras authorities convinced the ageing Coote to put off his retirement and relieve the fortress there. Hyder and Coote met in battle at Sholinghur, near Vellore. Hyder's artillery was ineffective, and the re-provisioned Vellore, which had been on the brink of surrender.
Lord Macartney, who had recently arrived to take the Governorship of Madras, also brought news that Britain was at war with the Dutch. Consequent to this, the Company was instructed to seize Dutch holdings in India, and Macartney had ordered a detachment from Tanjore, under Colonel Braithwaite, to capture the main Dutch post at Negapatam. Hyder made an agreement with the Dutch to provide troops for its defence, but was himself forced away from Negapatam by Braithwaite. The British took Negapatam after a three-week siege in October and November 1781. This setback forced Hyder to withdraw from most of Tanjore.
In January 1782, General Coote, his health failing, again set out to re-provision Vellore. Hyder did not prevent the re-supply, but shadowed the British back toward Tripassore, offering battle near Sholinghur. Coote successfully manoeuvred away from Hyder without battle. In February, Hyder detached Tipu with a sizeable force to recover Tanjore. Intelligence failures led the main British garrison to become surrounded by this superior force; Colonel Brathwaite and 2,000 men surrendered. Hyder was also pre-occupied by bad news from the west. A Mysorean force that had been besieging Tellicherry was broken, with its commander and his siege guns captured, and Coorg and Malabar were also descending into open rebellion. Hyder consequently sent forces west to deal with these matters, and was preparing to follow himself when word arrived on 10 March that a French force had landed at Porto Novo. Hyder immediately sent Tipu from Tanjore to meet with them, and followed himself from Arcot. At this time he had a celebrated meeting with the French Admiral Suffren, and the allies agreed on a plan to establish Cuddalore as a French base. Cuddalore was occupied without resistance on 8 April, and Hyder's army, joined by the French, marched toward Permacoil, which fell in May. Coote responded by marching toward Arni, where Hyder had a major supply depot. Hyder and the French had been considering an assault on Wandiwash, but abandoned that idea and marched to face Coote. They skirmished there on 2 June. In August the British landed a force on the Malabar coast, to which Hyder responded by sending additional troops under Tipu to the west. The onset of the monsoon season then suspended most military activity on the eastern plain, and Hyder established his camp near Chittoor.
He was a bold, an original, and an enterprising commander, skilful in tactics and fertile in resources, full of energy and never desponding in defeat. He was singularly faithful to his engagements and straightforward in his policy towards the British...his name is always mentioned in Mysore with respect, if not with admiration.
Bowring,
Hyder, who had suffered from a cancerous growth on his back, died in his camp on 6 December 1782. Some other accounts record it as 7 December 1782 and some historical accounts in the Persian language record the death in dates ranging from Hijri 1 Moharram 1197 to Hijri 4 Moharram 1197 in the Islamic calendar. The differences in recorded dates may be due to the lunar calendar and the differences in moon sightings in the surrounding kingdoms.
Hyder's advisers tried to keep his death a secret until Tipu could be recalled from the Malabar coast. Upon learning of his father's death Tipu immediately returned to Chittoor to assume the reins of power. His accession was not without problems: he had to put down an attempt by an uncle to place Tipu's brother Abdul Karim on the throne.  The British learned of his death within 48 hours of its occurrence, but the dilatory attitude of Coote's replacement, James Stuart, meant that they were unable to capitalise on it militarily.
Hyder Ali was buried at the Gumbaz in Seringapatam, the mausoleum raised by his son Tipu Sultan in 1782–84.
In 1763, Hyder Ali and Tipu Sultan established their first naval fleet on the Malabar Coast, under the command of Ali Raja Kunhi Amsa II a large and well armed fleet consisting of 10 dhows and 30 larger ketches in the Indian Ocean, in his attempts to conquer islands that had withstood the Mughal Emperor Aurangzeb. In 1763 his allies, the Ali Rajas, sailed from Lakshadweep and Cannanore carrying on board sepoys and on its pennons the colours and emblems of Hyder Ali, and captured the Maldives.
Ali Raja returned to Mysore to pay homage to Hyder Ali, presenting him the captured and blinded Sultan of the Maldives Hasan 'Izz ud-din. Outraged at this excess, Hyder Ali stripped Ali Raja of the command of his fleet.
Hyder Ali, like his son Tipu Sultan protected foreign merchant ships, and the Mysore navy is even known to have protected and convoyed Chinese merchant ships in the region.
In 1768, Hyder Ali lost two grabs and 10 gavilats in a naval skirmish with forces of the East India Company. He was left with eight garbs and ten galivats, most of them damaged beyond repair.
On 19 February 1775, two of Hyder Ali's ketches attacked HMS Seahorse, which drove them off after a brief exchange of fire.
On 8 December 1780 Edward Hughes attacked Hyder Ali's fleet causing considerable damage once again. Mysore is known to have lost some of the best ships it ever constructed in that naval-battle at Mangalore. But the British were unable to stop Suffren's fleet in 1781.
Hyder Ali was an innovator in the military use of rockets, which were used against positions and territories held by the East India Company during the Anglo-Mysore Wars. Although rocket technology originated in China and had made its way to India and Europe by the 13th century, development of accurate cannons had sidelined rockets as a military technology in Europe. Rocket technology was already in use when Hyder's father served (he commanded a company of 50 rocketmen), but it was Hyder who improved them and significantly expanded their use in the military. Technological innovations included the use of high-quality iron casing (better than was then available in Europe) for the combustion chamber, enabling the use of higher-powered explosive charges. He also organised companies of rocketmen who were experienced in aiming rockets based on the size of the rocket and the distance to the target. Rockets could also be mounted on carts that improved their mobility and made possible the firing of large numbers of them all at once. Rockets developed by Hyder and Tipu led to a renaissance of interest in the technology in Britain, where William Congreve, supplied with rocket cases from Mysore, developed what became known as Congreve rockets in the early 19th century.
In Hyder's time the Mysorean army had a rocket corps of as many as 1,200 men, which Tipu increased to 5,000. At the 1780 Battle of Pollilur, during the second war, Colonel William Baillie's ammunition stores are thought to have been detonated by a hit from one of Hyder's rockets, contributing to the British defeat.
The peak of Mysore's economic power was under Hyder Ali and his son Tipu Sultan in the post-Mughal era of the mid-late 18th century. They embarked on an ambitious program of economic development, aiming increase the wealth and revenue of Mysore.
Details are sketchy on Hyder's personal life. He had at least two wives. His second wife was Fatima, the mother of Tipu, his brother Karim, and a daughter. He may have also married the sister of Abdul Hakim Khan, the Nawab of Savanur; Bowring describes it as a marriage, but Punganuri Rao's translator, citing Wilks, claims this was a "concubine marriage". Karim and the daughter were both married to Abdul Hakim's children to cement an alliance in 1779.

The Congreve rocket was a type of rocket artillery designed by British inventor Sir William Congreve in 1804.
The design was based upon the rockets deployed by the Kingdom of Mysore against the East India Company during the Second, Third, and Fourth Anglo-Mysore Wars. Lieutenant general Thomas Desaguliers, colonel commandant of the Royal Artillery at Woolwich, was impressed by reports of their effectiveness, and undertook several unsuccessful experiments to produce his own rocket weapons. Several captured Mysorean rockets were sent to England following the annexation of the Mysorean kingdom into British India following the death of Tipu Sultan in the siege of Seringapatam.
The project was continued chiefly with William Congreve, who set up a research and development programme at the Woolwich Arsenal's laboratory. After development work was complete the rockets were manufactured in quantity further north, near Waltham Abbey, Essex. He was told that "the British at Seringapatam had suffered more from the rockets than from the shells or any other weapon used by the enemy." "In at least one instance", an eyewitness told Congreve, "a single rocket had killed three men and badly wounded others." The rockets were used by the British, the Russians and Paraguay during the nineteenth century.
The sultan of Mysore, Tipu Sultan and his father Hyder Ali developed the military tactic of using massed wave attacks supported by rocket artillery against enemy positions. In 1792, Tipu Sultan wrote a military manual called Fathul Mujahidin, in which two hundred artillerymen specialising in rocket artillery were prescribed to each Mysorean brigade (known as cushoons). Mysore had between sixteen and twenty-four cushoons of infantry. The areas of towns where rockets and fireworks were manufactured were known as "taramandal pet" ("galaxy market").
The rocket men were trained to launch their rockets at an angle calculated from the diameter of the cylinder and the distance of the target. In addition, wheeled rocket launchers were used in war that were capable of launching five to ten rockets almost simultaneously. 
Rockets could be of various sizes, but usually consisted of a cylindrical housing of soft hammered iron about 8 inches (200 mm) long and 1.5 to 3 inches (38 to 76 mm) in diameter, closed at one end, which was strapped to a shaft of bamboo about 4 ft long. The iron tube acted as a combustion chamber and contained well-packed black powder to act as the propellant. A rocket carrying about one pound of powder could travel almost 1,000 yards (910 m). In contrast, rockets in Europe were not iron cased and could not take large chamber pressures. As a consequence European rockets were not capable of reaching distances anywhere near as great.
Hyder Ali introduced the first iron-cased rockets in warfare.
Hyder Ali's father was the naik or chief constable at Budikote, and he commanded 50 rocketmen for the Nawab of Arcot. There was a regular rocket corps in the Mysore Army, beginning with about 1,200 men in Hyder Ali's time. 
At the Battle of Pollilur (1780) during the Second Anglo-Mysore War, Colonel William Baillie's ammunition stores are thought to have been detonated by a hit from one of Tipu Sultan's Mysorean rockets, which contributed to the British defeat.
In the Third Anglo-Mysore War in 1792, there is mention of two rocket units fielded by Tipu Sultan, 120 men and 131 men respectively. Lieutenant Colonel Knox was attacked by rockets near Srirangapatna on the night of 6 February 1792, while advancing towards the Kaveri River from the north. The rocket corps ultimately reached a strength of about 5,000 in Tipu Sultan's army. Mysore rockets were also used for ceremonial purposes. The Jacobin Club of Mysore sent a delegation to Tipu Sultan, and 500 rockets were launched as part of the gun salute.

During the Fourth Anglo-Mysore War, rockets were again used on several occasions. One of these involved Colonel Arthur Wellesley, later famous as the First Duke of Wellington and the hero of the Battle of Waterloo. Quoting Forrest:"At this point (near the village of Sultanpet, Figure 5) there was a large tope, or grove, which gave shelter to Tipu's rocketmen and had obviously to be cleaned out before the siege could be pressed closer to Srirangapattanam Island. The commander chosen for this operation was Col. Wellesley, but advancing towards the tope after dark on 5 April 1799, he was set upon with rockets and musket-fires, lost his way and, as Beatson politely puts it, had to "postpone the attack" until a more favourable opportunity should offer. Wellesley's failure was glossed over by Beatson and other chroniclers, but the next morning he failed to report when a force was being paraded to renew the attack."On 22 April , twelve days before the main battle, rocketeers worked their way around to the rear of the British encampment, then 'threw a great number of rockets at the same instant' to signal the beginning of an assault by 6,000 Indian infantry and a corps of Frenchmen, all directed by Mir Golam Hussain and Mohomed Hulleen Mir Mirans. The rockets had a range of about 1,000 yards. Some burst in the air like shells. Others, called ground rockets, on striking the ground, would rise again and bound along in a serpentine motion until their force was spent.According to one British observer, a young English officer named Bayly:
"So pestered were we with the rocket boys that there was no moving without danger from the destructive missiles ...". He continued: "the rockets and musketry from 20,000 of the enemy were incessant. No hail could be thicker. Every illumination of blue lights was accompanied by a shower of rockets, some of which entered the head of the column, passing through to the rear, causing death, wounds, and dreadful lacerations from the long bamboos of twenty or thirty feet, which are invariably attached to them'."During the decisive British victory at Srirangapattanam on 2 May 1799, a British shot struck a magazine of rockets within Tipu Sultan's fort, causing it to explode and send a towering cloud of black smoke up from the battlements, with cascades of exploding white light. Baird led the final attack on the fort on the afternoon of 4 May, and he was again met by "furious musket and rocket fire" - but it did not help much. The fort was taken in about an hour's time; in another hour or so, Tipu had been shot (the precise time of his death is not known) and the war was effectively over.
After the fall of Srirangapatna, 600 launchers, 700 serviceable rockets, and 9,000 empty rockets were found. Some of the rockets had pierced cylinders to allow them to act like incendiaries, while some had iron points or steel blades bound to the bamboo. These blades caused the rockets to become very unstable towards the end of their flight, causing the blades to spin around like flying scythes, cutting down all in their path.
Congreve began in 1804 by buying the best rockets on the London market, but found that their greatest range was only 600 yards. After spending ‘several hundred pounds’ of his own money on experiments, he was able to make a rocket that would travel 1,500 yards. He now ‘applied to Lord Chatham (the responsible minister in charge of the Ordnance Department)  for permission to have some large rockets made at Woolwich’. Permission was granted and ‘several six-pounder rockets’ made ‘on principles I had previously ascertained’ achieved a range of ‘full two thousand yards’. By the spring of 1806, he was producing 32-pounder rockets ranging 3,000 yards.
Congreve enjoyed the friendship of the Prince Regent, who supported his rocket projects and in whose household he served as an equerry from 1811. The Prince Regent was also the Elector of Hanover, and he was awarded the honorary rank of Lieutenant Colonel in the Hanoverian army's artillery in 1811. In 1813, Congreve declined the offer to command the Rocket Corps with rank in the Regiment of Artillery.
Congreve registered two patents and published three books on rocketry.
The initial rocket cases were constructed of cardboard, but by 1806 they were made from sheet iron. The propulsion was of the same ingredients as gunpowder, the mixture of which varied with the different sizes of rocket. The warheads had side-mounted brackets which were used to attach wooden sticks of differing lengths, according to the sizes of rocket.
Rocket sizes were designated by the calibre of the tube, using the then-standard British method of using weight in pounds as a measure of cannon bore. Larger diameter rockets also had correspondingly longer tubes.
By 1813, the rockets were made available in three classes:
The medium and light rockets could be case shot, shell, or explosive.
The 32-pounder was generally used for longer range bombardment, while a 12–pounder case shot was generally used for support of infantry and cavalry, with an extreme range of some 2,000 yards. The rockets could be fired from a wheeled bombarding frame, from a portable tripod, or even from a shallow trench or sloping bank. One in three horse artillerymen carried a launching trough for ground firing.
In December 1815, Congreve demonstrated a new design of rocket that had the rocket stick screwing centrally into the base of the case. This remained in service from 1817 until 1867, when it was replaced by the Hale rocket which required no stick and used clockwise rotation to impart stability in flight.
Contrary to popular belief, rockets could not out-range the equivalent smooth bore guns of the period. In real terms, the maximum effective range for the 12-pounder rockets and for the six-pounder gun was some 1,400 yards or about 1,280 meters. However, the rate of fire with rockets could be higher than the equivalent muzzle loading ordnance.  The absence of weighty ordnance meant that fewer horses were required. Captain Richard Bogue needed just 105 horses for his troop, compared with the 220 of Captain Cavalié Mercer's troop.
Rockets could be easily carried and readily deployed, particularly over difficult terrain or in small boats. This was amply demonstrated by the Royal Marine Artillery, as well as in the crossing of the river Adour and the Battle of Toulouse.
Their lack of specific accuracy with the larger rockets at long range was not a problem if the purpose was to set fire to a town or a number of moored ships; this was shown with the attack on the French Fleet in Aix and Basque roads and at the bombardment of Copenhagen. As Congreve himself had warned, however, they were of little use against fortified places, such as against Fort Henry, because of the lack of combustible structures.
The 12-pounder deployed at very close range was a fearsome weapon, as was seen at the battles of Göhrde and Leipzig in 1813, as well as the crossing of the Adour and the Battle of Toulouse in 1814.
Accuracy at medium range remained a problem. This is illustrated by Mercer's description of G Troop Royal Horse Artillery during the retreat from Quatre Bras on 17 June 1815:
The rocketeers had placed a little iron triangle in the road with a rocket lying on it. The order to fire is given - port-fire applied - the fidgety missile begins to sputter out sparks and wriggle its tail for a second or so, and then darts forth straight up the chaussée. A gun stands right in its way, between the wheels of which the shell in the head of the rocket bursts, the gunners fall right and left… our rocketeers kept shooting off rockets, none of which ever followed the course of the first; most of them, on arriving about the middle of the ascent, took a vertical direction, whilst some actually turned back upon ourselves - and one of these, following me like a squib until its shell exploded, actually put me in more danger than all the fire of the enemy throughout the day.The main user of Congreve rockets during the Napoleonic Wars was the Royal Navy, and men from the Royal Marine Artillery became experts in their use. The navy converted HMS Galgo and Erebus into rocket ships. The army became involved and was represented by various rocket detachments that changed into the two Rocket Troops, Royal Horse Artillery, on 1 January 1814.
In the autumn of 1805, the government decided upon an attack on Boulogne for the first test. William Sidney Smith was chosen to lead the expedition, accompanied by Congreve. Strong winds and rough seas hampered the operations on both the 20th and 21st, and the attack was not successful.
In April 1806, Rear Admiral Sidney Smith took rockets on a little-known mission to the Mediterranean to aid Sicily and the Kingdom of Naples in their struggle against the French. It was perhaps at Gaeta, near Naples, that Congreve's rockets had their first successful debut in battle. The second Boulogne rocket expedition, however, is more famous and is usually considered the first successful attempt.
On 8–9 October 1806, Commodore Edward Owen attacked the French flotilla at Boulogne. Captain William Jackson aboard HMS Musquito directed the boats firing 32 pound Congreve rockets. As night drew in on the channel, 24 cutters fitted with rocket frames formed a line and fired some 2,000 rockets at Boulogne. The barrage took only 30 minutes. Apparently the attack set a number of fires, but otherwise had limited effect. Still, it was enough to lead the British to employ rockets on a number of further occasions.
In 1807, Copenhagen was bombarded by more than 14,000 various missiles in the form of metal balls, explosive and incendiary bombs from cannons and mortars, and about 300 Congreve rockets. The rockets contributed to the conflagration of the city.
The lighter, six-pounder battlefield rockets had been sent on the second Egyptian campaign in 1807, a further field trial which proved to be unsuccessful.
Congreve accompanied Lord Cochrane in the fire-ship, rocket, and shell attack on the French Fleet in Aix and Basque roads on 11 April 1809.
The Walcheren Campaign in 1809 saw the deployment of HMS Galgo, a merchant sloop converted to a warship and then converted to fire Congreve rockets from 21 "rocket scuttles"' installed in her broadside. This rocket ship was deployed at the naval bombardment of Flushing, where they wrought such havoc that ‘General Monnet, the French commandant, made a formal protest to Lord Chatham’ against their use.  Congreve was also present at this engagement and commanded five land frames.
In 1810, Wellington agreed to a field trial of Congreve's new 12-pounder rocket carrying case shot. It was not successful and was withdrawn.
In May 1813, a detachment which had been training with rockets at Woolwich under Second Captain Richard Bogue RHA was inspected by a committee of Royal Artillery officers who recommended that it be tried in combat. On 7 June 1813, Bogue's unit was designated the "Rocket Brigade". At the same time as being granted its new title, The Rocket Brigade was ordered to be augmented and to proceed on active service, with orders to join the Army of the North commanded by Bernadotte, the Crown Prince of Sweden. Using the modified 12-pounder at low trajectory from ground firing-troughs, the brigade saw action at the Battle of Gohrde and at the Battle of Leipzig on 18 October 1813, where it was successfully employed to attack the French stronghold of Paunsdorf, occupied by five French and Saxon battalions. Captain Bogue was however killed by a sharpshooter in the subsequent cavalry charge, and the village of Paunsdorf was eventually retaken by the French Imperial Guard. In the continuing campaign, the Rocket Brigade was also used in the sieges of Frederiksfort and Glückstadt, which surrendered on 13 December 1813 and 5 January 1814, respectively.  On 1 January 1814, the unit assumed the title of the "2nd Rocket Troop RHA" and on 18 January it received orders to join the force under the orders of Sir Thomas Graham in Holland.
In September 1813, Wellington agreed, with much reservation, that rockets could be sent out to join the army in Spain. On 3 October 1813, another Royal Artillery detachment embarked from Woolwich, trained in firing rockets. This group was called the "Rocket Company" and consisted of almost sixty men under Captain Lane. On 1 January 1814, together with another detachment under Captain Eliot, it assumed the title of the "1st Rocket Troop RHA". Captain Lane's rockets were very successfully deployed at the crossing of the Adour on 23 February 1814 and in the final battle in the Peninsular War at the Battle of Toulouse on 10 April 1814. Later that year, they were sent to be part of the disastrous expedition against the American Army at New Orleans, in Louisiana.  
By the time of the Waterloo campaign on 30 April 1815, the command of the 2nd Rocket Troop was formally taken over by Captain Whinyates. Wellington remained averse to rockets, so Whinyates took just 800 rockets into the field, as well as five 6-pounder guns; it would appear that the rockets replaced the usual howitzer in the structure of the troop.
The Royal Marine Artillery used Congreve rockets in several engagements during this conflict. Two battalions of Royal Marines were sent to North America in 1813. Attached to each battalion was a rocket detachment, each with an establishment of 25 men, commanded by lieutenants Balchild and John Harvey Stevens. Both rocket detachments were embarked aboard the transport vessel Mariner  Rockets were used in the engagements at Fort Oswego and Lundy's Lane.
The British used the Congreve rocket on U.S. soil for the first time in an attack on Lewes, DE, April 6 &amp; 7, 1813.  The town was bombarded for 22 hours.
A third battalion of Royal Marines arrived in North America in 1814, with an attached rocket detachment commanded by Lieutenant John Lawrence, which subsequently participated in the Chesapeake campaign. During this campaign, the British used rockets at the Battle of Bladensburg to rout the American forces (which led to the capture and burning of Washington, D.C.), and at the Battle of North Point.
It was the use of ship-launched Congreve rockets by the British in the bombardment of Fort McHenry in the US in 1814 that inspired a phrase in the fifth line of the first verse of the United States' National Anthem, "The Star-Spangled Banner": "the rockets’ red glare". HMS Erebus fired the rockets from a 32-pound rocket battery installed below the main deck, which fired through portholes or scuttles pierced in the ship's side.
In Canada, rockets were used by the British at the Second Battle of Lacolle Mills, March 30, 1814. Rockets fired by a detachment of the Royal Marine Artillery, though inaccurate, unnerved the attacking American forces, and contributed to the defense of the blockhouse and mill.  Rockets were used again at the Battle of Cook's Mills, October 19, 1814.  An American force, sent to destroy General Gordon Drummond's source of flour, was challenged by a contingent of infantry which was supported by a light field cannon and a frame of Congreve rockets.  The rockets succeeded in discouraging the Americans from forming lines on the battlefield.
Captain Henry Lane's 1st Rocket Troop of the Royal Horse Artillery embarked at the end of 1814 in the transport vessel Mary with 40 artillerymen and 500 rockets and disembarked near New Orleans. Lieutenant Lawrence's rocket detachment took part in the final land engagement of the War of 1812 at Fort Bowyer in February 1815.
Algiers had been the centre for pirates for some years, and her fleet had reached considerable proportions. Things reached a head after a particular atrocity; Britain decided to stamp out their activities, and the Netherlands agreed to assist. The combined fleet was composed of six British ships of the line and four frigates, plus five Dutch frigates; there were also 37 gun boats, 10 mortar boats, and eight rocket boats. Lieutenant JT Fuller and 19 other ranks from the Rocket Troop accompanied the expedition, together with 2,500 rockets, and were engaged alongside the Royal Marine Artillery.
In the bombardment of Algiers the rocket boats, gun boats, and mortar boats engaged the enemy's fleet moored inside the harbour. "It was by their fire that all the ships in the port, with the exception of the outer frigate, were in flames which extended rapidly over the whole arsenal, storehouses and gun boats, exhibiting a spectacle of awful grandeur".
The following day, the Dey capitulated and accepted all the surrender terms.
On her voyage to the Greenland whale fishery in 1821 Fame carried Congreve rockets. Sir William Congreve equipped her with rockets at his own expense to test their utility in whaling hunting. The Master General of Ordnance and the First Lord of the Admiralty had Lieutenant Colquhoun and two Marine artillerymen accompany the rockets. Captain Scoresby wrote a letter from the Greenland fishery in June reporting that the rockets had been a great success. Subsequent reports made clear that the rockets were fired from about 40 yards and were highly effective in killing whales that had already been conventionally harpooned.
A separate trial took place on another whaler. A letter from Captain Kay, of the ship Margaret, of London, dated September 7, addressed to Lieut. Colquhoun, R.A., says — " I have taken the liberty of inclosing you an account of a few trials I have made of Congreve's Rockets. Fearing the harpooners would not fire it correctly, I had determined to try its effect myself, and it was not until the 8th June that an opportunity presented. Early on that morning a whale, of the largest size, was discovered near the ship ; I immediately pursued it, and when sufficiently near, fired a rocket into its side; the effect it had on the fish was tremendous — every joint in its body shook, and, after lying for a few seconds in this agitated way, it turned on its back and died. It appeared on flinching, that the rocket had penetrated through the blubber and exploded in the crann (sic) near the ribs ; the stick and lower part of the rocket was taken out entire, the upper part was blown to pieces. My next attempt was on the 9th July, on a whale of the same size as the former, but owing to the rapid motion of the fish, and a heavy swell of the sea, which rendered the boat unsteady, the rocket entered below the middle part of the body, in consequence of which its effect was considerably lessened, its frame, however, was much shook by the explosion, and it immediately sunk, but rose again, blowing an immense quantity of blood : it was then struck with a harpoon, and killed with lances. On flinching, part of the stick of the rocket could only by found; it therefore appears probable that the rocket had burst in the inside of the fish.
In December Lieutenant Colquohon demonstrated the use of the rockets at Annapolis, Maryland. A newspaper story gave a detailed account of the experiments he performed.
A new shipment of Congreve rockets – which the Burmese had never seen – were used in the closing phase of the Battle of Yangon (May–December 1824) and in the subsequent battle of Danubyu (March–April 1825) where rocket fire stopped fighting elephants.
Having witnessed the effects of incendiary rockets on grain warehouses of Danzig in 1813, artillery captain Józef Bem of the Kingdom of Poland started his own experiments with what was then called in Polish raca kongrewska. These culminated in his 1819 report Notes sur les fusées incendiaires (German edition: Erfahrungen über die Congrevischen Brand-Raketen bis zum Jahre 1819 in der Königlichen Polnischen Artillerie gesammelt, Weimar, 1820). The research took place in the Warsaw Arsenal, where captain Józef Kosiński also developed the multiple-rocket launchers adapted from horse artillery gun carriage.
The 1st Rocketeer Corps was formed in 1822 under the command of brigade general Pierre Charles Bontemps  and received its launchers in 1823. The unit received its baptism of fire during the Polish–Russian War 1830–31.
The rocket salvos fired by captain Karol Skalski's rocketeers during the twilight hours of the Battle of Olszynka Grochowska (25 February 1831) disrupted the Russian cavalry charges and forced them to retreat, which changed the tide of battle.
The rockets were also used several times (over a thousand stockpiled) by Polish freedom fighters during the final Battle of Warsaw (September 1831) in defense of the city.
Congreve rockets were used from the bombardment of the Canton ports, by Nemesis (1839) in January 1841, to their use at the Battle of Palikao, in September 1860.
During the period of the New Zealand Wars, the British Army used Congreve rockets against Māori fortifications—along with cannon fire—but found that simply digging trenches were sufficient to ensure the rockets became militarily ineffective so much that, like cannon, they became virtually unused and pointless.
The Confederate forces reportedly experimented with Congreve rockets.
During the war between Brazil, Uruguay and Argentina against Paraguay (1865–1870), the Paraguayans deployed Congreve rockets in several battles including: Battle of the Riachuelo (June 11, 1865) when land-based rockets were used against Brazilian naval forces without success; Battle of Paso de las Cuevas (August 12, 1865) when the Paraguayans utilized artillery and rockets;  combat of Corrales (January 31, 1866) when Paraguayan rockets were used against the Allied infantry and cavalry; Battle of Tuyutí (May 24, 1866) when Paraguayan forces used them to attack the advancing allied cavalry; battle of Yatayti Corá (July 10, 1866) where Paraguay used two launchers of 68-pound rockets; again in "Yatayti Cora" (January 20, 1867) when Paraguayan rockets caused a fire in the Argentinian camp. The Brazilian Navy employed them during the Battle of Curupayti (September 22, 1866), trying to destroy the reinforced Paraguayan trench field, but the rockets went short.
During the Crimean War (1853 - 1856) and the Indian Rebellion of 1857, marines and sailors from the Royal Navy used Congreve Rockets. "Bluejackets" armed with rockets from HMS Shannon and HMS Pearl, under the command of Captain William Peel, were among the Naval Brigade participating in the force led by Sir Colin Campbell at the Second Relief of Lucknow. There is an eye-witness narrative of the taking of the heavily-fortified Shah Najaf mosque written by William Forbes Mitchell: at a late stage Captain Peel had ... brought his infernal machine, known as a rocket battery, to the front, and sent a volley of rockets through the crowd on the ramparts.. After a second salvo from the rocket battery, many of the rebels fled and the mosque was finally taken by storm. When Forbes-Mitchell entered the enclosure he found only numerous dead defenders. According to a modern historian, "Peel's rockets had tipped the scale and the Shah Najaf fell to the British just as they had been about to fall back"..
As a weapon it remained in use until the 1850s, when it was superseded by the improved spinning design of William Hale.  In the 1870s, Congreve rockets were used to carry rescue lines to vessels in distress, gradually superseding the mortar of Captain Manby in operation from 1808 and rockets designed by John Dennett (1780-1852) that had been deployed in the late 1820s, first used to carry out a rescue in 1832 and used in the Irex rescue as late as 1890.
A wide variety of Congreve rockets were displayed at Firepower - The Royal Artillery Museum in South-East London, ranging in size from 3 to 300 pounds (1.4 to 136.1 kg).
The Science Museum has two eighteenth-century Indian war rockets in its collection. The Musée national de la Marine in Paris also features one rocket. The Stonington Historical Society in Stonington, Connecticut has one rocket in their collection that was fired at the town by the British in August 1814 during the Battle of Stonington.
Other examples in the United States can be seen at The Smithsonian National Museum and the Fort McHenry Museum. The Wittenburg Museum in Germany has a later-era rocket, and there is a reproduction of it in the Leipzig Museum; there is also one in a private collection in Leipzig.

Sir William Congreve, 2nd Baronet KCH FRS (20 May 1772 – 16 May 1828) was an English soldier, publisher, inventor,  rocket artillery pioneer renowned for his development and deployment of Congreve rockets, and a Tory Member of Parliament (MP).
He was the eldest son of Rebeca Elmston and Lt. General Sir William Congreve, 1st Baronet, the Comptroller of the Royal Laboratories at the Royal Arsenal and raised in Kent, England. He was educated at Newcome's school in Hackney, Wolverhampton Grammar School and Singlewell School in Kent. He then studied law at Trinity College, Cambridge, graduating BA in 1793 and MA in 1796. In 1814 he succeeded his father as second Baronet Congreve.
In 1803 he was a volunteer in the London and Westminster Light Horse, and was a London businessman who published a polemical newspaper, the Royal Standard and Political Register, which was Tory, pro-government and anti-Cobbett. Following a damaging libel action against it in 1804, Congreve withdrew from publishing and applied himself to inventing. Many years previously, several unsuccessful experiments had been made at the Royal Laboratory in Woolwich by Lt. General Thomas Desaguliers. In 1804, at his own expense, he began experimenting with rockets at Woolwich. Congreve was named as comptroller of the Royal Laboratory at Woolwich from 1814 until his death. (Congreve's father had also held the same post.)
Congreve was awarded the honorary rank of lieutenant colonel in the Hanoverian army's artillery in 1811, and was often referred to as "Colonel Congreve", later made major general in the same army. He was elected a Fellow of the Royal Society in March that year.  He was also awarded the Order of St. George following the Battle of Leipzig in 1813 and 1816 he was made Knight Commander of the Royal Guelphic Order (KCH). In 1821 he was awarded the Order of the Sword by the King of Sweden.
He enjoyed the friendship of the Prince Regent, in whose household he served as an equerry from 1811, carrying on the service when the Prince, who supported his rocket projects, became King George IV in 1820.
Earlier in 1812 he offered to contest for Parliament the borough of Liverpool but withdrew before polling for lack of support. He entered Parliament later that year when he was nominated as MP for the rotten borough of Gatton in 1812, but withdrew at the next elections in 1814 in favour of the son of the borough's proprietor Sir Mark Wood. In 1818 he was returned as Member for Plymouth, a seat he held until his death.
After living with a mistress and fathering two illegitimate sons, he married in December 1824, at Wessel, Prussia, Isabella Carvalho (or Charlotte), a young woman of Portuguese descent and widow of Henry Nisbett McEvoy. They had two sons and a daughter.
In 1824 he became general manager of the English Association for Gas Lighting on the Continent, a sizeable business producing gas for several cities in mainland Europe, including Berlin.
In later years he became a businessman and was chairman of the Equitable Loan Bank, director of the Arigna Iron and Coal Company, the Palladium Insurance Company and the Peruvian Mining Company. After a major fraud case began against him in 1826 in connection with the Arigna company, he fled to France, where he was taken seriously ill. He was prosecuted in his absence, the Lord Chancellor ultimately ruling, just before Congreve's death, that the transaction was "clearly fraudulent" and designed to profit Congreve and others.
He died in Toulouse, France, in May 1828, aged 55, and was buried there in the Protestant and Jewish cemetery of Chemin du Béarnais.
Congreve's name was used for a kind of friction match, "Congreaves", though he was not involved in their invention or production.
Mysorean rockets were the first iron-cased rockets that were successfully deployed for military use. Hyder Ali, the 18th century ruler of Mysore and his son and successor Tipu Sultan used them against the forces of the East India Company during the Anglo-Mysore Wars, beginning in 1780 with the Battle of Pollilur. In battles at Seringapatam in 1792 and 1799 these rockets were used with minimal effect against the British.
The experiences of the British with Mysorean rockets, mentioned in Munro's book of 1789, eventually led to the Royal Arsenal beginning a military rocket R&amp;D program in 1801. Several rocket cases were collected from Mysore and sent to Britain for analysis. The development was chiefly the work of William Congreve, who set up a research and development programme at the Woolwich Arsenal's laboratory. After development work was complete the rockets were manufactured in quantity further north, near Waltham Abbey, Essex. He was told that "the British at Seringapatam had suffered more from the rockets than from the shells or any other weapon used by the enemy." "In at least one instance", an eyewitness told Congreve, "a single rocket had killed three men and badly wounded others."
It has been suggested that Congreve may have adapted iron-cased gunpowder rockets for use by the British military from prototypes created by the Irish nationalist Robert Emmet during Emmet's Rebellion in 1803. But this seems far less likely given the fact that the British had been exposed to Indian rockets since 1780 at the latest, and that a vast quantity of unused rockets and their construction equipment fell into British hands at the end of the Anglo-Mysore Wars in 1799, 4 years before Emmet's rockets.
Congreve first demonstrated solid fuel rockets at the Royal Arsenal in 1805. He considered his work sufficiently advanced to engage in two Royal Navy attacks on the French fleet at Boulogne, France, one that year and one the next. In 1807 Congreve and sixteen Ordnance Department civilian employees were present at the Bombardment of Copenhagen, during which 300 rockets contributed to the conflagration of the city.
Congreve rockets were successfully used for the remainder of the Napoleonic Wars, with the most important employment of the weapon being at the Battle of Leipzig in 1813. The "rockets' red glare" in the American national anthem describes their firing at Fort McHenry during the War of 1812. In January 1814 the Royal Artillery absorbed the various companies armed with rockets into two Rocket Troops within the Royal Horse Artillery. They remained in the arsenal of the United Kingdom until the 1850s. Congreve organized the impressive firework displays in London for the peace of 1814 and for the coronation of George IV in 1821.
Besides his rockets, Congreve was a prolific (if indifferently successful) inventor for the remainder of his life. He registered 18 patents, of which 2 were for rockets. Congreve invented a gun-recoil mounting, a time-fuze, a rocket parachute attachment, a hydropneumatic canal lock (installed at Hampstead Road Lock, north London) and sluice (1813), a perpetual motion machine, a process of colour printing (1821) which was widely used in Germany, a new form of steam engine, and a method of consuming smoke (which was applied at the Royal Laboratory). He also took out patents for a clock in which time was measured by a ball rolling along a zig-zag track on an inclined plane; for protecting buildings against fire; inlaying and combining metals; unforgeable bank note paper; a method of killing whales by means of rockets; improvements in the manufacture of gunpowder; stereotype plates; fireworks; and gas meters.
Congreve's unsuccessful perpetual motion scheme involved an endless band which should raise more water by its capillary action on one side than on the other. He used capillary action of fluids that would disobey the law of never rising above their own level, so to produce a continual ascent and overflow. The device had an inclined plane over pulleys. At the top and bottom, there travelled an endless band of sponge, a bed, and, over this, again an endless band of heavy weights jointed together. The whole stood over the surface of still water. The capillary action raised the water, whereas the same thing could not happen in the part, since the weights would squeeze the water out. Hence, it was heavier than the other; but as "we know that if it were the same weight, there would be equilibrium, if the heavy chain be also uniform". Therefore, the extra weight of it would cause the chain to move round in the direction of the arrow, and this would go on, supposedly, continually.
In 1804 Congreve published A concise account of the origin and progress of the rocket system. Publication of A Concise Account of the Origin and Progress of the Rocket System by William Congreve was in 1807. In 1814 Congreve published The details of the rocket system. In 1827 The Congreve Rocket System was published in London. His other publications were: An Elementary Treatise on the Mounting of Naval Ordnance (1812); A Description of the Hydropneumatical Lock (1815); A New Principle of Steam-Engine (1819); Resumption of Cash Payments (1819) and Systems of Currency (1819).

Other coalition members: 100,000 regulars and militia at peak strength (1813)
Thousands more permanently injured.
Thousands of horses dead, captured or missing, unknown number of cannons, forts, wagons and buildings captured and destroyed.
Very heavy damage to industry and infrastructure (Spain, Russia, Prussia, Austria and Portugal) worth est.€2,000,000 
Unknown number of ships captured or destroyed.
€700,000 total war reparations by Prussia and Austria to France (1805–12)
Thousands more permanently injured.
Thousands of horses dead, captured or missing, unknown number of cannons, forts, wagons and buildings captured and destroyed.
Very heavy damage to industry and infrastructure (France, Belgium, Netherlands, Germany, Italy and French colonies) Worth est. €1,200,000
Dozens of ships captured or destroyed.
The Napoleonic Wars (1803–1815) were a series of major global conflicts pitting the French Empire and its allies, led by Napoleon I, against a fluctuating array of European states formed into various coalitions. It produced a period of French domination over most of continental Europe. The wars stemmed from the unresolved disputes associated with the French Revolution and the French Revolutionary Wars consisting of the War of the First Coalition (1792–1797) and the War of the Second Coalition (1798–1802). The Napoleonic wars are often described as five conflicts, each termed after the coalition that fought Napoleon: the Third Coalition (1803–1806), the Fourth (1806–07), the Fifth (1809), the Sixth (1813–14), and the Seventh (1815) plus the Peninsular War (1807–1814) and the French invasion of Russia (1812).
Napoleon, upon ascending to First Consul of France in 1799, had inherited a republic in chaos; he subsequently created a state with stable finances, a strong bureaucracy, and a well-trained army. In December 1805 Napoleon achieved what is considered his greatest victory, defeating the allied Russo-Austrian army at Austerlitz. At sea, the British severely defeated the joint Franco-Spanish navy in the Battle of Trafalgar on 21 October 1805. This victory secured British control of the seas and prevented the invasion of Britain. Concerned about increasing French power, Prussia led the creation of the Fourth Coalition with Russia, Saxony, and Sweden, which resumed war in October 1806. Napoleon quickly defeated the Prussians at Jena and the Russians at Friedland, bringing an uneasy peace to the continent. The peace failed, though, as war broke out in 1809, with the badly prepared Fifth Coalition, led by Austria. At first, the Austrians won a stunning victory at Aspern-Essling, but were quickly defeated at Wagram.
Hoping to isolate and weaken Britain economically through his Continental System, Napoleon launched an invasion of Portugal, the only remaining British ally in continental Europe. After occupying Lisbon in November 1807, and with the bulk of French troops present in Spain, Napoleon seized the opportunity to turn against his former ally, depose the reigning Spanish royal family and declare his brother King of Spain in 1808 as José I. The Spanish and Portuguese revolted with British support and expelled the French from Iberia in 1814 after six years of fighting.
Concurrently, Russia, unwilling to bear the economic consequences of reduced trade, routinely violated the Continental System, prompting Napoleon to launch a massive invasion of Russia in 1812. The resulting campaign ended in disaster for France and the near destruction of Napoleon's Grande Armée.
Encouraged by the defeat, Austria, Prussia, Sweden, and Russia formed the Sixth Coalition and began a new campaign against France, decisively defeating Napoleon at Leipzig in October 1813 after several inconclusive engagements. The Allies then invaded France from the east, while the Peninsular War spilled over into southwestern France. Coalition troops captured Paris at the end of March 1814 and forced Napoleon to abdicate in April. He was exiled to the island of Elba, and the Bourbons were restored to power. But Napoleon escaped in February 1815, and reassumed control of France for around one hundred days. After forming the Seventh Coalition, the allies defeated him at Waterloo in June 1815 and exiled him to the island of Saint Helena, where he died six years later.
The Congress of Vienna redrew the borders of Europe and brought a period of relative peace. The wars had profound consequences on global history, including the spread of nationalism and liberalism, the rise of Britain as the world's foremost naval and economic power, the appearance of independence movements in Latin America and subsequent decline of the Spanish and Portuguese Empires, the fundamental reorganization of German and Italian territories into larger states, and the introduction of radically new methods of conducting warfare, as well as civil law. After the end of the Napoleonic Wars there was a period of relative peace in continental Europe, lasting until the Crimean War in 1853.
Napoleon seized power in 1799, creating a military dictatorship. There are a number of opinions on the date to use as the formal beginning of the Napoleonic Wars; 18 May 1803 is often used, when Britain and France ended the only short period of peace between 1792 and 1814. The Napoleonic Wars began with the War of the Third Coalition, which was the first of the Coalition Wars against the First French Republic after Napoleon's accession as leader of France.
Britain ended the Treaty of Amiens and declared war on France in May 1803. Among the reasons were Napoleon's changes to the international system in Western Europe, especially in Switzerland, Germany, Italy, and the Netherlands. Historian Frederick Kagan argues that Britain was irritated in particular by Napoleon's assertion of control over Switzerland. Furthermore, Britons felt insulted when Napoleon stated that their country deserved no voice in European affairs, even though King George III was an elector of the Holy Roman Empire. For its part, Russia decided that the intervention in Switzerland indicated that Napoleon was not looking toward a peaceful resolution of his differences with the other European powers.
The British hastily enforced a naval blockade of France to starve it of resources. Napoleon responded with economic embargoes against Britain, and sought to eliminate Britain's Continental allies to break the coalitions arrayed against him. The so-called Continental System formed a league of armed neutrality to disrupt the blockade and enforce free trade with France. The British responded by capturing the Danish fleet, breaking up the league, and later secured dominance over the seas, allowing it to freely continue its strategy. But Napoleon won the War of the Third Coalition at Austerlitz, forcing the Austrian Empire out of the war and formally dissolving the Holy Roman Empire. Within months, Prussia declared war, triggering a War of the Fourth Coalition. This war ended disastrously for Prussia, defeated and occupied within 19 days of the beginning of the campaign. Napoleon subsequently defeated Russia at Friedland, creating powerful client states in Eastern Europe and ending the fourth coalition.
Concurrently, the refusal of Portugal to commit to the Continental System, and Spain's failure to maintain it led to the Peninsular War and the outbreak of the War of the Fifth Coalition. The French occupied Spain and formed a Spanish client kingdom, ending the alliance between the two. Heavy British involvement in the Iberian Peninsula soon followed while a British effort to capture Antwerp failed. Napoleon oversaw the situation in Iberia, defeating the Spanish, and expelling the British from the Peninsula. Austria, keen to recover territory lost during the War of the Third Coalition, invaded France's client states in Eastern Europe. Napoleon defeated the fifth coalition at Wagram.
Anger at British naval actions helped push the United States to declare war on Britain in the War of 1812, but it did not become an ally of France. Grievances over control of Poland, and Russia's withdrawal from the Continental System, led to Napoleon invading Russia in June 1812. The invasion was an unmitigated disaster for Napoleon; scorched earth tactics, desertion, French strategic failures and the onset of the Russian winter compelled Napoleon to retreat with massive losses. Napoleon suffered further setbacks; French power in the Iberian Peninsula was broken at Battle of Vitoria the following summer, and a new coalition began the War of the Sixth Coalition.
The coalition defeated Napoleon at Leipzig, precipitating his fall from power and eventual abdication on 6 April 1814. The victors exiled Napoleon to Elba and restored the Bourbon monarchy. Napoleon escaped from Elba in 1815, gathering enough support to overthrow the monarchy of Louis XVIII, triggering a seventh, and final, coalition against him. Napoleon was decisively defeated at Waterloo, and he abdicated again on 22 June. On 15 July, he surrendered to the British at Rochefort, and was permanently exiled to remote Saint Helena. The Treaty of Paris, signed on 20 November 1815, formally ended the war.
The Bourbon monarchy was restored once more, and the victors began the Congress of Vienna to restore peace to the continent. As a direct result of the war, the Kingdom of Prussia rose to become a great power on the continent, while Great Britain, with its unequalled Royal Navy and growing Empire, became the world's dominant superpower, beginning the Pax Britannica. The Holy Roman Empire was dissolved, and the philosophy of nationalism that emerged early in the war contributed greatly to the later unification of the German states, and those of the Italian peninsula. The war in Iberia greatly weakened Spanish power, and the Spanish Empire began to unravel; Spain would lose nearly all of its American possessions by 1833. The Portuguese Empire shrank, with Brazil declaring independence in 1822.
The wars revolutionised European warfare; the application of mass conscription and total war led to campaigns of unprecedented scale, as whole nations committed all their economic and industrial resources to a collective war effort. Tactically, the French Army redefined the role of artillery, while Napoleon emphasised mobility to offset numerical disadvantages, and aerial surveillance was used for the first time in warfare. The highly successful Spanish guerrillas demonstrated the capability of a people driven by fervent nationalism against an occupying force.
Due to the longevity of the wars, the extent of Napoleon's conquests, and the popularity of the ideals of the French Revolution, the ideals had a deep impact on European social culture. Many subsequent revolutions, such as that of Russia, looked to the French as their source of inspiration, while its core founding tenets greatly expanded the arena of human rights and shaped modern political philosophies in use today.
The outbreak of the French Revolution had been received with great alarm by the rulers of Europe's continental powers, which had been further exacerbated by the execution of Louis XVI of France, and the overthrow of the French monarchy. In 1793, the Austrian Empire, the Kingdom of Sardinia, the Kingdom of Naples, Prussia, the Spanish Empire, and the Kingdom of Great Britain formed the First Coalition to curtail the growing unrest in France. Measures such as mass conscription, military reforms, and total war allowed France to defeat the coalition, despite the concurrent civil war in France. Napoleon, then a general in the French army, forced the Austrians to sign the Treaty of Campo Formio, leaving only Great Britain opposed to the fledgling French Republic.
A Second Coalition was formed in 1798 by Great Britain, Austria, Naples, the Ottoman Empire, the Papal States, Portugal, Russia, and Sweden. The French Republic, under the Directory, suffered from heavy levels of corruption and internal strife. The new republic also lacked funds, and no longer enjoyed the services of Lazare Carnot, the minister of war who had guided France to its victories during the early stages of the Revolution. Bonaparte, commander of the Armée d'Italie in the latter stages of the First Coalition, had launched a campaign in Egypt, intending to disrupt the British economic powerhouse of India. Pressed from all sides, the Republic suffered a string of successive defeats against revitalised enemies, supported by Britain's financial help.
Bonaparte returned to France from Egypt on 23 August 1799, his campaign there having failed. He seized control of the French government on 9 November, in a bloodless coup d'état, replacing the Directory with the Consulate and transforming the republic into a de facto dictatorship. He further reorganised the French military forces, establishing a large reserve army positioned to support campaigns on the Rhine or in Italy. Russia had already been knocked out of the war, and, under Napoleon's leadership, the French decisively defeated the Austrians in June 1800, crippling Austrian capabilities in Italy. Austria was definitively defeated that December, by Moreau's forces in Bavaria. The Austrian defeat was sealed by the Treaty of Lunéville early the following year, further compelling the British to sign the Treaty of Amiens with France, establishing a tenuous peace.
No consensus exists as to when the French Revolutionary Wars ended and the Napoleonic Wars began. Possible dates include 9 November 1799, when Bonaparte seized power on 18 Brumaire, the date according to the Republican Calendar then in use; 18 May 1803, when Britain and France ended the one short period of peace between 1792 and 1814; or 2 December 1804, when Bonaparte crowned himself Emperor.
British historians occasionally refer to the nearly continuous period of warfare from 1792 to 1815 as the Great French War, or as the final phase of the Anglo-French Second Hundred Years' War, spanning the period 1689 to 1815. Historian Mike Rapport (2013) suggested using the term "French Wars" to unambiguously describe the entire period from 1792 to 1815.
In France, the Napoleonic Wars are generally integrated with the French Revolutionary Wars: Les guerres de la Révolution et de l'Empire.
German historiography may count the War of the Second Coalition (1798/9–1801/2), during which Napoleon seized power, as the Erster Napoleonischer Krieg ("First Napoleonic War").
In Dutch historiography, it is common to refer to the seven major wars between 1792 and 1815 as the Coalition Wars (coalitieoorlogen), referring to the first two as the French Revolution Wars (Franse Revolutieoorlogen).
Napoleon was, and remains, famous for his battlefield victories, and historians have spent enormous attention in analysing them. In 2008, Donald Sutherland wrote:
The ideal Napoleonic battle was to manipulate the enemy into an unfavourable position through manoeuvre and deception, force him to commit his main forces and reserve to the main battle and then undertake an enveloping attack with uncommitted or reserve troops on the flank or rear. Such a surprise attack would either produce a devastating effect on morale or force him to weaken his main battle line. Either way, the enemy's own impulsiveness began the process by which even a smaller French army could defeat the enemy's forces one by one.After 1807, Napoleon's creation of a highly mobile, well-armed artillery force gave artillery usage increased tactical importance. Napoleon, rather than relying on infantry to wear away the enemy's defences, could now use massed artillery as a spearhead to pound a break in the enemy's line. Once that was achieved he sent in infantry and cavalry.
Britain was irritated by several French actions following the Treaty of Amiens. Bonaparte had annexed Piedmont and Elba, made himself President of the Italian Republic, a state in northern Italy that France had set up, and failed to evacuate Holland, as it had agreed to do in the treaty. France continued to interfere with British trade despite peace having been made and complained about Britain harbouring certain individuals and not cracking down on the anti-French press.
Malta had been captured by Britain during the war and was subject to a complex arrangement in the 10th article of the Treaty of Amiens where it was to be restored to the Knights of St. John with a Neapolitan garrison and placed under the guarantee of third powers. The weakening of the Knights of St. John by the confiscation of their assets in France and Spain along with delays in obtaining guarantees prevented the British from evacuating it after three months as stipulated in the treaty.
The Helvetic Republic had been set up by France when it invaded Switzerland in 1798. France had withdrawn its troops, but violent strife broke out against the government, which many Swiss saw as overly centralised. Bonaparte reoccupied the country in October 1802 and imposed a compromise settlement. This caused widespread outrage in Britain, which protested that this was a violation of the Treaty of Lunéville. Although continental powers were unprepared to act, the British decided to send an agent to help the Swiss obtain supplies, and also ordered their military not to return Cape Colony to Holland as they had committed to do in the Treaty of Amiens.
Swiss resistance collapsed before anything could be accomplished, and after a month Britain countermanded the orders not to restore Cape Colony. At the same time, Russia finally joined the guarantee with regard to Malta. Concerned that there would be hostilities when Bonaparte found out that Cape Colony had been retained, the British began to procrastinate on the evacuation of Malta. In January 1803 a government paper in France published a report from a commercial agent which noted the ease with which Egypt could be conquered. The British seized on this to demand satisfaction and security before evacuating Malta, which was a convenient stepping stone to Egypt. France disclaimed any desire to seize Egypt and asked what sort of satisfaction was required but the British were unable to give a response. There was still no thought of going to war; Prime Minister Addington publicly affirmed that Britain was in a state of peace.
In early March 1803, the Addington ministry received word that Cape Colony had been re-occupied by the British army in accordance with the orders which had subsequently been countermanded. On 8 March they ordered military preparations to guard against possible French retaliation and justified them by falsely claiming that it was only in response to French preparations and that they were conducting serious negotiations with France. In a few days, it was known that Cape Colony had been surrendered in accordance with the counter-orders, but it was too late. Bonaparte berated the British ambassador in front of 200 spectators over the military preparations.
The Addington ministry realised they would face an inquiry over their false reasons for the military preparations, and during April unsuccessfully attempted to secure the support of William Pitt the Younger to shield them from damage. In the same month the ministry issued an ultimatum to France demanding the retention of Malta for at least ten years, the permanent acquisition of the island of Lampedusa from the Kingdom of Sicily, and the evacuation of Holland. They also offered to recognise French gains in Italy if they evacuated Switzerland and compensated the King of Sardinia for his territorial losses. France offered to place Malta in the hands of Russia to satisfy British concerns, pull out of Holland when Malta was evacuated, and form a convention to give satisfaction to Britain on other issues. The British falsely denied that Russia had made an offer and their ambassador left Paris. Desperate to avoid war, Bonaparte sent a secret offer where he agreed to let Britain retain Malta if France could occupy the Otranto peninsula in Naples. All efforts were futile and Britain declared war on 18 May 1803.
Britain ended the uneasy truce created by the Treaty of Amiens when it declared war on France in May 1803. The British were increasingly angered by Napoleon's reordering of the international system in Western Europe, especially in Switzerland, Germany, Italy and the Netherlands. Kagan argues that Britain was especially alarmed by Napoleon's assertion of control over Switzerland. The British felt insulted when Napoleon said it deserved no voice in European affairs (even though King George was an elector of the Holy Roman Empire) and sought to restrict the London newspapers that were vilifying him.
Britain had a sense of loss of control, as well as loss of markets, and was worried by Napoleon's possible threat to its overseas colonies. McLynn argues that Britain went to war in 1803 out of a "mixture of economic motives and national neuroses – an irrational anxiety about Napoleon's motives and intentions." McLynn concludes that it proved to be the right choice for Britain because, in the long run, Napoleon's intentions were hostile to the British national interest. Napoleon was not ready for war and so this was the best time for Britain to stop them. Britain seized upon the Malta issue, refusing to follow the terms of the Treaty of Amiens and evacuate the island.
The deeper British grievance was their perception that Napoleon was taking personal control of Europe, making the international system unstable, and forcing Britain to the sidelines. Numerous scholars have argued that Napoleon's aggressive posture made him enemies and cost him potential allies. As late as 1808, the continental powers affirmed most of his gains and titles, but the continuing conflict with Britain led him to start the Peninsular War and the invasion of Russia, which many scholars see as a dramatic miscalculation.

There was one serious attempt to negotiate peace with France during the war, made by Charles James Fox in 1806. The British wanted to retain their overseas conquests and have Hanover restored to George III in exchange for accepting French conquests on the continent. The French were willing to cede Malta, Cape Colony, Tobago, and French Indian posts to Britain but wanted to obtain Sicily in exchange for the restoration of Hanover, a condition the British refused.
Unlike its many coalition partners, Britain remained at war throughout the period of the Napoleonic Wars. Protected by naval supremacy (in the words of Admiral Jervis to the House of Lords "I do not say, my Lords, that the French will not come. I say only they will not come by sea"), Britain did not have to spend the entire war defending itself and could therefore focus on supporting its embattled allies, maintaining low-intensity land warfare on a global scale for over a decade. The British government paid out large sums of money to other European states so that they could pay armies in the field against France. These payments are colloquially known as the Golden Cavalry of St George. The British Army provided long-term support to the Spanish rebellion in the Peninsular War of 1808–1814, assisted by Spanish guerrilla ('little war') tactics. Anglo-Portuguese forces under Arthur Wellesley supported the Spanish, which campaigned successfully against the French armies, eventually driving them from Spain and allowing Britain to invade southern France. By 1815, the British Army played the central role in the final defeat of Napoleon at Waterloo.
Beyond minor naval actions against British imperial interests, the Napoleonic Wars were much less global in scope than preceding conflicts such as the Seven Years' War, which historians term a "world war".
In response to the naval blockade of the French coasts enacted by the British government on 16 May 1806, Napoleon issued the Berlin Decree on 21 November 1806, which brought into effect the Continental System. This policy aimed to eliminate the threat from Britain by closing French-controlled territory to its trade. Britain maintained a standing army of 220,000 at the height of the Napoleonic Wars, of whom less than half were available for campaigning. The rest were necessary for garrisoning Ireland and the colonies and providing security for Britain. France's strength peaked at around 2,500,000 full-time and part-time soldiers including several hundred thousand National Guardsmen whom Napoleon could draft into the military if necessary. Both nations enlisted large numbers of sedentary militia who were unsuited for campaigning and were mostly employed to release regular forces for active duty.
The Royal Navy disrupted France's extra-continental trade by seizing and threatening French shipping and colonial possessions, but could do nothing about France's trade with the major continental economies and posed little threat to French territory in Europe. France's population and agricultural capacity far outstripped that of Britain. Britain had the greatest industrial capacity in Europe, and its mastery of the seas allowed it to build up considerable economic strength through trade. This ensured that France could never consolidate its control over Europe in peace. Many in the French government believed that cutting Britain off from the Continent would end its economic influence over Europe and isolate it.
A key element in British success was its ability to mobilise the nation's industrial and financial resources and apply them to defeating France. Although the UK had a population of approximately 16 million against France's 30 million, the French numerical advantage was offset by British subsidies that paid for many of the Austrian and Russian soldiers, peaking at about 450,000 men in 1813. Under the Anglo–Russian agreement of 1803, Britain paid a subsidy of £1.5 million for every 100,000 Russian soldiers in the field.
British national output remained strong, and the well-organised business sector channeled products into what the military needed. Britain used its economic power to expand the Royal Navy, doubling the number of frigates, adding 50% more large ships of the line, and increasing the number of sailors from 15,000 to 133,000 in eight years after the war began in 1793. France saw its navy shrink by more than half. The smuggling of finished products into the continent undermined French efforts to weaken the British economy by cutting off markets. Subsidies to Russia and Austria kept them in the war. The British budget in 1814 reached £98 million, including £10 million for the Royal Navy, £40 million for the army, £10 million for the allies, and £38 million as interest on the national debt, which soared to £679 million, more than double the GDP. This debt was supported by hundreds of thousands of investors and taxpayers, despite the higher taxes on land and a new income tax. The cost of the war came to £831 million. In contrast, the French financial system was inadequate and Napoleon's forces had to rely in part on requisitions from conquered lands.
From London in 1813 to 1815, Nathan Mayer Rothschild was instrumental in almost single-handedly financing the British war effort, organising the shipment of bullion to the Duke of Wellington's armies across Europe, as well as arranging the payment of British financial subsidies to their continental allies.
Britain gathered together allies to form the Third Coalition against The French Empire after Napoleon self-proclaimed as emperor. In response, Napoleon seriously considered an invasion of Great Britain, and massed 180,000 troops at Boulogne. Before he could invade, he needed to achieve naval superiority—or at least to pull the British fleet away from the English Channel. A complex plan to distract the British by threatening their possessions in the West Indies failed when a Franco-Spanish fleet under Admiral Villeneuve turned back after an indecisive action off Cape Finisterre on 22 July 1805. The Royal Navy blockaded Villeneuve in Cádiz until he left for Naples on 19 October; the British squadron caught and overwhelmingly defeated the combined enemy fleet in the Battle of Trafalgar on 21 October (the British commander, Lord Nelson, died in the battle). Napoleon never again had the opportunity to challenge the British at sea, nor to threaten an invasion. He again turned his attention to enemies on the Continent.
In April 1805, Britain and Russia signed a treaty with the aim of removing the French from the Batavian Republic (roughly present-day Netherlands) and the Swiss Confederation. Austria joined the alliance after the annexation of Genoa and the proclamation of Napoleon as King of Italy on 17 March 1805. Sweden, which had already agreed to lease Swedish Pomerania as a military base for British troops against France, entered the coalition on 9 August.
The Austrians began the war by invading Bavaria on 8 September 1805 with an army of about 70,000 under Karl Mack von Leiberich, and the French army marched out from Boulogne in late July 1805 to confront them. At Ulm (25 September – 20 October) Napoleon surrounded Mack's army, forcing its surrender without significant losses.
With the main Austrian army north of the Alps defeated (another army under Archduke Charles fought against André Masséna's French army in Italy), Napoleon occupied Vienna on 13 November. Far from his supply lines, he faced a larger Austro–Russian army under the command of Mikhail Kutuzov, with the Emperor Alexander I of Russia personally present. On 2 December, Napoleon crushed the Austro–Russian force in Moravia at Austerlitz (usually considered his greatest victory). He inflicted 25,000 casualties on a numerically superior enemy army while sustaining fewer than 7,000 in his own force.
Austria signed the Treaty of Pressburg (26 December 1805) and left the coalition. The treaty required the Austrians to give up Venetia to the French-dominated Kingdom of Italy and the Tyrol to Bavaria. With the withdrawal of Austria from the war, stalemate ensued. Napoleon's army had a record of continuous unbroken victories on land, but the full force of the Russian army had not yet come into play. Napoleon had now consolidated his hold on France, had taken control of Belgium, the Netherlands, Switzerland, and most of Western Germany and northern Italy. His admirers say that Napoleon wanted to stop now, but was forced to continue in order to gain greater security from the countries that refused to accept his conquests. Esdaile rejects that explanation and instead says that it was a good time to stop expansion, for the major powers were ready to accept Napoleon as he was:
Within months of the collapse of the Third Coalition, the Fourth Coalition (1806–1807) against France was formed by Britain, Prussia, Russia, Saxony, and Sweden. In July 1806, Napoleon formed the Confederation of the Rhine out of the many tiny German states which constituted the Rhineland and most other western parts of Germany. He amalgamated many of the smaller states into larger electorates, duchies, and kingdoms to make the governance of non-Prussian Germany smoother. Napoleon elevated the rulers of the two largest Confederation states, Saxony and Bavaria, to the status of kings.
In August 1806, the Prussian king, Frederick William III, decided to go to war independently of any other great power. The army of Russia, a Prussian ally, in particular, was too far away to assist. On 8 October 1806, Napoleon unleashed all the French forces east of the Rhine into Prussia. Napoleon defeated a Prussian army at Jena (14 October 1806), and Davout defeated another at Auerstädt on the same day. 160,000 French soldiers (increasing in number as the campaign went on) attacked Prussia, moving with such speed that they destroyed the entire Prussian army as an effective military force. Out of 250,000 troops the Prussians sustained 25,000 casualties, lost a further 150,000 as prisoners, 4,000 artillery pieces, and over 100,000 muskets. At Jena, Napoleon had fought only a detachment of the Prussian force. The battle at Auerstädt involved a single French corps defeating the bulk of the Prussian army. Napoleon entered Berlin on 27 October 1806. He visited the tomb of Frederick the Great and instructed his marshals to remove their hats there, saying, "If he were alive we wouldn't be here today". Napoleon had taken only 19 days from beginning his attack on Prussia to knock it out of the war with the capture of Berlin and the destruction of its principal armies at Jena and Auerstädt. Saxony left Prussia, and together with small states from north Germany, allied with France.
In the next stage of the war, the French drove Russian forces out of Poland and employed many Polish and German soldiers in several sieges in Silesia and Pomerania, with the assistance of Dutch and Italian soldiers in the latter case. Napoleon then turned north to confront the remainder of the Russian army and to try to capture the temporary Prussian capital at Königsberg. A tactical draw at Eylau (7–8 February 1807), followed by capitulation at Danzig (24 May 1807) and the Battle of Heilsberg (10 June 1807), forced the Russians to withdraw further north. Napoleon decisively beat the Russian army at Friedland (14 June 1807), following which Alexander had to make peace with Napoleon at Tilsit (7 July 1807). In Germany and Poland, new Napoleonic client states, such as the Kingdom of Westphalia, Duchy of Warsaw, and Republic of Danzig, were established.
By September, Marshal Guillaume Brune completed the occupation of Swedish Pomerania, allowing the Swedish army to withdraw with all its munitions of war.
Britain's first response to Napoleon's Continental System was to launch a major naval attack against Denmark. Although ostensibly neutral, Denmark was under heavy French and Russian pressure to pledge its fleet to Napoleon. London could not take the chance of ignoring the Danish threat. In August 1807, the Royal Navy besieged and bombarded Copenhagen, leading to the capture of the Dano–Norwegian fleet, and assuring use of the sea lanes in the North and Baltic seas for the British merchant fleet. Denmark joined the war on the side of France, but without a fleet it had little to offer, beginning an engagement in a naval guerrilla war in which small gunboats attacking larger British ships in Danish and Norwegian waters. Denmark also committed themselves to participate in a war against Sweden together with France and Russia.
At Tilsit, Napoleon and Alexander had agreed that Russia should force Sweden to join the Continental System, which led to a Russian invasion of Finland in February 1808, followed by a Danish declaration of war in March. Napoleon also sent an auxiliary corps, consisting of troops from France, Spain and the Netherlands, led by Marshal Jean-Baptiste Bernadotte, to Denmark to participate in the invasion of Sweden. But British naval superiority prevented the armies from crossing the Øresund strait, and the war came mainly to be fought along the Swedish–Norwegian border. At the Congress of Erfurt (September–October 1808), France and Russia further agreed on the division of Sweden into two parts separated by the Gulf of Bothnia, where the eastern part became the Russian Grand Duchy of Finland. British voluntary attempts to assist Sweden with humanitarian aid remained limited and did not prevent Sweden from adopting a more Napoleon-friendly policy.
The war between Denmark and Britain effectively ended with a British victory at the battle of Lyngør in 1812, involving the destruction of the last large Dano–Norwegian ship—the frigate Najaden.
In 1807 Napoleon created a powerful outpost of his empire in Central Europe. Poland had recently been partitioned by its three large neighbours, but Napoleon created the Grand Duchy of Warsaw, which depended on France from the very beginning. The duchy consisted of lands seized by Austria and Prussia; its Grand Duke was Napoleon's ally the king of Saxony, but Napoleon appointed the intendants who ran the country. The population of 4.3 million was released from occupation and by 1814 sent about 200,000 men to Napoleon's armies. That included about 90,000 who marched with him to Moscow; few marched back. The Russians strongly opposed any move towards an independent Poland and one reason Napoleon invaded Russia in 1812 was to punish them. The Grand Duchy was dissolved in 1815; Poland did not become a state again until 1918, following the dissolution of the Russian Empire. Napoleon's impact on Poland was huge, including the Napoleonic legal code, the abolition of serfdom, and the introduction of modern middle-class bureaucracies.
The Iberian conflict began when Portugal continued trade with Britain despite French restrictions. When Spain failed to maintain the Continental System, the uneasy Spanish alliance with France ended in all but name. French troops gradually encroached on Spanish territory until they occupied Madrid, and installed a client monarchy. This provoked an explosion of popular rebellions across Spain. Heavy British involvement soon followed.
After defeats in Spain suffered by France, Napoleon took charge and enjoyed success, retaking Madrid, defeating the Spanish and forcing a withdrawal of the heavily out-numbered British army from the Iberian Peninsula (Battle of Corunna, 16 January 1809). But when he left, the guerrilla war against his forces in the countryside continued to tie down great numbers of troops. The outbreak of the War of the Fifth Coalition prevented Napoleon from successfully wrapping up operations against British forces by necessitating his departure for Austria, and he never returned to the Peninsular theatre. The British then sent in a fresh army under Sir Arthur Wellesley (later the Duke of Wellington). For a time, the British and Portuguese remained restricted to the area around Lisbon (behind their impregnable lines of Torres Vedras), while their Spanish allies were besieged in Cadiz.
The Peninsular war proved a major disaster for France. Napoleon did well when he was in direct charge, but severe losses followed his departure, as he severely underestimated how much manpower would be needed. The effort in Spain was a drain on money, manpower and prestige. Historian David Gates called it the "Spanish ulcer." Napoleon realised it had been a disaster for his cause, writing later, "That unfortunate war destroyed me ... All the circumstances of my disasters are bound up in that fatal knot."
The Peninsular campaigns witnessed 60 major battles and 30 major sieges, more than any other of the Napoleonic conflicts, and lasted over six years, far longer than any of the others. France and her allies lost at least 91,000 killed in action and 237,000 wounded in the peninsula. From 1812, the Peninsular War merged with the War of the Sixth Coalition.
The Fifth Coalition (1809) of Britain and Austria against France formed as Britain engaged in the Peninsular War in Spain and Portugal. The sea became a major theatre of war against Napoleon's allies. Austria, previously an ally of France, took the opportunity to attempt to restore its imperial territories in Germany as held prior to Austerlitz. During the time of the Fifth Coalition, the Royal Navy won a succession of victories in the French colonies. On land the major battles included Battle of Raszyn, Battle of Eckmuhl, Battle of Raab, Battle of Aspern-Essling, and Battle of Wagram.
On land, the Fifth Coalition attempted few extensive military endeavours. One, the Walcheren Expedition of 1809, involved a dual effort by the British Army and the Royal Navy to relieve Austrian forces under intense French pressure. It ended in disaster after the Army commander, John Pitt, 2nd Earl of Chatham, failed to capture the objective, the naval base of French-controlled Antwerp. For the most part of the years of the Fifth Coalition, British military operations on land (apart from the Iberian Peninsula) remained restricted to hit-and-run operations executed by the Royal Navy, which dominated the sea after having beaten down almost all substantial naval opposition from France and its allies and blockading what remained of France's naval forces in heavily fortified French-controlled ports. These rapid-attack operations were aimed mostly at destroying blockaded French naval and mercantile shipping and the disruption of French supplies, communications, and military units stationed near the coasts. Often, when British allies attempted military actions within several dozen miles or so of the sea, the Royal Navy would arrive, land troops and supplies, and aid the coalition's land forces in a concerted operation. Royal Navy ships even provided artillery support against French units when fighting strayed near enough to the coastline. The ability and quality of the land forces governed these operations. For example, when operating with inexperienced guerrilla forces in Spain, the Royal Navy sometimes failed to achieve its objectives because of the lack of manpower that the Navy's guerrilla allies had promised to supply.
Austria achieved some initial victories against the thinly spread army of Marshal Berthier. Napoleon had left Berthier with only 170,000 men to defend France's entire eastern frontier (in the 1790s, 800,000 men had carried out the same task, but holding a much shorter front).
In the east, the Austrians drove into the Duchy of Warsaw but suffered defeat at the Battle of Raszyn on 19 April 1809. The Polish army captured West Galicia following its earlier success. Napoleon assumed personal command and bolstered the army for a counter-attack on Austria. After a few small battles, the well-run campaign forced the Austrians to withdraw from Bavaria, and Napoleon advanced into Austria. His hurried attempt to cross the Danube resulted in the major Battle of Aspern-Essling (22 May 1809) – Napoleon's first significant tactical defeat. But the Austrian commander, Archduke Charles, failed to follow up on his indecisive victory, allowing Napoleon to prepare and seize Vienna in early July. He defeated the Austrians at Wagram, on 5–6 July. (It was during the middle of that battle that Marshal Bernadotte was stripped of his command after retreating contrary to Napoleon's orders. Shortly thereafter, Bernadotte took up the offer from Sweden to fill the vacant position of Crown Prince there. Later he actively participated in wars against his former Emperor.)
The War of the Fifth Coalition ended with the Treaty of Schönbrunn (14 October 1809). In the east, only the Tyrolese rebels led by Andreas Hofer continued to fight the French-Bavarian army until finally defeated in November 1809. In the west, the Peninsular War continued. Economic warfare between Britain and France continued: The British continued a naval blockade of French-controlled territory. Due to military shortages and lack of organisation in French territory, many breaches of the Continental System occurred and the French Continental System was largely ineffective and did little economic damage to Great Britain. Both sides entered further conflicts in attempts to enforce their blockade. As Napoleon realised that extensive trade was going through Spain and Russia, he invaded those two countries.; the British fought the United States in the War of 1812 (1812–1815).
In 1810, the French Empire reached its greatest extent. Napoleon married Marie-Louise, an Austrian Archduchess, with the aim of ensuring a more stable alliance with Austria and of providing the Emperor with an heir (something his first wife, Josephine, had failed to do). As well as the French Empire, Napoleon controlled the Swiss Confederation, the Confederation of the Rhine, the Duchy of Warsaw and the Kingdom of Italy. Territories allied with the French included:
and Napoleon's former enemies, Sweden, Prussia and Austria.
The Napoleonic Wars were the direct cause of wars in the Americas and elsewhere.
The War of 1812 coincided with the War of the Sixth Coalition. Historians in the United States and Canada see it as a war in its own right, while Europeans often see it as a minor theatre of the Napoleonic Wars. The United States declared war on Britain because of British interference with American merchant ships and forced enlistment into the Royal Navy. France had interfered as well, and the United States considered declaring war on France. The war ended in a military stalemate, and there were no boundary changes at the Treaty of Ghent, which took effect in early 1815 when Napoleon was on Elba.
The abdication of Kings Carlos IV and Fernando VII of Spain and the installation of Napoleon's brother as King José provoked civil wars and revolutions leading to the independence of most of Spain's mainland American colonies. In Spanish America many local elites formed juntas and set up mechanisms to rule in the name of Ferdinand VII, whom they considered the legitimate Spanish monarch. The outbreak of the Spanish American wars of independence in most of the empire was a result of Napoleon's destabilizing actions in Spain and led to the rise of strongmen in the wake of these wars. The defeat of Napoleon at Waterloo in 1815 caused an exodus of French soldiers into Latin America where they joined ranks with the armies of the independence movements. While these officials had a role in various victories such as the Capture of Valdivia (1820) some are held responsible for significant defeats at the hands of the royalist as is the case of Second Battle of Cancha Rayada (1818).
In contrast, the Portuguese royal family escaped to Brazil and established the court there, resulting in political stability for Portuguese America. With the defeat of Napoleon and the return of the Braganza monarchy to Portugal, the heir remained in Brazil and declared Brazilian independence, achieving it peacefully with the territory intact.
The Haitian Revolution began in 1791, just before the French Revolutionary Wars, and continued until 1804. France's defeat resulted in the independence of Saint-Domingue and led Napoleon to sell the territory making up the Louisiana Purchase to the United States.
During the Napoleonic Wars, the United States, Sweden, and Sicily fought against the Barbary pirates in the Mediterranean.
The Treaty of Tilsit in 1807 resulted in the Anglo–Russian War (1807–12). Emperor Alexander I declared war on Britain after the British attack on Denmark in September 1807. British men-of-war supported the Swedish fleet during the Finnish War and won victories over the Russians in the Gulf of Finland in July 1808 and August 1809. The success of the Russian army on land, however, forced Sweden to sign peace treaties with Russia in 1809 and with France in 1810, and to join the blockade against Britain. But Franco–Russian relations became progressively worse after 1810, and the Russian war with Britain effectively ended. In April 1812, Britain, Russia and Sweden signed secret agreements directed against Napoleon.
The central issue for both Napoleon and Tsar Alexander I was control over Poland. Each wanted a semi-independent Poland he could control. As Esdaile notes, "Implicit in the idea of a Russian Poland was, of course, a war against Napoleon." Schroeder says Poland was "the root cause" of Napoleon's war with Russia but Russia's refusal to support the Continental System was also a factor.
In 1812, at the height of his power, Napoleon invaded Russia with a pan-European Grande Armée, consisting of 450,000 men (200,000 Frenchmen, and many soldiers of allies or subject areas). The French forces crossed the Niemen River on 24 June 1812. Russia proclaimed a Patriotic War, and Napoleon proclaimed a Second Polish war. The Poles supplied almost 100,000 men for the invasion force, but against their expectations, Napoleon avoided any concessions to Poland, having in mind further negotiations with Russia.
The Grande Armée marched through Russia, winning some relatively minor engagements and the major Battle of Smolensk on 16–18 August. In the same days, part of the French Army led by Marshal Nicolas Oudinot was stopped in the Battle of Polotsk by the right wing of the Russian Army, under command of General Peter Wittgenstein. This prevented the French march on the Russian capital, Saint Petersburg; the fate of the invasion was decided in Moscow, where Napoleon led his forces in person.
Russia used scorched-earth tactics, and harried the Grande Armée with light Cossack cavalry. The Grande Armée did not adjust its operational methods in response. This led to most of the losses of the main column of the Grande Armée, which in one case amounted to 95,000 men, including deserters, in a week.
The main Russian army retreated for almost three months. This constant retreat led to the unpopularity of Field Marshal Michael Andreas Barclay de Tolly and a veteran, Prince Mikhail Kutuzov, was made the new Commander-in-Chief by Tsar Alexander I. Finally, the two armies engaged in the Battle of Borodino on 7 September, in the vicinity of Moscow. The battle was the largest and bloodiest single-day action of the Napoleonic Wars, involving more than 250,000 men and resulting in at least 70,000 casualties. It was indecisive; the French captured the main positions on the battlefield but failed to destroy the Russian army. Logistical difficulties meant that French casualties could not be replaced, unlike Russian ones.
Napoleon entered Moscow on 14 September, after the Russian Army had retreated yet again. By then, the Russians had largely evacuated the city and released criminals from the prisons to inconvenience the French; the governor, Count Fyodor Rostopchin, ordered the city to be burnt. Alexander I refused to capitulate, and the peace talks attempted by Napoleon failed. In October, with no sign of clear victory in sight, Napoleon began the disastrous Great Retreat from Moscow.
At the Battle of Maloyaroslavets the French tried to reach Kaluga, where they could find food and forage supplies. The replenished Russian Army blocked the road, and Napoleon was forced to retreat the same way he had come to Moscow, through the heavily ravaged areas along the Smolensk road. In the following weeks, the Grande Armée was dealt a catastrophic blow by the onset of the Russian Winter, the lack of supplies and constant guerrilla warfare by Russian peasants and irregular troops.
When the remnants of Napoleon's army crossed the Berezina River in November, only 27,000 fit soldiers survived, with 380,000 men dead or missing and 100,000 captured. Napoleon then left his men and returned to Paris to prepare the defence against the advancing Russians. The campaign effectively ended on 14 December 1812, when the last enemy troops left Russia. The Russians had lost around 210,000 men, but with their shorter supply lines, they soon replenished their armies. For every twelve soldiers of the Grande Armée that entered Russia, only two would make it out in fighting condition.
Seeing an opportunity in Napoleon's historic defeat, Prussia, Sweden, Austria, and several other German states switched sides joining Russia, the United Kingdom and others opposing Napoleon. Napoleon vowed that he would create a new army as large as the one he had sent into Russia, and quickly built up his forces in the east from 30,000 to 130,000 and eventually to 400,000. Napoleon inflicted 40,000 casualties on the Allies at Lützen (2 May 1813) and Bautzen (20–21 May 1813). Both battles involved forces of over 250,000, making them some of the largest conflicts of the wars so far. Metternich in November 1813 offered Napoleon the Frankfurt proposals. They would allow Napoleon to remain Emperor but France would be reduced to its "natural frontiers" and lose control of most of Italy and Germany and the Netherlands. Napoleon still expected to win the wars, and rejected the terms. By 1814, as the Allies were closing in on Paris, Napoleon did agree to the Frankfurt proposals, but it was too late and he rejected the new harsher terms proposed by the Allies.
In the Peninsular War, Arthur Wellesley, 1st Duke of Wellington, renewed the Anglo-Portuguese advance into Spain just after New Year in 1812, besieging and capturing the fortified towns of Ciudad Rodrigo, Badajoz, and in the Battle of Salamanca (which was a damaging defeat of the French). As the French regrouped, the Anglo-Portuguese entered Madrid and advanced towards Burgos, before retreating all the way to Portugal when renewed French concentrations threatened to trap them. As a consequence of the Salamanca campaign, the French were forced to end their long siege of Cadiz and to permanently evacuate the provinces of Andalusia and Asturias.
In a strategic move, Wellesley planned to move his supply base from Lisbon to Santander. The Anglo-Portuguese forces swept northwards in late May and seized Burgos. On 21 June, at Vitoria, the combined Anglo-Portuguese and Spanish armies won against Joseph Bonaparte, finally breaking French power in Spain. The French had to retreat from the Iberian peninsula, over the Pyrenees.
The belligerents declared an armistice from 4 June 1813 (continuing until 13 August) during which time both sides attempted to recover from the loss of approximately a quarter of a million men in the preceding two months. During this time coalition negotiations finally brought Austria out in open opposition to France. Two principal Austrian armies took the field, adding 300,000 men to the coalition armies in Germany. The Allies now had around 800,000 front-line soldiers in the German theatre, with a strategic reserve of 350,000 formed to support the front-line operations.
Napoleon succeeded in bringing the imperial forces in the region to around 650,000—although only 250,000 came under his direct command, with another 120,000 under Nicolas Charles Oudinot and 30,000 under Davout. The remainder of imperial forces came mostly from the Confederation of the Rhine, especially Saxony and Bavaria. In addition, to the south, Murat's Kingdom of Naples and Eugène de Beauharnais's Kingdom of Italy had 100,000 armed men. In Spain, another 150,000 to 200,000 French troops steadily retreated before Anglo-Portuguese forces numbering around 100,000. Thus around 900,000 Frenchmen in all theatres faced around 1,800,000 coalition soldiers (including the strategic reserve under formation in Germany). The gross figures may mislead slightly, as most of the German troops fighting on the side of the French fought at best unreliably and stood on the verge of defecting to the Allies. One can reasonably say that Napoleon could count on no more than 450,000 men in Germany—which left him outnumbered about four to one.
Following the end of the armistice, Napoleon seemed to have regained the initiative at Dresden (August 1813), where he once again defeated a numerically superior coalition army and inflicted enormous casualties, while sustaining relatively few. The failures of his marshals and a slow resumption of the offensive on his part cost him any advantage that this victory might have secured. At the Battle of Leipzig in Saxony (16–19 October 1813), also called the "Battle of the Nations", 191,000 French fought more than 300,000 Allies, and the defeated French had to retreat into France. After the French withdrawal from Germany, Napoleon's remaining ally, Denmark–Norway, became isolated and fell to the coalition.
Napoleon then fought a series of battles in France, including the Battle of Arcis-sur-Aube, but the overwhelming numbers of the Allies steadily forced him back. The Allies entered Paris on 30 March 1814. During this time Napoleon fought his Six Days' Campaign, in which he won many battles against the enemy forces advancing towards Paris. During this entire campaign, he never managed to field more than 70,000 men against more than half a million coalition soldiers. At the Treaty of Chaumont (9 March 1814), the Allies agreed to preserve the coalition until Napoleon's total defeat.
Napoleon determined to fight on, even now, incapable of fathoming his fall from power. During the campaign, he had issued a decree for 900,000 fresh conscripts, but only a fraction of these materialised, and Napoleon's schemes for victory eventually gave way to the reality of his hopeless situation. Napoleon abdicated on 6 April. Occasional military actions continued in Italy, Spain, and Holland in early 1814. An armistice was signed with the Allied Powers on 23 April 1814. The First Treaty of Paris, signed on 30 May 1814, officially ended the War of the Sixth Coalition.
The victors exiled Napoleon to the island of Elba and restored the French Bourbon monarchy in the person of Louis XVIII. They signed the Treaty of Fontainebleau (11 April 1814) and initiated the Congress of Vienna to redraw the map of Europe.
The Seventh Coalition (1815) pitted Britain, Russia, Prussia, Sweden, Switzerland, Austria, the Netherlands and several smaller German states against France. The period known as the Hundred Days began after Napoleon escaped from Elba and landed at Cannes (1 March 1815). Travelling to Paris, picking up support as he went, he eventually overthrew the restored Louis XVIII. The Allies rapidly gathered their armies to meet him again. Napoleon raised 280,000 men, whom he distributed among several armies. To add to the 90,000-strong standing army, he recalled well over a quarter of a million veterans from past campaigns and issued a decree for the eventual draft of around 2.5 million new men into the French army, which was never achieved. This faced an initial coalition force of about 700,000—although coalition campaign plans provided for one million front-line soldiers, supported by around 200,000 garrison, logistics and other auxiliary personnel.
Napoleon took about 124,000 men of the Army of the North on a pre-emptive strike against the Allies in Belgium. He intended to attack the coalition armies before they combined, in hope of driving the British into the sea and the Prussians out of the war. His march to the frontier achieved the surprise he had planned, catching the Anglo-Dutch Army in a dispersed arrangement. The Prussians had been more wary, concentrating 75% of their army in and around Ligny. The Prussians forced the Armée du Nord to fight all the day of the 15th to reach Ligny in a delaying action by the Prussian 1st Corps. He forced Prussia to fight at Ligny on 16 June 1815, and the defeated Prussians retreated in disorder. On the same day, the left wing of the Armée du Nord, under the command of Marshal Michel Ney, succeeded in stopping any of Wellington's forces going to aid Blücher's Prussians by fighting a blocking action at Quatre Bras. Ney failed to clear the cross-roads and Wellington reinforced the position. But with the Prussian retreat, Wellington too had to retreat. He fell back to a previously reconnoitred position on an escarpment at Mont St Jean, a few miles south of the village of Waterloo.
Napoleon took the reserve of the Army of the North, and reunited his forces with those of Ney to pursue Wellington's army, after he ordered Marshal Grouchy to take the right wing of the Army of the North and stop the Prussians re-grouping. In the first of a series of miscalculations, both Grouchy and Napoleon failed to realise that the Prussian forces were already reorganised and were assembling at the village of Wavre. The French army did nothing to stop a rather leisurely retreat that took place throughout the night and into the early morning by the Prussians. As the 4th, 1st, and 2nd Prussian Corps marched through the town towards Waterloo the 3rd Prussian Corps took up blocking positions across the river, and although Grouchy engaged and defeated the Prussian rearguard under the command of Lt-Gen von Thielmann in the Battle of Wavre (18–19 June) it was 12 hours too late. In the end, 17,000 Prussians had kept 33,000 badly needed French reinforcements off the field.
Napoleon delayed the start of fighting at the Battle of Waterloo on the morning of 18 June for several hours while he waited for the ground to dry after the previous night's rain. By late afternoon, the French army had not succeeded in driving Wellington's forces from the escarpment on which they stood. When the Prussians arrived and attacked the French right flank in ever-increasing numbers, Napoleon's strategy of keeping the coalition armies divided had failed and a combined coalition general advance drove his army from the field in confusion.
Grouchy organised a successful and well-ordered retreat towards Paris, where Marshal Davout had 117,000 men ready to turn back the 116,000 men of Blücher and Wellington. General Vandamme was defeated at the Battle of Issy and negotiations for surrender had begun.
On arriving at Paris three days after Waterloo, Napoleon still clung to the hope of a concerted national resistance; but the temper of the legislative chambers, and of the public generally, did not favour his view. Lacking support Napoleon abdicated again on 22 June 1815, and on 15 July he surrendered to the British squadron at Rochefort. The Allies exiled him to the remote South Atlantic island of Saint Helena, where he died on 5 May 1821.
In Italy, Joachim Murat, whom the Allies had allowed to remain King of Naples after Napoleon's initial defeat, once again allied with his brother-in-law, triggering the Neapolitan War (March to May 1815). Hoping to find support among Italian nationalists fearing the increasing influence of the Habsburgs in Italy, Murat issued the Rimini Proclamation inciting them to war. The proclamation failed and the Austrians soon crushed Murat at the Battle of Tolentino (2–3 May 1815), forcing him to flee. The Bourbons returned to the throne of Naples on 20 May 1815. Murat tried to regain his throne, but after that failed, he was executed by firing squad on 13 October 1815.
The Second Treaty of Paris, signed on 20 November 1815, officially marked the end of the Napoleonic Wars.
The Napoleonic Wars brought radical changes to Europe, but the reactionary forces returned and restored the Bourbon house to the French throne. Napoleon had succeeded in bringing most of Western Europe under one rule. In most European countries, subjugation in the French Empire brought with it many liberal features of the French Revolution including democracy, due process in courts, abolition of serfdom, reduction of the power of the Catholic Church, and demand for constitutional limits on monarchs. The increasing voice of the middle classes with rising commerce and industry meant that restored European monarchs found it difficult to restore pre-revolutionary absolutism and had to retain many of the reforms enacted during Napoleon's rule. Institutional legacies remain to this day in the form of civil law, with clearly defined codes of law—an enduring legacy of the Napoleonic Code.
France's constant warfare with the combined forces of different combinations of, and eventually all, of the other major powers of Europe for over two decades finally took its toll. By the end of the Napoleonic Wars, France no longer held the role of the dominant power in Continental Europe, as it had since the times of Louis XIV, as the Congress of Vienna produced a "balance of power" by resizing the main powers so they could balance each other and remain at peace. In this regard, Prussia was restored in its former borders, and also received large chunks of Poland and Saxony. Greatly enlarged, Prussia became a permanent Great Power. In order to drag Prussia's attention towards the west and France, the Congress also gave the Rhineland and Westphalia to Prussia. These industrial regions transformed agrarian Prussia into an industrial leader in the nineteenth century. Britain emerged as the most important economic power, and its Royal Navy held unquestioned naval superiority across the globe well into the 20th century.
After the Napoleonic period, nationalism, a relatively new movement, became increasingly significant. This shaped much of the course of future European history. Its growth spelled the beginning of some states and the end of others, as the map of Europe changed dramatically in the hundred years following the Napoleonic Era. Rule by fiefdoms and aristocracy was widely replaced by national ideologies based on shared origins and culture. Bonaparte's reign over Europe sowed the seeds for the founding of the nation-states of Germany and Italy by starting the process of consolidating city-states, kingdoms and principalities. At the end of the war, Denmark was forced to cede Norway to Sweden mainly as a compensation for the loss of Finland which the other coalition members agreed to, but because Norway had signed its own constitution on 17 May 1814 Sweden initiated the Swedish–Norwegian War of 1814. The war was a short one taking place between 26 July – 14 August 1814 and was a Swedish victory that put Norway into a personal union with Sweden under Charles XIV John of Sweden. The union was peacefully dissolved in 1905. The United Kingdom of the Netherlands created as a buffer state against France dissolved rapidly with the independence of Belgium in 1830.
The Napoleonic wars also played a key role in the independence of the Latin American colonies from Spain and Portugal. The conflict weakened the authority and military power of Spain, especially after the Battle of Trafalgar. There were many uprisings in Spanish America, leading to the wars of independence. In Portuguese America, Brazil experienced greater autonomy as it now served as seat of the Portuguese Empire and ascended politically to the status of Kingdom. These events also contributed to the Portuguese Liberal Revolution in 1820 and the Independence of Brazil in 1822.
The century of relative transatlantic peace, after the Congress of Vienna, enabled the "greatest intercontinental migration in human history" beginning with "a big spurt of immigration after the release of the dam erected by the Napoleonic Wars." Immigration inflows relative to the US population rose to record levels (peaking at 1.6% in 1850–51) as 30 million Europeans relocated to the United States between 1815 and 1914.
Another concept emerged from the Congress of Vienna – that of a unified Europe. After his defeat, Napoleon deplored the fact that his dream of a free and peaceful "European association" remained unaccomplished. Such a European association would share the same principles of government, system of measurement, currency and Civil Code. One-and-a-half centuries later, and after two world wars several of these ideals re-emerged in the form of the European Union.
Until the time of Napoleon, European states employed relatively small armies, made up of both national soldiers and mercenaries. These regulars were highly drilled, professional soldiers. Ancien Régime armies could only deploy small field armies due to rudimentary staffs and comprehensive yet cumbersome logistics. Both issues combined to limit field forces to approximately 30,000 men under a single commander.
Military innovators in the mid-18th century began to recognise the potential of an entire nation at war: a "nation in arms".
The scale of warfare dramatically enlarged during the Revolutionary and subsequent Napoleonic Wars. During Europe's major pre-revolutionary war, the Seven Years' War of 1756–1763, few armies ever numbered more than 200,000 with field forces often numbering less than 30,000. The French innovations of separate corps (allowing a single commander to efficiently command more than the traditional command span of 30,000 men) and living off the land (which allowed field armies to deploy more men without requiring an equal increase in supply arrangements such as depots and supply trains) allowed the French republic to field much larger armies than their opponents. Napoleon ensured during the time of the French republic that separate French field armies operated as a single army under his control, often allowing him to substantially outnumber his opponents. This forced his continental opponents to also increase the size of their armies, moving away from the traditional small, well-drilled Ancien Régime armies of the 18th century to mass conscript armies.
The Battle of Marengo, which largely ended the War of the Second Coalition, was fought with fewer than 60,000 men on both sides. The Battle of Austerlitz which ended the War of the Third Coalition involved fewer than 160,000 men. The Battle of Friedland which led to peace with Russia in 1807 involved about 150,000 men.
After these defeats, the continental powers developed various forms of mass conscription to allow them to face France on even terms, and the size of field armies increased rapidly. The battle of Wagram of 1809 involved 300,000 men, and 500,000 fought at Leipzig in 1813, of whom 150,000 were killed or wounded.
About a million French soldiers became casualties (wounded, invalided or killed), a higher proportion than in the First World War. The European total may have reached 5,000,000 military deaths, including disease.
France had the second-largest population in Europe by the end of the 18th century (27 million, as compared to Britain's 12 million and Russia's 35 to 40 million). It was well poised to take advantage of the levée en masse. Before Napoleon's efforts, Lazare Carnot played a large part in the reorganisation of the French army from 1793 to 1794—a time which saw previous French misfortunes reversed, with Republican armies advancing on all fronts.
The French army peaked in size in the 1790s with 1.5 million Frenchmen enlisted although battlefield strength was much less. Haphazard bookkeeping, rudimentary medical support and lax recruitment standards ensured that many soldiers either never existed, fell ill or were unable to withstand the physical demands of soldiering.
About 2.8 million Frenchmen fought on land and about 150,000 at sea, bringing the total for France to almost 3 million combatants during almost 25 years of warfare.
Britain had 750,000 men under arms between 1792 and 1815 as its army expanded from 40,000 men in 1793 to a peak of 250,000 men in 1813. Over 250,000 sailors served in the Royal Navy. In September 1812, Russia had 900,000 enlisted men in its land forces, and between 1799 and 1815 2.1 million men served in its army. Another 200,000 served in the Russian Navy. Out of the 900,000 men, the field armies deployed against France numbered less than 250,000.
There are no consistent statistics for other major combatants. Austria's forces peaked at about 576,000 (during the War of the Sixth Coalition) and had little or no naval component yet never fielded more than 250,000 men in field armies. After Britain, Austria proved the most persistent enemy of France; more than a million Austrians served during the long wars. Its large army was overall quite homogeneous and solid and in 1813 operated in Germany (140,000 men), Italy and the Balkans (90,000 men at its peak, about 50,000 men during most of the campaigning on these fronts). Austria's manpower was becoming quite limited towards the end of the wars, leading its generals to favour cautious and conservative strategies, to limit their losses.
Prussia never had more than 320,000 men under arms at any time. In 1813–1815, the core of its army (about 100,000 men) was characterised by competence and determination, but the bulk of its forces consisted of second- and third-line troops, as well as militiamen of variable strength. Many of these troops performed reasonably well and often displayed considerable bravery but lacked the professionalism of their regular counterparts and were not as well equipped. Others were largely unfit for operations, except sieges. During the 1813 campaign, 130,000 men were used in the military operations, with 100,000 effectively participating in the main German campaign, and about 30,000 being used to besiege isolated French garrisons.
Spain's armies also peaked at around 200,000 men, not including more than 50,000 guerrillas scattered over Spain. In addition the Maratha Confederation, the Ottoman Empire, Italy, Naples and the Duchy of Warsaw each had more than 100,000 men under arms. Even small nations now had armies rivalling the size of the Great Powers' forces of past wars but most of these were poor quality forces only suitable for garrison duties. The size of their combat forces remained modest yet they could still provide a welcome addition to the major powers. The percentage of French troops in the Grande Armee which Napoleon led into Russia was about 50% while the French allies also provided a significant contribution to the French forces in Spain. As these small nations joined the coalition forces in 1813–1814, they provided a useful addition to the coalition while depriving Napoleon of much-needed manpower.
The initial stages of the Industrial Revolution had much to do with larger military forces—it became easy to mass-produce weapons and thus to equip larger forces. Britain was the largest single manufacturer of armaments in this period. It supplied most of the weapons used by the coalition powers throughout the conflicts. France produced the second-largest total of armaments, equipping its own huge forces as well as those of the Confederation of the Rhine and other allies.
Napoleon showed innovative tendencies in his use of mobility to offset numerical disadvantages, as demonstrated in the rout of the Austro–Russian forces in 1805 in the Battle of Austerlitz. The French Army redefined the role of artillery, forming independent, mobile units, as opposed to the previous tradition of attaching artillery pieces in support of troops.
The semaphore system had allowed the French War-Minister, Carnot, to communicate with French forces on the frontiers throughout the 1790s. The French continued to use this system throughout the Napoleonic wars. Aerial surveillance was used for the first time when the French used a hot-air balloon to survey coalition positions before the Battle of Fleurus, on 26 June 1794.
Historians have explored how the Napoleonic wars became total wars. Most historians argue that the escalation in size and scope came from two sources. First was the ideological clash between revolutionary/egalitarian and conservative/hierarchical belief systems. Second was the emergence of nationalism in France, Germany, Spain, and elsewhere that made these "people's wars" instead of contests between monarchs. Bell has argued that even more important than ideology and nationalism were the intellectual transformations in the culture of war that came about through the Enlightenment. One factor, he says, is that war was no longer a routine event but a transforming experience for societies—a total experience. Secondly, the military emerged in its own right as a separate sphere of society distinct from the ordinary civilian world. The French Revolution made every civilian a part of the war machine, either as a soldier through universal conscription, or as a vital cog in the home front machinery supporting and supplying the army. Out of that, says Bell, came "militarism," the belief that the military role was morally superior to the civilian role in times of great national crisis. The fighting army represented the essence of the nation's soul. As Napoleon proclaimed, "It is the soldier who founds a Republic and it is the soldier who maintains it."
Napoleon said on his career "I closed the gulf of anarchy and brought order out of chaos. I rewarded merit regardless of birth or wealth, wherever I found it. I abolished feudalism and restored equality to all religion and before the law. I fought the decrepit monarchies of the Old Regime because the alternative was the destruction of all this. I purified the Revolution."
Intelligence played a pivotal factor throughout the Napoleonic Wars and could very well have changed the tide of war. The use and misuse of military intelligence dictated the course of many major battles during the Napoleonic Wars. Some of the major battles that were dictated by the use of intelligence include: The Battle of Waterloo, Battle of Leipzig, Battle of Salamanca, and the Battle of Vitoria. A major exception to the greater use of superior military intelligence to claim victory was the Battle of Jena in 1806. At the Battle of Jena even Prussian superior military intelligence was not enough to counter the sheer military force of Napoleons' armies.
The use of intelligence varied greatly across the major world powers of the war. Napoleon at this time had more supply of intelligence given to him than any French general before him. However, Napoleon was not an advocate of military intelligence at this time as he often found it unreliable and inaccurate when compared to his own preconceived notions of the enemy. Napoleon rather studied his enemy via domestic newspapers, diplomatic publications, maps, and prior documents of military engagements in the theaters of war in which he would operate. It was this stout and constant study of the enemy which made Napoleon the military mastermind of his time. Whereas, his opponents—Britain, Austria, Prussia, and Russia—were much more reliant on traditional intelligence-gathering methods and were much more quickly and willing to act on them.
The methods of Intelligence during these wars were to include the formation of vast and complex networks of corresponding agents, codebreaking, and cryptanalysis. The greatest cipher to be used to hide military operations during this time was known as the Great Paris Cipher used by the French. However, thanks to the hard work of British codebreakers like George Scovell, the British were able to crack French ciphers and gain vast amounts of military intelligence on Napoleon and his armies.
The Napoleonic Wars were a defining event of the early 19th century, and inspired many works of fiction, from then until the present day.

Francis Scott Key (August 1, 1779 – January 11, 1843) was an American lawyer, author, and amateur poet from Frederick, Maryland, who is best known for writing the lyrics for the American national anthem "The Star-Spangled Banner".
Key observed the British bombardment of Fort McHenry in 1814 during the War of 1812. He was inspired upon seeing the American flag still flying over the fort at dawn and wrote the poem "Defence of Fort M'Henry"; it was published within a week with the suggested tune of the popular song "To Anacreon in Heaven". The song with Key's lyrics became known as "The Star-Spangled Banner" and slowly gained in popularity as an unofficial anthem, finally achieving official status more than a century later under President Herbert Hoover as the national anthem. 
Key was a lawyer in Maryland and Washington D.C. for four decades and worked on important cases, including the Burr conspiracy trial, and he argued numerous times before the Supreme Court. He was nominated for District Attorney for the District of Columbia by President Andrew Jackson, where he served from 1833 to 1841. Key was a devout Episcopalian.
Key owned slaves from 1800, during which time abolitionists ridiculed his words, claiming that America was more like the "Land of the Free and Home of the Oppressed". As District Attorney, he suppressed abolitionists, and in 1836 lost a case against Reuben Crandall where he accused the defendant's abolitionist publications of instigating slaves to rebel. He was also a leader of the American Colonization Society which sent freed slaves to Africa. He freed some of his slaves in the 1830s, paying one ex-slave as his farm foreman to supervise his other slaves. He publicly criticized slavery and gave free legal representation to some slaves seeking freedom, but he also represented owners of runaway slaves. At the time of his death he owned eight slaves.
Key's father John Ross Key was a lawyer, a commissioned officer in the Continental Army, and a judge of English descent. His mother Ann Phoebe Dagworthy Charlton was born (February 6, 1756 – 1830), to Arthur Charlton, a tavern keeper, and his wife, Eleanor Harrison of Frederick in the colony of Maryland.
Key grew up on the family plantation Terra Rubra in Frederick County, Maryland (now Carroll County). He graduated from St. John's College, Annapolis, Maryland, in 1796 and read law under his uncle Philip Barton Key who was loyal to the British Crown during the War of Independence. He married Mary Tayloe Lloyd on January 1, 1802, daughter of Edward Lloyd IV of Wye House and Elizabeth Tayloe, daughter of John Tayloe II of Mount Airy and sister of John Tayloe III of The Octagon House.
During the War of 1812, following the Burning of Washington in August 1814, on September 7,1814, Key and American Agent for Prisoners of War, Colonel John Stuart Skinner dined aboard HMS Tonnant as the guests of Vice Admiral Alexander Cochrane, Rear Admiral George Cockburn, and Major General Robert Ross. Skinner and Key were there to plead for the release of Dr. William Beanes, an elderly resident of Upper Marlboro, Maryland, and a friend of Key, who had been captured in his home on August 28, 1814. Beanes was accused of aiding the arrest of some British soldiers (stragglers withdrawing after the Washington campaign) who were pillaging homes.  Skinner, Key, and the released Beanes were allowed to return to their own truce ship, under guard, but not allowed to leave the fleet because they had become familiar with the strength and position of the British units and their intention to launch an attack upon Baltimore. Key was unable to do anything but watch the 25-hour bombardment of the American forces at Fort McHenry during the Battle of Baltimore from dawn of September 13 through the morning of the 14th, 1814.  
At dawn, Key was able to see a large American flag waving over the fort, and he started writing a poem about his experience, on the back of a letter he had kept in his pocket. On September 16, Key, Skinner and Beanes were released from the fleet. When they arrived in Baltimore that evening, Key completed the poem at the Indian Queen Hotel, where he was staying, His finished manuscript was untitled and unsigned. When printed as a broadside the next day, it was given the title "Defence of Fort M'Henry” with the notation: "Tune – Anacreon in Heaven" This was a popular tune that Key had already used as a setting for his 1805 song "When the Warrior Returns", celebrating American heroes of the First Barbary War. It was soon published in newspapers first in Baltimore and then across the nation, and given the new title The Star-Spangled Banner. It was somewhat difficult to sing, yet it became increasingly popular, competing with "Hail, Columbia" (1796) as the de facto national anthem by the time of the Mexican–American War and the American Civil War. The song was finally adopted as the American national anthem more than a century after its first publication by Act of Congress in 1931 signed by President Herbert Hoover.
The third verse of the Star-Spangled Banner makes disparaging mention of blacks and demonstrates Key's opinion of their seeking freedom at the time by escaping to fight with the British, who promised them freedom from American enslavement.
Key was a leading attorney in Frederick, Maryland, and Washington, D.C., for many years, with an extensive real estate and trial practice. He and his family settled in Georgetown in 1805 or 1806, near the new national capital. He assisted his uncle Philip Barton Key in the sensational conspiracy trial of Aaron Burr and in the expulsion of Senator John Smith of Ohio. He made the first of his many arguments before the United States Supreme Court in 1807. In 1808, he assisted President Thomas Jefferson's attorney general in United States v. Peters.
In 1829, Key assisted in the prosecution of Tobias Watkins, former U.S. Treasury auditor under President John Quincy Adams, for misappropriating public funds. He also handled the Petticoat affair concerning Secretary of War John Eaton, and he served as the attorney for Sam Houston in 1832 during his trial for assaulting Representative William Stanbery of Ohio. After years as an adviser to President Jackson, Key was nominated by the President to District Attorney for the District of Columbia in 1833. He served from 1833 to 1841 while also handling his own private legal cases. In 1835, he prosecuted Richard Lawrence for his attempt to assassinate President Jackson at the top steps of the Capitol, the first attempt to kill an American president.
Key purchased his first slave in 1800 or 1801 and owned six slaves in 1820. He freed seven of his slaves in the 1830s, and owned eight slaves when he died. One of his freed slaves continued to work for him for wages as his farm's foreman, supervising several slaves. Key also represented several slaves seeking their freedom, as well as several slave-owners seeking return of their runaway slaves. Key was one of the executors of John Randolph of Roanoke's will, which freed his 400 slaves, and Key fought to enforce the will for the next decade and to provide the freed slaves with land to support themselves.
Key is known to have publicly criticized slavery's cruelties, and a newspaper editorial stated that "he often volunteered to defend the downtrodden sons and daughters of Africa." The editor said that Key "convinced me that slavery was wrong—radically wrong".
A quote increasingly credited to Key stating that free blacks are "a distinct and inferior race of people, which all experience proves to be the greatest evil that afflicts a community" is erroneous. The quote is taken from an 1838 letter that Key wrote to Reverend Benjamin Tappan of Maine who had sent Key a questionnaire about the attitudes of Southern religious institutions about slavery. Rather than representing a statement by Key identifying his personal thoughts, the words quoted are offered by Key to describe the attitudes of others who assert that formerly enslaved blacks could not remain in the U.S. as paid laborers. This was the official policy of the American Colonization Society. Key was an ACS leader and fundraiser for the organization, but he himself did not send the men and women he freed to Africa upon their emancipation. The original confusion around this quote arises from ambiguities in the 1937 biography of Key by Edward S. Delaplaine.
Key was a founding member and active leader of the American Colonization Society (ACS), whose primary goal was to send free blacks to Africa. Though many free blacks were born in the United States by this time, historians argue that upper-class American society, of which Key was a part, could never "envision a multiracial society". The ACS was not supported by most abolitionists or free blacks of the time, but the organization's work would eventually lead to the creation of Liberia in 1847.
In the early 1830s American thinking on slavery changed quite abruptly. Considerable opposition to the American Colonization Society's project emerged. Led by newspaper editor and publisher Wm. Lloyd Garrison, a growing portion of the population noted that only a very small number of free blacks were actually moved, and they faced brutal conditions in West Africa, with very high mortality. Free blacks made it clear that few of them wanted to move, and if they did, it would be to Canada, Mexico, or Central America, not Africa. The leaders of the American Colonization Society, including Key, were predominantly slaveowners. The Society was intended to preserve slavery, rather than eliminate it. In the words of philanthropist Gerrit Smith, it was "quite as much an Anti-Abolition, as Colonization Society". "This Colonization Society had, by an invisible process, half conscious, half unconscious, been transformed into a serviceable organ and member of the Slave Power."
The alternative to the colonization of Africa, project of the American Colonization Society, was the total and immediate abolition of slavery in the United States. This Key was firmly against, with or without slaveowner compensation, and he used his position as District Attorney to attack abolitionists. In 1833, he secured a grand jury indictment against Benjamin Lundy, editor of the anti-slavery publication Genius of Universal Emancipation, and his printer William Greer, for libel after Lundy published an article that declared, "There is neither mercy nor justice for colored people in this district ". Lundy's article, Key said in the indictment, "was intended to injure, oppress, aggrieve, and vilify the good name, fame, credit &amp; reputation of the Magistrates and constables" of Washington. Lundy left town rather than face trial; Greer was acquitted.
In a larger unsuccessful prosecution, in August 1836 Key obtained an indictment against Reuben Crandall, brother of controversial Connecticut teacher Prudence Crandall, who had recently moved to Washington, D.C. It accused Crandall of "seditious libel" after two marshals (who operated as slave catchers in their off hours) found Crandall had a trunk full of anti-slavery publications in his Georgetown residence/office, five days after the Snow riot, caused by rumors that a mentally ill slave had attempted to kill an elderly white woman. In an April 1837 trial that attracted nationwide attention and that congressmen attended, Key charged that Crandall's publications instigated slaves to rebel. Crandall's attorneys acknowledged he opposed slavery, but denied any intent or actions to encourage rebellion. Evidence was introduced that the anti-slavery publications were packing materials used by his landlady in shipping his possessions to him. He had not "published" anything; he had given one copy to one man who had asked for it.
Key, in his final address to the jury said:
Are you willing, gentlemen, to abandon your country, to permit it to be taken from you, and occupied by the abolitionist, according to whose taste it is to associate and amalgamate with the negro? Or, gentlemen, on the other hand, are there laws in this community to defend you from the immediate abolitionist, who would open upon you the floodgates of such extensive wickedness and mischief?The jury acquitted Crandall of all charges. This public and humiliating defeat, as well as family tragedies in 1835, diminished Key's political ambition. He resigned as District Attorney in 1840. He remained a staunch proponent of African colonization and a strong critic of the abolition movement until his death.
Crandall died shortly after his acquittal of pneumonia contracted in the Washington jail.
Key was a devout and prominent Episcopalian. In his youth, he almost became an Episcopal priest rather than a lawyer. Throughout his life he sprinkled biblical references in his correspondence. He was active in All Saints Parish in Frederick, Maryland, near his family's home. He also helped found or financially support several parishes in the new national capital, including St. John's Episcopal Church in Georgetown, Trinity Episcopal Church in present-day Judiciary Square, and Christ Church in Alexandria (at the time, in the District of Columbia).
From 1818 until his death in 1843, Key was associated with the American Bible Society. He successfully opposed an abolitionist resolution presented to that group around 1838.
Key also helped found two Episcopal seminaries, one in Baltimore and the other across the Potomac River in Alexandria (the Virginia Theological Seminary). Key also published a prose work called The Power of Literature, and Its Connection with Religion in 1834.
On January 11, 1843, Key died at the home of his daughter Elizabeth Howard in Baltimore from pleurisy at age 63. He was initially interred in Old Saint Paul's Cemetery in the vault of John Eager Howard but in 1866, his body was moved to his family plot in Frederick at Mount Olivet Cemetery.
The Key Monument Association erected a memorial in 1898 and the remains of both Francis Scott Key and his wife, Mary Tayloe Lloyd, were placed in a crypt in the base of the monument.
Despite several efforts to preserve it, the Francis Scott Key residence was ultimately dismantled in 1947. The residence had been located at 3516–18 M Street in Georgetown.
Though Key had written poetry from time to time, often with heavily religious themes, these works were not collected and published until 14 years after his death. Two of his religious poems used as Christian hymns include "Before the Lord We Bow" and "Lord, with Glowing Heart I'd Praise Thee".
In 1806, Key's sister, Anne Phoebe Charlton Key, married Roger B. Taney, who would later become Chief Justice of the United States. In 1846 one daughter, Alice, married U.S. Senator George H. Pendleton and another, Ellen Lloyd, married Simon F. Blunt. In 1859, Key's son Philip Barton Key II, who also served as United States Attorney for the District of Columbia, was shot and killed by Daniel Sickles‍—‌a U.S. Representative from New York who would serve as a general in the American Civil War‍—‌after he discovered that Philip Barton Key was having an affair with his wife. Sickles was acquitted in the first use of the temporary insanity defense. In 1861, Key's grandson Francis Key Howard was imprisoned in Fort McHenry with the Mayor of Baltimore George William Brown and other locals deemed to be Confederate sympathizers.
Key was a distant cousin and the namesake of F. Scott Fitzgerald, whose full name was Francis Scott Key Fitzgerald. His direct descendants include geneticist Thomas Hunt Morgan, guitarist Dana Key, and American fashion designer and socialite Pauline de Rothschild.
Fort McHenry is a historical American coastal pentagonal bastion fort on Locust Point, now a neighborhood of Baltimore, Maryland. It is best known for its role in the War of 1812, when it successfully defended Baltimore Harbor from an attack by the British navy from the Chesapeake Bay on September 13–14, 1814.  It was first built in 1798 and was used continuously by the U.S. armed forces through World War I and by the Coast Guard in World War II. It was designated a national park in 1925, and in 1939 was redesignated a "National Monument and Historic Shrine".
During the War of 1812 an American storm flag, 17 by 25 feet (5.2 m × 7.6 m), was flown over Fort McHenry during the bombardment. It was replaced early on the morning of September 14, 1814, with a larger American garrison flag, 30 by 42 feet (9.1 m × 12.8 m). The larger flag signaled American victory over the British in the Battle of Baltimore. The sight of the ensign inspired Francis Scott Key to write the poem "Defence of Fort M'Henry" that was later set to the tune "To Anacreon in Heaven" and became known as "The Star-Spangled Banner", the national anthem of the United States.
Fort McHenry was built on the site of the former Fort Whetstone, which had defended Baltimore from 1776 to 1797. Fort Whetstone stood on Whetstone Point (today's residential and industrial area of Locust Point) peninsula, which juts into the opening of Baltimore Harbor between the Basin (today's Inner Harbor) and Northwest branch on the north side and the Middle and Ferry (now Southern) branches of the Patapsco River on the south side.
The Frenchman Jean Foncin designed the fort in 1798, and it was built between 1798 and 1800. The new fort's purpose was to improve the defenses of the increasingly important Port of Baltimore from future enemy attacks.
The new fort was a bastioned pentagon,  surrounded by a dry moat—a deep, broad trench. The moat would serve as a shelter from which infantry might defend the fort from a land attack. In case of such an attack on this first line of defense, each point, or bastion could provide a crossfire of cannon and small arms fire.
Fort McHenry was named after early American statesman James McHenry (1753–1816), a Scots-Irish immigrant and surgeon-soldier. He was a delegate to the Continental Congress from Maryland and a signer of the United States Constitution. Afterwards, he was appointed United States Secretary of War (1796–1800), serving under Presidents George Washington and John Adams.
Beginning at 6:00 a.m. on September 13, 1814, British warships under the command of Vice Admiral Alexander Cochrane continuously bombarded Fort McHenry for 25 hours. The American defenders had 18-, 24- and 32-pounder (8, 11, and 16 kg) cannons. The British guns had a range of 2 miles (3 km), and the British rockets had a 1.75-mile (2.8 km) range, but neither guns nor rockets were accurate. The British ships were unable to pass Fort McHenry and penetrate Baltimore Harbor because of its defenses, including a chain of 22 sunken ships, and the American cannons. The British vessels were only able to fire their rockets and mortars at the fort at the weapons' maximum range. The poor accuracy on both sides resulted in very little damage to either side before the British, having depleted their ammunition, ceased their attack on the morning of September 14. Thus the naval part of the British invasion of Baltimore had been repulsed. Only one British warship, a bomb vessel, received a direct hit from the fort's return fire, which wounded one crewman.
The Americans, under the command of Major George Armistead, lost four killed—including one black soldier, Private William Williams, and a woman who was cut in half by a bomb as she carried supplies to the troops—and 24 wounded. At one point during the bombardment, a bomb crashed through the fort's powder magazine. However, either the rain extinguished the fuse or the bomb was a dud.
Francis Scott Key, a Washington lawyer who had come to Baltimore to negotiate the release of Dr. William Beanes, a civilian prisoner of war, witnessed the bombardment from a nearby truce ship. An oversized American flag had been sewn by Mary Pickersgill for $405.90 in anticipation of the British attack on the fort. When Key saw the flag emerge intact in the dawn of September 14, he was so moved that he began that morning to compose "Defence of Fort M'Henry" set to the tune "To Anacreon in Heaven" which would later be renamed "The Star-Spangled Banner" and become the United States' national anthem.
During the American Civil War the area where Fort McHenry sits served as a military prison, confining both Confederate soldiers, as well as a large number of Maryland political figures who were suspected of being Confederate sympathizers. The imprisoned included newly elected Baltimore Mayor George William Brown, the city council, and the new police commissioner, George P. Kane, and members of the Maryland General Assembly along with several newspaper editors and owners. Francis Scott Key's grandson, Francis Key Howard, was one of these political detainees. Some of the cells used still exist and can be visited at the fort. Fort McHenry also served to train artillery at this time; this service is the origin of the Rodman guns presently located and displayed at the fort.
On 25 May 1861 John Merryman was arrested in Baltimore County and imprisoned in Fort McHenry.  Merryman had had a role in destroying bridges in Maryland to impede the movement of Union troops. Merryman petitioned Supreme Court Chief Justice Roger B. Taney for a writ of habeas corpus, and Taney granted the petition, demanding that Merryman appear in his courtroom the next day and sending U.S. Marshals to the fort to enforce the ruling. A famous and dramatic standoff then occurred at the gates of the fort between the Federal Marshals and General George Cadwalader, the commander of Union troops of the Fort.  The commander refused to comply with the order on the grounds that he was acting under orders from President Abraham Lincoln, who had suspended habeas corpus.  The court case, Ex parte Merryman, remains unresolved, and the Executive Branch continued to refuse to comply with Taney's ruling.
During World War I, an additional hundred-odd buildings were built on the land surrounding the fort in order to convert the entire facility into an enormous U.S. Army hospital for the treatment of troops returning from the European conflict. None of those buildings remain, while the original fort has been preserved and restored to essentially its condition during the War of 1812.
During World War II, Fort McHenry served as a Coast Guard base.  Used for training, the historic sections remained open to the public.
The fort was made a national park in 1925; on August 11, 1939, it was redesignated a "National Monument and Historic Shrine", the only such doubly designated place in the United States. It was placed on the National Register of Historic Places on October 15, 1966. It has become national tradition that when a new flag is designed it first flies over Fort McHenry. The first official 49- and 50-star American flags were flown over the fort and are still located on the premises.
The fort has become a center of recreation for the Baltimore locals as well as a prominent tourist destination. Thousands of visitors come each year to see the "Birthplace of the Star-Spangled Banner." It's easily accessible by water taxi from the popular Baltimore Inner Harbor. However, to prevent abuse of the parking lots at the Fort, the National Park Service does not permit passengers to take the water taxi back to the Inner Harbor unless they have previously used it to arrive at the monument.
Several authorized archaeological digs have been conducted, and found artifacts are on display in one of the buildings surrounding the Parade Ground.  These structures, as well as the Visitor Center, have numerous other exhibits as well that show the fort's use over time.
Every September, the City of Baltimore commemorates Defenders Day in honor of the Battle of Baltimore. It is the biggest celebration of the year at the Fort, accompanied by a weekend of programs, events, and fireworks.
In 2005 the living history volunteer unit, the Fort McHenry Guard, was awarded the George B. Hartzog award for serving the National Park Service as the best volunteer unit. Among the members of the unit is Martin O'Malley, the former mayor of Baltimore and Governor of Maryland, who was made the unit's honorary colonel in 2003.
The flag that flew over Fort McHenry, the Star-Spangled Banner Flag, has deteriorated to an extremely fragile condition. After undergoing restoration at the National Museum of American History, it is now on display there in a special exhibit that allows it to lie at a slight angle in dim light.
The United States Code currently authorizes Fort McHenry's closure to the public in the event of a national emergency for use by the military for the duration of such an emergency.
In 2013, Fort McHenry National Monument and Historic Shrine was honored with its own quarter under the America the Beautiful Quarters Program.
On September 10–16, 2014, Fort McHenry celebrated the bicentennial of the writing of the Star-Spangled Banner called the Star-Spangled Spectacular.  The event included a parade of tall ships, a large fireworks show, and the Navy's Blue Angels
As of 2015, restoration efforts began to preserve the original brick used in construction of the Fort, primarily through mortar replacement.
On August 26, 2020, when due to the COVID-19 pandemic a normal Republican National Convention could not be held, vice president Mike Pence held his acceptance speech after being nominated for a second term as vice president of the United States at Fort McHenry.
Historical re-enactment at Fort McHenry
The sally port (main entrance) into Fort McHenry.
Adjacent to Fort McHenry lies a monument of Orpheus that is dedicated to the soldiers of the fort and Francis Scott Key.
Fort McHenry
Fort McHenry map: 237 

William Moore (fl. 1806 – c. 1823) was a British mathematician and early contributor to rocket theory. He worked at the Royal Military Academy, Woolwich. His 1813 Treatise was the first exposition of rocket mechanics based on Newton's third law of motion.  Little is known of his life, because many relevant historical documents were destroyed by German bombing in World War II.

This article about a United Kingdom mathematician is a stub. You can help Wikipedia by expanding it.Alexander Dmitrievich Zasyadko (Russian: Александр Дмитриевич Засядко; 1779 – 8 June  1837), was artillery engineer of the Russian Imperial Army, of Ukrainian origin, lieutenant general of artillery. Designer and specialist in missile weapons development.
In 1797, Zasyadko graduated from the Artillery and Engineering School in Saint Petersburg (currently Mozhaysky Military Engineering-Space University). 
In 1815, Zasyadko began his work on creating military gunpowder rockets. He constructed rocket-launching platforms, which allowed to fire in salvos (6 rockets at a time), and gun-laying devices. Zasyadko elaborated a tactics for military use of rocket weaponry. In 1820, Zasyadko was appointed head of the Petersburg Armory, Okhtensky Powder Factory, pyrotechnic laboratory and the first Higher Artillery School in Russia. In 1827, Zasyadko was in charge of Artillery Headquarters of the Russian army and took part in the Russo-Turkish War of 1828-1829, his rockets played important role during siege of Brăila and Varna.
He organized rocket production in a special rocket workshop and created the first rocket sub-unit in the Russian army.
In 1834, Zasyadko retired due to his illness. The crater Zasyadko on the far side of the Moon is named after him.

A salvo  is the simultaneous discharge of artillery or firearms including the firing of guns either to hit a target or to perform a salute. As a tactic in warfare, the intent is to cripple an enemy in one blow and prevent them from fighting back.
Troops armed with muzzleloaders required time to refill their arms with gunpowder and shot. Gun drills were designed to enable an almost continuous rain of fire on the enemy by lining troops into ranks, allowing one rank to fire a salvo, or volley, while the other ranks prepared their guns for firing.
The term is commonly used to describe the firing of broadsides by warships, especially battleships. During fleet engagements in the days of sail, from 17th century until the 19th century, ships of the line were maneuvered with the objective of bringing the greatest possible number of cannon to bear on the enemy and to discharge them in a salvo, causing enough damage and confusion as to allow time for the cannon to be swabbed out and reloaded. Crossing the T entailed cutting across the enemy's line of battle to enable broadsides to be fired through the enemy's bow or stern along the whole length of the ship, with every shot likely to cause the maximum carnage. The opportunity was a passing one and the most had to be made of it.
With the coming of HMS Dreadnought, with her turreted main armament, the heavy guns were directed by firing a salvo of half-broadside in order to observe the fall of shot, allowing enough time to adjust for range and direction before firing the other half-broadside. This way, shells were kept in flight while each half-battery was reloaded. Reloading a battleship guns, arriving at a firing solution and lining the guns up to fire took as long as 30 seconds, especially when the fall of shot needed to be observed and corrections made before firing again. A target ship moving at 18 knots (33 km/h) traveled 0.15 nautical miles (0.28 km) in 30 seconds, and would often maneuver to "spoil" the range measurement. The "spread" of the salvo would have one shot fire "over" the estimated range, one shot "under", and two on the estimated range. When a four-shot "salvo" "straddled" the target with one splashing over, one splashing under and two landing on or near the target, fire control officers knew they had the correct range. All turret-mounted guns on battleships and cruisers were directed by the gunnery officer, positioned high in the ship and equipped with a visual rangefinder and other mechanisms for directing fire. Instructions to the gunlayers in the turrets were passed by voice pipe, messenger and, later, by telephone. Guns could also be laid by remote control by the gunnery director, with the appropriate technology. Late in World War II, guns were directed by radar.

William Hale (21 October 1797 – 30 March 1870), was a British inventor and rocket pioneer.
Hale was born in Colchester, England in 1797. He was self-taught although his grandfather, the educator William Cole, is believed to have tutored him. By 1827 he had obtained his first patent; he also won a first class Gold Medal of the Royal Society of Arts in Paris for his paper on ship propulsion using an early form of jet propulsion.
Hale was inducted into the International Space Hall of Fame in 2004.
In 1844, Hale patented a new form of rotary rocket that improved on the earlier Congreve rocket design. Hale removed the guidestick from the design, instead vectoring part of the thrust through canted exhaust holes to provide rotation of the rocket, which improved its stability in flight.
These rockets could weigh up to 60 pounds (27 kg) and were noted for their glare and noise on ignition.
Hale rockets were first used by the United States Army in the Mexican–American War of 1846–1848. Although the British Army experimented with Hale rockets during the Crimean War of 1853–1856 they did not officially adopt them until 1867. In the American Civil War of 1861-1865 the Union forces deployed the Hale rocket launcher, a metal tube that fired 7-inch (18 cm) and 10-inch (25 cm) long spin-stabilized rockets up to 2,000 yards (1.8 km). It was only generally used by the U.S. Navy.
 Media related to William Hale (British inventor) at Wikimedia Commons

This article about an engineer, inventor or industrial designer from the United Kingdom or its predecessor states is a stub. You can help Wikipedia by expanding it.Edward Mounier Boxer (1822-1898) was an English inventor.
Edward M. Boxer was a colonel of the Royal Artillery.
In 1855 he was appointed Superintendent of the Royal Laboratory of the Royal Arsenal at Woolwich. 
in 1858 he was elected a Fellow of the Royal Society.
He is known primarily for two of his inventions:

William Leitch (20 May 1814 – 9 May 1864) was a Scottish astronomer, naturalist and mathematician, and a minister of the Church of Scotland. Leitch studied mathematics and science at the University of Glasgow, and moved to Canada in 1860 to take the post of principal at Queen's University.
Space historian Robert Godwin published in October 2015 his discovery that Leitch gave the first modern scientific explanation of the potential for space exploration using rockets (1861). Leitch was said to be "a distinguished astronomer, naturalist and mathematician", and his proposal for rocket spaceflight came four decades prior to more well-known proposals by Konstantin Tsiolkovsky (1903), Robert Esnault-Pelterie (1913), Robert H. Goddard (1914), and Hermann Oberth (1923). Leitch's rocket spaceflight description was first provided in his 1861 essay "A Journey Through Space", which was later published in his book God's Glory in the Heavens (1862). This description correctly attributed rocket thrust to the "internal reaction" (Newton's laws of motion) and correctly identified that rocket thrust is most effective in the vacuum of space. In the third edition of his biography of Leitch, Godwin explained Leitch's connections to the Boston community, and specifically how both Leitch and Robert Hutchings Goddard both knew and corresponded with patent attorney Orson Desaix Munn.
A serious accident confined Leitch to his room for many months at age 14, during which he studied mathematics and science. After the grammar school at Greenock, he attended the University of Glasgow and earned a Bachelor of Arts degree with highest honours in 1837. He followed this with a Master of Arts the next year, and became observatory assistant to Professor John Pringle Nichol, and lectured on astronomy. He then studied for two years at the university's Divinity Hall, and became a licensed minister of the Church of Scotland in 1839.
Leitch stayed with the Kirk through the Disruption of 1843, when he was ordained and worked in the parish of Monimail, in the presbytery of Cupar for 16 years. During this time he worked with the church's Sabbath schools and became interested in popular education. He was awarded the degree of Doctor of Divinity by Glasgow University in 1860.
In 1859, two trustees of Queen's University in Kingston, Ontario came to Scotland to find a successor to the retiring principal. Leitch accepted the post and moved to Ontario in October 1860.

Konstantin Eduardovich Tsiolkovsky (Russian: Константи́н Эдуа́рдович Циолко́вский; 17 September  1857 – 19 September 1935) was a Russian and Soviet rocket scientist who pioneered astronautic theory. Along with the Frenchman Robert Esnault-Pelterie, the  Germans Hermann Oberth and Fritz von Opel, and the American Robert H. Goddard, he is one of the founding fathers of modern rocketry and astronautics. His works later inspired leading Soviet rocket-engineers Sergei Korolev and Valentin Glushko, who contributed to the success of the Soviet space program.
Tsiolkovsky spent most of his life in a log house on the outskirts of Kaluga, about 200 km (120 mi) southwest of Moscow. A recluse by nature, his unusual habits made him seem bizarre to his fellow townsfolk.

Tsiolkovsky was born in Izhevskoye (now in Spassky District, Ryazan Oblast), in the Russian Empire, to a middle-class family. His father, Makary Edward Erazm Ciołkowski, was a Polish forester of Roman Catholic faith who emigrated to Russia; his Russian Orthodox mother was of mixed Volga Tatar  and Russian origin. His father was successively a forester, teacher, and minor government official.  At the age of 10, Konstantin caught scarlet fever and became hard of hearing. When he was 13, his mother died. He was not admitted to elementary schools because of his hearing problem, so he was self-taught. As a reclusive home-schooled child, he passed much of his time by reading books and became interested in mathematics and physics. As a teenager, he began to contemplate the possibility of space travel.
Tsiolkovsky spent three years attending a Moscow library where Russian cosmism proponent Nikolai Fyodorov worked. He later came to believe that colonizing space would lead to the perfection of the human species, with immortality and a carefree existence.
Additionally, inspired by the fiction of Jules Verne, Tsiolkovsky theorized many aspects of space travel and rocket propulsion. He is considered the father of spaceflight and the first person to conceive the space elevator, becoming inspired in 1895 by the newly constructed Eiffel Tower in Paris.
Despite the youth's growing knowledge of physics, his father was concerned that he would not be able to provide for himself financially as an adult and brought him back home at the age of 19 after learning that he was overworking himself and going hungry. Afterwards, Tsiolkovsky passed the teacher's exam and went to work at a school in Borovsk near Moscow. He also met and married his wife Varvara Sokolova during this time. Despite being stuck in Kaluga, a small town far from major learning centers, Tsiolkovsky managed to make scientific discoveries on his own.
The first two decades of the 20th century were marred by personal tragedy. Tsiolkovsky's son Ignaty committed suicide in 1902, and in 1908 many of his accumulated papers were lost in a flood. In 1911, his daughter Lyubov was arrested for engaging in revolutionary activities.
Tsiolkovsky stated that he developed the theory of rocketry only as a supplement to philosophical research on the subject. He wrote more than 400 works including approximately 90 published pieces on space travel and related subjects. Among his works are designs for rockets with steering thrusters, multistage boosters, space stations, airlocks for exiting a spaceship into the vacuum of space, and closed-cycle biological systems to provide food and oxygen for space colonies.
Tsiolkovsky's first scientific study dates back to 1880–1881. He wrote a paper called "Theory of Gases," in which he outlined the basis of the kinetic theory of gases, but after submitting it to the Russian Physico-Chemical Society (RPCS), he was informed that his discoveries had already been made 25 years earlier. Undaunted, he pressed ahead with his second work, "The Mechanics of the Animal Organism". It received favorable feedback, and Tsiolkovsky was made a member of the Society. Tsiolkovsky's main works after 1884 dealt with four major areas: the scientific rationale for the all-metal balloon (airship), streamlined airplanes and trains, hovercraft, and rockets for interplanetary travel.
In 1892, he was transferred to a new teaching post in Kaluga where he continued to experiment. During this period, Tsiolkovsky began working on a problem that would occupy much of his time during the coming years: an attempt to build an all-metal dirigible that could be expanded or shrunk in size.
Tsiolkovsky developed the first aerodynamics laboratory in Russia in his apartment. In 1897, he built the first Russian wind tunnel with an open test section and developed a method of experimentation using it. In 1900, with a grant from the Academy of Sciences, he made a survey using models of the simplest shapes and determined the drag coefficients of the sphere, flat plates, cylinders, cones, and other bodies. Tsiolkovsky's work in the field of aerodynamics was a source of ideas for Russian scientist Nikolay Zhukovsky, the father of modern aerodynamics and hydrodynamics. Tsiolkovsky described the airflow around bodies of different geometric shapes, but because the RPCS did not provide any financial support for this project, he was forced to pay for it largely out of his own pocket.
Tsiolkovsky studied the mechanics of lighter-than-air powered flying machines. He first proposed the idea of an all-metal dirigible and built a model of it. The first printed work on the airship was "A Controllable Metallic Balloon" (1892), in which he gave the scientific and technical rationale for the design of an airship with a metal sheath. Tsiolkovsky was not supported on the airship project, and the author was refused a grant to build the model. An appeal to the General Aviation Staff of the Russian army also had no success. In 1892, he turned to the new and unexplored field of heavier-than-air aircraft. Tsiolkovsky's idea was to build an airplane with a metal frame. In the article "An Airplane or a Birdlike (Aircraft) Flying Machine" (1894) are descriptions and drawings of a monoplane, which in its appearance and aerodynamics anticipated the design of aircraft that would be constructed 15 to 18 years later. In an Aviation Airplane, the wings have a thick profile with a rounded front edge and the fuselage is faired. But work on the airplane, as well as on the airship, did not receive recognition from the official representatives of Russian science, and Tsiolkovsky's further research had neither monetary nor moral support. In 1914, he displayed his models of all-metal dirigibles at the Aeronautics Congress in St. Petersburg but met with a lukewarm response.
Disappointed at this, Tsiolkovsky gave up on space and aeronautical problems with the onset of World War I and instead turned his attention to the problem of alleviating poverty. This occupied his time during the war years until the Russian Revolution in 1917.
Starting in 1896, Tsiolkovsky systematically studied the theory of motion of rocket apparatus. Thoughts on the use of the rocket principle in the cosmos were expressed by him as early as 1883, and a rigorous theory of rocket propulsion was developed in 1896. Tsiolkovsky derived the formula, which he called the "formula of aviation", establishing the relationship between:
After writing out this equation, Tsiolkovsky recorded the date: 10 May 1897. In the same year, the formula for the motion of a body of variable mass was published in the thesis of the Russian mathematician I. V. Meshchersky ("Dynamics of a Point of Variable Mass," I. V. Meshchersky, St. Petersburg, 1897).
His most important work, published in May 1903, was Exploration of Outer Space by Means of Rocket Devices (Russian: Исследование мировых пространств реактивными приборами). Tsiolkovsky calculated, using the Tsiolkovsky equation,: 1  that the horizontal speed required for a minimal orbit around the Earth is 8,000 m/s (5 miles per second) and that this could be achieved by means of a multistage rocket fueled by liquid oxygen and liquid hydrogen. In the article "Exploration of Outer Space by Means of Rocket Devices", it was suggested for the first time that a rocket could perform space flight. In this article and its sequels (1911 and 1914), he developed some ideas of missiles and considered the use of liquid rocket engines.
The outward appearance of Tsiolkovsky's spacecraft design, published in 1903, was a basis for modern spaceship design.  The design had a hull divided into three main sections. The pilot and copilot were in the first section, the second and third sections held the liquid oxygen and liquid hydrogen needed to fuel the spacecraft.
However, the result of the first publication was not what Tsiolkovsky expected. No foreign scientists appreciated his research, which today is a major scientific discipline. In 1911, he published the second part of the work "Exploration of Outer Space by Means of Rocket Devices". Here Tsiolkovsky evaluated the work needed to overcome the force of gravity, determined the speed needed to propel the device into the solar system ("escape velocity"), and examined calculation of flight time. The publication of this article made a splash in the scientific world, Tsiolkovsky found many friends among his fellow scientists.
In 1926–1929, roughly at the same time when Fritz von Opel's rocket-powered Opel RAK land vehicles and aircraft were demonstrated to the public,Tsiolkovsky solved the practical problem regarding the role played by rocket fuel in getting to escape velocity and leaving the Earth. He showed that the final speed of the rocket depends on the rate of gas flowing from it and on how the weight of the fuel relates to the weight of the empty rocket.
Tsiolkovsky conceived a number of ideas that have been later used in rockets. They include: gas rudders (graphite) for controlling a rocket's flight and changing the trajectory of its center of mass, the use of components of the fuel to cool the outer shell of the spacecraft (during re-entry to Earth) and the walls of the combustion chamber and nozzle, a pump system for feeding the fuel components, the optimal descent trajectory of the spacecraft while returning from space, etc. In the field of rocket propellants, Tsiolkovsky studied a large number of different oxidizers and combustible fuels and recommended specific pairings: liquid oxygen and hydrogen, and oxygen with hydrocarbons. Tsiolkovsky did much fruitful work on the creation of the theory of jet aircraft, and invented his chart Gas Turbine Engine.  In 1927, he published the theory and design of a train on an air cushion. He first proposed a "bottom of the retractable body" chassis. However, space flight and the airship were the main problems to which he devoted his life. Tsiolkovsky had been developing the idea of the hovercraft since 1921, publishing a fundamental paper on it in 1927, entitled "Air Resistance and the Express Train" (Russian: Сопротивление воздуха и скорый по́езд). In 1929, Tsiolkovsky proposed the construction of multistage rockets in his book Space Rocket Trains (Russian: Космические ракетные поезда).
Tsiolkovsky championed the idea of the diversity of life in the universe and was the first theorist and advocate of human spaceflight.
Tsiolkovsky never built a rocket; he apparently did not expect many of his theories to ever be implemented.
Hearing problems did not prevent the scientist from having a good understanding of music, as outlined in his work "The Origin of Music and Its Essence."
Tsiolkovsky supported the Bolshevik Revolution, and eager to promote science and technology, the new Soviet government elected him a member of the Socialist Academy in 1918.: 1–2, 8  He worked as a high school mathematics teacher until retiring in 1920 at the age of 63. In 1921, he received a lifetime pension.: 1–2, 8 
In his late lifetime Tsiolkovsky was honored for his pioneering work. However, from the mid 1920s onwards the importance of his other work was acknowledged, and he was honoured for it and the Soviet state provided financial backing for his research. He was initially popularized in Soviet Russia in 1931–1932 mainly by two writers: Yakov Perelman and Nikolai Rynin. Tsiolkovsky died in Kaluga on 19 September 1935 after undergoing an operation for stomach cancer. He bequeathed his life's work to the Soviet state.
Although many called his ideas impractical,: 8, 117  Tsiolkovsky influenced later rocket scientists throughout Europe, like Wernher von Braun. Soviet search teams at Peenemünde found a German translation of a book by Tsiolkovsky of which "almost every page...was embellished by von Braun's comments and notes.": 27  Leading Soviet rocket-engine designer Valentin Glushko and rocket designer Sergey Korolev studied Tsiolkovsky's works as youths,: 6–7, 333  and both sought to turn Tsiolkovsky's theories into reality.: 3, 166, 182, 187, 205–206, 208  In particular, Korolev saw traveling to Mars as the more important priority,: 208, 333, 337  until in 1964 he decided to compete with the American Project Apollo for the Moon.: 404 
In 1989, Tsiolkovsky was inducted into the International Air &amp; Space Hall of Fame at the San Diego Air &amp; Space Museum.
Tsiolkovsky wrote a book called The Will of the Universe. The Unknown Intelligence in 1928 in which he propounded a philosophy of panpsychism.  He believed humans would eventually colonize the Milky Way galaxy. His thought preceded the Space Age by several decades, and some of what he foresaw in his imagination has come into being since his death. Tsiolkovsky also did not believe in traditional religious cosmology, but instead (and to the chagrin of the Soviet authorities) he believed in a cosmic being that governed humans as "marionettes, mechanical puppets, machines, movie characters", thereby adhering to a mechanical view of the universe, which he believed would be controlled in the millennia to come through the power of human science and industry. In a short article in 1933, he explicitly formulated what was later to be known as the Fermi paradox.
He wrote a few works on ethics, espousing negative utilitarianism.


in Europe (dark grey)The United Kingdom of Great Britain and Northern Ireland, commonly known as the United Kingdom (UK) or Britain, is a sovereign country in Europe, off the north-western coast of the continental mainland. It comprises England, Wales, Scotland, and Northern Ireland. The United Kingdom includes the island of Great Britain, the north-eastern part of the island of Ireland, and many smaller islands within the British Isles. Northern Ireland shares a land border with the Republic of Ireland; otherwise, the United Kingdom is surrounded by the Atlantic Ocean, the North Sea, the English Channel, the Celtic Sea and the Irish Sea. The total area of the United Kingdom is 93,628 square miles (242,500 km2), with an estimated 2020 population of more than 67 million people.
The United Kingdom is a unitary parliamentary democracy and constitutional monarchy. The monarch, Queen Elizabeth II, has reigned since 1952. The capital and largest city is London, a global city and financial centre with a metropolitan area population of over 14 million. Other major cities include Birmingham, Manchester, Glasgow, Liverpool and Leeds. Scotland, Wales, and Northern Ireland have their own devolved governments, each with varying powers.
The United Kingdom has evolved from a series of annexations, unions and separations of constituent countries over several hundred years. The Treaty of Union between the Kingdom of England (which included Wales, annexed in 1542) and the Kingdom of Scotland in 1707 formed the Kingdom of Great Britain. Its union in 1801 with the Kingdom of Ireland created the United Kingdom of Great Britain and Ireland. Most of Ireland seceded from the UK in 1922, leaving the present United Kingdom of Great Britain and Northern Ireland, which formally adopted that name in 1927.
The nearby Isle of Man, Guernsey and Jersey are not part of the UK, being Crown Dependencies with the British Government responsible for defence and international representation. There are also 14 British Overseas Territories, the last remnants of the British Empire which, at its height in the 1920s, encompassed almost a quarter of the world's landmass and a third of the world's population, and was the largest empire in history. British influence can be observed in the language, culture and the legal and political systems of many of its former colonies.
The United Kingdom has the world's sixth-largest economy by nominal gross domestic product (GDP), and the eighth-largest by purchasing power parity (PPP). It has a high-income economy and a very high human development index rating, ranking 13th in the world. It also performs well in international rankings of education, healthcare, life expectancy and human development. It remains a great power in global affairs. The UK became the world's first industrialised country and was the world's foremost power during the 19th and early 20th centuries. Today the UK remains one of the world's great powers, with considerable economic, cultural, military, scientific, technological and political influence internationally. It is a recognised nuclear state and is ranked fourth globally in military expenditure. It has been a permanent member of the United Nations Security Council since its first session in 1946.
The United Kingdom is a member of the Commonwealth of Nations, the Council of Europe, the G7, the Group of Ten, the G20, the United Nations, NATO, AUKUS, the Organisation for Economic Co-operation and Development (OECD), Interpol, and the World Trade Organization (WTO). It was a member state of the European Communities (EC) and its successor, the European Union (EU), from its accession in 1973 until its withdrawal in 2020 following a referendum held in 2016.

The Acts of Union 1707 declared that the Kingdom of England and Kingdom of Scotland were "United into One Kingdom by the Name of Great Britain". The term "United Kingdom" has occasionally been used as a description for the former kingdom of Great Britain, although its official name from 1707 to 1800 was simply "Great Britain". The Acts of Union 1800 united the kingdoms of Great Britain and Ireland in 1801, forming the United Kingdom of Great Britain and Ireland. Following the partition of Ireland and the independence of the Irish Free State in 1922, which left Northern Ireland as the only part of the island of Ireland within the United Kingdom, the name was changed to the "United Kingdom of Great Britain and Northern Ireland".
Although the United Kingdom is a sovereign country, England, Scotland, Wales and Northern Ireland are also widely referred to as countries. The UK Prime Minister's website has used the phrase "countries within a country" to describe the United Kingdom. Some statistical summaries, such as those for the twelve NUTS 1 regions of the United Kingdom refer to Scotland, Wales and Northern Ireland as "regions". Northern Ireland is also referred to as a "province". With regard to Northern Ireland, the descriptive name used "can be controversial, with the choice often revealing one's political preferences".
The term "Great Britain" conventionally refers to the island of Great Britain, or politically to England, Scotland and Wales in combination. It is sometimes used as a loose synonym for the United Kingdom as a whole.
The term "Britain" is used both as a synonym for Great Britain, and as a synonym for the United Kingdom. Usage is mixed: the UK Government prefers to use the term "UK" rather than "Britain" or "British" on its own website (except when referring to embassies), while acknowledging that both terms refer to the United Kingdom and that elsewhere "British government" is used at least as frequently as "United Kingdom government". The UK Permanent Committee on Geographical Names recognises "United Kingdom", "UK" and "U.K." as shortened and abbreviated geopolitical terms for the United Kingdom of Great Britain and Northern Ireland in its toponymic guidelines; it does not list "Britain" but notes that "it is only the one specific nominal term 'Great Britain' which invariably excludes Northern Ireland". The BBC historically preferred to use "Britain" as shorthand only for Great Britain, though the present style guide does not take a position except that "Great Britain" excludes Northern Ireland.
The adjective "British" is commonly used to refer to matters relating to the United Kingdom and is used in law to refer to United Kingdom citizenship and matters to do with nationality. People of the United Kingdom use a number of different terms to describe their national identity and may identify themselves as being British, English, Scottish, Welsh, Northern Irish, or Irish; or as having a combination of different national identities. The official designation for a citizen of the United Kingdom is "British citizen".
Settlement by anatomically modern humans of what was to become the United Kingdom occurred in waves beginning by about 30,000 years ago. By the end of the region's prehistoric period, the population is thought to have belonged, in the main, to a culture termed Insular Celtic, comprising Brittonic Britain and Gaelic Ireland.
The Roman conquest, beginning in 43 AD, and the 400-year rule of southern Britain, was followed by an invasion by Germanic Anglo-Saxon settlers, reducing the Brittonic area mainly to what was to become Wales, Cornwall and, until the latter stages of the Anglo-Saxon settlement, the Hen Ogledd (northern England and parts of southern Scotland). Most of the region settled by the Anglo-Saxons became unified as the Kingdom of England in the 10th century. Meanwhile, Gaelic-speakers in north-west Britain (with connections to the north-east of Ireland and traditionally supposed to have migrated from there in the 5th century) united with the Picts to create the Kingdom of Scotland in the 9th century.
In 1066, the Normans invaded England from northern France. After conquering England, they seized large parts of Wales, conquered much of Ireland and were invited to settle in Scotland, bringing to each country feudalism on the Northern French model and Norman-French culture. The Anglo-Norman ruling class greatly influenced, but eventually assimilated with, each of the local cultures. Subsequent medieval English kings completed the conquest of Wales and made unsuccessful attempts to annex Scotland. Asserting its independence in the 1320 Declaration of Arbroath, Scotland maintained its independence thereafter, albeit in near-constant conflict with England.
The English monarchs, through inheritance of substantial territories in France and claims to the French crown, were also heavily involved in conflicts in France, most notably the Hundred Years War, while the Kings of Scots were in an alliance with the French during this period.
Early modern Britain saw religious conflict resulting from the Reformation and the introduction of Protestant state churches in each country. Wales was fully incorporated into the Kingdom of England, and Ireland was constituted as a kingdom in personal union with the English crown. In what was to become Northern Ireland, the lands of the independent Catholic Gaelic nobility were confiscated and given to Protestant settlers from England and Scotland.
In 1603, the kingdoms of England, Scotland and Ireland were united in a personal union when James VI, King of Scots, inherited the crowns of England and Ireland and moved his court from Edinburgh to London; each country nevertheless remained a separate political entity and retained its separate political, legal, and religious institutions.
In the mid-17th century, all three kingdoms were involved in a series of connected wars (including the English Civil War) which led to the temporary overthrow of the monarchy, with the execution of King Charles I, and the establishment of the short-lived unitary republic of the Commonwealth of England, Scotland and Ireland. During the 17th and 18th centuries, British sailors were involved in acts of piracy (privateering), attacking and stealing from ships off the coast of Europe and the Caribbean.
Although the monarchy was restored, the Interregnum (along with the Glorious Revolution of 1688 and the subsequent Bill of Rights 1689, and the Claim of Right Act 1689) ensured that, unlike much of the rest of Europe, royal absolutism would not prevail, and a professed Catholic could never accede to the throne. The British constitution would develop on the basis of constitutional monarchy and the parliamentary system. With the founding of the Royal Society in 1660, science was greatly encouraged. During this period, particularly in England, the development of naval power and the interest in voyages of discovery led to the acquisition and settlement of overseas colonies, particularly in North America and the Caribbean.
Though previous attempts at uniting the two kingdoms within Great Britain in 1606, 1667, and 1689 had proved unsuccessful, the attempt initiated in 1705 led to the Treaty of Union of 1706 being agreed and ratified by both parliaments.
On 1 May 1707, the Kingdom of Great Britain was formed, the result of Acts of Union being passed by the parliaments of England and Scotland to ratify the 1706 Treaty of Union and so unite the two kingdoms.
In the 18th century, cabinet government developed under Robert Walpole, in practice the first prime minister (1721–1742). A series of Jacobite Uprisings sought to remove the Protestant House of Hanover from the British throne and restore the Catholic House of Stuart. The Jacobites were finally defeated at the Battle of Culloden in 1746, after which the Scottish Highlanders were brutally suppressed. The British colonies in North America that broke away from Britain in the American War of Independence became the United States of America, recognised by Britain in 1783. British imperial ambition turned towards Asia, particularly to India.
Britain played a leading part in the Atlantic slave trade, mainly between 1662 and 1807 when British or British-colonial Slave ships transported nearly 3.3 million slaves from Africa. The slaves were taken to work on plantations in British possessions, principally in the Caribbean but also North America. Slavery coupled with the Caribbean sugar industry had a significant role in strengthening and developing the British economy in the 18th century. However, Parliament banned the trade in 1807, banned slavery in the British Empire in 1833, and Britain took a leading role in the movement to abolish slavery worldwide through the blockade of Africa and pressing other nations to end their trade with a series of treaties. The world's oldest international human rights organisation, Anti-Slavery International, was formed in London in 1839.
The term "United Kingdom" became official in 1801 when the parliaments of Great Britain and Ireland each passed an Act of Union, uniting the two kingdoms and creating the United Kingdom of Great Britain and Ireland.
After the defeat of France at the end of the French Revolutionary Wars and Napoleonic Wars (1792–1815), the United Kingdom emerged as the principal naval and imperial power of the 19th century (with London the largest city in the world from about 1830). Unchallenged at sea, British dominance was later described as Pax Britannica ("British Peace"), a period of relative peace among the Great Powers (1815–1914) during which the British Empire became the global hegemon and adopted the role of global policeman. By the time of the Great Exhibition of 1851, Britain was described as the "workshop of the world". From 1853 to 1856, Britain took part in the Crimean War, allied with the Ottoman Empire in the fight against the Russian Empire, participating in the naval battles of the Baltic Sea known as the Åland War in the Gulf of Bothnia and the Gulf of Finland, among others. The British Empire was expanded to include India, large parts of Africa and many other territories throughout the world. Alongside the formal control it exerted over its own colonies, British dominance of much of world trade meant that it effectively controlled the economies of many regions, such as Asia and Latin America. Domestically, political attitudes favoured free trade and laissez-faire policies and a gradual widening of the voting franchise. During the century, the population increased at a dramatic rate, accompanied by rapid urbanisation, causing significant social and economic stresses. To seek new markets and sources of raw materials, the Conservative Party under Disraeli launched a period of imperialist expansion in Egypt, South Africa, and elsewhere. Canada, Australia and New Zealand became self-governing dominions. After the turn of the century, Britain's industrial dominance was challenged by Germany and the United States. Social reform and home rule for Ireland were important domestic issues after 1900. The Labour Party emerged from an alliance of trade unions and small socialist groups in 1900, and suffragettes campaigned from before 1914 for women's right to vote.
Britain fought alongside France, Russia and (after 1917) the United States, against Germany and its allies in the First World War (1914–1918). British armed forces were engaged across much of the British Empire and in several regions of Europe, particularly on the Western front. The high fatalities of trench warfare caused the loss of much of a generation of men, with lasting social effects in the nation and a great disruption in the social order.
After the war, Britain received the League of Nations mandate over a number of former German and Ottoman colonies. The British Empire reached its greatest extent, covering a fifth of the world's land surface and a quarter of its population. Britain had suffered 2.5 million casualties and finished the war with a huge national debt.
By the mid-1920s most of the British population could listen to BBC radio programmes. Experimental television broadcasts began in 1929 and the first scheduled BBC Television Service commenced in 1936.
The rise of Irish nationalism, and disputes within Ireland over the terms of Irish Home Rule, led eventually to the partition of the island in 1921. The Irish Free State became independent, initially with Dominion status in 1922, and unambiguously independent in 1931. Northern Ireland remained part of the United Kingdom. The 1928 Act widened suffrage by giving women electoral equality with men. A wave of strikes in the mid-1920s culminated in the General Strike of 1926. Britain had still not recovered from the effects of the war when the Great Depression (1929–1932) occurred. This led to considerable unemployment and hardship in the old industrial areas, as well as political and social unrest in the 1930s, with rising membership in communist and socialist parties. A coalition government was formed in 1931.
Nonetheless, "Britain was a very wealthy country, formidable in arms, ruthless in pursuit of its interests and sitting at the heart of a global production system." After Nazi Germany invaded Poland, Britain entered the Second World War by declaring war on Germany in 1939. Winston Churchill became prime minister and head of a coalition government in 1940. Despite the defeat of its European allies in the first year of the war, Britain and its Empire continued the fight alone against Germany. Churchill engaged industry, scientists, and engineers to advise and support the government and the military in the prosecution of the war effort. In 1940, the Royal Air Force defeated the German Luftwaffe in a struggle for control of the skies in the Battle of Britain. Urban areas suffered heavy bombing during the Blitz. The Grand Alliance of Britain, the United States and the Soviet Union formed in 1941 leading the Allies against the Axis powers. There were eventual hard-fought victories in the Battle of the Atlantic, the North Africa campaign and the Italian campaign. British forces played an important role in the Normandy landings of 1944 and the liberation of Europe, achieved with its allies the United States, the Soviet Union and other Allied countries. The British Army led the Burma campaign against Japan and the British Pacific Fleet fought Japan at sea. British scientists contributed to the Manhattan Project which led to the surrender of Japan.
During the Second World War, the UK was one of the Big Three powers (along with the U.S. and the Soviet Union) who met to plan the post-war world; it was an original signatory to the Declaration by United Nations. After the war, the UK became one of the five permanent members of the United Nations Security Council and worked closely with the United States to establish the IMF, World Bank and NATO. The war left the UK severely weakened and financially dependent on the Marshall Plan, but it was spared the total war that devastated eastern Europe. In the immediate post-war years, the Labour government initiated a radical programme of reforms, which had a significant effect on British society in the following decades. Major industries and public utilities were nationalised, a welfare state was established, and a comprehensive, publicly funded healthcare system, the National Health Service, was created. The rise of nationalism in the colonies coincided with Britain's now much-diminished economic position, so that a policy of decolonisation was unavoidable. Independence was granted to India and Pakistan in 1947. Over the next three decades, most colonies of the British Empire gained their independence, with all those that sought independence supported by the UK, during both the transition period and afterwards. Many became members of the Commonwealth of Nations.
The UK was the third country to develop a nuclear weapons arsenal (with its first atomic bomb test, Operation Hurricane, in 1952), but the new post-war limits of Britain's international role were illustrated by the Suez Crisis of 1956. The international spread of the English language ensured the continuing international influence of its literature and culture. As a result of a shortage of workers in the 1950s, the government encouraged immigration from Commonwealth countries. In the following decades, the UK became a more multi-ethnic society than before. Despite rising living standards in the late 1950s and 1960s, the UK's economic performance was less successful than many of its main competitors such as France, West Germany and Japan.
In the decades-long process of European integration, the UK was a founding member of the alliance called the Western European Union, established with the London and Paris Conferences in 1954. In 1960 the UK was one of the seven founding members of the European Free Trade Association (EFTA), but in 1973 it left to join the European Communities (EC). When the EC became the European Union (EU) in 1992, the UK was one of the 12 founding member states. The Treaty of Lisbon, signed in 2007, forms the constitutional basis of the European Union since then.
From the late 1960s, Northern Ireland suffered communal and paramilitary violence (sometimes affecting other parts of the UK) conventionally known as the Troubles. It is usually considered to have ended with the Belfast "Good Friday" Agreement of 1998.
Following a period of widespread economic slowdown and industrial strife in the 1970s, the Conservative government of the 1980s under Margaret Thatcher initiated a radical policy of monetarism, deregulation, particularly of the financial sector (for example, the Big Bang in 1986) and labour markets, the sale of state-owned companies (privatisation), and the withdrawal of subsidies to others. From 1984, the economy was helped by the inflow of substantial North Sea oil revenues.
Around the end of the 20th century, there were major changes to the governance of the UK with the establishment of devolved administrations for Scotland, Wales and Northern Ireland. The statutory incorporation followed acceptance of the European Convention on Human Rights. The UK is still a key global player diplomatically and militarily. It plays leading roles in the UN and NATO.
The UK broadly supported the United States' approach to the War on Terror in the early years of the 21st century. Controversy surrounded some of Britain's overseas military deployments, particularly in Afghanistan and Iraq.
The 2008 global financial crisis severely affected the UK economy. The Cameron–Clegg coalition government of 2010 introduced austerity measures intended to tackle the substantial public deficits which resulted. The devolved Scottish Government and UK government agreed for a referendum to be held on Scottish independence in 2014. This referendum resulted in the electorate in Scotland voting by 55.3 to 44.7% for Scotland to remain part of the United Kingdom.
In 2016, 51.9 per cent of voters in the United Kingdom voted to leave the European Union. The UK left the EU on 31 January 2020 and completed its withdrawal in full at the end of that year. The COVID-19 pandemic had a major impact on the UK in 2020 and 2021.
The total area of the United Kingdom is approximately 244,820 square kilometres (94,530 sq mi). The country occupies the major part of the British Isles archipelago and includes the island of Great Britain, the north-eastern one-sixth of the island of Ireland and some smaller surrounding islands. It lies between the North Atlantic Ocean and the North Sea with the southeast coast coming within 22 miles (35 km) of the coast of northern France, from which it is separated by the English Channel. In 1993 10 per cent of the UK was forested, 46 per cent used for pastures and 25 per cent cultivated for agriculture. The Royal Greenwich Observatory in London was chosen as the defining point of the Prime Meridian in Washington, DC, in 1884, although due to more accurate modern measurement the meridian actually lies 100 metres to the east of the observatory.
The United Kingdom lies between latitudes 49° and 61° N, and longitudes 9° W and 2° E. Northern Ireland shares a 224-mile (360 km) land boundary with the Republic of Ireland. The coastline of Great Britain is 11,073 miles (17,820 km) long. It is connected to continental Europe by the Channel Tunnel, which at 31 miles (50 km) (24 miles (38 km) underwater) is the longest underwater tunnel in the world.
England accounts for just over half (53 per cent) of the total area of the UK, covering 130,395 square kilometres (50,350 sq mi). Most of the country consists of lowland terrain, with more upland and some mountainous terrain northwest of the Tees-Exe line; including the Lake District, the Pennines, Exmoor and Dartmoor. The main rivers and estuaries are the Thames, Severn and the Humber. England's highest mountain is Scafell Pike (978 metres (3,209 ft)) in the Lake District.
Scotland accounts for just under one-third (32 per cent) of the total area of the UK, covering 78,772 square kilometres (30,410 sq mi). This includes nearly 800 islands, predominantly west and north of the mainland; notably the Hebrides, Orkney Islands and Shetland Islands. Scotland is the most mountainous country in the UK and its topography is distinguished by the Highland Boundary Fault – a geological rock fracture – which traverses Scotland from Arran in the west to Stonehaven in the east. The fault separates two distinctively different regions; namely the Highlands to the north and west and the Lowlands to the south and east. The more rugged Highland region contains the majority of Scotland's mountainous land, including Ben Nevis which at 1,345 metres (4,413 ft) is the highest point in the British Isles. Lowland areas – especially the narrow waist of land between the Firth of Clyde and the Firth of Forth known as the Central Belt – are flatter and home to most of the population including Glasgow, Scotland's largest city, and Edinburgh, its capital and political centre, although upland and mountainous terrain lies within the Southern Uplands.
Wales accounts for less than one-tenth (9 per cent) of the total area of the UK, covering 20,779 square kilometres (8,020 sq mi). Wales is mostly mountainous, though South Wales is less mountainous than North and mid Wales. The main population and industrial areas are in South Wales, consisting of the coastal cities of Cardiff, Swansea and Newport, and the South Wales Valleys to their north. The highest mountains in Wales are in Snowdonia and include Snowdon (Welsh: Yr Wyddfa) which, at 1,085 metres (3,560 ft), is the highest peak in Wales. Wales has over 2,704 kilometres (1,680 miles) of coastline. Several islands lie off the Welsh mainland, the largest of which is Anglesey (Ynys Môn) in the north-west.
Northern Ireland, separated from Great Britain by the Irish Sea and North Channel, has an area of 14,160 square kilometres (5,470 sq mi) and is mostly hilly. It includes Lough Neagh which, at 388 square kilometres (150 sq mi), is the largest lake in the British Isles by area. The highest peak in Northern Ireland is Slieve Donard in the Mourne Mountains at 852 metres (2,795 ft).
The UK contains four terrestrial ecoregions: Celtic broadleaf forests, English Lowlands beech forests, North Atlantic moist mixed forests, and Caledon conifer forests. The country had a 2019 Forest Landscape Integrity Index mean score of 1.65/10, ranking it 161th globally out of 172 countries.
Most of the United Kingdom has a temperate climate, with generally cool temperatures and plentiful rainfall all year round. The temperature varies with the seasons seldom dropping below 0 °C (32 °F) or rising above 30 °C (86 °F). Some parts, away from the coast, of upland England, Wales, Northern Ireland and most of Scotland, experience a subpolar oceanic climate (Cfc). Higher elevations in Scotland experience a continental subarctic climate (Dfc) and the mountains experience a tundra climate (ET). The prevailing wind is from the southwest and bears frequent spells of mild and wet weather from the Atlantic Ocean, although the eastern parts are mostly sheltered from this wind since the majority of the rain falls over the western regions the eastern parts are therefore the driest. Atlantic currents, warmed by the Gulf Stream, bring mild winters; especially in the west where winters are wet and even more so over high ground. Summers are warmest in the southeast of England and coolest in the north. Heavy snowfall can occur in winter and early spring on high ground, and occasionally settles to great depth away from the hills.
United Kingdom is ranked 4 out of 180 countries in the Environmental Performance Index. A law has been passed that UK greenhouse gas emissions will be net zero by 2050.
The United Kingdom is a unitary state under a constitutional monarchy. Queen Elizabeth II is the monarch and head of state of the UK, as well as 14 other independent countries. These 15 countries are sometimes referred to as "Commonwealth realms". The monarch has "the right to be consulted, the right to encourage, and the right to warn". The Constitution of the United Kingdom is uncodified and consists mostly of a collection of disparate written sources, including statutes, judge-made case law and international treaties, together with constitutional conventions. The UK Parliament can carry out constitutional reform by passing acts of parliament, and thus has the political power to change or abolish almost any written or unwritten element of the constitution. No sitting parliament can pass laws that future parliaments cannot change.
The UK is a parliamentary democracy and a constitutional monarchy. The Parliament of the United Kingdom is sovereign. It is made up of the House of Commons, the House of Lords and the Crown. The main business of parliament takes place in the two houses, but royal assent is required for a bill to become an act of parliament (law).
For general elections (elections to the House of Commons), the UK is divided into 650 constituencies, each of which is represented by a member of Parliament (MP). MPs hold office for up to five years and are always up for re-election in general elections. The Conservative Party, Labour Party and Scottish National Party are, respectively, the current first, second and third largest parties (by number of MPs) in the House of Commons.
The prime minister is the head of government in the United Kingdom. Nearly all prime ministers have served as First Lord of the Treasury and all prime ministers have continuously served as First Lord of the Treasury since 1905, Minister for the Civil Service since 1968 and Minister for the Union since 2019. In modern times, the prime minister is, by constitutional convention, an MP. The prime minister is appointed by the monarch and their appointment is governed by constitutional conventions. However, they are normally the leader of the political party with the most seats in the House of Commons and hold office by virtue of their ability to command the confidence of the House of Commons.
The prime minister not only has statutory functions (alongside other ministers), but is the monarch's principal adviser and it is for them to advise the monarch on the exercise of the royal prerogative in relation to government. In particular, the prime minister recommends the appointment of ministers and chairs the Cabinet.
The geographical division of the United Kingdom into counties or shires began in England and Scotland in the early Middle Ages and was complete throughout Great Britain and Ireland by the early Modern Period. Administrative arrangements were developed separately in each country of the United Kingdom, with origins which often predated the formation of the United Kingdom. Modern local government by elected councils, partly based on the ancient counties, was introduced separately: in England and Wales in a 1888 act, Scotland in a 1889 act and Ireland in a 1898 act, meaning there is no consistent system of administrative or geographic demarcation across the United Kingdom.
Until the 19th century there was little change to those arrangements, but there has since been a constant evolution of role and function.
The organisation of local government in England is complex, with the distribution of functions varying according to local arrangements. The upper-tier subdivisions of England are the nine regions, now used primarily for statistical purposes. One region, Greater London, has had a directly elected assembly and mayor since 2000 following popular support for the proposal in a referendum. It was intended that other regions would also be given their own elected regional assemblies, but a proposed assembly in the North East region was rejected by a referendum in 2004. Since 2011, ten combined authorities have been established in England. Eight of these have elected mayors, the first elections for which took place on 4 May 2017. Below the regional tier, some parts of England have county councils and district councils and others have unitary authorities, while London consists of 32 London boroughs and the City of London. Councillors are elected by the first-past-the-post system in single-member wards or by the multi-member plurality system in multi-member wards.
For local government purposes, Scotland is divided into 32 council areas, with wide variation in both size and population. The cities of Glasgow, Edinburgh, Aberdeen and Dundee are separate council areas, as is the Highland Council, which includes a third of Scotland's area but only just over 200,000 people. Local councils are made up of elected councillors, of whom there are 1,223; they are paid a part-time salary. Elections are conducted by single transferable vote in multi-member wards that elect either three or four councillors. Each council elects a Provost, or Convenor, to chair meetings of the council and to act as a figurehead for the area.
Local government in Wales consists of 22 unitary authorities. All unitary authorities are led by a leader and cabinet elected by the council itself. These include the cities of Cardiff, Swansea and Newport, which are unitary authorities in their own right. Elections are held every four years under the first-past-the-post system.
Local government in Northern Ireland has since 1973 been organised into 26 district councils, each elected by single transferable vote. Their powers are limited to services such as collecting waste, controlling dogs and maintaining parks and cemeteries. In 2008 the executive agreed on proposals to create 11 new councils and replace the present system.
Scotland, Wales and Northern Ireland each have their own government or executive, led by a first minister (or, in the case of Northern Ireland, a diarchal first minister and deputy first minister), and a devolved unicameral legislature. England, the largest country of the United Kingdom, has no devolved executive or legislature and is administered and legislated for directly by the UK's government and parliament on all issues. This situation has given rise to the so-called West Lothian question, which concerns the fact that members of parliament from Scotland, Wales and Northern Ireland can vote, sometimes decisively, on matters that affect only England. The 2013 McKay Commission on this recommended that laws affecting only England should need support from a majority of English members of parliament.
The Scottish Government and Parliament have wide-ranging powers over any matter that has not been specifically reserved to the UK Parliament, including education, healthcare, Scots law and local government. Their power over economic issues is significantly constrained by an act of the UK parliament passed in 2020.
The Welsh Government and the Senedd (Welsh Parliament; formerly the National Assembly for Wales) have more limited powers than those devolved to Scotland. The Senedd is able to legislate on any matter not specifically reserved to the UK Parliament through Acts of Senedd Cymru.
The Northern Ireland Executive and Assembly have powers similar to those devolved to Scotland. The Executive is led by a diarchy representing unionist and nationalist members of the Assembly. Devolution to Northern Ireland is contingent on participation by the Northern Ireland administration in the North-South Ministerial Council, where the Northern Ireland Executive cooperates and develops joint and shared policies with the Government of Ireland. The British and Irish governments co-operate on non-devolved matters affecting Northern Ireland through the British–Irish Intergovernmental Conference, which assumes the responsibilities of the Northern Ireland administration in the event of its non-operation.
The UK does not have a codified constitution and constitutional matters are not among the powers devolved to Scotland, Wales or Northern Ireland. Under the doctrine of parliamentary sovereignty, the UK Parliament could, in theory, therefore, abolish the Scottish Parliament, Senedd or Northern Ireland Assembly. Indeed, in 1972, the UK Parliament unilaterally prorogued the Parliament of Northern Ireland, setting a precedent relevant to contemporary devolved institutions. In practice, it would be politically difficult for the UK Parliament to abolish devolution to the Scottish Parliament and the Senedd, given the political entrenchment created by referendum decisions. The political constraints placed upon the UK Parliament's power to interfere with devolution in Northern Ireland are even greater than in relation to Scotland and Wales, given that devolution in Northern Ireland rests upon an international agreement with the Government of Ireland. The UK Parliament restricts the three devolved parliaments' legislative competence in economic areas through an Act passed in 2020.
The United Kingdom has responsibility for 17 territories that do not form part of the United Kingdom itself: 14 British Overseas Territories and three Crown Dependencies.
The 14 British Overseas Territories are remnants of the British Empire: Anguilla; Bermuda; the British Antarctic Territory; the British Indian Ocean Territory; the British Virgin Islands; the Cayman Islands; the Falkland Islands; Gibraltar; Montserrat; Saint Helena, Ascension and Tristan da Cunha; the Turks and Caicos Islands; the Pitcairn Islands; South Georgia and the South Sandwich Islands; and Akrotiri and Dhekelia on the island of Cyprus. British claims in Antarctica have limited international recognition. Collectively Britain's overseas territories encompass an approximate land area of 480,000 square nautical miles (640,000 sq mi; 1,600,000 km2), with a total population of approximately 250,000. The overseas territories also give the UK the world's fifth largest exclusive economic zone at 6,805,586 km2 (2,627,651 sq mi). A 1999 UK government white paper stated that: " Overseas Territories are British for as long as they wish to remain British. Britain has willingly granted independence where it has been requested; and we will continue to do so where this is an option." Self-determination is also enshrined in the constitutions of several overseas territories and three have specifically voted to remain under British sovereignty (Bermuda in 1995, Gibraltar in 2002 and the Falkland Islands in 2013).
The Crown dependencies are possessions of the Crown, as opposed to overseas territories of the UK. They comprise three independently administered jurisdictions: the Channel Islands of Jersey and Guernsey in the English Channel, and the Isle of Man in the Irish Sea. By mutual agreement, the British Government manages the islands' foreign affairs and defence and the UK Parliament has the authority to legislate on their behalf. Internationally, they are regarded as "territories for which the United Kingdom is responsible". The power to pass legislation affecting the islands ultimately rests with their own respective legislative assemblies, with the assent of the Crown (Privy Council or, in the case of the Isle of Man, in certain circumstances the Lieutenant-Governor). Since 2005 each Crown dependency has had a Chief Minister as its head of government.
The United Kingdom does not have a single legal system as Article 19 of the 1706 Treaty of Union provided for the continuation of Scotland's separate legal system. Today the UK has three distinct systems of law: English law, Northern Ireland law and Scots law. A new Supreme Court of the United Kingdom came into being in October 2009 to replace the Appellate Committee of the House of Lords. The Judicial Committee of the Privy Council, including the same members as the Supreme Court, is the highest court of appeal for several independent Commonwealth countries, the British Overseas Territories and the Crown Dependencies.
Both English law, which applies in England and Wales, and Northern Ireland law are based on common-law principles. The essence of common law is that, subject to statute, the law is developed by judges in courts, applying statute, precedent and common sense to the facts before them to give explanatory judgements of the relevant legal principles, which are reported and binding in future similar cases (stare decisis). The courts of England and Wales are headed by the Senior Courts of England and Wales, consisting of the Court of Appeal, the High Court of Justice (for civil cases) and the Crown Court (for criminal cases). The Supreme Court is the highest court in the land for both criminal and civil appeal cases in England, Wales and Northern Ireland and any decision it makes is binding on every other court in the same jurisdiction, often having a persuasive effect in other jurisdictions.
Scots law is a hybrid system based on both common-law and civil-law principles. The chief courts are the Court of Session, for civil cases, and the High Court of Justiciary, for criminal cases. The Supreme Court of the United Kingdom serves as the highest court of appeal for civil cases under Scots law. Sheriff courts deal with most civil and criminal cases including conducting criminal trials with a jury, known as sheriff solemn court, or with a sheriff and no jury, known as sheriff summary Court. The Scots legal system is unique in having three possible verdicts for a criminal trial: "guilty", "not guilty" and "not proven". Both "not guilty" and "not proven" result in an acquittal.
Crime in England and Wales increased in the period between 1981 and 1995, though since that peak there has been an overall fall of 66 per cent in recorded crime from 1995 to 2015, according to crime statistics. The prison population of England and Wales has increased to 86,000, giving England and Wales the highest rate of incarceration in Western Europe at 148 per 100,000. Her Majesty's Prison Service, which reports to the Ministry of Justice, manages most of the prisons within England and Wales. The murder rate in England and Wales has stabilised in the first half of the 2010s with a murder rate around 1 per 100,000 which is half the peak in 2002 and similar to the rate in the 1980s Crime in Scotland fell slightly in 2014–2015 to its lowest level in 39 years in with 59 killings for a murder rate of 1.1 per 100,000. Scotland's prisons are overcrowded but the prison population is shrinking.
The UK is a permanent member of the United Nations Security Council, a member of NATO, AUKUS, the Commonwealth of Nations, the G7 finance ministers, the G7 forum, the G20, the OECD, the WTO, the Council of Europe and the OSCE. The UK is said to have a "Special Relationship" with the United States and a close partnership with France – the "Entente cordiale" – and shares nuclear weapons technology with both countries; the Anglo-Portuguese Alliance is considered to be the oldest binding military alliance in the world. The UK is also closely linked with the Republic of Ireland; the two countries share a Common Travel Area and co-operate through the British-Irish Intergovernmental Conference and the British-Irish Council. Britain's global presence and influence is further amplified through its trading relations, foreign investments, official development assistance and military engagements. Canada, Australia and New Zealand, all of which are former colonies of the British Empire which share Queen Elizabeth II as their head of state, are the most favourably viewed countries in the world by British people.
Her Majesty's Armed Forces consist of three professional service branches: the Royal Navy and Royal Marines (forming the Naval Service), the British Army and the Royal Air Force. The armed forces of the United Kingdom are managed by the Ministry of Defence and controlled by the Defence Council, chaired by the Secretary of State for Defence. The Commander-in-Chief is the British monarch, to whom members of the forces swear an oath of allegiance. The Armed Forces are charged with protecting the UK and its overseas territories, promoting the UK's global security interests and supporting international peacekeeping efforts. They are active and regular participants in NATO, including the Allied Rapid Reaction Corps, the Five Power Defence Arrangements, RIMPAC and other worldwide coalition operations. Overseas garrisons and facilities are maintained in Ascension Island, Bahrain, Belize, Brunei, Canada, Cyprus, Diego Garcia, the Falkland Islands, Germany, Gibraltar, Kenya, Oman, Qatar and Singapore.
The British armed forces played a key role in establishing the British Empire as the dominant world power in the 18th, 19th and early 20th centuries. By emerging victorious from conflicts, Britain has often been able to decisively influence world events. Since the end of the British Empire, the UK has remained a major military power. Following the end of the Cold War, defence policy has a stated assumption that "the most demanding operations" will be undertaken as part of a coalition.
According to sources which include the Stockholm International Peace Research Institute and the International Institute for Strategic Studies, the UK has either the fourth- or the fifth-highest military expenditure. Total defence spending amounts to 2.0 per cent of national GDP.
The UK has a partially regulated market economy. Based on market exchange rates, the UK is today the fifth-largest economy in the world and the second-largest in Europe after Germany. HM Treasury, led by the Chancellor of the Exchequer, is responsible for developing and executing the government's public finance policy and economic policy. The Bank of England is the UK's central bank and is responsible for issuing notes and coins in the nation's currency, the pound sterling. Banks in Scotland and Northern Ireland retain the right to issue their own notes, subject to retaining enough Bank of England notes in reserve to cover their issue. The pound sterling is the world's fourth-largest reserve currency (after the US dollar, euro, and Japanese Yen). Since 1997 the Bank of England's Monetary Policy Committee, headed by the Governor of the Bank of England, has been responsible for setting interest rates at the level necessary to achieve the overall inflation target for the economy that is set by the Chancellor each year.
The UK service sector makes up around 79 per cent of GDP. London is one of the world's largest financial centres, ranking 2nd in the world, behind New York City, in the Global Financial Centres Index in 2020. London also has the largest city GDP in Europe. Edinburgh ranks 17th in the world, and 6th in Western Europe in the Global Financial Centres Index in 2020. Tourism is very important to the British economy; with over 27 million tourists arriving in 2004, the United Kingdom is ranked as the sixth major tourist destination in the world and London has the most international visitors of any city in the world. The creative industries accounted for 7 per cent GVA in 2005 and grew at an average of 6 per cent per annum between 1997 and 2005.
Following the United Kingdom's withdrawal from the European Union, the functioning of the UK internal economic market is enshrined by the United Kingdom Internal Market Act 2020 which ensures trade in goods and services continues without internal barriers across the four countries of the United Kingdom.
The Industrial Revolution started in the UK with an initial concentration on the textile industry, followed by other heavy industries such as shipbuilding, coal mining and steelmaking. British merchants, shippers and bankers developed overwhelming advantage over those of other nations allowing the UK to dominate international trade in the 19th century. As other nations industrialised, coupled with economic decline after two world wars, the United Kingdom began to lose its competitive advantage and heavy industry declined, by degrees, throughout the 20th century. Manufacturing remains a significant part of the economy but accounted for only 16.7 per cent of national output in 2003.
The automotive industry employs around 800,000 people, with a turnover in 2015 of £70 billion, generating £34.6 billion of exports (11.8 per cent of the UK's total export goods). In 2015, the UK produced around 1.6 million passenger vehicles and 94,500 commercial vehicles. The UK is a major centre for engine manufacturing: in 2015 around 2.4 million engines were produced. The UK motorsport industry employs around 41,000 people, comprises around 4,500 companies and has an annual turnover of around £6 billion.
The aerospace industry of the UK is the second- or third-largest national aerospace industry in the world depending upon the method of measurement and has an annual turnover of around £30 billion.
BAE Systems plays a critical role in some of the world's biggest defence aerospace projects. In the UK, the company makes large sections of the Typhoon Eurofighter and assembles the aircraft for the Royal Air Force. It is also a principal subcontractor on the F35 Joint Strike Fighter – the world's largest single defence project – for which it designs and manufactures a range of components. It also manufactures the Hawk, the world's most successful jet training aircraft. Airbus UK also manufactures the wings for the A400 m military transporter. Rolls-Royce is the world's second-largest aero-engine manufacturer. Its engines power more than 30 types of commercial aircraft and it has more than 30,000 engines in service in the civil and defence sectors.
The UK space industry was worth £9.1bn in 2011 and employed 29,000 people. It is growing at a rate of 7.5 per cent annually, according to its umbrella organisation, the UK Space Agency. In 2013, the British Government pledged £60 m to the Skylon project: this investment will provide support at a "crucial stage" to allow a full-scale prototype of the SABRE engine to be built.
The pharmaceutical industry plays an important role in the UK economy and the country has the third-highest share of global pharmaceutical R&amp;D expenditures.
Agriculture is intensive, highly mechanised and efficient by European standards, producing about 60 per cent of food needs with less than 1.6 per cent of the labour force (535,000 workers). Around two-thirds of production is devoted to livestock, one-third to arable crops. The UK retains a significant, though much reduced fishing industry. It is also rich in a number of natural resources including coal, petroleum, natural gas, tin, limestone, iron ore, salt, clay, chalk, gypsum, lead, silica and an abundance of arable land.
In 2020, coronavirus lockdown measures caused the UK economy to suffer its biggest slump on record, shrinking by 20.4 per cent between April and June compared to the first three months of the year, to push it officially into recession for the first time in 11 years.
The UK has an external debt of $9.6 trillion dollars, which is the second-highest in the world after the US. As a percentage of GDP, external debt is 408 per cent, which is the third-highest in the world after Luxembourg and Iceland.
England and Scotland were leading centres of the Scientific Revolution from the 17th century. The United Kingdom led the Industrial Revolution from the 18th century, and has continued to produce scientists and engineers credited with important advances. Major theorists from the 17th and 18th centuries include Isaac Newton, whose laws of motion and illumination of gravity have been seen as a keystone of modern science; from the 19th century Charles Darwin, whose theory of evolution by natural selection was fundamental to the development of modern biology, and James Clerk Maxwell, who formulated classical electromagnetic theory; and more recently Stephen Hawking, who advanced major theories in the fields of cosmology, quantum gravity and the investigation of black holes.
Major scientific discoveries from the 18th century include hydrogen by Henry Cavendish; from the 20th century penicillin by Alexander Fleming, and the structure of DNA, by Francis Crick and others. Famous British engineers and inventors of the Industrial Revolution include James Watt, George Stephenson, Richard Arkwright, Robert Stephenson and Isambard Kingdom Brunel. Other major engineering projects and applications by people from the UK include the steam locomotive, developed by Richard Trevithick and Andrew Vivian; from the 19th century the electric motor by Michael Faraday, the first computer designed by Charles Babbage, the first commercial electrical telegraph by William Fothergill Cooke and Charles Wheatstone, the incandescent light bulb by Joseph Swan, and the first practical telephone, patented by Alexander Graham Bell; and in the 20th century the world's first working television system by John Logie Baird and others, the jet engine by Frank Whittle, the basis of the modern computer by Alan Turing, and the World Wide Web by Tim Berners-Lee.
Scientific research and development remains important in British universities, with many establishing science parks to facilitate production and co-operation with industry. Between 2004 and 2008 the UK produced 7 per cent of the world's scientific research papers and had an 8 per cent share of scientific citations, the third and second-highest in the world (after the United States and China, respectively). Scientific journals produced in the UK include Nature, the British Medical Journal and The Lancet. The United Kingdom was ranked 4th in the Global Innovation Index 2020 and 2021, up from 5th in 2019.
A radial road network totals 29,145 miles (46,904 km) of main roads, 2,173 miles (3,497 km) of motorways and 213,750 miles (344,000 km) of paved roads. The M25, encircling London, is the largest and busiest bypass in the world. In 2009 there were a total of 34 million licensed vehicles in Great Britain.
The rail network in the UK is the oldest such network in the world. The system consists of five high-speed main lines (the West Coast, East Coast, Midland, Great Western and Great Eastern), which radiate from London to the rest of the country, augmented by regional rail lines and dense commuter networks within the major cities. High Speed 1 is operationally separate from the rest of the network. The world's first passenger railway running on steam was the Stockton and Darlington Railway, opened in 1825. Just under five years later the world's first intercity railway was the Liverpool and Manchester Railway, designed by George Stephenson. The network grew rapidly as a patchwork of hundreds of separate companies during the Victorian era.
The UK has a railway network of 10,072 miles (16,209 km) in Great Britain and 189 miles (304 km) in Northern Ireland. Railways in Northern Ireland are operated by NI Railways, a subsidiary of state-owned Translink. In Great Britain, the British Rail network was privatised between 1994 and 1997, which was followed by a rapid rise in passenger numbers. The UK was ranked eighth among national European rail systems in the 2017 European Railway Performance Index assessing intensity of use, quality of service and safety. HS2 is a new high speed railway under construction linking up London, the Midlands, the North and Scotland serving over 25 stations, including eight of Britain's 10 largest cities and connecting around 30 million. Crossrail, opened in 2022, was Europe's largest construction project with a £15 billion projected cost.
Great British Railways is a planned state-owned public body that will oversee rail transport in Great Britain from 2023. In 2014, there were 5.2 billion bus journeys in the UK, 2.4 billion of which were in London. The red double-decker bus has entered popular culture as an internationally recognised icon of England. The London bus network is extensive, with over 6,800 scheduled services every weekday carrying about six million passengers on over 700 different routes making it one of the most extensive bus systems in the world and the largest in Europe.
In the year from October 2009 to September 2010 UK airports handled a total of 211.4 million passengers. In that period the three largest airports were London Heathrow Airport (65.6 million passengers), Gatwick Airport (31.5 million passengers) and London Stansted Airport (18.9 million passengers). London Heathrow Airport, located 15 miles (24 km) west of the capital, has the most international passenger traffic of any airport in the world and is the hub for the UK flag carrier British Airways, as well as Virgin Atlantic.
In 2006, the UK was the world's ninth-largest consumer of energy and the 15th-largest producer. The UK is home to a number of large energy companies, including two of the six oil and gas "supermajors" – BP and Shell.
In 2013, the UK produced 914 thousand barrels per day (bbl/d) of oil and consumed 1,507 thousand bbl/d. Production is now in decline and the UK has been a net importer of oil since 2005. In 2010 the UK had around 3.1 billion barrels of proven crude oil reserves, the largest of any EU member state.
In 2009, the UK was the 13th-largest producer of natural gas in the world and the largest producer in the EU. Production is now in decline and the UK has been a net importer of natural gas since 2004.
Coal production played a key role in the UK economy in the 19th and 20th centuries. In the mid-1970s, 130 million tonnes of coal were produced annually, not falling below 100 million tonnes until the early 1980s. During the 1980s and 1990s the industry was scaled back considerably. In 2011, the UK produced 18.3 million tonnes of coal. In 2005 it had proven recoverable coal reserves of 171 million tons. The UK Coal Authority has stated there is a potential to produce between 7 billion tonnes and 16 billion tonnes of coal through underground coal gasification (UCG) or 'fracking', and that, based on current UK coal consumption, such reserves could last between 200 and 400 years. Environmental and social concerns have been raised over chemicals getting into the water table and minor earthquakes damaging homes.
In the late 1990s, nuclear power plants contributed around 25 per cent of total annual electricity generation in the UK, but this has gradually declined as old plants have been shut down and ageing-related problems affect plant availability. In 2012, the UK had 16 reactors normally generating about 19 per cent of its electricity. All but one of the reactors will be retired by 2023. Unlike Germany and Japan, the UK intends to build a new generation of nuclear plants from about 2018.
The total of all renewable electricity sources provided for 38.9 per cent of the electricity generated in the United Kingdom in the third quarter of 2019, producing 28.8TWh of electricity. The UK is one of the best sites in Europe for wind energy, and wind power production is its fastest-growing supply, in 2019 it generated almost 20 per cent of the UK's total electricity.
Access to improved water supply and sanitation in the UK is universal. It is estimated that 96.7 per cent of households are connected to the sewer network. According to the Environment Agency, total water abstraction for public water supply in the UK was 16,406 megalitres per day in 2007.
In England and Wales water and sewerage services are provided by 10 private regional water and sewerage companies and 13 mostly smaller private "water only" companies. In Scotland, water and sewerage services are provided by a single public company, Scottish Water. In Northern Ireland water and sewerage services are also provided by a single public entity, Northern Ireland Water.
A census is taken simultaneously in all parts of the UK every 10 years. In the 2011 census the total population of the United Kingdom was 63,181,775. It is the fourth-largest in Europe (after Russia, Germany and France), the fifth-largest in the Commonwealth and the 22nd-largest in the world. In mid-2014 and mid-2015 net long-term international migration contributed more to population growth. In mid-2012 and mid-2013 natural change contributed the most to population growth. Between 2001 and 2011 the population increased by an average annual rate of approximately 0.7 per cent. This compares to 0.3 per cent per year in the period 1991 to 2001 and 0.2 per cent in the decade 1981 to 1991. The 2011 census also showed that, over the previous 100 years, the proportion of the population aged 0–14 fell from 31 per cent to 18 per cent, and the proportion of people aged 65 and over rose from 5 to 16 per cent. In 2018 the median age of the UK population was 41.7 years.
England's population in 2011 was 53 million, representing some 84 per cent of the UK total. It is one of the most densely populated countries in the world, with 420 people resident per square kilometre in mid-2015, with a particular concentration in London and the south-east. The 2011 census put Scotland's population at 5.3 million, Wales at 3.06 million and Northern Ireland at 1.81 million.
In 2017 the average total fertility rate (TFR) across the UK was 1.74 children born per woman. While a rising birth rate is contributing to population growth, it remains considerably below the baby boom peak of 2.95 children per woman in 1964, or the high of 6.02 children born per woman in 1815, below the replacement rate of 2.1, but higher than the 2001 record low of 1.63. In 2011, 47.3 per cent of births in the UK were to unmarried women. The Office for National Statistics published a bulletin in 2015 showing that, out of the UK population aged 16 and over, 1.7 per cent identify as gay, lesbian, or bisexual (2.0 per cent of males and 1.5 per cent of females); 4.5 per cent of respondents responded with "other", "I don't know", or did not respond. The number of transgender people in the UK was estimated to be between 65,000 and 300,000 by research between 2001 and 2008.
Historically, indigenous British people were thought to be descended from the various ethnic groups that settled there before the 12th century: the Celts, Romans, Anglo-Saxons, Norse and the Normans. Welsh people could be the oldest ethnic group in the UK. A 2006 genetic study shows that more than 50 per cent of England's gene pool contains Germanic Y chromosomes. Another 2005 genetic analysis indicates that "about 75 per cent of the traceable ancestors of the modern British population had arrived in the British isles by about 6,200 years ago, at the start of the British Neolithic or Stone Age", and that the British broadly share a common ancestry with the Basque people.
The UK has a history of non-white immigration with Liverpool having the oldest Black population in the country dating back to at least the 1730s during the period of the African slave trade. During this period it is estimated the Afro-Caribbean population of Great Britain was 10,000 to 15,000 which later declined due to the abolition of slavery. The UK also has the oldest Chinese community in Europe, dating to the arrival of Chinese seamen in the 19th century. In 1950 there were probably fewer than 20,000 non-white residents in Britain, almost all born overseas. In 1951 there were an estimated 94,500 people living in Britain who had been born in South Asia, China, Africa and the Caribbean, just under 0.2 per cent of the UK population. By 1961 this number had more than quadrupled to 384,000, just over 0.7 per cent of the United Kingdom population.
Since 1948 substantial immigration from Africa, the Caribbean and South Asia has been a legacy of ties forged by the British Empire. Migration from new EU member states in Central and Eastern Europe since 2004 has resulted in growth in these population groups, although some of this migration has been temporary. Since the 1990s, there has been substantial diversification of the immigrant population, with migrants to the UK coming from a much wider range of countries than previous waves, which tended to involve larger numbers of migrants coming from a relatively small number of countries.
Academics have argued that the ethnicity categories employed in British national statistics, which were first introduced in the 1991 census, involve confusion between the concepts of ethnicity and race. In 2011, 87.2 per cent of the UK population identified themselves as white, meaning 12.8 per cent of the UK population identify themselves as of one of number of ethnic minority groups. In the 2001 census, this figure was 7.9 per cent of the UK population.
Because of differences in the wording of the census forms used in England and Wales, Scotland and Northern Ireland, data on the Other White group is not available for the UK as a whole, but in England and Wales this was the fastest-growing group between the 2001 and 2011 censuses, increasing by 1.1 million (1.8 percentage points). Amongst groups for which comparable data is available for all parts of the UK level, the Other Asian category increased from 0.4 per cent to 1.4 per cent of the population between 2001 and 2011, while the Mixed category rose from 1.2 per cent to 2 per cent.
Ethnic diversity varies significantly across the UK. 30.4 per cent of London's population and 37.4 per cent of Leicester's was estimated to be non-white in 2005, whereas less than 5 per cent of the populations of North East England, Wales and the South West were from ethnic minorities, according to the 2001 census. In 2016, 31.4 per cent of primary and 27.9 per cent of secondary pupils at state schools in England were members of an ethnic minority. The 1991 census was the first UK census to have a question on ethnic group. In the 1991 UK census 94.1 per cent of people reported themselves as being White British, White Irish or White Other with 5.9 per cent of people reporting themselves as coming from other minority groups.
The UK's de facto official language is English. It is estimated that 95 per cent of the UK's population are monolingual English speakers. 5.5 per cent of the population are estimated to speak languages brought to the UK as a result of relatively recent immigration. South Asian languages are the largest grouping which includes Punjabi, Urdu, Bengali, Sylheti, Hindi and Gujarati. According to the 2011 census, Polish has become the second-largest language spoken in England and has 546,000 speakers. In 2019, some three quarters of a million people spoke little or no English.
Three indigenous Celtic languages are spoken in the UK: Welsh, Irish and Scottish Gaelic. Cornish, which became extinct as a first language in the late 18th century, is subject to revival efforts and has a small group of second language speakers. In the 2011 Census, approximately one-fifth (19 per cent) of the population of Wales said they could speak Welsh, an increase from the 1991 Census (18 per cent). In addition, it is estimated that about 200,000 Welsh speakers live in England. In the same census in Northern Ireland 167,487 people (10.4 per cent) stated that they had "some knowledge of Irish" (see Irish language in Northern Ireland), almost exclusively in the nationalist (mainly Catholic) population. Over 92,000 people in Scotland (just under 2 per cent of the population) had some Gaelic language ability, including 72 per cent of those living in the Outer Hebrides. The number of children being taught either Welsh or Scottish Gaelic is increasing. Among emigrant-descended populations some Scottish Gaelic is still spoken in Canada (principally Nova Scotia and Cape Breton Island), and Welsh in Patagonia, Argentina.
Scots, a language descended from early northern Middle English, has limited recognition alongside its regional variant, Ulster Scots in Northern Ireland, without specific commitments to protection and promotion.
As of April 2020, there are said to be around 151,000 users of British Sign Language (BSL), a sign language used by deaf people, in the UK. BSL was recognised as a language of England, Scotland and Wales in law in 2022. It is compulsory for pupils to study a second language from the age of seven in England. French and Spanish are the two most commonly taught second languages in the United Kingdom. All pupils in Wales are either taught Welsh as a second language up to age 16, or are taught in Welsh as a first language. Welsh was recognised as having official status in Wales in 2011.
Forms of Christianity have dominated religious life in what is now the United Kingdom for over 1,400 years. Although a majority of citizens still identify with Christianity in many surveys, regular church attendance has fallen dramatically since the middle of the 20th century, while immigration and demographic change have contributed to the growth of other faiths, most notably Islam. This has led some commentators to variously describe the UK as a multi-faith, secularised, or post-Christian society.
In the 2001 census, 71.6 per cent of all respondents indicated that they were Christians, with the next largest faiths being Islam (2.8 per cent), Hinduism (1.0 per cent), Sikhism (0.6 per cent), Judaism (0.5 per cent), Buddhism (0.3 per cent) and all other religions (0.3 per cent). 15 per cent of respondents stated that they had no religion, with a further 7 per cent not stating a religious preference. A Tearfund survey in 2007 showed only one in 10 Britons actually attend church weekly. Between the 2001 and 2011 census there was a decrease in the number of people who identified as Christian by 12 per cent, whilst the percentage of those reporting no religious affiliation doubled. This contrasted with growth in the other main religious group categories, with the number of Muslims increasing by the most substantial margin to a total of about 5 per cent. The Muslim population has increased from 1.6 million in 2001 to 2.7 million in 2011, making it the second-largest religious group in the United Kingdom.
In a 2016 survey conducted by BSA (British Social Attitudes) on religious affiliation; 53 per cent of respondents indicated 'no religion', while 41 per cent indicated they were Christians, followed by 6 per cent who affiliated with other religions (e.g. Islam, Hinduism, Judaism, etc.). Among Christians, adherents to the Church of England constituted 15 per cent, Catholic Church 9 per cent, and other Christians (including Presbyterians, Methodists, other Protestants, as well as Eastern Orthodox), 17 per cent. 71 per cent of young people aged 18––24 said they had no religion.
The Church of England is the established church in England. It retains a representation in the UK Parliament and the British monarch is its Supreme Governor. In Scotland, the Church of Scotland is recognised as the national church. It is not subject to state control, and the British monarch is an ordinary member, required to swear an oath to "maintain and preserve the Protestant Religion and Presbyterian Church Government" upon his or her accession. The Church in Wales was disestablished in 1920 and, as the Church of Ireland was disestablished in 1870 before the partition of Ireland, there is no established church in Northern Ireland. Although there are no UK-wide data in the 2001 census on adherence to individual Christian denominations, it has been estimated that 62 per cent of Christians are Anglican, 13.5 per cent Catholic, 6 per cent Presbyterian, and 3.4 per cent Methodist, with small numbers of other Protestant denominations such as Plymouth Brethren, and Orthodox churches.
The United Kingdom has experienced successive waves of migration. The Great Famine in Ireland, then part of the United Kingdom, resulted in perhaps a million people migrating to Great Britain. Throughout the 19th century a small population of 28,644 German immigrants built up in England and Wales. London held around half of this population, and other small communities existed in Manchester, Bradford and elsewhere. The German immigrant community was the largest group until 1891, when it became second to Russian Jews. After 1881, Russian Jews suffered bitter persecutions and 2,000,000 left the Russian Empire by 1914. Around 120,000 settled permanently in Britain, becoming the largest ethnic minority from outside the British Isles; this population had increased to 370,000 by 1938. Unable to return to Poland at the end of World War II, over 120,000 Polish veterans remained in the UK permanently. After the Second World War, many people immigrated from colonies and former-colonies in the Caribbean and Indian subcontinent, as a legacy of empire or driven by labour shortages. In 1841, 0.25 per cent of the population of England and Wales was born in a foreign country, increasing to 1.5 per cent by 1901, 2.6 per cent by 1931 and 4.4 per cent in 1951.
In 2014 the immigration net increase was 318,000: Immigration was at 641,000, up from 526,000 in 2013, while the number of emigrants leaving for over a year was 323,000. A recent migration trend has been the arrival of workers from the new EU member states in Eastern Europe, known as the A8 countries. In 2011, citizens of new EU member states made up 13 per cent of immigrants. The UK applied temporary restrictions to citizens of Romania and Bulgaria, which joined the EU in January 2007. Research conducted by the Migration Policy Institute for the Equality and Human Rights Commission suggests that, between May 2004 and September 2009, 1.5 million workers migrated from the new EU member states to the UK, most of them Polish. Many subsequently returned home, resulting in a net increase in the number of nationals of the new member states in the UK. The late-2000s recession in the UK reduced economic incentive for Poles to migrate to the UK, making migration temporary and circular. The proportion of foreign-born people in the UK remains slightly below that of many other European countries.
Immigration is now contributing to a rising population, with arrivals and UK-born children of migrants accounting for about half of the population increase between 1991 and 2001. 27 per cent of UK live births in 2014 were to mothers born outside the UK, according to official statistics released in 2015. The ONS reported that net migration rose from 2009 to 2010 by 21 per cent to 239,000.
In 2013, approximately 208,000 foreign nationals were naturalised as British citizens, the highest number since 1962. This figure fell to around 125,800 in 2014. Between 2009 and 2013, the average British citizenships granted annually was 195,800. The most common previous nationalities of those naturalised in 2014 were India, Pakistan, the Philippines, Nigeria, Bangladesh, Nepal, China, South Africa, Poland and Somalia. The total number of grants of settlement, which confer permanent residence in the UK but not citizenship, was approximately 154,700 in 2013, higher than the previous two years.
In 2008, the British Government introduced a points-based immigration system for immigration from outside the European Economic Area to replace former schemes, including the Scottish Government's Fresh Talent Initiative. In June 2010 a temporary limit on immigration from outside the EU was introduced, aiming to discourage applications before a permanent cap was imposed in April 2011.
Emigration was an important feature of British society in the 19th century. Between 1815 and 1930, around 11.4 million people emigrated from Britain and 7.3 million from Ireland. Estimates show that by the end of the 20th century, some 300 million people of British and Irish descent were permanently settled around the globe. Today, at least 5.5 million UK-born people live abroad, mainly in Australia, Spain, the United States and Canada.
Education in the United Kingdom is a devolved matter, with each country having a separate education system.
Considering the four systems together, about 38 per cent of the United Kingdom population has a university or college degree, which is the highest percentage in Europe, and among the highest percentages in the world. The United Kingdom trails only the United States in terms of representation on lists of top 100 universities.
A government commission's report in 2014 found that privately educated people comprise 7 per cent of the general population of the UK but much larger percentages of the top professions, the most extreme case quoted being 71 per cent of senior judges.
Whilst education in England is the responsibility of the Secretary of State for Education, the day-to-day administration and funding of state schools is the responsibility of local authorities. Universally free of charge state education was introduced piecemeal between 1870 and 1944. Education is now mandatory from ages 5 to 16, and in England youngsters must stay in education or training until they are 18. In 2011, the Trends in International Mathematics and Science Study (TIMSS) rated 13–14-year-old pupils in England and Wales 10th in the world for maths and 9th for science. The majority of children are educated in state-sector schools, a small proportion of which select on the grounds of academic ability. Two of the top 10 performing schools in terms of GCSE results in 2006 were state-run grammar schools. In 2010, over half of places at the University of Oxford and the University of Cambridge were taken by students from state schools, while the proportion of children in England attending private schools is around 7 per cent.
Education in Scotland is the responsibility of the Cabinet Secretary for Education and Lifelong Learning, with day-to-day administration and funding of state schools the responsibility of Local Authorities. Two non-departmental public bodies have key roles in Scottish education. The Scottish Qualifications Authority is responsible for the development, accreditation, assessment and certification of qualifications other than degrees which are delivered at secondary schools, post-secondary colleges of further education and other centres. Learning and Teaching Scotland provides advice, resources and staff development to education professionals. Scotland first legislated for compulsory education in 1496. The proportion of children in Scotland attending private schools is just over 4 per cent in 2016, but it has been falling slowly in recent years. Scottish students who attend Scottish universities pay neither tuition fees nor graduate endowment charges, as fees were abolished in 2001 and the graduate endowment scheme was abolished in 2008.
The Welsh Government's Minister for Education has responsibility for education in Wales. State funded education is available to children from the age of three whilst the legal obligation for parents to have their children educated, usually at school, begins at age five. A sizeable minority of pupils are educated in Welsh whilst the rest are obliged to study the language until the age of 16. Wales' performance in Pisa testing, which compares the academic performance of adolescents around the world, has improved in recent years but remains lower than other parts of the UK. In 2019, just under 60% of entrants passed their main English and math GCSEs. The obligation to receive education in Wales ends at the age of 16. In 2017 and 2018, just under 80% of 16 to 18 and just under 40% of 19 to 24-year-olds were in some kind of education or training.
Education in Northern Ireland is the responsibility of the Minister of Education, although responsibility at a local level is administered by the Education Authority which is further sub-divided into five geographical areas. The Council for the Curriculum, Examinations &amp; Assessment (CCEA) is the body responsible for advising the government on what should be taught in Northern Ireland's schools, monitoring standards and awarding qualifications.
Healthcare in the United Kingdom is a devolved matter and each country has its own system of private and publicly funded healthcare. Public healthcare is provided to all UK permanent residents and is mostly free at the point of need, being paid for from general taxation. The World Health Organization, in 2000, ranked the provision of healthcare in the United Kingdom as fifteenth best in Europe and eighteenth in the world.
Since 1979 expenditure on healthcare has been increased significantly. The 2018 OECD data, which incorporates in health a chunk of what in the UK is classified as social care, has the UK spending £3,121 per head. In 2017 the UK spent £2,989 per person on healthcare, around the median for members of the Organisation for Economic Co-operation and Development.
Regulatory bodies are organised on a UK-wide basis such as the General Medical Council, the Nursing and Midwifery Council and non-governmental-based, such as the Royal Colleges. Political and operational responsibility for healthcare lies with four national executives; healthcare in England is the responsibility of the UK Government; healthcare in Northern Ireland is the responsibility of the Northern Ireland Executive; healthcare in Scotland is the responsibility of the Scottish Government; and healthcare in Wales is the responsibility of the Welsh Government. Each National Health Service has different policies and priorities, resulting in contrasts.
The culture of the United Kingdom has been influenced by many factors including: the nation's island status; its history as a western liberal democracy and a major power; as well as being a political union of four countries with each preserving elements of distinctive traditions, customs and symbolism. As a result of the British Empire, British influence can be observed in the language, culture and legal systems of many of its former colonies including Australia, Canada, India, Ireland, New Zealand, Pakistan, South Africa and the United States; a common culture coined today as the Anglosphere. The substantial cultural influence of the United Kingdom has led it to be described as a "cultural superpower". A global opinion poll for the BBC saw the United Kingdom ranked the third most positively viewed nation in the world (behind Germany and Canada) in 2013 and 2014.
"British literature" refers to literature associated with the United Kingdom, the Isle of Man and the Channel Islands. Most British literature is in the English language. In 2005, some 206,000 books were published in the United Kingdom and in 2006 it was the largest publisher of books in the world.
The English playwright and poet William Shakespeare is widely regarded as the greatest dramatist of all time. The 20th-century English crime writer Agatha Christie is the best-selling novelist of all time.
Twelve of the top 25 of 100 novels by British writers chosen by a BBC poll of global critics were written by women; these included works by George Eliot, Virginia Woolf, Charlotte and Emily Brontë, Mary Shelley, Jane Austen, Doris Lessing and Zadie Smith.
Scotland's contributions include Arthur Conan Doyle (the creator of Sherlock Holmes), Sir Walter Scott, J. M. Barrie, Robert Louis Stevenson and the poet Robert Burns. More recently Hugh MacDiarmid and Neil M. Gunn contributed to the Scottish Renaissance, with grimmer works from Ian Rankin and Iain Banks. Scotland's capital, Edinburgh, was UNESCO's first worldwide City of Literature.
Britain's oldest known poem, Y Gododdin, was composed most likely in the late 6th century. It was written in Cumbric or Old Welsh and contains the earliest known reference to King Arthur. The Arthurian legend was further developed by Geoffrey of Monmouth. Poet Dafydd ap Gwilym (fl. 1320–1370) is regarded as one of the greatest European poets of his age. Daniel Owen is credited as the first Welsh-language novelist, publishing Rhys Lewis in 1885. The best-known of the Anglo-Welsh poets are Dylan Thomas and R. S. Thomas, the latter nominated for the Nobel Prize in Literature in 1996. Leading Welsh novelists of the twentieth century include Richard Llewellyn and Kate Roberts.
Irish writers, living at a time when all of Ireland was part of the United Kingdom, include Oscar Wilde, Bram Stoker and George Bernard Shaw.
There have been a number of authors whose origins were from outside the United Kingdom but who moved to the UK. These include Joseph Conrad, T. S. Eliot, Kazuo Ishiguro, Sir Salman Rushdie and Ezra Pound.
Various styles of music have become popular in the UK, including the indigenous folk music of England, Wales, Scotland and Northern Ireland. Historically, there has been exceptional Renaissance music from the Tudor period, with masses, madrigals and lute music by Thomas Tallis, John Taverner, William Byrd, Orlando Gibbons and John Dowland. After the Stuart Restoration, an English tradition of dramatic masques, anthems and airs became established, led by Henry Purcell, followed by Thomas Arne and others. The German-born composer George Frideric Handel became a naturalised British citizen in 1727, when he composed the anthem Zadok the Priest for the coronation of George II; it became the traditional ceremonial music for anointing all future monarchs. Handel's many oratorios, such as his famous Messiah, were written in the English language. Ceremonial music is also performed to mark Remembrance Sunday across the UK, including the Traditional Music played at the Cenotaph. In the second half of the 19th century, as Arthur Sullivan and his librettist W. S. Gilbert wrote their popular Savoy operas, Edward Elgar's wide range of music rivalled that of his contemporaries on the continent. Increasingly, however, composers became inspired by the English countryside and its folk music, notably Gustav Holst, Ralph Vaughan Williams, and Benjamin Britten, a pioneer of modern British opera. Among the many post-war composers, some of the most notable have made their own personal choice of musical identity: Peter Maxwell Davies (Orkney), Harrison Birtwistle (mythological), and John Tavener (religious).
The UK is also home to world-renowned symphonic orchestras and choruses such as the BBC Symphony Orchestra and the London Symphony Chorus. Notable British conductors include Sir Henry Wood, Sir John Barbirolli, Sir Malcolm Sargent, Sir Charles Groves, Sir Charles Mackerras and Sir Simon Rattle; at the same time international conductors, not British-born, like Georg Solti and Bernard Haitink, have become at the forefront in Britain for performances of symphonic music and opera.
Some of the notable film score composers include William Walton, Eric Coates, John Barry, Clint Mansell, Mike Oldfield, John Powell, Craig Armstrong, David Arnold, John Murphy, Monty Norman and Harry Gregson-Williams. Andrew Lloyd Webber is a prolific composer of musical theatre. His works have dominated London's West End since the late 20th century and have also been a commercial success worldwide.
According to the website of The New Grove Dictionary of Music and Musicians, the term "pop music" originated in Britain in the mid-1950s to describe rock and roll's fusion with the "new youth music". The Oxford Dictionary of Music states that artists such as The Beatles and The Rolling Stones drove pop music to the forefront of popular music in the early 1960s. In the following years, Britain widely occupied a part in the development of rock music, with British acts pioneering hard rock; raga rock; art rock; heavy metal; space rock; glam rock; new wave; Gothic rock, and ska punk. In addition, British acts developed progressive rock; psychedelic rock; and punk rock. Besides rock music, British acts also developed neo soul and created dubstep.
The Beatles have international sales of over 1 billion units and are the biggest-selling and most influential band in the history of popular music. Other prominent British contributors to have influenced popular music over the last 50 years include The Rolling Stones, Pink Floyd, Queen, Led Zeppelin, the Bee Gees, and Elton John, all of whom have worldwide record sales of 200 million or more. The Brit Awards are the BPI's annual music awards, and some of the British recipients of the Outstanding Contribution to Music award include; The Who, David Bowie, Eric Clapton, Rod Stewart, The Police, and Fleetwood Mac (who are a British-American band). More recent UK music acts that have had international success include George Michael, Oasis, Spice Girls, Radiohead, Coldplay, Arctic Monkeys, Robbie Williams, Amy Winehouse, Adele, Ed Sheeran, One Direction and Harry Styles.
A number of UK cities are known for their music. Acts from Liverpool have had 54 UK chart number 1 hit singles, more per capita than any other city worldwide. Glasgow's contribution to music was recognised in 2008 when it was named a UNESCO City of Music. Manchester played a role in the spread of dance music such as acid house, and from the mid-1990s, Britpop. London and Bristol are closely associated with the origins of electronic music sub-genres such as drum and bass and trip hop. Birmingham became known as the birthplace of heavy metal, with the band Black Sabbath starting there in the 1960s.
Pop remains the most popular music genre by sales and streams of singles, with 33.4 per cent of that market in 2016, followed by hip-hop and R&amp;B at 24.5 per cent. Rock is not far behind, at 22.6 per cent. The modern UK is known to produce some of the most prominent English-speaking rappers along with the United States, including Stormzy, Kano, Yxng Bane, Ramz and Skepta.
The history of British visual art forms part of western art history. Major British artists include: the Romantics William Blake, John Constable, Samuel Palmer and J.M.W. Turner; the portrait painters Sir Joshua Reynolds and Lucian Freud; the landscape artists Thomas Gainsborough and L. S. Lowry; the pioneer of the Arts and Crafts Movement William Morris; the figurative painter Francis Bacon; the Pop artists Peter Blake, Richard Hamilton and David Hockney; the pioneers of Conceptual art movement Art &amp; Language; the collaborative duo Gilbert and George; the abstract artist Howard Hodgkin; and the sculptors Antony Gormley, Anish Kapoor and Henry Moore. During the late 1980s and 1990s the Saatchi Gallery in London helped to bring to public attention a group of multi-genre artists who would become known as the "Young British Artists": Damien Hirst, Chris Ofili, Rachel Whiteread, Tracey Emin, Mark Wallinger, Steve McQueen, Sam Taylor-Wood and the Chapman Brothers are among the better-known members of this loosely affiliated movement.
The Royal Academy in London is a key organisation for the promotion of the visual arts in the United Kingdom. Major schools of art in the UK include: the six-school University of the Arts London, which includes the Central Saint Martins College of Art and Design and Chelsea College of Art and Design; Goldsmiths, University of London; the Slade School of Fine Art (part of University College London); the Glasgow School of Art; the Royal College of Art; and The Ruskin School of Drawing and Fine Art (part of the University of Oxford). The Courtauld Institute of Art is a leading centre for the teaching of the history of art. Important art galleries in the United Kingdom include the National Gallery, National Portrait Gallery, Tate Britain and Tate Modern (the most-visited modern art gallery in the world, with around 4.7 million visitors per year).
The United Kingdom has had a considerable influence on the history of the cinema. The British directors Alfred Hitchcock, whose film Vertigo is considered by some critics as the best film of all time, and David Lean are among the most critically acclaimed of all time. Many British actors have achieved international fame and critical success. Some of the most commercially successful films of all time have been produced in the United Kingdom, including two of the highest-grossing film franchises (Harry Potter and James Bond). Ealing Studios has a claim to being the oldest continuously working film studio in the world.
In 2009, British films grossed around $2 billion worldwide and achieved a market share of around 7 per cent globally and 17 per cent in the United Kingdom. UK box-office takings totalled £944 million in 2009, with around 173 million admissions. The annual British Academy Film Awards are hosted by the British Academy of Film and Television Arts.
British cuisine developed from various influences reflective of its land, settlements, arrivals of new settlers and immigrants, trade and colonialism. Celtic agriculture and animal breeding produced a wide variety of foodstuffs for indigenous Celts and Britons. Anglo-Saxon England developed meat and savoury herb stewing techniques before the practice became common in Europe. The Norman conquest introduced exotic spices into England in the Middle Ages. The British Empire facilitated a knowledge of Indian cuisine with its "strong, penetrating spices and herbs". British cuisine has absorbed the cultural influence of those who have settled in Britain, producing many hybrid dishes, such as the Anglo-Indian chicken tikka masala. Vegan and vegetarian diets have increased in Britain in recent years. In 2021, a survey found that 8% of British respondents eat a plant-based diet and 36% of respondents have a favourable view of plant-based diets.
The BBC, founded in 1922, is the UK's publicly funded radio, television and Internet broadcasting corporation, and is the oldest and largest broadcaster in the world. It operates numerous television and radio stations in the UK and abroad and its domestic services are funded by the television licence. The BBC World Service is an international broadcaster owned and operated by the BBC. It is the world's largest of any kind. It broadcasts radio news, speech and discussions in more than 40 languages.
Other major players in the UK media include ITV plc, which operates 11 of the 15 regional television broadcasters that make up the ITV Network, and Sky. Newspapers produced in the United Kingdom include The Times, The Guardian, The Observer, The Economist, and the Financial Times. Magazines and journals published in the United Kingdom that have achieved worldwide circulation include Nature, New Scientist, The Spectator, Prospect, NME, Radio Times, and The Economist.
London dominates the media sector in the UK: national newspapers and television and radio are largely based there, although Manchester is also a significant national media centre. Edinburgh and Glasgow, and Cardiff, are important centres of newspaper and broadcasting production in Scotland and Wales, respectively. The UK publishing sector, including books, directories and databases, journals, magazines and business media, newspapers and news agencies, has a combined turnover of around £20 billion and employs around 167,000 people. In 2015, the UK published 2,710 book titles per million inhabitants, more than any other country, much of this being exported to other Anglophone countries.
In 2009, it was estimated that individuals viewed a mean of 3.75 hours of television per day and 2.81 hours of radio. In that year the main BBC public service broadcasting channels accounted for an estimated 28.4 per cent of all television viewing; the three main independent channels accounted for 29.5 per cent and the increasingly important other satellite and digital channels for the remaining 42.1 per cent. Sales of newspapers have fallen since the 1970s and in 2010 41 per cent of people reported reading a daily national newspaper. In 2010, 82.5 per cent of the UK population were Internet users, the highest proportion amongst the 20 countries with the largest total number of users in that year.
The United Kingdom is famous for the tradition of 'British Empiricism', a branch of the philosophy of knowledge that states that only knowledge verified by experience is valid, and 'Scottish Philosophy', sometimes referred to as the 'Scottish School of Common Sense'. The most famous philosophers of British Empiricism are John Locke, George Berkeley and David Hume; while Dugald Stewart, Thomas Reid and William Hamilton were major exponents of the Scottish "common sense" school. Two Britons are also notable for the ethical theory of utilitarianism, a moral philosophy first used by Jeremy Bentham and later by John Stuart Mill in his short work Utilitarianism.
Association football, tennis, table tennis, badminton, rugby union, rugby league, rugby sevens, golf, boxing, netball, water polo, field hockey, billiards, darts, rowing, rounders and cricket originated or were substantially developed in the UK, with the rules and codes of many modern sports invented and codified in the late 19th century Victorian Britain. In 2012, the President of the IOC, Jacques Rogge, stated, "This great, sports-loving country is widely recognised as the birthplace of modern sport. It was here that the concepts of sportsmanship and fair play were first codified into clear rules and regulations. It was here that sport was included as an educational tool in the school curriculum".
A 2003 poll found that football is the most popular sport in the United Kingdom. England is recognised by FIFA as the birthplace of club football, and The Football Association is the oldest of its kind, with the rules of football first drafted in 1863 by Ebenezer Cobb Morley. Each of the Home Nations has its own football association, national team and league system and individually are the governing members of the International Football Association Board alongside FIFA. The English top division, the Premier League, is the most watched football league in the world. The first international football match was contested by England and Scotland on 30 November 1872. England, Scotland, Wales and Northern Ireland usually compete as separate countries in international competitions.
In 2003, rugby union was ranked the second most popular sport in the UK. The sport was created in Rugby School, Warwickshire, and the first rugby international took place on 27 March 1871 between England and Scotland. England, Scotland, Wales, Ireland, France and Italy compete in the Six Nations Championship; the premier international tournament in the northern hemisphere. Sport governing bodies in England, Scotland, Wales and Ireland organise and regulate the game separately. Every four years, England, Ireland, Scotland and Wales make a combined team known as the British and Irish Lions. The team tours Australia, New Zealand and South Africa.
Cricket was invented in England, and its laws were established by the Marylebone Cricket Club in 1788. The England cricket team, controlled by the England and Wales Cricket Board, and the Irish cricket team, controlled by Cricket Ireland are the only national teams in the UK with Test status. Team members are drawn from the main county sides, and include both English and Welsh players. Cricket is distinct from football and rugby where Wales and England field separate national teams, although Wales had fielded its own team in the past. Scottish players have played for England because Scotland does not have Test status and has only recently started to play in One Day Internationals. Scotland, England (and Wales), and Ireland (including Northern Ireland) have competed at the Cricket World Cup, with England winning the tournament in 2019. There is a professional league championship in which clubs representing 17 English counties and 1 Welsh county compete.
The modern game of tennis originated in Birmingham, England, in the 1860s, before spreading around the world. The world's oldest tennis tournament, the Wimbledon championships, first occurred in 1877, and today the event takes place over two weeks in late June and early July.
The UK is closely associated with motorsport. Many teams and drivers in Formula One (F1) are based in the UK, and the country has won more drivers' and constructors' titles than any other. The UK hosted the first F1 Grand Prix in 1950 at Silverstone, the location of the British Grand Prix held each year in July.
Golf is the sixth most popular sport, by participation, in the UK. Although The Royal and Ancient Golf Club of St Andrews in Scotland is the sport's home course, the world's oldest golf course is actually Musselburgh Links' Old Golf Course. In 1764, the standard 18-hole golf course was created at St Andrews when members modified the course from 22 to 18 holes. The oldest golf tournament in the world, and the first major championship in golf, The Open Championship, is played annually on the weekend of the third Friday in July.
Rugby league originated in Huddersfield, West Yorkshire in 1895 and is generally played in Northern England. A single 'Great Britain Lions' team had competed in the Rugby League World Cup and Test match games, but this changed in 2008 when England, Scotland and Ireland competed as separate nations. Great Britain is still retained as the full national team. Super League is the highest level of professional rugby league in the UK and Europe. It consists of 11 teams from Northern England, and one each from London, Wales and France.
The 'Queensberry rules', the code of general rules in boxing, was named after John Douglas, 9th Marquess of Queensberry in 1867, and formed the basis of modern boxing. Snooker is another of the UK's popular sporting exports, with the world championships held annually in Sheffield. In Northern Ireland, Gaelic football and hurling are popular team sports, both in terms of participation and spectating. Irish expatriates in the UK and the US also play them. Shinty (or camanachd) is popular in the Scottish Highlands. Highland games are held in spring and summer in Scotland, celebrating Scottish and celtic culture and heritage, especially that of the Scottish Highlands.
The United Kingdom has hosted the Summer Olympic Games on three occasions in 1908, 1948 and 2012, with London acting as the host city for all three games respectively. The upcoming 2022 Commonwealth Games, scheduled to be hosted in Birmingham, will mark the seventh time the UK has hosted the Commonwealth Games.
The flag of the United Kingdom is the Union Flag (also referred to as the Union Jack). It was created in 1606 by the superimposition of the Flag of England, representing Saint George, on the Flag of Scotland, representing Saint Andrew, and was updated in 1801 with the addition of Saint Patrick's Flag. Wales is not represented in the Union Flag, as Wales had been conquered and annexed to England prior to the formation of the United Kingdom. The possibility of redesigning the Union Flag to include representation of Wales has not been completely ruled out. The national anthem of the United Kingdom is "God Save the Queen", with "Queen" replaced with "King" in the lyrics whenever the monarch is a man.
Britannia is a national personification of the United Kingdom, originating from Roman Britain. Britannia is symbolised as a young woman with brown or golden hair, wearing a Corinthian helmet and white robes. She holds Poseidon's three-pronged trident and a shield, bearing the Union Flag.
Beside the lion and the unicorn and the dragon of heraldry, the bulldog is an iconic animal and commonly represented with the Union Jack. It has been associated with Winston Churchill's defiance of Nazi Germany. A now rare personification is a character originating in the 18th century, John Bull, a portly country gentleman dressed in a top hat and tailcoat with a Union Jack waistcoat, often accompanied by a bulldog.
The floral emblems of the three kingdoms are the Tudor rose for England, the thistle for Scotland and the shamrock for Northern Ireland; they are sometimes shown intertwined to represent unity. The daffodil and the leek are the symbols of Wales. Alternatives include the Royal Oak for England and the flax flower for Northern Ireland.
Click on a coloured area to see an article about English in that country or region
Coordinates: 55°N 3°W﻿ / ﻿55°N 3°W﻿ / 55; -3

The Royal Flying Corps (RFC) was the air arm of the British Army before and during the First World War until it merged with the Royal Naval Air Service on 1 April 1918 to form the Royal Air Force.  During the early part of the war, the RFC supported the British Army by artillery co-operation and photographic reconnaissance. This work gradually led RFC pilots into aerial battles with German pilots and later in the war included the strafing of enemy infantry and emplacements, the bombing of German military airfields and later the strategic bombing of German industrial and transport facilities.
At the start of World War I the RFC, commanded by Brigadier-General Sir David Henderson, consisted of five squadrons – one observation balloon squadron (RFC No 1 Squadron) and four aeroplane squadrons. These were first used for aerial spotting on 13 September 1914 but only became efficient when they perfected the use of wireless communication at Aubers Ridge on 9 May 1915. Aerial photography was attempted during 1914, but again only became effective the next year. By 1918, photographic images could be taken from 15,000 feet and were interpreted by over 3,000 personnel. Parachutes were not available to pilots of heavier-than-air craft in the RFC – nor were they used by the RAF during the First World War – although the Calthrop Guardian Angel parachute (1916 model) was officially adopted just as the war ended. By this time parachutes had been used by balloonists for three years.
On 17 August 1917, South African General Jan Smuts presented a report to the War Council on the future of air power. Because of its potential for the 'devastation of enemy lands and the destruction of industrial and populous centres on a vast scale', he recommended a new air service be formed that would be on a level with the Army and Royal Navy. The formation of the new service would also make the under-used men and machines of the Royal Naval Air Service (RNAS) available for action on the Western Front and end the inter-service rivalries that at times had adversely affected aircraft procurement. On 1 April 1918, the RFC and the RNAS were amalgamated to form a new service, the Royal Air Force (RAF), under the control of the new Air Ministry. After starting in 1914 with some 2,073 personnel, by the start of 1919 the RAF had 4,000 combat aircraft and 114,000 personnel in some 150 squadrons.
With the growing recognition of the potential for aircraft as a cost-effective method of reconnaissance and artillery observation, the Committee of Imperial Defence established a sub-committee to examine the question of military aviation in November 1911.  On 28 February 1912 the sub-committee reported its findings which recommended that a flying corps be formed and that it consist of a naval wing, a military wing, a central flying school and an aircraft factory.  The recommendations of the committee were accepted and on 13 April 1912 King George V signed a royal warrant establishing the Royal Flying Corps. The Air Battalion of the Royal Engineers became the Military Wing of the Royal Flying Corps a month later on 13 May.
The Flying Corps' initial allowed strength was 133 officers, and by the end of that year it had 12 manned balloons and 36 aeroplanes.  The RFC originally came under the responsibility of Brigadier-General Henderson, the Director of Military Training, and had separate branches for the Army and the Navy. Major Sykes commanded the Military Wing and Commander C R Samson commanded the Naval Wing. The Royal Navy however, with different priorities to that of the Army and wishing to retain greater control over its aircraft, formally separated its branch and renamed it the Royal Naval Air Service on 1 July 1914, although a combined central flying school was retained.
The RFC's motto was Per ardua ad astra ("Through adversity to the stars"). This remains the motto of the Royal Air Force (RAF) and other Commonwealth air forces.
The RFC's first fatal crash was on 5 July 1912 near Stonehenge on Salisbury Plain; Captain Eustace B. Loraine and his observer, Staff Sergeant R.H.V. Wilson, flying from Larkhill Aerodrome, were killed.  An order was issued after the crash stating "Flying will continue this evening as usual", thus beginning a tradition.
In August 1912 RFC Lieutenant Wilfred Parke RN became the first aviator to be observed to recover from an accidental spin when the Avro G cabin biplane, with which he had just broken a world endurance record, entered a spin at 700 feet above ground level at Larkhill. Four months later on 11 December 1912 Parke was killed when the Handley Page monoplane in which he was flying from Hendon to Oxford crashed.
Aircraft used during the war by the RFC included:
On its inception in 1912 the Royal Flying Corps consisted of a Military and a Naval Wing, with the Military Wing consisting of three squadrons each commanded by a major.  The Naval Wing, with fewer pilots and aircraft than the Military Wing, did not organise itself into squadrons until 1914; it separated from the RFC that same year.  By November 1914 the Royal Flying Corps, even taking the loss of the Naval Wing into account, had expanded sufficiently to warrant the creation of wings consisting of two or more squadrons.  These wings were commanded by lieutenant-colonels. In October 1915 the Corps had undergone further expansion which justified the creation of brigades, each commanded by a brigadier-general.  Further expansion led to the creation of divisions, with the Training Division being established in August 1917 and RFC Middle East being raised to divisional status in December 1917.  Additionally, although the Royal Flying Corps in France was never titled as a division, by March 1916 it comprised several brigades and its commander (Trenchard) had received a promotion to major-general, giving it in effect divisional status.  Finally, the air raids on London and the south-east of England led to the creation of the London Air Defence Area in August 1917 under the command of Ashmore who was promoted to major-general.
Two of the first three RFC squadrons were formed from the Air Battalion of the Royal Engineers: No. 1 Company (a balloon company) becoming No. 1 Squadron, RFC, and  No. 2 Company (a 'heavier-than-air' company) becoming No. 3 Squadron, RFC. A second heavier-than-air squadron, No. 2 Squadron, RFC, was also formed on the same day.
No. 4 Squadron, RFC was formed from No. 2 Sqn in August 1912, and No. 5 Squadron, RFC from No. 3 Sqn in July 1913.
By the end of March 1918, the Royal Flying Corps comprised some 150 squadrons.
The composition of an RFC squadron varied depending on its designated role, although the commanding officer was usually a major (in a largely non-operational role), with the squadron 'flights' (annotated A, B, C etc.) the basic tactical and operational unit, each commanded by a captain. A 'recording officer' (of captain/lieutenant rank) would act as intelligence officer and adjutant, commanding two or three NCOs and ten other ranks in the administration section of the squadron. Each flight contained on average between six and ten pilots (and a corresponding number of observers, if applicable) with a senior sergeant and thirty-six other ranks (as fitters, riggers, metalsmiths, armourers, etc.). The average squadron also had on complement an equipment officer, armaments officer (each with five other ranks) and a transport officer, in charge of twenty-two other ranks.
The squadron transport establishment typically included one car, five light tenders, seven heavy tenders, two repair lorries, eight motorcycles and eight trailers.
Wings in the Royal Flying Corps consisted of a number of squadrons.
When the Royal Flying Corps was established it was intended to be a joint service. Owing to the rivalry between the British Army and Royal Navy, new terminology was thought necessary in order to avoid marking the Corps out as having a particularly Army or Navy ethos.  Accordingly, the Corps was originally split into two wings: a Military Wing (i.e. an army wing) and a Naval Wing.  By 1914, the Naval Wing had become the Royal Naval Air Service, having gained its independence from the Royal Flying Corps.
By November 1914 the Flying Corps had significantly expanded and it was felt necessary to create organizational units which would control collections of squadrons; the term "wing" was reused for these new organizational units.
The Military Wing was abolished and its units based in Great Britain were regrouped as the Administrative Wing. The RFC squadrons in France were grouped under the newly established 1st Wing and the 2nd Wing.  The 1st Wing was assigned to the support of the 1st Army whilst the 2nd Wing supported the 2nd Army.
As the Flying Corps grew, so did the number of wings.  The 3rd Wing was established on 1 March 1915 and on 15 April the 5th Wing came into existence.  By August that year the 6th Wing had been created and in November 1915 a 7th Wing and 8th Wing had also been stood up. Additional wings continued to be created throughout World War I in line with the incessant demands for air units.  The last RFC wing to be created was the 54th Wing in March 1918, just prior to the creation of the RAF.
Following the creation of brigades, wings took on specialised functions. Corps wings undertook artillery observation and ground liaison duties, with one squadron detached to each army corps. Army wings were responsible for air superiority, bombing and strategic reconnaissance. United Kingdom based forces were organised into home defence and training wings. By March 1918, wings controlled as many as nine squadrons.
Following Sir David Henderson's return from France to the War Office in August 1915, he submitted a scheme to the Army Council which was intended to expand the command structure of the Flying Corps.  The Corps' wings would be grouped in pairs to form brigades and the commander of each brigade would hold the temporary rank of brigadier-general.  The scheme met with Lord Kitchener's approval and although some staff officers opposed it, the scheme was adopted.
In the field, most brigades were assigned to the army. Initially a brigade consisted of an army wing and corps wing; beginning in November 1916 a balloon wing was added to control the observation balloon companies. Logistics support was provided by an army aircraft park, aircraft ammunition column and reserve lorry park.
All operating locations were officially called "Royal Flying Corps Station name". A typical Squadron may have been based at four Stations – an Aerodrome for the HQ, and three Landing Grounds, one per each flight. Stations tended to be named after the local railway station, to simplify the administration of rail travel warrants.
Typically a training airfield consisted of a 2,000 feet (610 m) grass square. There were three pairs plus one single hangar, constructed of wood or brick, 180 feet (55 m) x 100 feet (30 m) in size. There were up to 12 canvas Bessonneau hangars as the aircraft, constructed from wood, wire and fabric, were liable to weather damage. Other airfield buildings were typically wooden or Nissen huts.
Landing Grounds were often L-shaped, usually arrived at by removing a hedge boundary between two fields, and thereby allowing landing runs in two directions of 400–500 metres (1,300–1,600 ft). Typically they would be manned by only two or three airmen, whose job was to guard the fuel stores and assist any aircraft which had occasion to land. Accommodation for airmen and pilots was often in tents, especially on the Western Front. Officers would be billeted to local country houses, or commandeered châteaux when posted abroad, if suitable accommodation had not been built on the Station.
Landing Grounds were categorised according to their lighting and day or night capabilities:
Stations that were heavily used or militarily important grew by compulsorily purchasing extra land, changing designations as necessary. Aerodromes would often grow into sprawling sites, due to the building of headquarters/administration offices, mess buildings, fuel and weapon stores, wireless huts and other support structures as well as the aircraft hangarage and repair facilities. Narborough and Marham both started off as Night Landing Grounds a few miles apart. One was an RNAS Station, the other RFC. Narborough grew to be the largest aerodrome in Britain at 908 acres (367 ha) with 30 acres (12 ha) of buildings including seven large hangars, seven motorised transport (MT) garages, five workshops, two coal yards, two Sergeants' Messes, three dope sheds and a guardhouse. Marham was 80 acres (32 ha). Both these Stations are now lost beneath the present RAF Marham. Similarly, Stations at Easton-on-the-Hill and Stamford merged into modern day RAF Wittering although they are in different counties.
The Royal Flying Corps Canada was established by the RFC in 1917 to train aircrew in Canada. Air Stations were established in southern Ontario at the following locations:
The RFC was also responsible for the manning and operation of observation balloons on the Western front. When the British Expeditionary Force (BEF) arrived in France in August 1914, it had no observation balloons and it was not until April 1915 that the first balloon company was on strength, albeit on loan from the French Aérostiers. The first British unit arrived 8 May 1915, and commenced operations during the Battle of Aubers Ridge. Operations from balloons thereafter continued throughout the war. Highly hazardous in operation, a balloon could only be expected to last a fortnight before damage or destruction. Results were also highly dependent on the expertise of the observer and was subject to the weather conditions.
To keep the balloon out of the range of artillery fire, it was necessary to locate the balloons some distance away from the front line or area of military operations. However, the stable platform offered by a kite-balloon made it more suitable for the cameras of the day than an aircraft.
For the first half of the war, as with the land armies deployed, the French air force vastly outnumbered the RFC, and accordingly did more of the fighting.  Despite the primitive aircraft, aggressive leadership by RFC commander Hugh Trenchard and the adoption of a continually offensive stance operationally in efforts to pin the enemy back led to many brave fighting exploits and high casualties – over 700 in 1916, the rate worsening thereafter, until the RFC's nadir in April 1917 which was dubbed 'Bloody April'.
This aggressive, if costly, doctrine did however provide the Army General Staff with vital and up-to-date intelligence on German positions and numbers through continual photographic and observational reconnaissance throughout the war.
At the start of the war, numbers 2, 3, 4 and 5 Squadrons were equipped with aeroplanes. No. 1 Squadron had been equipped with balloons but all these were transferred to the Naval Wing in 1913; thereafter No. 1 Squadron reorganised itself as an 'aircraft park' for the British Expeditionary Force. The RFC's first casualties were before the Corps even arrived in France: Lt Robert R. Skene and Air Mechanic Ray Barlow were killed on 12 August 1914 when their (probably overloaded) plane crashed at Netheravon on the way to rendezvous with the rest of the RFC near Dover. Skene had been the first Englishman to perform a loop in an aeroplane.
On 13 August 1914, 2, 3, and 4 squadrons, comprising 60 machines, departed from Dover for the British Expeditionary Force in France and 5 Squadron joined them a few days later. The aircraft took a route across the English Channel from Dover to Boulogne, then followed the French coast to the Bay of the Somme and followed the river to Amiens. When the BEF moved forward to Maubeuge the RFC accompanied them. On 19 August the Corps undertook its first action of the war, with two of its aircraft performing aerial reconnaissance.  The mission was not a great success; to save weight each aircraft carried a pilot only instead of the usual pilot and observer.  Because of this, and poor weather, both of the pilots lost their way and only one was able to complete his task.
On 22 August 1914, the first British aircraft was lost to German fire. The crew—pilot Second Lieutenant Vincent Waterfall and observer Lt. Charles George Gordon Bayly, of 5 Squadron—flying an Avro 504 over Belgium, were killed by infantry fire. Also on 22 August 1914, Captain L E O Charlton (observer) and his pilot, Lieutenant Vivian Hugh Nicholas Wadham, made the crucial observation of the 1st German Army's approach towards the flank of the British Expeditionary Force. This allowed the BEF Commander-in-Chief Field Marshal Sir John French to realign his front and save his army around Mons.
Next day, the RFC found itself fighting in the Battle of Mons and two days after that, gained its first air victory. On 25 August, Lt C. W. Wilson and Lt C. E. C. Rabagliati forced down a German Etrich Taube, which had approached their aerodrome while they were refuelling their Avro 504.  Another RFC machine landed nearby and the RFC observer chased the German pilot into nearby woods. After the Great Retreat from Mons, the Corps fell back to the Marne where in September, the RFC again proved its value by identifying von Kluck's First Army's left wheel against the exposed French flank.  This information was significant as the First Army's manoeuvre allowed French forces to make an effective counter-attack at the Battle of the Marne.
Sir John French's (the British Expeditionary Force commander) first official dispatch on 7 September included the following: "I wish particularly to bring to your Lordships' notice the admirable work done by the Royal Flying Corps under Sir David Henderson.  Their skill, energy, and perseverance has been beyond all praise.  They have furnished me with most complete and accurate information, which has been of incalculable value in the conduct of operations.  Fired at constantly by friend and foe, and not hesitating to fly in every kind of weather, they have remained undaunted throughout.  Further, by actually fighting in the air, they have succeeded in destroying five of the enemy's machines."
Early in the war RFC aircraft were not systematically marked with any national insignia. At a squadron level, Union Flag markings in various styles were often painted on the wings (and sometimes the fuselage sides and/or rudder). However, there was a danger of the large red St George's Cross being mistaken for the German Eisernes Kreuz (iron cross) marking, and so of RFC aircraft being fired upon by friendly ground forces. By late 1915, therefore, the RFC had adopted a modified version of the French cockade (or roundel) marking, with the colours reversed (the blue circle outermost). In contrast to usual French practice, the roundel was applied to the fuselage sides as well as the wings. To minimise the likelihood of "friendly" attack, the rudders of RFC aircraft were painted to match the French, with the blue, white and red stripes – going from the forward (rudder hingeline) to aft (trailing edge) – of the French tricolour. Later in the war, a "night roundel" was adopted for night flying aircraft (especially Handley Page O/400 heavy bombers), which omitted the conspicuous white circle of the "day" marking.
Later in September, 1914, during the First Battle of the Aisne, the RFC made use of wireless telegraphy to assist with artillery targeting and took aerial photographs for the first time.
From 16,000 feet a photographic plate could cover some 2 by 3 miles (3.2 km × 4.8 km) of front line in sharp detail. In 1915 Lieutenant-Colonel JTC Moore-Brabrazon designed the first practical aerial camera. These semi-automatic cameras became a high priority for the Corps and photo-reconnaissance aircraft were soon operational in numbers with the RFC. The camera was usually fixed to the side of the fuselage, or operated through a hole in the floor. The increasing need for surveys of the western front and its approaches, made extensive aerial photography essential. Aerial photographs were exclusively used in compiling the British Army's highly detailed 1:10,000 scale maps introduced in mid-1915. Such were advances in aerial photography that the entire Somme Offensive of July–November 1916 was based on the RFC's air-shot photographs.
One of the initial and most important uses of RFC aircraft was observing artillery fire behind the enemy front line at targets that could not be seen by ground observers.  The fall of shot of artillery fire were easy enough for the pilot to see, providing he was looking in the right place at the right time; apart from this the problem was communicating corrections to the battery.
Development of procedures had been the responsibility of No 3 Squadron and the Royal Artillery in 1912–13.  These methods usually depended on the pilot being tasked to observe the fire against a specific target and report the fall of shot relative to the target, the battery adjusted their aim, fired and the process was repeated until the target was effectively engaged. One early communication method was for the flier to write a note and drop it to the ground where it could be recovered but various visual signalling methods were also used.  This meant the pilots had to observe the battery to see when it fired and see if it had laid out a visual signal using white marker panels on the ground.
The Royal Engineers' Air Battalion had pioneered experiments with wireless telegraphy in airships and aircraft before the RFC was created.  Unfortunately the early transmitters weighed 75 pounds and filled a seat in the cockpit.  This meant that the pilot had to fly the aircraft, navigate, observe the fall of the shells and transmit the results by morse code by himself.  Also, the wireless in the aircraft could not receive.  Originally only a special Wireless Flight attached to No. 4 Squadron RFC had the wireless equipment.  Eventually this flight was expanded into No. 9 Squadron under Major Hugh Dowding.  However, in early 1915 the Sterling lightweight wireless became available and was widely used. In 1915 each corps in the BEF was assigned a RFC squadron solely for artillery observation and reconnaissance duties.  The transmitter filled the cockpit normally used by the observer and a trailing wire antenna was used which had to be reeled in prior to landing.
The RFC's wireless experiments under Major Herbert Musgrave, included research into how wireless telegraphy could be used by military aircraft. However, the most important officers in wireless development were Lieutenants Donald Lewis and Baron James in the RFC HQ wireless unit formed in France in September 1914.  They developed both equipment and procedures in operational sorties.
An important development was the Zone Call procedure in 1915.  By this time maps were 'squared' and a target location could be reported from the air using alphanumeric characters transmitted in Morse code.  Batteries were allocated a Zone, typically a quarter of a mapsheet, and it was the duty of the RFC signallers on the ground beside the battery command post to pick out calls for fire in their battery's Zone.  Once ranging started the airman reported the position of the ranging round using the clock code, the battery adjusted their firing data and fired again, and the process was repeated until the pilot observed an on-target or close round.  The battery commander then decided how much to fire at the target.
The results were mixed.  Observing artillery fire, even from above, requires training and skill.  Within artillery units, ground observers received mentoring to develop their skill, which was not available to RFC aircrew. There were undoubtedly some very skilled artillery observers in the RFC, but there were many who were not and there was a tendency for 'optimism bias' – reported on-target rounds that weren't.  The procedures were also time-consuming.
The ground stations were generally attached to heavy artillery units, such as Royal Garrison Artillery Siege Batteries, and were manned by RFC wireless operators, such as Henry Tabor. These wireless operators had to fend for themselves as their squadrons were situated some distance away and they were not posted to the battery they were colocated with.  This led to concerns as to who had responsibility for them and in November 1916 squadron commanders had to be reminded "that it is their duty to keep in close touch with the operators attached to their command, and to make all necessary arrangements for supplying them with blankets, clothing, pay, etc" (Letter from Headquarters, 2nd Brigade RFC dated 18 November 1916 – Public Records Office AIR/1/864)
The wireless operators' work was often carried out under heavy artillery fire in makeshift dug-outs.  The artillery batteries were important targets and antennas were a lot less robust than the guns, hence prone to damage requiring immediate repair.  As well as taking down and interpreting the numerous signals coming in from the aircraft, the operator had to communicate back to the aircraft by means of cloth strips laid out on the ground or a signalling lamp to give visual confirmation that the signals had been received.  The wireless communication was one way as no receiver was mounted in the aircraft and the ground station could not transmit. Details from: .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:linear-gradient(transparent,transparent),url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:#d33}.mw-parser-output .cs1-visible-error{color:#d33}.mw-parser-output .cs1-maint{display:none;color:#3a3;margin-left:0.3em}.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}"Henry Tabor's 1916 War Diary".
By May 1916, 306 aircraft and 542 ground stations were equipped with wireless.
An unusual mission for the RFC was the delivery of spies behind enemy lines.  The first mission took place on the morning of 13 September 1915 and was not a success.  The plane crashed, the pilot and spy were badly injured and they were both captured (two years later the pilot, Captain T.W. Mulcahy-Morgan escaped and returned to England).  Later missions were more successful.  In addition to delivering the spies the RFC was also responsible for keeping them supplied with the carrier pigeons that were used to send reports back to base.  In 1916 a Special Duty Flight was formed as part of the Headquarters Wing to handle these and other unusual assignments.
The obvious potential for aerial bombardment of the enemy was not lost on the RFC, and despite the poor payload of early war aircraft, bombing missions were undertaken. Front line squadrons (at the prompting of the more inventive pilots) devised several methods of carrying, aiming and dropping bombs. Lieutenant Conran of No 3 Squadron attacked an enemy troop column by dropping hand grenades over the side of his cockpit; the noise of the grenades caused the horses to stampede. At No 6 Squadron, Captain Louis Strange managed to destroy two canvas-covered trucks with home-made petrol bombs.
In March 1915 a bombing raid was flown, with Captain Strange flying a modified Royal Aircraft Factory B.E.2c, to carry four 20 lb Cooper bombs on wing racks released by pulling a cable fitted in the cockpit. Attacking Courtrai railway station. Strange approached from low level and hit a troop train causing 75 casualties. The same day Captain Carmichael of No 5 Squadron dropped a 100 lb bomb from a Martinsyde S1 on the railway junction at Menin. Days later, Lieutenant William Barnard Rhodes-Moorhouse of No 2 Squadron was posthumously awarded the Victoria Cross after bombing Courtrai station in a BE2c.
In October 1917 No 41 Wing was formed to attack strategic targets in Germany. Consisting of No 55 Squadron (Airco DH.4), No 100 (Royal Aircraft Factory F.E.2b) and No 16 (Naval) Squadron (Handley Page 0/100) the wing was based at Ochey commanded by Lt Colonel Cyril Newall. Its first attack was on Saarbrücken on 17 October with 11 DH-4s and a week later nine Handley Page O/100s carried out a night attack against factories in Saarbrücken, while 16 F.E.2bs bombed railways nearby. Four aircraft failed to return. The wing was expanded with the later addition of Nos 99 and 104 Squadrons, both flying the DH-4 into the Independent Air Force.
Aircraft were increasingly engaged in ground attack operations as the war wore on, aimed at disrupting enemy forces at or near the front line and during offensives. While formal tactical bombing raids were planned and usually directed at specific targets, ground-attack was usually carried out by individual pilots or small flights against targets of opportunity. Although the fitted machine guns were the primary armament for ground attack, bomb racks holding 20 lb Cooper bombs were soon fitted to many single-seat aircraft. Ground attack sorties were carried out at very low altitude and were often highly effective, in spite of the primitive nature of the weaponry involved, compared with later conflicts. The moral effect on ground troops subjected to air attack could even be decisive. Such operations became increasingly hazardous for the attacking aircraft, as one hit from small arms fire could bring an aircraft down and troops learned deflection shooting to hit relatively slow moving enemy aeroplanes.
During the Battle of Messines in June 1917, Trenchard ordered the British crews to fly low over the lines and strafe all available targets. Techniques for Army and RFC co-operation quickly evolved and improved and during the Third Battle of Ypres over 300 aircraft from 14 RFC squadrons, including the Sopwith Camel, armed with four 9 kg (20 lb) bombs, constantly raided enemy trenches, troop concentrations, artillery positions and strongholds in co-operation with tanks and infantry.
The cost to the RFC was high, with a loss rate of ground attack aircraft approaching 30 per cent. The first British production armoured type, the Sopwith Salamander, did not see service during the First World War.
In the UK the RFC Home Establishment was not only responsible for training air and ground crews and preparing squadrons to deploy to France, but providing squadrons for home defence, countering the German Zeppelin raids and later Gotha raids. The RFC (and the Royal Naval Air Service) initially had limited success against the German raids, largely through the problem of locating the attackers and having aircraft of sufficient performance to reach the operating altitude of the German raiders.
With the bulk of the operational squadrons engaged in France few could be spared for home defence in the UK. Therefore, training squadrons were called on to supply home defence aircraft and aircrews for the duration of the war. Night flying and defence missions were often flown by instructors in aircraft deemed worn-out and often obsolete for front-line service, although the pilots selected as instructors were often among the most experienced in the RFC.
The RFC officially took over the role of Home Defence in December 1915 and at that time had 10 permanent airfields. By December 1916 there were 11 RFC home defence squadrons:
As the war moved into the period of the mobile warfare commonly called the Race to the Sea, the Corps moved forward again.  On 8 October 1914 the RFC arrived in Saint-Omer and a headquarters was established at the aerodrome next to the local race course.  Over the next few days the four squadrons arrived and for the next four years Saint-Omer was a focal point for all RFC operations in the field.  Although most squadrons only used Saint-Omer as a transit camp before moving on to other locations, the base grew in importance as it increased its logistic support to the RFC.
Hugh Trenchard was the commander of the Royal Flying Corps in France from August 1915 until January 1918.  Trenchard's time in command was characterised by three priorities.  First was his emphasis on support to and co-ordination with ground forces.  This support started with reconnaissance and artillery co-ordination and later encompassed tactical low-level bombing of enemy ground forces.  While Trenchard did not oppose the strategic bombing of Germany in principle, he opposed moves to divert his forces on to long-range bombing missions as he believed the strategic role to be less important and his resource to be too limited.  Secondly, he stressed the importance of morale, not only of his own airmen, but more generally the detrimental effect that the presence of an aircraft had upon the morale of opposing ground troops.  Finally, Trenchard had an unswerving belief in the importance of offensive action.  Although this belief was widely held by senior British commanders, the RFC's offensive posture resulted in the loss of many men and machines and some doubted its effectiveness.
Before the Battle of the Somme the RFC mustered 421 aircraft, with 4 kite-balloon squadrons and 14 balloons. These made up four brigades, which worked with the four British armies. By the end of the Somme offensive in November 1916, the RFC had lost 800 aircraft and 252 aircrew killed (all causes) since July 1916, with 292 tons of bombs dropped and 19,000 Recce photographs taken.
As 1917 dawned the Allied Air Forces felt the effect of the German Air Force's increasing superiority in both organisation and equipment (if not numbers). The recently formed Jastas, equipped with the Albatros fighter, inflicted very heavy losses on the RFC's obsolescent aircraft, culminating in Bloody April, the nadir of the RFC's fortunes in World War I.
To support the Battle of Arras beginning on 9 April 1917, the RFC deployed 25 squadrons, totalling 365 aircraft, a third of which were fighters (scouts). The British lost 245 aircraft with 211 aircrew killed or missing &amp; 108 as prisoners of war. The German Air Services lost just 66 aircraft from all causes.
By the summer of 1917, the introduction of the next generation of technically advanced combat aircraft (such as the SE5, Sopwith Camel and Bristol Fighter) ensured losses fell and damage inflicted on the enemy increased.
Close support and battlefield co-operation tactics with the British Army were further developed by November 1917, when low-flying fighter aircraft co-operated highly effectively with advancing columns of tanks and infantry during the Battle of Cambrai.
1917 saw 2,094 RFC aircrew killed in action or missing.
The disastrous defeat of the Italian Army by Austro-Hungarian and German forces in the Battle of Caporetto led to the transfer of 3 RFC Sopwith Camel fighter squadrons (28, 45 and 66), two two-seater squadrons (34 and 42, with RE8s) and No. 4 Balloon Wing to the Italian Front in November 1917. No. 139 Squadron (Bristol Fighters) were added in July 1918.
RFC Squadrons were also deployed to the Middle East and the Balkans. In July 1916 the Middle-East Brigade of the RFC was formed under the command of Brigadier General W G H Salmond, concentrating RFC units based in Macedonia, Mesopotamia, Palestine and East Africa under one unified command.
In the Middle East units had to make do with older, often obsolete equipment before being given more modern aircraft. The Palestine Brigade of the RFC was formed in October 1917 to support General Allenby's ground offensive against the Ottomans in Palestine.
Despite their relatively small numbers the RFC gave valuable assistance to the Army in the eventual defeat of Ottoman forces in Palestine, Trans Jordan and Mesopotamia (Iraq).
The German Offensive in March 1918 was an all-out effort to win the war before the German economy collapsed from the pressures exerted on it by the Royal Navy's blockade and the strains of war In the weeks following the launch of the attack, RFC crews flew unceasingly, with all types of aircraft bombing and strafing ground forces, often from extremely low level, meantime also bringing back vital reports of the fluid ground fighting.
The RFC contributed significantly to slowing the German advance and ensuring the controlled retreat of the Allied Armies did not turn into a rout. The battle reached its peak on 12 April, when the newly formed RAF dropped more bombs, and flew more missions than any other day during the war. The cost of halting the German advance was high however, with over 400 aircrew killed and 1000 aircraft lost to enemy action.
On 17 August 1917, General Jan Smuts presented a report to the War Council on the future of air power.  Because of its potential for the 'devastation of enemy lands and the destruction of industrial and populous centres on a vast scale', he recommended a new air service be formed that would be on a level with the Army and Royal Navy. Pilots were seconded to the RFC from other regiments and could return when they were no longer able to fly but in a separate service this would be impossible. The formation of the new service would make underused RNAS resources available for the Western Front, as well as ending the inter-service rivalry that at times had adversely affected aircraft procurement. On 1 April 1918, the RFC and the RNAS were amalgamated to form the Royal Air Force, under the control of a new Air Ministry. After starting in 1914 with some 2,073 personnel by the start of 1919 the RAF had 4,000 combat aircraft and 114,000 personnel.
Many pilots were initially seconded to the RFC from their original regiments by becoming an observer. Some RFC ground crew (often NCO's or below) also volunteered for these flying duties as they then received supplementary flying pay. There was no formal training for observers until 1917 and many were sent on their first sortie with only a brief introduction to the aircraft from the pilot.  Once certified as fully qualified the observer was awarded the coveted half-wing brevet.  Once awarded this could not be forfeited so it essentially amounted to a decoration.  Originally in the RFC, as in most early air forces, the observer was nominally in command of the aircraft with the pilot having the role of a "chauffeur".  In practice, this was reversed at an early stage in the RFC, so that the pilot normally commanded the aircraft.  Most operational two seaters of the period did not have dual controls (an exception was the F.K. 8), so that the death or incapacity of the pilot normally meant an inevitable crash – but nonetheless many observers gained at least rudimentary piloting skills, and it was very common for experienced observers to be selected for pilot training.
Applicants for aircrew generally entered the RFC as a cadet via the depot pool for basic training. The cadet would then generally pass on to the School of Military Aeronautics at either Reading or Oxford. Following this period of theoretical learning the cadet was posted to a Training Squadron, either in the UK or overseas.
Colonel Robert Smith-Barry, a former CO of 60 Squadron, appalled at the poor standard of newly trained pilots and high fatality rate during training in 1915–16, formulated a comprehensive curriculum for pilot training, and with the agreement of Trenchard, returned to the UK to implement his training ethos at Gosport in 1917. The immediate effect was to halve fatalities in training.
The curriculum was based on a combination of classroom theory and dual flight instruction.  Students were not to be discouraged from potentially dangerous manoeuvres but were exposed to them in a controlled environment so that the student could learn to safely rectify errors of judgement.
Dual flying training usually weeded out those not suitable for flying training (approximately 45% of the initial class intake) before the remaining cadets were taught in the air by an instructor ( initially a 'tour-expired' pilot sent for a rest from an operational squadron in France, without any specific training on how to instruct). After flying 10 to 20 hours dual instruction, the pupil would be ready to 'go solo'.
In May 1916 pilots under instruction were further trained for fighting in the air. Schools of Special Flying were set up at Turnberry, Marske, Sedgeforth, Feiston, East Fortune and Ayr, where finished pilots could simulate combat flying under the supervision of veteran instructors.
During 1917 experienced pilots were redeployed from the Sinai and Palestine campaign to set up a new flying school and train pilots in Egypt and staff another in Australia. Seven Training Squadrons were eventually established in Egypt at five Training Depot Stations.
In 1917, the American, British, and Canadian Governments agreed to join forces for training. Between April 1917 and January 1919, Camp Borden in Ontario hosted instruction on flying, wireless, air gunnery and photography, training 1,812 RFC Canada pilots and 72 for the United States. Training also took place at several other Ontario locations.
During winter 1917–18, RFC instructors trained with the Aviation Section, US Signal Corps on three airfields in the United States accommodating about six thousand men, at Camp Taliaferro near Fort Worth, Texas. Training was hazardous; 39 RFC officers and cadets died in Texas. Eleven remain there, reinterred in 1924 at a Commonwealth War Graves Commission cemetery where a monument honours their sacrifice.
As the war drew on the RFC increasingly drew on men from across the British Empire including South Africa, Canada and Australia. As well as individual personnel, the separate Australian Flying Corps (AFC) deployed Nos 1, 2, 3 and 4 Squadrons AFC (which the RFC referred to as 67, 68, 69 and 71 Squadrons). Over 200 Americans joined the RFC before the United States became a combatant. Eventually Canadians made up nearly a third of RFC aircrew.
Although as the war progressed and training became far safer, by the end of the war, some 8,000 had been killed while training or in flying accidents.
Parachuting from balloons and aircraft, with very few accidents, had been a popular "stunt" for several years before the war. In 1915 inventor Everard Calthrop offered the RFC his patented parachute. On 13 January 1917, Captain Clive Collett, a New Zealander, made the first British military parachute jump from a heavier-than-air craft.  The jump, from 600 feet, was successful but although parachutes were issued to the crews of observation balloons, the higher authorities in the RFC and the Air Board were opposed to the issuing of parachutes to pilots of heavier-than-air craft.  It was felt at the time that a parachute might tempt a pilot to abandon his aircraft in an emergency rather than continuing the fight. The parachutes of the time were also heavy and cumbersome, and the added weight was frowned upon by some experienced pilots as it adversely affected aircraft with already marginal performance. It was not until 16 September 1918 that an order was issued for all single-seater aircraft to be fitted with parachutes, and this did not eventuate until after the war.
At the end of the war there were 5,182 pilots in service (constituting 2% of total RAF personnel). In comparison, the casualties from the RFC/RNAS/RAF for 1914–18 totalled 9,378 killed or missing, with 7,245 wounded. Some 900,000 flying hours on operations were logged, and 6,942 tons of bombs dropped. The RFC claimed some 7,054 German aircraft and balloons either destroyed, sent 'down out of control' or 'driven down'.
Eleven RFC members received the Victoria Cross during the First World War.  Initially the RFC did not believe in publicising the victory totals and exploits of their aces.  Eventually, however, public interest and the newspapers' demand for heroes led to this policy being abandoned, with the feats of aces such as Captain Albert Ball raising morale in the service as well as on the "home front". More than 1000 airmen are considered as "aces" (see List of World War I flying aces from the British Empire). However, the British criteria for confirming air victories were much lower compared to those from Germany or France and do not meet modern standards (see Aerial victory standards of World War I).
For a short time after the formation of the RAF, pre-RAF ranks such as Lieutenant, Captain and Major continued to exist, a practice which officially ended on 15 September 1919. For this reason some early RAF memorials and gravestones show ranks which no longer exist in the modern RAF. A typical example is James McCudden's grave, though there are many others.
The following had command of the RFC in the Field:
The following served as chief of staff for the RFC in the Field:


Allied victory
World War I or the First World War, often abbreviated as WWI or WW1, began on 28 July 1914 and ended on 11 November 1918. Referred to by contemporaries as the "Great War", its belligerents included much of Europe, the Russian Empire, the United States, and the Ottoman Empire, with fighting also expanding into the Middle East, Africa, and parts of Asia. One of the deadliest conflicts in history, an estimated 9 million people were killed in combat, while over 5 million civilians died from military occupation, bombardment, hunger, and disease. Millions of additional deaths resulted from genocides within the Ottoman Empire and the 1918 influenza pandemic, which was exacerbated by the movement of combatants during the war.
By 1914, the European great powers were divided into the Triple Entente of France, Russia, and Britain; and the Triple Alliance of Germany, Austria-Hungary, and Italy. Tensions in the Balkans came to a head on 28 June 1914 following the assassination of Archduke Franz Ferdinand, the Austro-Hungarian heir, by Gavrilo Princip, a Bosnian Serb. Austria-Hungary blamed Serbia, which led to the July Crisis, an unsuccessful attempt to avoid conflict through diplomacy. Russia came to Serbia's defense following Austria-Hungary's declaration of war on the latter on 28 July, and by 4 August, the system of alliances drew in Germany, France, and Britain, along with their respective colonies. In November, the Ottoman Empire, Germany, and Austria-Hungary formed the Central Powers, while in April 1915, Italy switched sides to join Britain, France, Russia, and Serbia in forming the Allies of World War I.
Facing a war on two fronts, German strategy in 1914 was first to defeat France, then shift its forces to Eastern Europe and knock out Russia in what was known as the Schlieffen Plan. However, Germany's advance into France failed, and by the end of 1914, the two sides faced each other along the Western Front, a continuous series of trench lines stretching from the English Channel to Switzerland that changed little until 1917. By contrast, the Eastern Front was far more fluid, with Austria-Hungary and Russia gaining and then losing large swathes of territory. Other significant theatres included the Middle Eastern Theatre, the Italian Front, and the Balkans Theatre, drawing Bulgaria, Romania, and Greece into the war.
By early 1915 Russia had been seeing defeat after defeat in the twin Battle of Tannenberg and the Battle of the Masurian Lakes. The Russians had suffered around 450,000 casualties in all of those battles, by then their armies were demoralized and the Germans had sent the bulk of their armies towards the Eastern Front. The siege of Przemyśl had been a success for the Russians but by April the Germans had begun drawing up plans to liberate Galicia. By May the Germans had launched the Gorlice–Tarnów offensive, an offensive which eventually turned into a Russian retreat. By the 5th of August, Warsaw had been occupied by the Germans. The battle finally ended in September 1915 with the entirety of Poland and parts of Minsk being occupied.
Shortages caused by the Allied naval blockade led Germany to initiate unrestricted submarine warfare in early 1917, bringing the previously-neutral United States into the war on 6 April 1917. In Russia, the Bolsheviks seized power in the October Revolution of 1917, and made peace in the Treaty of Brest-Litovsk on 3 March 1918, freeing up a large number of German troops. By transferring these forces to the Western Front, the German General Staff hoped to win a decisive victory before American reinforcements could impact the war, and launched the German spring offensive in March 1918. Despite initial success, it was soon halted by heavy casualties and ferocious defence; in August, the Allies launched the Hundred Days Offensive and although the Imperial German Army continued to fight hard, it could no longer halt their advance.
Towards the end of 1918, the Central Powers began to collapse; Bulgaria signed an armistice on 29 September, followed by the Ottomans on 31 October, then Austria-Hungary on 3 November. Isolated, facing the German Revolution at home and a military on the verge of mutiny, Kaiser Wilhelm abdicated on 9 November, and the new German government signed the Armistice of 11 November 1918, bringing the conflict to a close. The Paris Peace Conference of 1919–1920 imposed various settlements on the defeated powers, with the best-known of these being the Treaty of Versailles. The dissolution of the Russian, German, Ottoman, and Austro-Hungarian empires led to numerous uprisings and the creation of independent states, including Poland, Czechoslovakia, and Yugoslavia. For reasons that are still debated, failure to manage the instability that resulted from this upheaval during the interwar period ended with the outbreak of World War II in September 1939.
The term world war was first coined in September 1914 by German biologist and philosopher Ernst Haeckel. He claimed that "there is no doubt that the course and character of the feared 'European War' ... will become the first world war in the full sense of the word," in The Indianapolis Star on 20 September 1914.
The term "First World War" had been used by Lt-Col. Charles à Court Repington, as a title for his memoirs (published in 1920); he had noted his discussion on the matter with a Major Johnstone of Harvard University in his diary entry of 10 September 1918.
Prior to World War II, the events of 1914–1918 were generally known as the Great War or simply the World War. In August 1914, The Independent magazine wrote "This is the Great War. It names itself". In October 1914, the Canadian magazine Maclean's similarly wrote, "Some wars name themselves. This is the Great War." Contemporary Europeans also referred to it as "the war to end war" and it was also described as "the war to end all wars" due to their perception of its then-unparalleled scale, devastation, and loss of life. After World War II began in 1939, the terms became more standard, with British Empire historians, including Canadians, favouring "The First World War" and Americans "World War I".
For much of the 19th century, the major European powers maintained a tenuous balance of power among themselves, known as the Concert of Europe. After 1848, this was challenged by a variety of factors, including Britain's withdrawal into so-called splendid isolation, the decline of the Ottoman Empire and the rise of Prussia under Otto von Bismarck. The 1866 Austro-Prussian War established Prussian hegemony in Germany, while victory in the 1870–1871 Franco-Prussian War allowed Bismarck to consolidate the German states into a German Empire under Prussian leadership. Avenging the defeat of 1871, or revanchism, and recovering the provinces of Alsace-Lorraine became the principal objects of French policy for the next forty years.
In order to isolate France and avoid a war on two fronts, Bismarck negotiated the League of the Three Emperors (German: Dreikaiserbund) between Austria-Hungary, Russia and Germany. After Russian victory in the 1877–1878 Russo-Turkish War, the League was dissolved due to Austrian concerns over Russian influence in the Balkans, an area they considered of vital strategic interest. Germany and Austria-Hungary then formed the 1879 Dual Alliance, which became the Triple Alliance when Italy joined in 1882. For Bismarck, the purpose of these agreements was to isolate France by ensuring the three Empires resolved any disputes between themselves; when this was threatened in 1880 by British and French attempts to negotiate directly with Russia, he reformed the League in 1881, which was renewed in 1883 and 1885. After the agreement lapsed in 1887, he replaced it with the Reinsurance Treaty, a secret agreement between Germany and Russia to remain neutral if either were attacked by France or Austria-Hungary.
Bismarck viewed peace with Russia as the foundation of German foreign policy but after becoming Kaiser in 1890, Wilhelm II forced him to retire and was persuaded not to renew the Reinsurance Treaty by Leo von Caprivi, his new Chancellor. This provided France an opportunity to counteract the Triple Alliance, by signing the Franco-Russian Alliance in 1894, followed by the 1904 Entente Cordiale with Britain, and the Triple Entente was completed by the 1907 Anglo-Russian Convention. While these were not formal alliances, by settling long-standing colonial disputes in Africa and Asia, British entry into any future conflict involving France or Russia became a possibility. British and Russian support for France against Germany during the Agadir Crisis in 1911 reinforced their relationship and increased Anglo-German estrangement, deepening the divisions that would erupt in 1914.
After 1871, the creation of a unified Reich, supported by French indemnity payments and the annexation of Alsace-Lorraine, led to a huge increase in German industrial strength. Backed by Wilhelm II, Admiral Alfred von Tirpitz sought to exploit this to build a Kaiserliche Marine, or Imperial German Navy, able to compete with the British Royal Navy for world naval supremacy. He was greatly influenced by US naval strategist Alfred Thayer Mahan, who argued possession of a blue-water navy was vital for global power projection; Tirpitz had his books translated into German, while Wilhelm made them required reading for his advisors and senior military personnel.
However, it was also an emotional decision, driven by Wilhelm's simultaneous admiration for the Royal Navy and desire to outdo it. Bismarck calculated Britain would not interfere in Europe so long as its maritime supremacy remained secure but his dismissal in 1890 led to a change in policy and an Anglo-German naval arms race. Despite the vast sums spent by Tirpitz, the launch of HMS Dreadnought in 1906 gave the British a technological advantage over their German rival which they never lost. Ultimately, the race diverted huge resources to creating a German navy large enough to antagonise Britain, but not defeat it; in 1911, Chancellor Theobald von Bethmann Hollweg acknowledged defeat, leading to the Rüstungswende or 'armaments turning point', when he switched expenditure from the navy to the army.
This was driven by concern over Russia's recovery from defeat in the 1905 Russo-Japanese War and the subsequent revolution. Economic reforms backed by French funding led to a significant post-1908 expansion of railways and infrastructure, particularly in its western border regions. Germany and Austria-Hungary relied on faster mobilisation to compensate for fewer numbers and it was the potential threat posed by the closing of this gap that led to the end of the naval race, rather than a reduction in tensions. When Germany expanded its standing army by 170,000 men in 1913, France extended compulsory military service from two to three years; similar measures taken by the Balkan powers and Italy, which led to increased expenditure by the Ottomans and Austria-Hungary. Absolute figures are hard to calculate due to differences in categorising expenditure, since they often omit civilian infrastructure projects with a military use, such as railways. However, from 1908 to 1913, defence spending by the six major European powers increased by over 50% in real terms.
The years before 1914 were marked by a series of crises in the Balkans as other powers sought to benefit from Ottoman decline. While Pan-Slavic and Orthodox Russia considered itself the protector of Serbia and other Slav states, they preferred the strategically vital Bosporus straits be controlled by a weak Ottoman government, rather than an ambitious Slav power like Bulgaria. Since Russia had its own ambitions in Eastern Turkey and their clients had over-lapping claims in the Balkans, balancing them divided Russian policy makers and added to regional instability.
Austrian statesmen viewed the Balkans as essential for the continued existence of their Empire and Serbian expansion as a direct threat. The 1908–1909 Bosnian Crisis began when Austria annexed the former Ottoman territory of Bosnia and Herzegovina, which it had occupied since 1878. Timed to coincide with the Bulgarian Declaration of Independence from the Ottoman Empire, this unilateral action was denounced by the European powers but accepted as there was no consensus on how to reverse it. Some historians see this as a significant escalation, ending any chance of Austria co-operating with Russia in the Balkans while damaging relations with Serbia and Italy, both of whom had their own expansionist ambitions in the area.
Tensions increased after the 1911 to 1912 Italo-Turkish War demonstrated Ottoman weakness and led to the formation of the Balkan League, an alliance of Serbia, Bulgaria, Montenegro, and Greece. The League quickly over-ran most of European Turkey in the 1912 to 1913 First Balkan War, much to the surprise of outside observers. The Serbian capture of ports on the Adriatic resulted in partial Austrian mobilisation on 21 November 1912, including units along the Russian border in Galicia. In a meeting the next day, the Russian government decided not to mobilise in response, unwilling to precipitate a war for which they were not yet prepared.
The Great Powers sought to re-assert control through the 1913 Treaty of London, which created an independent Albania, while enlarging the territories of Bulgaria, Serbia, Montenegro and Greece. However, disputes between the victors sparked the 33-day Second Balkan War, when Bulgaria attacked Serbia and Greece on 16 June 1913; it was defeated, losing most of Macedonia to Serbia and Greece, and Southern Dobruja to Romania. The result was that even countries which benefited from the Balkan Wars, such as Serbia and Greece, felt cheated of their "rightful gains", while for Austria it demonstrated the apparent indifference with which other powers viewed their concerns, including Germany. This complex mix of resentment, nationalism and insecurity helps explain why the pre-1914 Balkans became known as the "powder keg of Europe".
On 28 June 1914, Archduke Franz Ferdinand of Austria, heir presumptive to Emperor Franz Joseph, visited Sarajevo, capital of the recently annexed provinces of Bosnia and Herzegovina. Six assassins  from the movement known as Young Bosnia, or Mlada Bosna, took up positions along the route taken by the Archduke's motorcade, with the intention of assassinating him. Supplied with arms by extremists within the Serbian Black Hand intelligence organisation, they hoped his death would free Bosnia from Austrian rule, although there was little agreement on what would replace it.
Nedeljko Čabrinović threw a grenade at the Archduke's car and injured two of his aides, who were taken to hospital while the convoy carried on. The other assassins were also unsuccessful but an hour later, as Ferdinand was returning from visiting the injured officers, his car took a wrong turn into a street where Gavrilo Princip was standing. He stepped forward and fired two pistol shots, fatally wounding Ferdinand and his wife Sophie, who both died shortly thereafter. Although Emperor Franz Joseph was shocked by the incident, political and personal differences meant the two men were not close; allegedly, his first reported comment was "A higher power has re-established the order which I, alas, could not preserve".
According to historian Zbyněk Zeman, his reaction was reflected more broadly in Vienna, where "the event almost failed to make any impression whatsoever. On Sunday 28 June and Monday 29th, the crowds listened to music and drank wine, as if nothing had happened." Nevertheless, the impact of the murder of the heir to the throne was significant, and has been described by historian Christopher Clark as a "9/11 effect, a terrorist event charged with historic meaning, transforming the political chemistry in Vienna".
The Austro-Hungarian authorities encouraged the subsequent anti-Serb riots in Sarajevo, in which Bosnian Croats and Bosniaks killed two Bosnian Serbs and damaged numerous Serb-owned buildings. Violent actions against ethnic Serbs were also organised outside Sarajevo, in other cities in Austro-Hungarian-controlled Bosnia and Herzegovina, Croatia and Slovenia. Austro-Hungarian authorities in Bosnia and Herzegovina imprisoned and extradited approximately 5,500 prominent Serbs, 700 to 2,200 of whom died in prison. A further 460 Serbs were sentenced to death. A predominantly Bosniak special militia known as the Schutzkorps was established and carried out the persecution of Serbs.
The assassination initiated the July Crisis, a month of diplomatic manoeuvring between Austria-Hungary, Germany, Russia, France and Britain. Believing Serbian intelligence helped organise Franz Ferdinand's murder, Austrian officials wanted to use the opportunity to end their interference in Bosnia and saw war as the best way of achieving this. However, the Foreign Ministry had no solid proof of Serbian involvement and a dossier used to make its case was riddled with errors. On 23 July, Austria delivered an ultimatum to Serbia, listing ten demands made intentionally unacceptable to provide an excuse for starting hostilities.
Serbia ordered general mobilisation on 25 July, but accepted all the terms, except for those empowering Austrian representatives to suppress "subversive elements" inside Serbia, and take part in the investigation and trial of Serbians linked to the assassination. Claiming this amounted to rejection, Austria broke off diplomatic relations and ordered partial mobilisation the next day; on 28 July, they declared war on Serbia and began shelling Belgrade. Having initiated war preparations on 25 July, Russia now ordered general mobilisation in support of Serbia on 30th.
Anxious to ensure backing from the SDP political opposition by presenting Russia as the aggressor, Bethmann-Hollweg delayed commencement of war preparations until 31 July. That afternoon the Russian government were handed a note requiring them to "cease all war measures against Germany and Austria-Hungary" within 12 hours. A further German demand for neutrality was refused by the French who ordered general mobilisation but delayed declaring war. The German General Staff had long assumed they faced a war on two fronts; the Schlieffen Plan envisaged using 80% of the army to defeat France in the west, then switch to Russia. Since this required them to move quickly, mobilisation orders were issued that afternoon.
At a meeting on 29 July, the British cabinet had narrowly decided its obligations to Belgium under the 1839 Treaty of London did not require it to oppose a German invasion with military force. However, this was largely driven by Prime Minister Asquith's desire to maintain unity; he and his senior Cabinet ministers were already committed to support France, the Royal Navy had been mobilised and public opinion was strongly in favour of intervention. On 31 July, Britain sent notes to Germany and France, asking them to respect Belgian neutrality; France pledged to do so, Germany did not reply.
Once the German ultimatum to Russia expired on the morning of 1 August, the two countries were at war. Later the same day, Wilhelm was informed by his Ambassador in London, Prince Lichnowsky, that Britain would remain neutral if France was not attacked, and in any case might be stayed by a crisis in Ireland. Jubilant at this news, he ordered General Moltke, the German chief of staff, to "march the whole of the ... army to the East". Moltke protested that "it cannot be done. The deployment of millions cannot be improvised." Lichnowsky, in any case, quickly realised he was mistaken. Although Wilhelm insisted on waiting for a telegram from his cousin George V, once received, it confirmed there had been a misunderstanding and he told Moltke "Now do what you want."
Aware of German plans to attack through Belgium, French Commander-in-Chief Joseph Joffre asked his government for permission to cross the border and pre-empt such a move. To avoid a violation of Belgian neutrality, he was told any advance could come only after a German invasion. On 2 August, Germany occupied Luxembourg and exchanged fire with French units; on 3 August, they declared war on France and demanded free passage across Belgium, which was refused. Early on the morning of 4 August, the Germans invaded and Albert I of Belgium called for assistance under the Treaty of London. Britain sent Germany an ultimatum demanding they withdraw from Belgium; when this expired at midnight without a response, the two empires were at war.
The strategy of the Central Powers suffered from miscommunication. Germany had promised to support Austria-Hungary's invasion of Serbia, but interpretations of what this meant differed. Previously tested deployment plans had been replaced early in 1914, but those had never been tested in exercises. Austro-Hungarian leaders believed Germany would cover its northern flank against Russia. Germany, however, envisioned Austria-Hungary directing most of its troops against Russia, while Germany dealt with France. This confusion forced the Austro-Hungarian Army to divide its forces between the Russian and Serbian fronts.
Beginning on 12 August, the Austrian and Serbs clashed at the battles of the Cer and Kolubara; over the next two weeks, Austrian attacks were repulsed with heavy losses, dashing their hopes of a swift victory and marking the first major Allied victories of the war. As a result, Austria had to keep sizeable forces on the Serbian front, weakening its efforts against Russia. Serbia's defeat of the 1914 invasion has been called one of the major upset victories of the twentieth century. In spring 1915, the campaign saw the first use of anti-aircraft warfare after an Austrian plane was shot down with ground-to-air fire, as well as the first medical evacuation by the Serbian army in autumn 1915.
Upon mobilisation in 1914, 80% of the German Army was located on the Western Front, with the remainder acting as a screening force in the East; officially titled Aufmarsch II West, it is better known as the Schlieffen Plan after its creator, Alfred von Schlieffen, head of the German General Staff from 1891 to 1906. Rather than a direct attack across their shared frontier, the German right wing would sweep through the Netherlands and Belgium, then swing south, encircling Paris and trapping the French army against the Swiss border. Schlieffen estimated this would take six weeks, after which the German army would transfer to the East and defeat the Russians.
The plan was substantially modified by his successor, Helmuth von Moltke the Younger. Under Schlieffen, 85% of German forces in the west were assigned to the right wing, with the remainder holding along the frontier. By keeping his left wing deliberately weak, he hoped to lure the French into an offensive into the "lost provinces" of Alsace-Lorraine, which was in fact the strategy envisaged by their Plan XVII. However, Moltke grew concerned the French might push too hard on his left flank and as the German Army increased in size from 1908 to 1914, he changed the allocation of forces between the two wings from 85:15 to 70:30. He also considered Dutch neutrality essential for German trade and cancelled the incursion into the Netherlands, which meant any delays in Belgium threatened the entire viability of the plan. Historian Richard Holmes argues these changes meant the right wing was not strong enough to achieve decisive success and thus led to unrealistic goals and timings.
The initial German advance in the West was very successful and by the end of August the Allied left, which included the British Expeditionary Force, or "BEF", was in full retreat. At the same time, the French offensive in Alsace-Lorraine was a disastrous failure, with casualties exceeding 260,000, including 27,000 killed on 22 August during the Battle of the Frontiers. German planning provided broad strategic instructions, while allowing army commanders considerable freedom in carrying them out at the front; this worked well in 1866 and 1870 but in 1914, von Kluck used this freedom to disobey orders, opening a gap between the German armies as they closed on Paris. The French and British exploited this gap to halt the German advance east of Paris at the First Battle of the Marne from 5 to 12 September and push the German forces back some 50 km (31 mi).
In 1911, the Russian Stavka had agreed with the French to attack Germany within fifteen days of mobilisation, ten days before the Germans had anticipated, although it meant the two Russian armies that entered East Prussia on 17 August did so without many of their support elements. Although the Russian Second Army was effectively destroyed at the Battle of Tannenberg on 26–30 August, their advance caused the Germans to re-route their 8th Field Army from France to East Prussia, a factor in Allied victory on the Marne.
By the end of 1914, German troops held strong defensive positions inside France, controlled the bulk of France's domestic coalfields and had inflicted 230,000 more casualties than it lost itself. However, communications problems and questionable command decisions cost Germany the chance of a decisive outcome, while it had failed to achieve the primary objective of avoiding a long, two-front war. As was apparent to a number of German leaders, this amounted to a strategic defeat; shortly after the Marne, Crown Prince Wilhelm told an American reporter; "We have lost the war. It will go on for a long time but lost it is already."
On 30 August 1914, New Zealand occupied German Samoa, now the independent state of Samoa. On 11 September, the Australian Naval and Military Expeditionary Force landed on the island of New Britain, then part of German New Guinea. On 28 October, the German cruiser SMS Emden  sank the Russian cruiser Zhemchug in the Battle of Penang. Japan declared war on Germany prior to seizing territories in the Pacific which later became the South Seas Mandate, as well as German Treaty ports on the Chinese Shandong peninsula at Tsingtao. After Vienna refused to withdraw its cruiser SMS Kaiserin Elisabeth from Tsingtao, Japan declared war on Austria-Hungary as well, and the ship was sunk at Tsingtao in November 1914. Within a few months, Allied forces had seized all German territories in the Pacific, leaving only isolated commerce raiders and a few holdouts in New Guinea.
Some of the first clashes of the war involved British, French, and German colonial forces in Africa. On 6–7 August, French and British troops invaded the German protectorate of Togoland and Kamerun. On 10 August, German forces in South-West Africa attacked South Africa; sporadic and fierce fighting continued for the rest of the war. The German colonial forces in German East Africa, led by Colonel Paul von Lettow-Vorbeck, fought a guerrilla warfare campaign during World War I and only surrendered two weeks after the armistice took effect in Europe.
Germany attempted to use Indian nationalism and pan-Islamism to its advantage, instigating uprisings in India, and sending a mission that urged Afghanistan to join the war on the side of Central Powers. However, contrary to British fears of a revolt in India, the outbreak of the war saw an unprecedented outpouring of loyalty and goodwill towards Britain. Indian political leaders from the Indian National Congress and other groups were eager to support the British war effort since they believed that strong support for the war effort would further the cause of Indian Home Rule. The Indian Army in fact outnumbered the British Army at the beginning of the war; about 1.3 million Indian soldiers and labourers served in Europe, Africa, and the Middle East, while the central government and the princely states sent large supplies of food, money, and ammunition. In all, 140,000 men served on the Western Front and nearly 700,000 in the Middle East. Casualties of Indian soldiers totalled 47,746 killed and 65,126 wounded during World War I.
The suffering engendered by the war, as well as the failure of the British government to grant self-government to India after the end of hostilities, bred disillusionment and fuelled the campaign for full independence that would be led by Mohandas K. Gandhi and others.
Pre-war military tactics that emphasised open warfare and the individual rifleman proved obsolete when confronted with conditions prevailing in 1914. Technological advances allowed the creation of strong defensive systems largely impervious to massed infantry advances, such as barbed wire, machine guns and above all far more powerful artillery, which dominated the battlefield and made crossing open ground extremely difficult. Both sides struggled to develop tactics for breaching entrenched positions without suffering heavy casualties. In time, however, technology began to produce new offensive weapons, such as gas warfare and the tank.
After the First Battle of the Marne in September 1914, Allied and German forces unsuccessfully tried to outflank each other, a series of manoeuvres later known as the "Race to the Sea". By the end of 1914, the opposing forces confronted each other along an uninterrupted line of entrenched positions from the Channel to the Swiss border. Since the Germans were normally able to choose where to stand, they generally held the high ground; in addition, their trenches tended to be better built, since Anglo-French trenches were initially intended as "temporary," and would only be needed until the breaking of German defences.
Both sides tried to break the stalemate using scientific and technological advances. On 22 April 1915, at the Second Battle of Ypres, the Germans (violating the Hague Convention) used chlorine gas for the first time on the Western Front. Several types of gas soon became widely used by both sides, and though it never proved a decisive, battle-winning weapon, poison gas became one of the most-feared and best-remembered horrors of the war.
Neither side proved able to deliver a decisive blow for the next two years. Throughout 1915–17, the British Empire and France suffered more casualties than Germany, because of both the strategic and tactical stances chosen by the sides. Strategically, while the Germans mounted only one major offensive, the Allies made several attempts to break through the German lines.
In February 1916 the Germans attacked French defensive positions at the Battle of Verdun, lasting until December 1916. The Germans made initial gains, before French counter-attacks returned matters to near their starting point. Casualties were greater for the French, but the Germans bled heavily as well, with anywhere from 700,000 to 975,000 casualties suffered between the two combatants. Verdun became a symbol of French determination and self-sacrifice.
The Battle of the Somme was an Anglo-French offensive of July to November 1916. The opening day of the offensive (1 July 1916) was the bloodiest day in the history of the British Army, suffering 57,470 casualties, including 19,240 dead. The entire Somme offensive cost the British Army some 420,000 casualties. The French suffered another estimated 200,000 casualties and the Germans an estimated 500,000. Gun fire was not the only factor taking lives; the diseases that emerged in the trenches were a major killer on both sides. The living conditions made it so that countless diseases and infections occurred, such as trench foot, shell shock, blindness/burns from mustard gas, lice, trench fever, "cooties" (body lice) and the 'Spanish flu'.
At the start of the war, the German Empire had cruisers scattered across the globe, some of which were subsequently used to attack Allied merchant shipping. The British Royal Navy systematically hunted them down, though not without some embarrassment from its inability to protect Allied shipping. Before the beginning of the war, it was widely understood that Britain held the position of strongest, most influential navy in the world. The publishing of the book The Influence of Sea Power upon History by Alfred Thayer Mahan in 1890 was intended to encourage the United States to increase its naval power. Instead, this book made it to Germany and inspired its readers to try to over-power the British Royal Navy. For example, the German detached light cruiser SMS Emden, part of the East Asia Squadron stationed at Qingdao, seized or destroyed 15 merchantmen, as well as sinking a Russian cruiser and a French destroyer. However, most of the German East-Asia squadron—consisting of the armoured cruisers SMS Scharnhorst and Gneisenau, light cruisers Nürnberg and Leipzig and two transport ships—did not have orders to raid shipping and was instead underway to Germany when it met British warships. The German flotilla and Dresden sank two armoured cruisers at the Battle of Coronel, but was virtually destroyed at the Battle of the Falkland Islands in December 1914, with only Dresden and a few auxiliaries escaping, but after the Battle of Más a Tierra these too had been destroyed or interned.
Soon after the outbreak of hostilities, Britain began a naval blockade of Germany. The strategy proved effective, cutting off vital military and civilian supplies, although this blockade violated accepted international law codified by several international agreements of the past two centuries. Britain mined international waters to prevent any ships from entering entire sections of ocean, causing danger to even neutral ships. Since there was limited response to this tactic of the British, Germany expected a similar response to its unrestricted submarine warfare.
The Battle of Jutland (German: Skagerrakschlacht, or "Battle of the Skagerrak") in May/June 1916 developed into the largest naval battle of the war. It was the only full-scale clash of battleships during the war, and one of the largest in history. The Kaiserliche Marine's High Seas Fleet, commanded by Vice Admiral Reinhard Scheer, fought the Royal Navy's Grand Fleet, led by Admiral Sir John Jellicoe. The engagement was a stand off, as the Germans were outmanoeuvred by the larger British fleet, but managed to escape and inflicted more damage to the British fleet than they received. Strategically, however, the British asserted their control of the sea, and the bulk of the German surface fleet remained confined to port for the duration of the war.
German U-boats attempted to cut the supply lines between North America and Britain. The nature of submarine warfare meant that attacks often came without warning, giving the crews of the merchant ships little hope of survival. The United States launched a protest, and Germany changed its rules of engagement. After the sinking of the passenger ship RMS Lusitania in 1915, Germany promised not to target passenger liners, while Britain armed its merchant ships, placing them beyond the protection of the "cruiser rules", which demanded warning and movement of crews to "a place of safety" (a standard that lifeboats did not meet). Finally, in early 1917, Germany adopted a policy of unrestricted submarine warfare, realising the Americans would eventually enter the war. Germany sought to strangle Allied sea lanes before the United States could transport a large army overseas, but after initial successes eventually failed to do so.
The U-boat threat lessened in 1917, when merchant ships began travelling in convoys, escorted by destroyers. This tactic made it difficult for U-boats to find targets, which significantly lessened losses; after the hydrophone and depth charges were introduced, accompanying destroyers could attack a submerged submarine with some hope of success. Convoys slowed the flow of supplies since ships had to wait as convoys were assembled. The solution to the delays was an extensive program of building new freighters. Troopships were too fast for the submarines and did not travel the North Atlantic in convoys. The U-boats had sunk more than 5,000 Allied ships, at a cost of 199 submarines.
World War I also saw the first use of aircraft carriers in combat, with HMS Furious launching Sopwith Camels in a successful raid against the Zeppelin hangars at Tondern in July 1918, as well as blimps for antisubmarine patrol.
Faced with Russia in the east, Austria-Hungary could spare only one-third of its army to attack Serbia. After suffering heavy losses, the Austrians briefly occupied the Serbian capital, Belgrade. A Serbian counter-attack in the Battle of Kolubara succeeded in driving them from the country by the end of 1914. For the first ten months of 1915, Austria-Hungary used most of its military reserves to fight Italy. German and Austro-Hungarian diplomats, however, scored a coup by persuading Bulgaria to join the attack on Serbia. The Austro-Hungarian provinces of Slovenia, Croatia and Bosnia provided troops for Austria-Hungary in the fight with Serbia, Russia and Italy. Montenegro allied itself with Serbia.
Bulgaria declared war on Serbia on 14 October 1915 and joined in the attack by the Austro-Hungarian army under Mackensen's army of 250,000 that was already underway. Serbia was conquered in a little more than a month, as the Central Powers, now including Bulgaria, sent in 600,000 troops total. The Serbian army, fighting on two fronts and facing certain defeat, retreated into northern Albania. The Serbs suffered defeat in the Battle of Kosovo. Montenegro covered the Serbian retreat towards the Adriatic coast in the Battle of Mojkovac in 6–7 January 1916, but ultimately the Austrians also conquered Montenegro. The surviving Serbian soldiers were evacuated by ship to Greece. After conquest, Serbia was divided between Austro-Hungary and Bulgaria.
In late 1915, a Franco-British force landed at Salonica in Greece to offer assistance and to pressure its government to declare war against the Central Powers. However, the pro-German King Constantine I dismissed the pro-Allied government of Eleftherios Venizelos before the Allied expeditionary force arrived. The friction between the King of Greece and the Allies continued to accumulate with the National Schism, which effectively divided Greece between regions still loyal to the king and the new provisional government of Venizelos in Salonica. After intense negotiations and an armed confrontation in Athens between Allied and royalist forces (an incident known as Noemvriana), the King of Greece resigned and his second son Alexander took his place; Greece officially joined the war on the side of the Allies in June 1917.
The Macedonian front was initially mostly static. French and Serbian forces retook limited areas of Macedonia by recapturing Bitola on 19 November 1916 following the costly Monastir offensive, which brought stabilisation of the front.
Serbian and French troops finally made a breakthrough in September 1918 in the Vardar offensive, after most of the German and Austro-Hungarian troops had been withdrawn. The Bulgarians were defeated at the Battle of Dobro Pole, and by 25 September British and French troops had crossed the border into Bulgaria proper as the Bulgarian army collapsed. Bulgaria capitulated four days later, on 29 September 1918. The German high command responded by despatching troops to hold the line, but these forces were far too weak to re-establish a front.
The disappearance of the Macedonian front meant that the road to Budapest and Vienna was now opened to Allied forces. Hindenburg and Ludendorff concluded that the strategic and operational balance had now shifted decidedly against the Central Powers and, a day after the Bulgarian collapse, insisted on an immediate peace settlement.
The Ottomans threatened Russia's Caucasian territories and Britain's communications with India via the Suez Canal. As the conflict progressed, the Ottoman Empire took advantage of the European powers' preoccupation with the war and conducted large-scale ethnic cleansing of the indigenous Armenian, Greek, and Assyrian Christian populations, known as the Armenian genocide, Greek genocide, and Assyrian genocide.
The British and French opened overseas fronts with the Gallipoli (1915) and Mesopotamian campaigns (1914). In Gallipoli, the Ottoman Empire successfully repelled the British, French, and Australian and New Zealand Army Corps (ANZACs). In Mesopotamia, by contrast, after the defeat of the British defenders in the siege of Kut by the Ottomans (1915–16), British Imperial forces reorganised and captured Baghdad in March 1917. The British were aided in Mesopotamia by local Arab and Assyrian tribesmen, while the Ottomans employed local Kurdish and Turcoman tribes.
Further to the west, the Suez Canal was defended from Ottoman attacks in 1915 and 1916; in August, a German and Ottoman force was defeated at the Battle of Romani by the ANZAC Mounted Division and the 52nd (Lowland) Infantry Division. Following this victory, an Egyptian Expeditionary Force advanced across the Sinai Peninsula, pushing Ottoman forces back in the Battle of Magdhaba in December and the Battle of Rafa on the border between the Egyptian Sinai and Ottoman Palestine in January 1917.
Russian armies generally had success in the Caucasus campaign. Enver Pasha, supreme commander of the Ottoman armed forces, was ambitious and dreamed of re-conquering central Asia and areas that had been lost to Russia previously. He was, however, a poor commander. He launched an offensive against the Russians in the Caucasus in December 1914 with 100,000 troops, insisting on a frontal attack against mountainous Russian positions in winter. He lost 86% of his force at the Battle of Sarikamish.
The Ottoman Empire, with German support, invaded Persia (modern Iran) in December 1914 in an effort to cut off British and Russian access to petroleum reservoirs around Baku near the Caspian Sea. Persia, ostensibly neutral, had long been under the spheres of British and Russian influence. The Ottomans and Germans were aided by Kurdish and Azeri forces, together with a large number of major Iranian tribes, such as the Qashqai, Tangistanis, Lurs, and Khamseh, while the Russians and British had the support of Armenian and Assyrian forces. The Persian campaign was to last until 1918 and end in failure for the Ottomans and their allies. However, the Russian withdrawal from the war in 1917 led to Armenian and Assyrian forces, who had hitherto inflicted a series of defeats upon the forces of the Ottomans and their allies, being cut off from supply lines, outnumbered, outgunned and isolated, forcing them to fight and flee towards British lines in northern Mesopotamia.
General Yudenich, the Russian commander from 1915 to 1916, drove the Turks out of most of the southern Caucasus with a string of victories. During the 1916 campaign, the Russians defeated the Turks in the Erzurum offensive, also occupying Trabzon. In 1917, Russian Grand Duke Nicholas assumed command of the Caucasus front. Nicholas planned a railway from Russian Georgia to the conquered territories so that fresh supplies could be brought up for a new offensive in 1917. However, in March 1917 (February in the pre-revolutionary Russian calendar), the Tsar abdicated in the course of the February Revolution, and the Russian Caucasus Army began to fall apart.
The Arab Revolt, instigated by the Arab bureau of the British Foreign Office, started June 1916 with the Battle of Mecca, led by Sharif Hussein of Mecca, and ended with the Ottoman surrender of Damascus. Fakhri Pasha, the Ottoman commander of Medina, resisted for more than two and half years during the siege of Medina before surrendering in January 1919.
The Senussi tribe, along the border of Italian Libya and British Egypt, incited and armed by the Turks, waged a small-scale guerrilla war against Allied troops. The British were forced to dispatch 12,000 troops to oppose them in the Senussi campaign. Their rebellion was finally crushed in mid-1916.
Total Allied casualties on the Ottoman fronts amounted 650,000 men. Total Ottoman casualties were 725,000 (325,000 dead and 400,000 wounded).
Although Italy joined the Triple Alliance in 1882, a treaty with its traditional Austrian enemy was so controversial that subsequent governments denied its existence and the terms were only made public in 1915. This arose from nationalist designs on Austro-Hungarian territory in Trentino, the Austrian Littoral, Rijeka and Dalmatia, which were considered vital to secure the borders established in 1866. In 1902, Rome secretly agreed with France to remain neutral if the latter was attacked by Germany, effectively nullifying its role in the Triple Alliance.
When the war began in 1914, Italy argued the Triple Alliance was defensive in nature and it was not obliged to support an Austrian attack on Serbia. Opposition to joining the Central Powers increased when Turkey became a member in September, since in 1911 Italy had occupied Ottoman possessions in Libya and the Dodecanese islands. To secure Italian neutrality, the Central Powers offered them the French protectorate of Tunisia, while in return for an immediate entry into the war, the Allies agreed to their demands for Austrian territory and sovereignty over the Dodecanese. Although they remained secret, these provisions were incorporated into the April 1915 Treaty of London; Italy joined the Triple Entente and on 23 May declared war on Austria-Hungary, followed by Germany fifteen months later.
The pre-1914 Italian army was the weakest in Europe, short of officers, trained men, adequate transport and modern weapons; by April 1915, some of these deficiencies had been remedied but it was still unprepared for the major offensive required by the Treaty of London. The advantage of superior numbers was offset by the difficult terrain; much of the fighting took place at altitudes of over 3000 metres in the Alps and Dolomites, where trench lines had to be cut through rock and ice and keeping troops supplied was a major challenge. These issues were exacerbated by unimaginative strategies and tactics. Between 1915 and 1917, the Italian commander, Luigi Cadorna, undertook a series of frontal assaults along the Isonzo which made little progress and cost many lives; by the end of the war, total Italian combat deaths totalled around 548,000.
Although an Italian corps occupied southern Albania in May 1916, their main focus was the Isonzo front which after the capture of Gorizia in August 1916 remained static until October 1917. After a combined Austro-German force won a major victory at Caporetto, Cadorna was replaced by Armando Diaz who retreated more than 100 kilometres (62 mi) before holding positions along the Piave River. A second Austrian offensive was repulsed in June 1918 and by October it was clear the Central Powers had lost the war. On 24 October, Diaz launched the Battle of Vittorio Veneto and initially met stubborn resistance,  but with Austria-Hungary collapsing, Hungarian divisions in Italy now demanded they be sent home. When this was granted, many others followed and the Imperial army disintegrated, the Italians taking over 300,000 prisoners. On 3 November, the Armistice of Villa Giusti ended hostilities between Austria-Hungary and Italy which occupied Trieste and areas along the Adriatic Sea awarded to it in 1915.
Despite secretly agreeing to support the Triple Alliance in 1883, Romania increasingly found itself at odds with the Central Powers over their support for Bulgaria in the 1912 to 1913 Balkan Wars and the status of ethnic Romanian communities in Hungarian-controlled Transylvania, which comprised an estimated 2.8 million of the 5.0 million population. With the ruling elite split into pro-German and pro-Entente factions, Romania remained neutral in 1914, arguing like Italy that because Austria-Hungary had declared war on Serbia, it was under no obligation to join them. They maintained this position for the next two years, while allowing Germany and Austria to transport military supplies and advisors across Romanian territory.
In September 1914, Russia had acknowledged Romanian rights to Austro-Hungarian territories including Transylvania and Banat, whose acquisition had widespread popular support,  and Russian success against Austria led Romania to join the Entente in the August 1916 Treaty of Bucharest. Under the strategic plan known as Hypothesis Z, the Romanian army planned an offensive into Transylvania, while defending Southern Dobruja and Giurgiu against a possible Bulgarian counterattack. On 27 August 1916, they attacked Transylvania and occupied substantial parts of the province before being driven back by the recently formed German 9th Army, led by former Chief of Staff Falkenhayn. A combined German-Bulgarian-Turkish offensive captured Dobruja and Giurgiu, although the bulk of the Romanian army managed to escape encirclement and retreated to Bucharest, which surrendered to the Central Powers on 6 December 1916.
Approximately 16% of the pre-war Austro-Hungarian population consisted of ethnic Romanians, whose loyalty faded as the war progressed; by 1917, they made up more than 50% of the 300,000 deserters from the Imperial army. Prisoners of war held by the Russian Empire formed the Romanian Volunteer Corps who were repatriated to Romania in 1917.   Many fought in the battles of Mărăști, Mărășești and Oituz, where with Russian support the Romanian army managed to defeat an offensive by the Central Powers and even take back some territory. Left isolated after the October Revolution forced Russia out of the war, Romania signed an armistice on 9 December 1917. Shortly afterwards, fighting broke out in the adjacent Russian territory of Bessarabia between Bolsheviks and Romanian nationalists, who requested military assistance from their compatriots. Following their intervention, the independent Moldavian Democratic Republic was formed in February 1918, which voted for union with Romania on 27 March.
On 7 May 1918 Romania signed the Treaty of Bucharest with the Central Powers, which recognised Romanian sovereignty over Bessarabia in return for ceding control of passes in the Carpathian Mountains to Austria-Hungary and granting oil concessions to Germany. Although approved by Parliament, Ferdinand I refused to sign the treaty, hoping for an Allied victory; Romania re-entered the war on 10 November 1918 on the side of the Allies and the Treaty of Bucharest was formally annulled by the Armistice of 11 November 1918.  Between 1914 to 1918, an estimated 400,000 to 600,000 ethnic Romanians served with the Austro-Hungarian army, of whom up to 150,000 were killed in action; total military and civilian deaths within contemporary Romanian borders are estimated at around 748,000.
Russian plans for the start of the war called for simultaneous invasions of Austrian Galicia and East Prussia. Although Russia's initial advance into Galicia was largely successful, it was driven back from East Prussia by Hindenburg and Ludendorff at the battles of Tannenberg and the Masurian Lakes in August and September 1914. Russia's less developed industrial base and ineffective military leadership were instrumental in the events that unfolded. By the spring of 1915, the Russians had retreated from Galicia, and, in May, the Central Powers achieved a remarkable breakthrough on Poland's southern frontiers with their Gorlice–Tarnów offensive. On 5 August, they captured Warsaw and forced the Russians to withdraw from Poland.
Despite Russia's success in the June 1916 Brusilov offensive against the Austrians in eastern Galicia, the offensive was undermined by the reluctance of other Russian generals to commit their forces to support the victory. Allied and Russian forces were revived only briefly by Romania's entry into the war on 27 August and initial gains in Transylvania, as Romania was rapidly pushed back by a combined Central Powers offensive until only the region of Moldavia was left. Meanwhile, unrest grew in Russia as the Tsar remained at the front. The increasingly incompetent rule of Empress Alexandra drew protests and resulted in the murder of her favourite, Rasputin, at the end of 1916.
On 12 December 1916, after ten brutal months of the Battle of Verdun and a successful offensive against Romania, Germany attempted to negotiate a peace with the Allies. However, this attempt was rejected out of hand as a "duplicitous war ruse".
Soon after, the US president, Woodrow Wilson, attempted to intervene as a peacemaker, asking in a note for both sides to state their demands. Lloyd George's War Cabinet considered the German offer to be a ploy to create divisions amongst the Allies. After initial outrage and much deliberation, they took Wilson's note as a separate effort, signalling that the United States was on the verge of entering the war against Germany following the "submarine outrages". While the Allies debated a response to Wilson's offer, the Germans chose to rebuff it in favour of "a direct exchange of views". Learning of the German response, the Allied governments were free to make clear demands in their response of 14 January. They sought restoration of damages, the evacuation of occupied territories, reparations for France, Russia and Romania, and a recognition of the principle of nationalities. This included the liberation of Italians, Slavs, Romanians, Czecho-Slovaks, and the creation of a "free and united Poland". On the question of security, the Allies sought guarantees that would prevent or limit future wars, complete with sanctions, as a condition of any peace settlement. The negotiations failed and the Entente powers rejected the German offer on the grounds that Germany had not put forward any specific proposals.
By the end of 1916, Russian casualties totalled nearly five million killed, wounded or captured, with major urban areas affected by food shortages and high prices. In March 1917, Tsar Nicholas ordered the military to forcibly suppress a wave of strikes in Petrograd but the troops refused to fire on the crowds. Revolutionaries set up the Petrograd Soviet and fearing a left-wing takeover, the State Duma forced Nicholas to abdicate and established the Russian Provisional Government, which confirmed Russia's willingness to continue the war. However, the Petrograd Soviet refused to disband, creating competing power centres and caused confusion and chaos, with frontline soldiers becoming increasingly demoralised and unwilling to fight on.
In the summer of 1917 a Central Powers offensive began in Romania under the command of August von Mackensen to knock Romania out of the war. Resulting in the battles of Oituz, Mărăști and Mărășești where up to 1,000,000 Central Powers troops were present. The battles lasted from 22 July to 3 September and eventually the Romanian army was victorious. August von Mackensen could not plan for another offensive as he had to transfer troops to the Italian Front.
Following the Tsar's abdication, Vladimir Lenin—with the help of the German government—was ushered by train from Switzerland into Russia 16 April 1917. Discontent and the weaknesses of the Provisional Government led to a rise in the popularity of the Bolshevik Party, led by Lenin, which demanded an immediate end to the war. The Revolution of November was followed in December by an armistice and negotiations with Germany. At first, the Bolsheviks refused the German terms, but when German troops began marching across Ukraine unopposed, the new government acceded to the Treaty of Brest-Litovsk on 3 March 1918. The treaty ceded vast territories, including Finland, Estonia, Latvia, Lithuania, parts of Poland and Ukraine to the Central Powers. Despite this enormous German success, the manpower required by the Germans to occupy the captured territory may have contributed to the failure of their Spring Offensive, and secured relatively little food or other materiel for the Central Powers war effort.
With the Russian Empire out of the war, Romania found itself alone on the Eastern Front and signed the Treaty of Bucharest with the Central Powers in May 1918, ending the state of war between Romania and the Central Powers. Under the terms of the treaty, Romania had to give territory to Austria-Hungary and Bulgaria, and lease its oil reserves to Germany. However, the terms also included the Central Powers recognition of the union of Bessarabia with Romania.
The United States was a major supplier of war materiel to the Allies but remained neutral in 1914; many opposed the idea of involvement in "foreign wars", while German Americans made up over 10% of the total population in 1913. On 7 May 1915, 128 Americans died when the British Passenger ship Lusitania was sunk by a German submarine. President Woodrow Wilson demanded an apology and warned the United States would not tolerate unrestricted submarine warfare but refused to be drawn into the war. When more Americans died after the sinking of SS Arabic in August, Bethman-Hollweg ordered an end to such attacks. Wilson argued he was "too proud to fight", although former president Theodore Roosevelt denounced the idea of "setting a spiritual example  by sitting idle, uttering cheap platitudes and picking up their trade". Despite growing pro-war sentiment, Wilson was narrowly re-elected as president in 1916.
By the end of 1916, the British naval blockade was causing serious shortages in Germany and Wilhelm approved the resumption of unrestricted submarine warfare  on 1 February 1917. While the German government recognised this action was likely to bring America into the war, the navy claimed they could starve Britain into submission in less than six months. The military position also appeared stable, at least for the foreseeable future. Despite heavy losses at Verdun and the Somme during 1916, withdrawal to the newly created Hindenburg Line would enable the Westheer to conserve its troops, while it was clear Russia was on the brink of revolution. The combination meant Germany was willing to gamble it could force the Allies to make peace before the US could intervene in any meaningful way.
Although Wilson severed diplomatic relations on 2 February, he was reluctant to start hostilities without overwhelming public support. On 24 February, he was presented with the Zimmermann Telegram; drafted in January by German Foreign Secretary Arthur Zimmermann, it was intercepted and decoded by British intelligence, who shared it with their American counterparts. Already financing Russian Bolsheviks and anti-British Irish nationalists, Zimmermann hoped to exploit nationalist feelings in Mexico caused by American incursions during the Pancho Villa Expedition. He promised President Carranza support for a war against the United States and help in recovering Texas, New Mexico, and Arizona, although this offer was promptly rejected. Publication of the telegram on 1 March caused an upsurge in support for war but this quickly subsided.
The most significant factor in creating the support Wilson needed was the German submarine offensive, which not only cost American lives, but paralysed trade as ships were reluctant to put to sea. This caused food shortages in cities along the East Coast and on 22 March, Congress approved the arming of merchant ships. Now committed to war, in his speech to Congress on 2 April Wilson presented it as a crusade "against human greed and folly, against Germany, and for justice, peace and civilisation". On 6 April, Congress declared war on Germany as an "Associated Power" of the Allies. At this stage they were not at war with the other Central Powers.
The United States Navy sent a battleship group to Scapa Flow to join the Grand Fleet and provided convoy escorts. In April 1917, the United States Army had fewer than 300,000 men, including National Guard units, compared to British and French armies of 4.1 and 8.3 million respectively. The Selective Service Act of 1917 drafted 2.8 million men, although training and equipping such numbers was a huge logistical challenge. By June 1918, over 667,000 members of the American Expeditionary Forces, or AEF, had been transported to France, a figure which reached 2 million by the end of November. However, American tactical doctrine was still based on pre-1914 principles, a world away from the combined arms approach used by the French and British in 1918. US commanders were initially slow to accept such ideas, leading to heavy casualties and it was not until the last month of the war that these failings were rectified.
Despite his conviction Germany must be defeated, Wilson went to war to ensure the US played a leading role in shaping the peace, which meant preserving the AEF as a separate military force, rather than being absorbed into British or French units as his Allies wanted. He was strongly supported by AEF commander General John J. Pershing, a proponent of pre-1914 "open warfare" who considered the French and British emphasis on artillery as misguided and incompatible with American "offensive spirit". Much to the frustration of his Allies, who had suffered heavy losses in 1917, he insisted on retaining control of American troops and refused to commit them to the front line until able to operate as independent units. As a result, the first significant US involvement was the Meuse–Argonne offensive in late September 1918.
Verdun cost the French nearly 400,000 casualties, while the horrific conditions severely impacted morale, leading to a number of incidents of indiscipline. Although relatively minor, they reflected a belief among the rank and file that their sacrifices were not appreciated by their government or senior officers. Combatants on both sides claimed the battle was the most psychologically exhausting of the entire war; recognising this, Philippe Pétain frequently rotated divisions, a process known as the noria system. While this ensured units were withdrawn before their ability to fight was significantly eroded, it meant a high proportion of the French army was affected by the battle. By the beginning of 1917, morale was brittle, even in divisions with good combat records.
In December 1916, Robert Nivelle replaced Pétain as commander of French armies on the Western Front and began planning a spring attack in Champagne, part of a joint Franco-British operation. Nivelle claimed the capture of his main objective, the Chemin des Dames, would achieve a massive breakthrough and cost no more than 15,000 casualties. Poor security meant German intelligence was well informed on tactics and timetables, but despite this, when the attack began on 16 April the French made substantial gains, before being brought to a halt by the newly built and extremely strong defences of the Hindenburg Line. Nivelle persisted with frontal assaults and by 25 April the French had suffered nearly 135,000 casualties, including 30,000 dead, most incurred in the first two days.
Concurrent British attacks at Arras were more successful, although ultimately of little strategic value. Operating as a separate unit for the first time, the Canadian Corps capture of Vimy Ridge during the battle is viewed by many Canadians as a defining moment in creating a sense of national identity. Although Nivelle continued the offensive, on 3 May the 21st Division, which had been involved in some of the heaviest fighting at Verdun, refused orders to go into battle, initiating the French Army mutinies; within days, acts of "collective indiscipline" had spread to 54 divisions, while over 20,000 deserted. Unrest was almost entirely confined to the infantry, whose demands were largely non-political, including better economic support for families at home, and regular periods of leave, which Nivelle had ended.
Although the vast majority remained willing to defend their own lines, they refused to participate in offensive action, reflecting a complete breakdown of trust in the army leadership. Nivelle was removed from command on 15 May and replaced by Pétain, who resisted demands for drastic punishment and set about restoring morale by improving conditions. While exact figures are still debated, only 27 men were actually executed, with another 3,000 sentenced to periods of imprisonment; however, the psychological effects were long-lasting, one veteran commenting "Pétain has purified the unhealthy atmosphere...but they have ruined the heart of the French soldier".
The last large-scale offensive of this period was a British attack (with French support) at Passchendaele (July–November 1917). This offensive opened with great promise for the Allies, before bogging down in the October mud. Casualties, though disputed, were roughly equal, at some 200,000–400,000 per side.
The victory of the Central Powers at the Battle of Caporetto led the Allies to convene the Rapallo conference at which they formed the Supreme War Council to co-ordinate planning. Previously, British and French armies had operated under separate commands.
In December, the Central Powers signed an armistice with Russia, thus freeing large numbers of German troops for use in the west. With German reinforcements and new American troops pouring in, the outcome was to be decided on the Western Front. The Central Powers knew that they could not win a protracted war, but they held high hopes for success based on a final quick offensive. Furthermore, both sides became increasingly fearful of social unrest and revolution in Europe. Thus, both sides urgently sought a decisive victory.
In 1917, Emperor Charles I of Austria secretly attempted separate peace negotiations with Clemenceau, through his wife's brother Sixtus in Belgium as an intermediary, without the knowledge of Germany. Italy opposed the proposals. When the negotiations failed, his attempt was revealed to Germany, resulting in a diplomatic catastrophe.
In March and April 1917, at the First and Second Battles of Gaza, German and Ottoman forces stopped the advance of the Egyptian Expeditionary Force, which had begun in August 1916 at the Battle of Romani.
At the end of October, the Sinai and Palestine campaign resumed, when General Edmund Allenby's XXth Corps, XXI Corps and Desert Mounted Corps won the Battle of Beersheba. Two Ottoman armies were defeated a few weeks later at the Battle of Mughar Ridge and, early in December, Jerusalem was captured following another Ottoman defeat at the Battle of Jerusalem. About this time, Friedrich Freiherr Kress von Kressenstein was relieved of his duties as the Eighth Army's commander, replaced by Djevad Pasha, and a few months later the commander of the Ottoman Army in Palestine, Erich von Falkenhayn, was replaced by Otto Liman von Sanders.
In early 1918, the front line was extended and the Jordan Valley was occupied, following the First Transjordan and the Second Transjordan attacks by British Empire forces in March and April 1918. In March, most of the Egyptian Expeditionary Force's British infantry and Yeomanry cavalry were sent to the Western Front as a consequence of the Spring Offensive. They were replaced by Indian Army units. During several months of reorganisation and training of the summer, a number of attacks were carried out on sections of the Ottoman front line. These pushed the front line north to more advantageous positions for the Entente in preparation for an attack and to acclimatise the newly arrived Indian Army infantry. It was not until the middle of September that the integrated force was ready for large-scale operations.
The reorganised Egyptian Expeditionary Force, with an additional mounted division, broke Ottoman forces at the Battle of Megiddo in September 1918. In two days the British and Indian infantry, supported by a creeping barrage, broke the Ottoman front line and captured the headquarters of the Eighth Army (Ottoman Empire) at Tulkarm, the continuous trench lines at Tabsor, Arara, and the Seventh Army (Ottoman Empire) headquarters at Nablus. The Desert Mounted Corps rode through the break in the front line created by the infantry. During virtually continuous operations by Australian Light Horse, British mounted Yeomanry, Indian Lancers, and New Zealand Mounted Rifle brigades in the Jezreel Valley, they captured Nazareth, Afulah and Beisan, Jenin, along with Haifa on the Mediterranean coast and Daraa east of the Jordan River on the Hejaz railway. Samakh and Tiberias on the Sea of Galilee were captured on the way northwards to Damascus. Meanwhile, Chaytor's Force of Australian light horse, New Zealand mounted rifles, Indian, British West Indies and Jewish infantry captured the crossings of the Jordan River, Es Salt, Amman and at Ziza most of the Fourth Army (Ottoman Empire). The Armistice of Mudros, signed at the end of October, ended hostilities with the Ottoman Empire when fighting was continuing north of Aleppo.
On or shortly before 15 August 1917 Pope Benedict XV made a peace proposal suggesting:
Section to be continued.
Ludendorff drew up plans (codenamed Operation Michael) for the 1918 offensive on the Western Front. The Spring Offensive sought to divide the British and French forces with a series of feints and advances. The German leadership hoped to end the war before significant US forces arrived. The operation commenced on 21 March 1918 with an attack on British forces near Saint-Quentin. German forces achieved an unprecedented advance of 60 kilometres (37 mi).
British and French trenches were penetrated using novel infiltration tactics, also named Hutier tactics after General Oskar von Hutier, by specially trained units called stormtroopers. Previously, attacks had been characterised by long artillery bombardments and massed assaults. In the Spring Offensive of 1918, however, Ludendorff used artillery only briefly and infiltrated small groups of infantry at weak points. They attacked command and logistics areas and bypassed points of serious resistance. More heavily armed infantry then destroyed these isolated positions. This German success relied greatly on the element of surprise.
The front moved to within 120 kilometres (75 mi) of Paris. Three heavy Krupp railway guns fired 183 shells on the capital, causing many Parisians to flee. The initial offensive was so successful that Kaiser Wilhelm II declared 24 March a national holiday. Many Germans thought victory was near. After heavy fighting, however, the offensive was halted. Lacking tanks or motorised artillery, the Germans were unable to consolidate their gains. The problems of re-supply were also exacerbated by increasing distances that now stretched over terrain that was shell-torn and often impassable to traffic.
Following Operation Michael, Germany launched Operation Georgette against the northern English Channel ports. The Allies halted the drive after limited territorial gains by Germany. The German Army to the south then conducted Operations Blücher and Yorck, pushing broadly towards Paris. Germany launched Operation Marne (Second Battle of the Marne) on 15 July, in an attempt to encircle Reims. The resulting counter-attack, which started the Hundred Days Offensive, marked the first successful Allied offensive of the war. By 20 July, the Germans had retreated across the Marne to their starting lines, having achieved little, and the German Army never regained the initiative. German casualties between March and April 1918 were 270,000, including many highly trained stormtroopers.
Meanwhile, Germany was falling apart at home. Anti-war marches became frequent and morale in the army fell. Industrial output was half the 1913 levels.
The Allied counteroffensive, known as the Hundred Days Offensive, began on 8 August 1918, with the Battle of Amiens. The battle involved over 400 tanks and 120,000 British, Dominion, and French troops, and by the end of its first day a gap 24 kilometres (15 mi) long had been created in the German lines. The defenders displayed a marked collapse in morale, causing Ludendorff to refer to this day as the "Black Day of the German army". After an advance as far as 23 kilometres (14 mi), German resistance stiffened, and the battle was concluded on 12 August.
Rather than continuing the Amiens battle past the point of initial success, as had been done so many times in the past, the Allies shifted attention elsewhere. Allied leaders had now realised that to continue an attack after resistance had hardened was a waste of lives, and it was better to turn a line than to try to roll over it. They began to undertake attacks in quick order to take advantage of successful advances on the flanks, then broke them off when each attack lost its initial impetus.
The day after the Offensive began, Ludendorff said: "We cannot win the war any more, but we must not lose it either." On 11 August he offered his resignation to the Kaiser, who refused it, replying, "I see that we must strike a balance. We have nearly reached the limit of our powers of resistance. The war must be ended." On 13 August, at Spa, Hindenburg, Ludendorff, the Chancellor, and Foreign Minister Hintz agreed that the war could not be ended militarily and, on the following day, the German Crown Council decided that victory in the field was now most improbable. Austria and Hungary warned that they could continue the war only until December, and Ludendorff recommended immediate peace negotiations. Prince Rupprecht warned Prince Maximilian of Baden: "Our military situation has deteriorated so rapidly that I no longer believe we can hold out over the winter; it is even possible that a catastrophe will come earlier."
British and Dominion forces launched the next phase of the campaign with the Battle of Albert on 21 August. The assault was widened by French and then further British forces in the following days. During the last week of August, the Allied pressure along a 110-kilometre (68 mi) front against the enemy was heavy and unrelenting. From German accounts, "Each day was spent in bloody fighting against an ever and again on-storming enemy, and nights passed without sleep in retirements to new lines."
Faced with these advances, on 2 September the German Oberste Heeresleitung ("Supreme Army Command") issued orders to withdraw in the south to the Hindenburg Line. This ceded without a fight the salient seized the previous April. According to Ludendorff, "We had to admit the necessity ... to withdraw the entire front from the Scarpe to the Vesle." In nearly four weeks of fighting beginning on 8 August, over 100,000 German prisoners were taken. The German High Command realised that the war was lost and made attempts to reach a satisfactory end. On 10 September Hindenburg urged peace moves to Emperor Charles of Austria, and Germany appealed to the Netherlands for mediation. On 14 September Austria sent a note to all belligerents and neutrals suggesting a meeting for peace talks on neutral soil, and on 15 September Germany made a peace offer to Belgium. Both peace offers were rejected.
In September the Allies advanced to the Hindenburg Line in the north and centre. The Germans continued to fight strong rear-guard actions and launched numerous counterattacks, but positions and outposts of the Line continued to fall, with the BEF alone taking 30,441 prisoners in the last week of September. On 24 September an assault by both the British and French came within 3 kilometres (2 mi) of St. Quentin. The Germans had now retreated to positions along or behind the Hindenburg Line. That same day, Supreme Army Command informed the leaders in Berlin that armistice talks were inevitable.
The final assault on the Hindenburg Line began with the Meuse-Argonne offensive, launched by French and American troops on 26 September. The following week, co-operating French and American units broke through in Champagne at the Battle of Blanc Mont Ridge, forcing the Germans off the commanding heights, and closing towards the Belgian frontier. On 8 October the line was pierced again by British and Dominion troops at the Battle of Cambrai. The German army had to shorten its front and use the Dutch frontier as an anchor to fight rear-guard actions as it fell back towards Germany.
When Bulgaria signed a separate armistice on 29 September, Ludendorff, having been under great stress for months, suffered something similar to a breakdown. It was evident that Germany could no longer mount a successful defence. The collapse of the Balkans meant that Germany was about to lose its main supplies of oil and food. Its reserves had been used up, even as US troops kept arriving at the rate of 10,000 per day. The Americans supplied more than 80% of Allied oil during the war, and there was no shortage.
News of Germany's impending military defeat spread throughout the German armed forces. The threat of mutiny was rife. Admiral Reinhard Scheer and Ludendorff decided to launch a last attempt to restore the "valour" of the German Navy.
In northern Germany, the German Revolution of 1918–1919 began at the end of October 1918. Units of the German Navy refused to set sail for a last, large-scale operation in a war they believed to be as good as lost, initiating the uprising. The sailors' revolt, which then ensued in the naval ports of Wilhelmshaven and Kiel, spread across the whole country within days and led to the proclamation of a republic on 9 November 1918, shortly thereafter to the abdication of Kaiser Wilhelm II, and to German surrender.
With the military faltering and with widespread loss of confidence in the Kaiser leading to his abdication and fleeing of the country, Germany moved towards surrender. Prince Maximilian of Baden took charge of a new government on 3 October as Chancellor of Germany to negotiate with the Allies. Negotiations with President Wilson began immediately, in the hope that he would offer better terms than the British and French. Wilson demanded a constitutional monarchy and parliamentary control over the German military. There was no resistance when the Social Democrat Philipp Scheidemann on 9 November declared Germany to be a republic. The Kaiser, kings and other hereditary rulers all were removed from power and Wilhelm fled to exile in the Netherlands. It was the end of Imperial Germany; a new Germany had been born as the Weimar Republic.
The collapse of the Central Powers came swiftly. Bulgaria was the first to sign an armistice, the Armistice of Salonica on 29 September 1918. German Emperor Wilhelm II in his telegram to Bulgarian Tsar Ferdinand I described situation: "Disgraceful! 62,000 Serbs decided the war!". On the same day, the German Supreme Army Command informed Kaiser Wilhelm II and the Imperial Chancellor Count Georg von Hertling, that the military situation facing Germany was hopeless.
On 24 October, the Italians began a push that rapidly recovered territory lost after the Battle of Caporetto. This culminated in the Battle of Vittorio Veneto, which marked the end of the Austro-Hungarian Army as an effective fighting force. The offensive also triggered the disintegration of the Austro-Hungarian Empire. During the last week of October, declarations of independence were made in Budapest, Prague, and Zagreb. On 29 October, the imperial authorities asked Italy for an armistice, but the Italians continued advancing, reaching Trento, Udine, and Trieste. On 3 November, Austria-Hungary sent a flag of truce to ask for an armistice (Armistice of Villa Giusti). The terms, arranged by telegraph with the Allied Authorities in Paris, were communicated to the Austrian commander and accepted. The Armistice with Austria was signed in the Villa Giusti, near Padua, on 3 November. Austria and Hungary signed separate armistices following the overthrow of the Habsburg monarchy. In the following days, the Italian Army occupied Innsbruck and all Tyrol with over 20,000 soldiers.
On 30 October, the Ottoman Empire capitulated, signing the Armistice of Mudros.
On 11 November, at 5:00 am, an armistice with Germany was signed in a railroad carriage at Compiègne. At 11 am on 11 November 1918—"the eleventh hour of the eleventh day of the eleventh month"—a ceasefire came into effect. During the six hours between the signing of the armistice and its taking effect, opposing armies on the Western Front began to withdraw from their positions, but fighting continued along many areas of the front, as commanders wanted to capture territory before the war ended. The occupation of the Rhineland took place following the Armistice. The occupying armies consisted of American, Belgian, British and French forces.
In November 1918, the Allies had ample supplies of men and materiel to invade Germany. Yet at the time of the armistice, no Allied force had crossed the German frontier, the Western Front was still some 720 kilometres (450 mi) from Berlin, and the Kaiser's armies had retreated from the battlefield in good order. These factors enabled Hindenburg and other senior German leaders to spread the story that their armies had not really been defeated. This resulted in the stab-in-the-back myth, which attributed Germany's defeat not to its inability to continue fighting (even though up to a million soldiers were suffering from the 1918 flu pandemic and unfit to fight), but to the public's failure to respond to its "patriotic calling" and the supposed intentional sabotage of the war effort, particularly by Jews, Socialists, and Bolsheviks.
The Allies had much more potential wealth they could spend on the war. One estimate (using 1913 US dollars) is that the Allies spent $58 billion on the war and the Central Powers only $25 billion. Among the Allies, the UK spent $21 billion and the US$17 billion; among the Central Powers Germany spent $20 billion.
In the aftermath of the war, four empires disappeared: the German, Austro-Hungarian, Ottoman, and Russian. Numerous nations regained their former independence, and new ones were created. Four dynasties, together with their ancillary aristocracies, fell as a result of the war: the Romanovs, the Hohenzollerns, the Habsburgs, and the Ottomans. Belgium and Serbia were badly damaged, as was France, with 1.4 million soldiers dead, not counting other casualties. Germany and Russia were similarly affected.
A formal state of war between the two sides persisted for another seven months, until the signing of the Treaty of Versailles with Germany on 28 June 1919. The United States Senate did not ratify the treaty despite public support for it, and did not formally end its involvement in the war until the Knox–Porter Resolution was signed on 2 July 1921 by President Warren G. Harding. For the United Kingdom and the British Empire, the state of war ceased under the provisions of the Termination of the Present War (Definition) Act 1918 with respect to:
After the Treaty of Versailles, treaties with Austria, Hungary, Bulgaria, and the Ottoman Empire were signed. The Ottoman Empire disintegrated, with much of its Levant territory awarded to various Allied powers as protectorates. The Turkish core in Anatolia was reorganised as the Republic of Turkey. The Ottoman Empire was to be partitioned by the Treaty of Sèvres of 1920. This treaty was never ratified by the Sultan and was rejected by the Turkish National Movement, leading to the victorious Turkish War of Independence and the much less stringent 1923 Treaty of Lausanne.
Some war memorials date the end of the war as being when the Versailles Treaty was signed in 1919, which was when many of the troops serving abroad finally returned home; by contrast, most commemorations of the war's end concentrate on the armistice of 11 November 1918. Legally, the formal peace treaties were not complete until the last, the Treaty of Lausanne, was signed. Under its terms, the Allied forces left Constantinople on 23 August 1923.
After the war, there grew a certain amount of academic focus on the causes of war and on the elements that could make peace flourish. In part, these led to the institutionalization of peace and conflict studies, security studies and International Relations (IR) in general. The Paris Peace Conference imposed a series of peace treaties on the Central Powers officially ending the war. The 1919 Treaty of Versailles dealt with Germany and, building on Wilson's 14th point, brought into being the League of Nations on 28 June 1919.
The Central Powers had to acknowledge responsibility for "all the loss and damage to which the Allied and Associated Governments and their nationals have been subjected as a consequence of the war imposed upon them by" their aggression. In the Treaty of Versailles, this statement was Article 231. This article became known as the War Guilt clause as the majority of Germans felt humiliated and resentful. Overall the Germans felt they had been unjustly dealt with by what they called the "diktat of Versailles". German historian Hagen Schulze said the Treaty placed Germany "under legal sanctions, deprived of military power, economically ruined, and politically humiliated." Belgian historian Laurence Van Ypersele emphasises the central role played by memory of the war and the Versailles Treaty in German politics in the 1920s and 1930s:
Active denial of war guilt in Germany and German resentment at both reparations and continued Allied occupation of the Rhineland made widespread revision of the meaning and memory of the war problematic. The legend of the "stab in the back" and the wish to revise the "Versailles diktat", and the belief in an international threat aimed at the elimination of the German nation persisted at the heart of German politics. Even a man of peace such as  Stresemann publicly rejected German guilt. As for the Nazis, they waved the banners of domestic treason and international conspiracy in an attempt to galvanise the German nation into a spirit of revenge. Like a Fascist Italy, Nazi Germany sought to redirect the memory of the war to the benefit of its own policies.Meanwhile, new nations liberated from German rule viewed the treaty as recognition of wrongs committed against small nations by much larger aggressive neighbours. The Peace Conference required all the defeated powers to pay reparations for all the damage done to civilians. However, owing to economic difficulties and Germany being the only defeated power with an intact economy, the burden fell largely on Germany.
Austria-Hungary was partitioned into several successor states, including Austria, Hungary, Czechoslovakia, and Yugoslavia, largely but not entirely along ethnic lines. Transylvania was awarded to Romania. The details were contained in the Saint-Germain-en-Laye and the Treaty of Trianon. As a result, Hungary lost 64% of its total population, decreasing from 20.9 million to 7.6 million and losing 31% (3.3 out of 10.7 million) of its ethnic Hungarians. According to the 1910 census, speakers of the Hungarian language included approximately 48% of the entire population of the kingdom, and 54% of the population of the territory referred to as "Hungary proper", i.e. excluding Croatia-Slavonia. Within the borders of "Hungary proper" numerous ethnic minorities were present: 16.1% Romanians, 10.5% Slovaks, 10.4% Germans, 2.5% Ruthenians, 2.5% Serbs and 8% others. Between 1920 and 1924, 354,000 Hungarians fled former Hungarian territories attached to Romania, Czechoslovakia, and Yugoslavia.
The Russian Empire, which had withdrawn from the war in 1917 after the October Revolution, lost much of its western frontier as the newly independent nations of Estonia, Finland, Latvia, Lithuania, and Poland were carved from it. Romania took control of Bessarabia in April 1918.
After 123 years, Poland re-emerged as an independent country. The Kingdom of Serbia and its dynasty, as a "minor Entente nation" and the country with the most casualties per capita, became the backbone of a new multinational state, the Kingdom of Serbs, Croats and Slovenes, later renamed Yugoslavia. Czechoslovakia, combining the Kingdom of Bohemia with parts of the Kingdom of Hungary, became a new nation. Romania would unite all Romanian-speaking people under a single state leading to Greater Romania. Russia became the Soviet Union and lost Finland, Estonia, Lithuania, and Latvia, which became independent countries. The Ottoman Empire was soon replaced by Turkey and several other countries in the Middle East.
In the British Empire, the war unleashed new forms of nationalism. In Australia and New Zealand, the Battle of Gallipoli became known as those nations' "Baptism of Fire". It was the first major war in which the newly established countries fought, and it was one of the first times that Australian troops fought as Australians, not just subjects of the British Crown. Anzac Day, commemorating the Australian and New Zealand Army Corps (ANZAC), celebrates this defining moment.
After the Battle of Vimy Ridge, where the Canadian divisions fought together for the first time as a single corps, Canadians began to refer to their country as a nation "forged from fire". Having succeeded on the same battleground where the "mother countries" had previously faltered, they were for the first time respected internationally for their own accomplishments. Canada entered the war as a Dominion of the British Empire and remained so, although it emerged with a greater measure of independence. When Britain declared war in 1914, the dominions were automatically at war; at the conclusion, Canada, Australia, New Zealand, and South Africa were individual signatories of the Treaty of Versailles.
Lobbying by Chaim Weizmann and fear that American Jews would encourage the United States to support Germany culminated in the British government's Balfour Declaration of 1917, endorsing creation of a Jewish homeland in Palestine. A total of more than 1,172,000 Jewish soldiers served in the Allied and Central Power forces in World War I, including 275,000 in Austria-Hungary and 450,000 in Tsarist Russia.
The establishment of the modern state of Israel and the roots of the continuing Israeli–Palestinian conflict are partially found in the unstable power dynamics of the Middle East that resulted from World War I. Before the end of the war, the Ottoman Empire had maintained a modest level of peace and stability throughout the Middle East. With the fall of the Ottoman government, power vacuums developed and conflicting claims to land and nationhood began to emerge. The political boundaries drawn by the victors of World War I were quickly imposed, sometimes after only cursory consultation with the local population. These continue to be problematic in the 21st-century struggles for national identity. While the dissolution of the Ottoman Empire at the end of World War I was pivotal in contributing to the modern political situation of the Middle East, including the Arab-Israeli conflict, the end of Ottoman rule also spawned lesser-known disputes over water and other natural resources.
The prestige of Germany and German things in Latin America remained high after the war but did not recover to its pre-war levels. Indeed, in Chile the war bought an end to a period of intense scientific and cultural influence writer Eduardo de la Barra scorningly called "the German bewitchment" (Spanish: el embrujamiento alemán).
The Czechoslovak Legion fought on the sides of the Entente, seeking to win support for an independent Czechoslovakia. The Legion in Russia was established in September 1914, in December 1917 in France (including volunteers from America) and in April 1918 in Italy. Czechoslovak Legion troops defeated the Austro-Hungarian army at the Ukrainian village of Zboriv, in July 1917. After this success, the number of Czechoslovak legionaries increased, as well as Czechoslovak military power. In the Battle of Bakhmach, the Legion defeated the Germans and forced them to make a truce.
In Russia, they were heavily involved in the Russian Civil War, siding with the Whites against the Bolsheviks, at times controlling most of the Trans-Siberian Railway and conquering all the major cities of Siberia. The presence of the Czechoslovak Legion near Yekaterinburg appears to have been one of the motivations for the Bolshevik execution of the Tsar and his family in July 1918. Legionaries arrived less than a week afterwards and captured the city. Because Russia's European ports were not safe, the corps was evacuated by a long detour via the port of Vladivostok. The last transport was the American ship Heffron in September 1920.
The Transylvanian and Bukovinian Romanians who were taken prisoners of war fought as the Romanian Volunteer Corps in Russia, Romanian Legion of Siberia and Romanian Legion in Italy. Taking part in the Eastern Front as part of the Russian Army and since summer 1917 in the Romanian front as part of the Romanian Army. As a supporter of the White movement with the Czechoslovak Legion against the Red Army during the Russian Civil War. In the battles of Montello, Vittorio Veneto, Sisemolet, Piave, Cimone, Monte Grappa, Nervesa and Ponte Delle Alpi as part of the Italian Army against Austria-Hungary and in 1919 as part of the Romanian Army in the Hungarian-Romanian War.
In the late spring of 1918, three new states were formed in the South Caucasus: the First Republic of Armenia, the Azerbaijan Democratic Republic, and the Democratic Republic of Georgia, which declared their independence from the Russian Empire. Two other minor entities were established, the Centrocaspian Dictatorship and South West Caucasian Republic (the former was liquidated by Azerbaijan in the autumn of 1918 and the latter by a joint Armenian-British task force in early 1919). With the withdrawal of the Russian armies from the Caucasus front in the winter of 1917–18, the three major republics braced for an imminent Ottoman advance, which commenced in the early months of 1918. Solidarity was briefly maintained when the Transcaucasian Federative Republic was created in the spring of 1918, but this collapsed in May when the Georgians asked for and received protection from Germany and the Azerbaijanis concluded a treaty with the Ottoman Empire that was more akin to a military alliance. Armenia was left to fend for itself and struggled for five months against the threat of a full-fledged occupation by the Ottoman Turks before defeating them at the Battle of Sardarabad.
Of the 60 million European military personnel who were mobilised from 1914 to 1918, 8 million were killed, 7 million were permanently disabled, and 15 million were seriously injured. Germany lost 15.1% of its active male population, Austria-Hungary lost 17.1%, and France lost 10.5%. France mobilised 7.8 million men, of which 1.4 million died and 3.2 million were injured. Among the soldiers mutilated and surviving in the trenches, approximately 15,000 sustained horrific facial injuries, causing them to undergo social stigma and marginalisation; they were called the gueules cassées. In Germany, civilian deaths were 474,000 higher than in peacetime, due in large part to food shortages and malnutrition that weakened resistance to disease. These excess deaths are estimated as 271,000 in 1918, plus another 71,000 in the first half of 1919 when the blockade was still in effect. By the end of the war, starvation caused by famine had killed approximately 100,000 people in Lebanon. Between 5 and 10 million people died in the Russian famine of 1921. By 1922, there were between 4.5 million and 7 million homeless children in Russia as a result of nearly a decade of devastation from World War I, the Russian Civil War, and the subsequent famine of 1920–1922. Numerous anti-Soviet Russians fled the country after the Revolution; by the 1930s, the northern Chinese city of Harbin had 100,000 Russians. Thousands more emigrated to France, England, and the United States.
The Australian prime minister, Billy Hughes, wrote to the British prime minister, David Lloyd George, "You have assured us that you cannot get better terms. I much regret it, and hope even now that some way may be found of securing agreement for demanding reparation commensurate with the tremendous sacrifices made by the British Empire and her Allies." Australia received £5,571,720 war reparations, but the direct cost of the war to Australia had been £376,993,052, and, by the mid-1930s, repatriation pensions, war gratuities, interest and sinking fund charges were £831,280,947. Of about 416,000 Australians who served, about 60,000 were killed and another 152,000 were wounded.
Diseases flourished in the chaotic wartime conditions. In 1914 alone, louse-borne epidemic typhus killed 200,000 in Serbia. From 1918 to 1922, Russia had about 25 million infections and 3 million deaths from epidemic typhus. In 1923, 13 million Russians contracted malaria, a sharp increase from the pre-war years. Starting in early 1918, a major influenza epidemic known as Spanish flu spread around the world, accelerated by the movement of large number of soldiers, often crammed together in camps and transport ships with poor sanitation. Overall, the Spanish flu killed at least 17 million to 25 million people, including an estimated 2.64 million Europeans and as many as 675,000 Americans. Moreover, between 1915 and 1926, an epidemic of encephalitis lethargica spread around the world affecting nearly five million people.
The social disruption and widespread violence of the Russian Revolution of 1917 and the ensuing Russian Civil War sparked more than 2,000 pogroms in the former Russian Empire, mostly in Ukraine. An estimated 60,000–200,000 civilian Jews were killed in the atrocities.
In the aftermath of World War I, Greece fought against Turkish nationalists led by Mustafa Kemal, a war that eventually resulted in a massive population exchange between the two countries under the Treaty of Lausanne. According to various sources, several hundred thousand Greeks died during this period, which was tied in with the Greek genocide.
World War I began as a clash of 20th-century technology and 19th-century tactics, with the inevitably large ensuing casualties. By the end of 1917, however, the major armies, now numbering millions of men, had modernised and were making use of telephone, wireless communication, armoured cars, tanks (especially with the advent of the first prototype tank, Little Willie), and aircraft. Infantry formations were reorganised, so that 100-man companies were no longer the main unit of manoeuvre; instead, squads of 10 or so men, under the command of a junior NCO, were favoured.
Artillery also underwent a revolution. In 1914, cannons were positioned in the front line and fired directly at their targets. By 1917, indirect fire with guns (as well as mortars and even machine guns) was commonplace, using new techniques for spotting and ranging, notably, aircraft and the often overlooked field telephone. Counter-battery missions became commonplace, also, and sound detection was used to locate enemy batteries.
Germany was far ahead of the Allies in using heavy indirect fire. The German Army employed 150 mm (6 in) and 210 mm (8 in) howitzers in 1914, when typical French and British guns were only 75 mm (3 in) and 105 mm (4 in). The British had a 6-inch (152 mm) howitzer, but it was so heavy it had to be hauled to the field in pieces and assembled. The Germans also fielded Austrian 305 mm (12 in) and 420 mm (17 in) guns and, even at the beginning of the war, had inventories of various calibres of Minenwerfer, which were ideally suited for trench warfare.
On 27 June 1917 the Germans used the biggest gun in the world, Batterie Pommern, nicknamed "Lange Max". This gun from Krupp was able to shoot 750 kg shells from Koekelare to Dunkirk, a distance of about 50 km (31 mi).
Much of the combat involved trench warfare, in which hundreds often died for each metre gained. Many of the deadliest battles in history occurred during World War I. Such battles include Ypres, the Marne, Cambrai, the Somme, Verdun, and Gallipoli. The Germans employed the Haber process of nitrogen fixation to provide their forces with a constant supply of gunpowder despite the British naval blockade. Artillery was responsible for the largest number of casualties and consumed vast quantities of explosives. The large number of head wounds caused by exploding shells and fragmentation forced the combatant nations to develop the modern steel helmet, led by the French, who introduced the Adrian helmet in 1915. It was quickly followed by the Brodie helmet, worn by British Imperial and US troops, and in 1916 by the distinctive German Stahlhelm, a design, with improvements, still in use today.
Gas! GAS! Quick, boys! – An ecstasy of fumbling,
Fitting the clumsy helmets just in time;
But someone still was yelling out and stumbling,
And flound'ring like a man in fire or lime ...
Dim, through the misty panes and thick green light,
As under a green sea, I saw him drowning.
The widespread use of chemical warfare was a distinguishing feature of the conflict. Gases used included chlorine, mustard gas and phosgene. Relatively few war casualties were caused by gas, as effective countermeasures to gas attacks were quickly created, such as gas masks. The use of chemical warfare and small-scale strategic bombing (as opposed to tactical bombing) were both outlawed by the Hague Conventions of 1899 and 1907, and both proved to be of limited effectiveness, though they captured the public imagination.
The most powerful land-based weapons were railway guns, weighing dozens of tons apiece. The German version were nicknamed Big Berthas, even though the namesake was not a railway gun. Germany developed the Paris Gun, able to bombard Paris from over 100 kilometres (62 mi), though shells were relatively light at 94 kilograms (210 lb).
Trenches, machine guns, air reconnaissance, barbed wire, and modern artillery with fragmentation shells helped bring the battle lines of World War I to a stalemate. The British and the French sought a solution with the creation of the tank and mechanised warfare. The British first tanks were used during the Battle of the Somme on 15 September 1916. Mechanical reliability was an issue, but the experiment proved its worth. Within a year, the British were fielding tanks by the hundreds, and they showed their potential during the Battle of Cambrai in November 1917, by breaking the Hindenburg Line, while combined arms teams captured 8,000 enemy soldiers and 100 guns. Meanwhile, the French introduced the first tanks with a rotating turret, the Renault FT, which became a decisive tool of the victory. The conflict also saw the introduction of light automatic weapons and submachine guns, such as the Lewis gun, the M1918 Browning Automatic Rifle, and the MP 18.
Another new weapon, the flamethrower, was first used by the German army and later adopted by other forces. Although not of high tactical value, the flamethrower was a powerful, demoralising weapon that caused terror on the battlefield.
Trench railways evolved to supply the enormous quantities of food, water, and ammunition required to support large numbers of soldiers in areas where conventional transportation systems had been destroyed. Internal combustion engines and improved traction systems for automobiles and trucks/lorries eventually rendered trench railways obsolete.
On the Western Front, neither side made impressive gains in the first three years of the war with attacks at Verdun, the Somme, Passchendaele, and Cambrai—the exception was Nivelle's Offensive in which the German defence gave ground while mauling the attackers so badly that there were mutinies in the French Army. In 1918 the Germans smashed through the defence lines in three great attacks: Michael, on the Lys, and on the Aisne, which displayed the power of their new tactics. The Allies struck back at Soissons, which showed the Germans that they must return to the defensive, and at Amiens; tanks played a prominent role in both these assaults, as they had the year before at Cambrai.
The areas in the East were larger. The Germans did well at the First Masurian Lakes driving the invaders from East Prussia, and at Riga, which led the Russians to sue for peace. The Austro-Hungarians and Germans joined for a great success at Gorlice–Tarnów, which drove the Russians out of Poland. In a series of attacks along with the Bulgarians, they occupied Serbia, Albania, Montenegro and most of Romania. The Allies successes came later in Palestine, the beginning of the end for the Ottomans, in Macedonia, which drove the Bulgarians out of the war, and at Vittorio Veneto, the final blow for the Austro-Hungarians. The area occupied in the East by the Central powers on 11 November 1918 was 1,042,600 km2 (402,600 sq mi).
Germany deployed U-boats (submarines) after the war began. Alternating between restricted and unrestricted submarine warfare in the Atlantic, the Imperial German Navy employed them to deprive the British Isles of vital supplies. The deaths of British merchant sailors and the seeming invulnerability of U-boats led to the development of depth charges (1916), hydrophones (sonar, 1917), blimps, hunter-killer submarines (HMS R-1, 1917), forward-throwing anti-submarine weapons, and dipping hydrophones (the latter two both abandoned in 1918). To extend their operations, the Germans proposed supply submarines (1916). Most of these would be forgotten in the interwar period until World War II revived the need.
Fixed-wing aircraft were first used militarily by the Italians in Libya on 23 October 1911 during the Italo-Turkish War for reconnaissance, soon followed by the dropping of grenades and aerial photography the next year. By 1914, their military utility was obvious. They were initially used for reconnaissance and ground attack. To shoot down enemy planes, anti-aircraft guns and fighter aircraft were developed. Strategic bombers were created, principally by the Germans and British, though the former used Zeppelins as well. Towards the end of the conflict, aircraft carriers were used for the first time, with HMS Furious launching Sopwith Camels in a raid to destroy the Zeppelin hangars at Tønder in 1918.
Manned observation balloons, floating high above the trenches, were used as stationary reconnaissance platforms, reporting enemy movements and directing artillery. Balloons commonly had a crew of two, equipped with parachutes, so that if there was an enemy air attack the crew could parachute to safety. At the time, parachutes were too heavy to be used by pilots of aircraft (with their marginal power output), and smaller versions were not developed until the end of the war; they were also opposed by the British leadership, who feared they might promote cowardice.
Recognised for their value as observation platforms, balloons were important targets for enemy aircraft. To defend them against air attack, they were heavily protected by anti-aircraft guns and patrolled by friendly aircraft; to attack them, unusual weapons such as air-to-air rockets were tried. Thus, the reconnaissance value of blimps and balloons contributed to the development of air-to-air combat between all types of aircraft, and to the trench stalemate, because it was impossible to move large numbers of troops undetected. The Germans conducted air raids on England during 1915 and 1916 with airships, hoping to damage British morale and cause aircraft to be diverted from the front lines, and indeed the resulting panic led to the diversion of several squadrons of fighters from France.
The introduction of radio telegraphy was a significant step in communication during World War I. The stations utilized at that time were spark-gap transmitters. As an example, the information of the start of World War I was transmitted to German South West Africa on 2 August 1914 via radio telegraphy from the Nauen transmitter station via a relay station in Kamina and Lomé in Togo to the radio station in Windhoek.
The German invaders treated any resistance—such as sabotaging rail lines—as illegal and immoral, and shot the offenders and burned buildings in retaliation. In addition, they tended to suspect that most civilians were potential francs-tireurs (guerrillas) and, accordingly, took and sometimes killed hostages from among the civilian population. The German army executed over 6,500 French and Belgian civilians between August and November 1914, usually in near-random large-scale shootings of civilians ordered by junior German officers. The German Army destroyed 15,000–20,000 buildings—most famously the university library at Louvain—and generated a wave of refugees of over a million people. Over half the German regiments in Belgium were involved in major incidents. Thousands of workers were shipped to Germany to work in factories. British propaganda dramatising the Rape of Belgium attracted much attention in the United States, while Berlin said it was both lawful and necessary because of the threat of franc-tireurs like those in France in 1870. The British and French magnified the reports and disseminated them at home and in the United States, where they played a major role in dissolving support for Germany.
Austria's propaganda machinery spread anti-Serb sentiment, with other things, the slogan "Serbien muss sterbien" (Serbia must die). During the war Austro-Hungarian officers in Serbia ordered troops to "exterminate and burn everything that is Serbian", and hangings and mass shootings were everyday occurrences. Austrian historian, Anton Holzer, wrote that the Austro-Hungarian army carried out "countless and systematic massacres…against the Serbian population. The soldiers invaded villages and rounded up unarmed men, women and children. They were either shot dead, bayoneted to death or hanged. The victims were locked into barns and burned alive. Women were sent up to the front lines and mass-raped. The inhabitants of whole villages were taken as hostages and humiliated and tortured."
A claim from a local spy that "traitors" were hiding in a certain house was enough to sentence the whole family to death by hanging. Priests were often hanged, under the accusation of spreading the spirit of treason among the people. Multiple source state that 30,000 Serbs, mostly civilians, were hanged by Austro-Hungarian forces in the first year of the war alone.
On 19 August 1915, the German submarine U-27 was sunk by the British Q-ship HMS Baralong. All German survivors were summarily executed by Baralong's crew on the orders of Lieutenant Godfrey Herbert, the captain of the ship. The shooting was reported to the media by American citizens who were on board the Nicosia, a British freighter loaded with war supplies, which was stopped by U-27 just minutes before the incident.
On 24 September, Baralong destroyed U-41, which was in the process of sinking the cargo ship Urbino. According to Karl Goetz, the submarine's commander, Baralong continued to fly the US flag after firing on U-41 and then rammed the lifeboat—carrying the German survivors, sinking it.
The Canadian hospital ship HMHS Llandovery Castle was torpedoed by the German submarine SM U-86 on 27 June 1918 in violation of international law. Only 24 of the 258 medical personnel, patients, and crew survived. Survivors reported that the U-boat surfaced and ran down the lifeboats, machine-gunning survivors in the water. The U-boat captain, Helmut Brümmer-Patzig, was charged with war crimes in Germany following the war, but escaped prosecution by going to the Free City of Danzig, beyond the jurisdiction of German courts.
After the war, the German government claimed that approximately 763,000 German civilians died from starvation and disease during the war because of the Allied blockade. An academic study done in 1928 put the death toll at 424,000. Germany protested that the Allies had used starvation as a weapon of war. Sally Marks argued that the German accounts of a hunger blockade are a "myth," as Germany did not face the starvation level of Belgium and the regions of Poland and northern France that it occupied. According to the British judge and legal philosopher Patrick Devlin, "The War Orders given by the Admiralty on 26 August  were clear enough. All food consigned to Germany through neutral ports was to be captured and all food consigned to Rotterdam was to be presumed consigned to Germany." According to Devlin, this was a serious breach of International Law, equivalent to German minelaying.
The German army was the first to successfully deploy chemical weapons during the Second Battle of Ypres (22 April – 25 May 1915), after German scientists working under the direction of Fritz Haber at the Kaiser Wilhelm Institute developed a method to weaponize chlorine. The use of chemical weapons was sanctioned by the German High Command in an effort to force Allied soldiers out of their entrenched positions, complementing rather than supplanting more lethal conventional weapons. In time, chemical weapons were deployed by all major belligerents throughout the war, inflicting approximately 1.3 million casualties, but relatively few fatalities: About 90,000 in total. For example, there were an estimated 186,000 British chemical weapons casualties during the war (80% of which were the result of exposure to the vesicant sulfur mustard, introduced to the battlefield by the Germans in July 1917, which burns the skin at any point of contact and inflicts more severe lung damage than chlorine or phosgene), and up to one-third of American casualties were caused by them. The Russian Army reportedly suffered roughly 500,000 chemical weapon casualties in World War I. The use of chemical weapons in warfare was in direct violation of the 1899 Hague Declaration Concerning Asphyxiating Gases and the 1907 Hague Convention on Land Warfare, which prohibited their use.
The effect of poison gas was not limited to combatants. Civilians were at risk from the gases as winds blew the poison gases through their towns, and they rarely received warnings or alerts of potential danger. In addition to absent warning systems, civilians often did not have access to effective gas masks. An estimated 100,000–260,000 civilian casualties were caused by chemical weapons during the conflict and tens of thousands more (along with military personnel) died from scarring of the lungs, skin damage, and cerebral damage in the years after the conflict ended. Many commanders on both sides knew such weapons would cause major harm to civilians but nonetheless continued to use them. British Field Marshal Douglas Haig wrote in his diary, "My officers and I were aware that such weapons would cause harm to women and children living in nearby towns, as strong winds were common in the battlefront. However, because the weapon was to be directed against the enemy, none of us were overly concerned at all."
The war damaged chemistry's prestige in European societies, in particular the German variety.
The ethnic cleansing of the Ottoman Empire's Armenian population, including mass deportations and executions, during the final years of the Ottoman Empire is considered genocide. The Ottomans carried out organised and systematic massacres of the Armenian population at the beginning of the war and manipulated acts of Armenian resistance by portraying them as rebellions to justify further extermination. In early 1915, a number of Armenians volunteered to join the Russian forces and the Ottoman government used this as a pretext to issue the Tehcir Law (Law on Deportation), which authorised the deportation of Armenians from the Empire's eastern provinces to Syria between 1915 and 1918. The Armenians were intentionally marched to death and a number were attacked by Ottoman brigands. While an exact number of deaths is unknown, the International Association of Genocide Scholars estimates 1.5 million. The government of Turkey has consistently denied the genocide, arguing that those who died were victims of inter-ethnic fighting, famine, or disease during World War I; these claims are rejected by most historians.
Other ethnic groups were similarly attacked by the Ottoman Empire during this period, including Assyrians and Greeks, and some scholars consider those events to be part of the same policy of extermination. At least 250,000 Assyrian Christians, about half of the population, and 350,000–750,000 Anatolian and Pontic Greeks were killed between 1915 and 1922.
Many pogroms accompanied the Russian Revolution of 1917 and the ensuing Russian Civil War. 60,000–200,000 civilian Jews were killed in the atrocities throughout the former Russian Empire (mostly within the Pale of Settlement in present-day Ukraine). There were an estimated 7–12 million casualties during the Russian Civil War, mostly civilians.
The British soldiers of the war were initially volunteers but increasingly were conscripted into service. Surviving veterans, returning home, often found they could discuss their experiences only amongst themselves. Grouping together, they formed "veterans' associations" or "Legions". A small number of personal accounts of American veterans have been collected by the Library of Congress Veterans History Project.
About eight million men surrendered and were held in POW camps during the war. All nations pledged to follow the Hague Conventions on fair treatment of prisoners of war, and the survival rate for POWs was generally much higher than that of combatants at the front. Individual surrenders were uncommon; large units usually surrendered en masse. At the siege of Maubeuge about 40,000 French soldiers surrendered, at the battle of Galicia Russians took about 100,000 to 120,000 Austrian captives, at the Brusilov Offensive about 325,000 to 417,000 Germans and Austrians surrendered to Russians, and at the Battle of Tannenberg, 92,000 Russians surrendered. When the besieged garrison of Kaunas surrendered in 1915, some 20,000 Russians became prisoners, at the battle near Przasnysz (February–March 1915) 14,000 Germans surrendered to Russians, and at the First Battle of the Marne about 12,000 Germans surrendered to the Allies. 25–31% of Russian losses (as a proportion of those captured, wounded, or killed) were to prisoner status; for Austria-Hungary 32%, for Italy 26%, for France 12%, for Germany 9%; for Britain 7%. Prisoners from the Allied armies totalled about 1.4 million (not including Russia, which lost 2.5–3.5 million men as prisoners). From the Central Powers about 3.3 million men became prisoners; most of them surrendered to Russians. Germany held 2.5 million prisoners; Russia held 2.2–2.9 million; while Britain and France held about 720,000. Most were captured just before the Armistice. The United States held 48,000. The most dangerous moment was the act of surrender when helpless soldiers were sometimes gunned down. Once prisoners reached a camp, conditions were, in general, satisfactory (and much better than in World War II), thanks in part to the efforts of the International Red Cross and inspections by neutral nations. However, conditions were terrible in Russia: starvation was common for prisoners and civilians alike; about 15–20% of the prisoners in Russia died, and in Central Powers imprisonment 8% of Russians. In Germany, food was scarce, but only 5% died.
The Ottoman Empire often treated POWs poorly. Some 11,800 British Empire soldiers, most of them Indians, became prisoners after the siege of Kut in Mesopotamia in April 1916; 4,250 died in captivity. Although many were in a poor condition when captured, Ottoman officers forced them to march 1,100 kilometres (684 mi) to Anatolia. A survivor said: "We were driven along like beasts; to drop out was to die." The survivors were then forced to build a railway through the Taurus Mountains.
In Russia, when the prisoners from the Czechoslovak Legion of the Austro-Hungarian army were released in 1917, they re-armed themselves and briefly became a military and diplomatic force during the Russian Civil War.
While the Allied prisoners of the Central Powers were quickly sent home at the end of active hostilities, the same treatment was not granted to Central Power prisoners of the Allies and Russia, many of whom served as forced labour, e.g., in France until 1920. They were released only after many approaches by the Red Cross to the Supreme War Council. German prisoners were still being held in Russia as late as 1924.
Military and civilian observers from every major power closely followed the course of the war. Many were able to report on events from a perspective somewhat akin to modern "embedded" positions within the opposing land and naval forces.
In the Balkans, Yugoslav nationalists such as the leader, Ante Trumbić, strongly supported the war, desiring the freedom of Yugoslavs from Austria-Hungary and other foreign powers and the creation of an independent Yugoslavia. The Yugoslav Committee, led by Trumbić, was formed in Paris on 30 April 1915 but shortly moved its office to London. In April 1918, the Rome Congress of Oppressed Nationalities met, including Czechoslovak, Italian, Polish, Transylvanian, and Yugoslav representatives who urged the Allies to support national self-determination for the peoples residing within Austria-Hungary.
In the Middle East, Arab nationalism soared in Ottoman territories in response to the rise of Turkish nationalism during the war, with Arab nationalist leaders advocating the creation of a pan-Arab state. In 1916, the Arab Revolt began in Ottoman-controlled territories of the Middle East in an effort to achieve independence.
In East Africa, Iyasu V of Ethiopia was supporting the Dervish state who were at war with the British in the Somaliland campaign. Von Syburg, the German envoy in Addis Ababa, said, "now the time has come for Ethiopia to regain the coast of the Red Sea driving the Italians home, to restore the Empire to its ancient size." The Ethiopian Empire was on the verge of entering World War I on the side of the Central Powers before Iyasu's overthrow at the Battle of Segale due to Allied pressure on the Ethiopian aristocracy. Iyasu was accused of converting to Islam. According to Ethiopian historian Bahru Zewde, the evidence used to prove Iyasu's conversion was a doctored photo of Iyasu wearing a turban provided by the Allies. Some historians claim the British spy T. E. Lawrence forged the Iyasu photo.
A number of socialist parties initially supported the war when it began in August 1914. But European socialists split on national lines, with the concept of class conflict held by radical socialists such as Marxists and syndicalists being overborne by their patriotic support for the war. Once the war began, Austrian, British, French, German, and Russian socialists followed the rising nationalist current by supporting their countries' intervention in the war.
Italian nationalism was stirred by the outbreak of the war and was initially strongly supported by a variety of political factions. One of the most prominent and popular Italian nationalist supporters of the war was Gabriele D'Annunzio, who promoted Italian irredentism and helped sway the Italian public to support intervention in the war. The Italian Liberal Party, under the leadership of Paolo Boselli, promoted intervention in the war on the side of the Allies and used the Dante Alighieri Society to promote Italian nationalism. Italian socialists were divided on whether to support the war or oppose it; some were militant supporters of the war, including Benito Mussolini and Leonida Bissolati. However, the Italian Socialist Party decided to oppose the war after anti-militarist protestors were killed, resulting in a general strike called Red Week. The Italian Socialist Party purged itself of pro-war nationalist members, including Mussolini. Mussolini, a syndicalist who supported the war on grounds of irredentist claims on Italian-populated regions of Austria-Hungary, formed the pro-interventionist Il Popolo d'Italia and the Fasci Rivoluzionario d'Azione Internazionalista ("Revolutionary Fasci for International Action") in October 1914 that later developed into the Fasci Italiani di Combattimento in 1919, the origin of fascism. Mussolini's nationalism enabled him to raise funds from Ansaldo (an armaments firm) and other companies to create Il Popolo d'Italia to convince socialists and revolutionaries to support the war.
On both sides there was large scale fundraising for soldiers' welfare, their dependents and for those injured. The Nail Men were a German example. Around the British empire there were many Patriotic Funds, including the Royal Patriotic Fund Corporation, Canadian Patriotic Fund, Queensland Patriotic Fund and, by 1919, there were 983 funds in New Zealand. At the start of the next world war the New Zealand funds were reformed, having been criticised as overlapping, wasteful and abused, but 11 were still functioning in 2002.
Once war was declared, many socialists and trade unions backed their governments. Among the exceptions were the Bolsheviks, the Socialist Party of America, the Italian Socialist Party, and people like Karl Liebknecht, Rosa Luxemburg, and their followers in Germany.
Pope Benedict XV, elected to the papacy less than three months into World War I, made the war and its consequences the main focus of his early pontificate. In stark contrast to his predecessor, five days after his election he spoke of his determination to do what he could to bring peace. His first encyclical, Ad beatissimi Apostolorum, given 1 November 1914, was concerned with this subject. Benedict XV found his abilities and unique position as a religious emissary of peace ignored by the belligerent powers. The 1915 Treaty of London between Italy and the Triple Entente included secret provisions whereby the Allies agreed with Italy to ignore papal peace moves towards the Central Powers. Consequently, the publication of Benedict's proposed seven-point Peace Note of August 1917 was roundly ignored by all parties except Austria-Hungary.
In Britain in 1914, the Public Schools Officers' Training Corps annual camp was held at Tidworth Pennings, near Salisbury Plain. Head of the British Army, Lord Kitchener, was to review the cadets, but the imminence of the war prevented him. General Horace Smith-Dorrien was sent instead. He surprised the two-or-three thousand cadets by declaring (in the words of Donald Christopher Smith, a Bermudian cadet who was present),
that war should be avoided at almost any cost, that war would solve nothing, that the whole of Europe and more besides would be reduced to ruin, and that the loss of life would be so large that whole populations would be decimated. In our ignorance I, and many of us, felt almost ashamed of a British General who uttered such depressing and unpatriotic sentiments, but during the next four years, those of us who survived the holocaust—probably not more than one-quarter of us—learned how right the General's prognosis was and how courageous he had been to utter it.Voicing these sentiments did not hinder Smith-Dorrien's career, or prevent him from doing his duty in World War I to the best of his abilities.
Many countries jailed those who spoke out against the conflict. These included Eugene Debs in the United States and Bertrand Russell in Britain. In the US, the Espionage Act of 1917 and Sedition Act of 1918 made it a federal crime to oppose military recruitment or make any statements deemed "disloyal". Publications at all critical of the government were removed from circulation by postal censors, and many served long prison sentences for statements of fact deemed unpatriotic.
A number of nationalists opposed intervention, particularly within states that the nationalists were hostile to. Although the vast majority of Irish people consented to participate in the war in 1914 and 1915, a minority of advanced Irish nationalists staunchly opposed taking part. The war began amid the Home Rule crisis in Ireland that had resurfaced in 1912, and by July 1914 there was a serious possibility of an outbreak of civil war in Ireland. Irish nationalists and Marxists attempted to pursue Irish independence, culminating in the Easter Rising of 1916, with Germany sending 20,000 rifles to Ireland to stir unrest in Britain. The UK government placed Ireland under martial law in response to the Easter Rising, though once the immediate threat of revolution had dissipated, the authorities did try to make concessions to nationalist feeling. However, opposition to involvement in the war increased in Ireland, resulting in the Conscription Crisis of 1918.
Other opposition came from conscientious objectors—some socialist, some religious—who refused to fight. In Britain, 16,000 people asked for conscientious objector status. Some of them, most notably prominent peace activist Stephen Hobhouse, refused both military and alternative service. Many suffered years of prison, including solitary confinement and bread and water diets. Even after the war, in Britain many job advertisements were marked "No conscientious objectors need apply".
The Central Asian revolt started in the summer of 1916, when the Russian Empire government ended its exemption of Muslims from military service.
In 1917, a series of French Army Mutinies led to dozens of soldiers being executed and many more imprisoned.
On 1–4 May 1917, about 100,000 workers and soldiers of Petrograd, and after them, the workers and soldiers of other Russian cities, led by the Bolsheviks, demonstrated under banners reading "Down with the war!" and "all power to the soviets!" The mass demonstrations resulted in a crisis for the Russian Provisional Government. In Milan, in May 1917, Bolshevik revolutionaries organised and engaged in rioting calling for an end to the war, and managed to close down factories and stop public transportation. The Italian army was forced to enter Milan with tanks and machine guns to face Bolsheviks and anarchists, who fought violently until 23 May when the army gained control of the city. Almost 50 people (including three Italian soldiers) were killed and over 800 people arrested.
In September 1917, Russian soldiers in France began questioning why they were fighting for the French at all and mutinied. In Russia, opposition to the war led to soldiers also establishing their own revolutionary committees, which helped foment the October Revolution of 1917, with the call going up for "bread, land, and peace". The Decree on Peace, written by Vladimir Lenin, was passed on 8 November 1917, following the success of the October Revolution. The Bolsheviks agreed to a peace treaty with Germany, the Treaty of Brest-Litovsk, despite its harsh conditions. The German Revolution of 1918–1919 led to the abdication of the Kaiser and German surrender.
Conscription was common in most European countries. However, it was controversial in English-speaking countries. It was especially unpopular among minority ethnic groups—especially the Irish Catholics in Ireland and Australia, and the French Catholics in Canada.
In Canada, the issue produced a major political crisis that permanently alienated the Francophones. It opened a political gap between French Canadian, who believed their true loyalty was to Canada and not to the British Empire, and members of the Anglophone majority, who saw the war as a duty to their British heritage.
Australia had a form of conscription at the outbreak of the war, as compulsory military training had been introduced in 1911. However, the Defence Act 1903 provided that unexempted males could be called upon only for home defence during times of war, not overseas service. Prime Minister Billy Hughes wished to amend the legislation to require conscripts to serve overseas, and held two non-binding referendums – one in 1916 and one in 1917 – in order to secure public support. Both were defeated by narrow margins, with farmers, the labour movement, the Catholic Church, and Irish-Australians combining to campaign for the "No" vote. The issue of conscription caused the 1916 Australian Labor Party split. Hughes and his supporters were expelled from the party, forming the National Labor Party and then the Nationalist Party. Despite the referendum results, the Nationalists won a landslide victory at the 1917 federal election.
In Britain, conscription resulted in the calling up of nearly every physically fit man in Britain—six of ten million eligible. Of these, about 750,000 lost their lives. Most deaths were those of young unmarried men; however, 160,000 wives lost husbands and 300,000 children lost fathers. Conscription during the First World War began when the British government passed the Military Service Act in 1916. The act specified that single men aged 18 to 40 years old were liable to be called up for military service unless they were widowed with children or ministers of a religion. There was a system of Military Service Tribunals to adjudicate upon claims for exemption upon the grounds of performing civilian work of national importance, domestic hardship, health, and conscientious objection. The law went through several changes before the war ended. Married men were exempt in the original Act, although this was changed in June 1916. The age limit was also eventually raised to 51 years old. Recognition of work of national importance also diminished, and in the last year of the war, there was some support for the conscription of clergy. Conscription lasted until mid-1919. Due to the political situation in Ireland, conscription was never applied there; only in England, Scotland and Wales.
In the United States, conscription began in 1917 and was generally well received, with a few pockets of opposition in isolated rural areas. The administration decided to rely primarily on conscription, rather than voluntary enlistment, to raise military manpower after only 73,000 volunteers enlisted out of the initial 1 million target in the first six weeks of the war. In 1917 10 million men were registered. This was deemed to be inadequate, so age ranges were increased and exemptions reduced, and so by the end of 1918 this increased to 24 million men that were registered with nearly 3 million inducted into the military services. The draft was universal and included blacks on the same terms as whites, although they served in different units. In all 367,710 black Americans were drafted (13% of the total), compared to 2,442,586 white (87%).
Forms of resistance ranged from peaceful protest to violent demonstrations and from humble letter-writing campaigns asking for mercy to radical newspapers demanding reform. The most common tactics were dodging and desertion, and many communities sheltered and defended their draft dodgers as political heroes. Many socialists were jailed for "obstructing the recruitment or enlistment service". The most famous was Eugene Debs, head of the Socialist Party of America, who ran for president in 1920 from his prison cell. In 1917 a number of radicals and anarchists challenged the new draft law in federal court, arguing that it was a direct violation of the Thirteenth Amendment's prohibition against slavery and involuntary servitude. The Supreme Court unanimously upheld the constitutionality of the draft act in the Selective Draft Law Cases on 7 January 1918.
Like all the armies of mainland Europe, Austria-Hungary relied on conscription to fill its ranks. Officer recruitment, however, was voluntary. The effect of this at the start of the war was that well over a quarter of the rank and file were Slavs, while more than 75% of the officers were ethnic Germans. This was much resented. The army has been described as being "run on colonial lines" and the Slav soldiers as "disaffected". Thus conscription contributed greatly to Austria's disastrous performance on the battlefield.
The non-military diplomatic and propaganda interactions among the nations were designed to build support for the cause or to undermine support for the enemy. For the most part, wartime diplomacy focused on five issues: propaganda campaigns; defining and redefining the war goals, which became harsher as the war went on; luring neutral nations (Italy, Ottoman Empire, Bulgaria, Romania) into the coalition by offering slices of enemy territory; and encouragement by the Allies of nationalistic minority movements inside the Central Powers, especially among Czechs, Poles, and Arabs. In addition, there were multiple peace proposals coming from neutrals, or one side or the other; none of them progressed very far.
... "Strange, friend," I said, "Here is no cause to mourn."
"None," said the other, "Save the undone years"... 
The first tentative efforts to comprehend the meaning and consequences of modern warfare began during the initial phases of the war, and this process continued throughout and after the end of hostilities, and is still underway, more than a century later. As late as 2007, signs warning visitors to keep off certain paths at battlefield sites like Verdun and Somme remained in place as unexploded ordnance continued to pose a danger to farmers living near former battlegrounds. In France and Belgium locals who discover caches of unexploded munitions are assisted by weapons disposal units. In some places, plant life has still not returned to normal.
Teaching World War I has presented special challenges. When compared with World War II, the First World War is often thought to be "a wrong war fought for the wrong reasons". It lacks the metanarrative of good versus evil that characterizes the Second World War. Lacking recognizable heroes and villains, it is often taught thematically, invoking tropes like the wastefulness of war, the folly of generals and the innocence of soldiers. The complexity of the conflict is mostly obscured by these oversimplifications.
Historian Heather Jones argues that the historiography has been reinvigorated by the cultural turn in recent years. Scholars have raised entirely new questions regarding military occupation, radicalisation of politics, race, medical science, gender and mental health. Furthermore, new research has revised our understanding of five major topics that historians have long debated: Why the war began, why the Allies won, whether generals were responsible for high casualty rates, how the soldiers endured the horrors of trench warfare, and to what extent the civilian homefront accepted and endorsed the war effort.
Memorials were erected in thousands of villages and towns. Close to battlefields, those buried in improvised burial grounds were gradually moved to formal graveyards under the care of organisations such as the Commonwealth War Graves Commission, the American Battle Monuments Commission, the German War Graves Commission, and Le Souvenir français. Many of these graveyards also have central monuments to the missing or unidentified dead, such as the Menin Gate Memorial to the Missing and the Thiepval Memorial to the Missing of the Somme.
In 1915 John McCrae, a Canadian army doctor, wrote the poem In Flanders Fields as a salute to those who perished in the Great War. Published in Punch on 8 December 1915, it is still recited today, especially on Remembrance Day and Memorial Day.
National World War I Museum and Memorial in Kansas City, Missouri, is a memorial dedicated to all Americans who served in World War I. The Liberty Memorial was dedicated on 1 November 1921, when the supreme Allied commanders spoke to a crowd of more than 100,000 people.
The UK Government has budgeted substantial resources to the commemoration of the war during the period 2014 to 2018. The lead body is the Imperial War Museum. On 3 August 2014, French President François Hollande and German President Joachim Gauck together marked the centenary of Germany's declaration of war on France by laying the first stone of a memorial in Vieil Armand, known in German as Hartmannswillerkopf, for French and German soldiers killed in the war. During the Armistice centenary commemorations, French President Emmanuel Macron and German Chancellor Angela Merkel visited the site of the signing of the Armistice of Compiègne and unveiled a plaque to reconciliation.
World War I had a lasting impact on collective memory. It was seen by many in Britain as signalling the end of an era of stability stretching back to the Victorian period, and across Europe many regarded it as a watershed. Historian Samuel Hynes explained:
A generation of innocent young men, their heads full of high abstractions like Honour, Glory and England, went off to war to make the world safe for democracy. They were slaughtered in stupid battles planned by stupid generals. Those who survived were shocked, disillusioned and embittered by their war experiences, and saw that their real enemies were not the Germans, but the old men at home who had lied to them. They rejected the values of the society that had sent them to war, and in doing so separated their own generation from the past and from their cultural inheritance.This has become the most common perception of World War I, perpetuated by the art, cinema, poems, and stories published subsequently. Films such as All Quiet on the Western Front, Paths of Glory and King and Country have perpetuated the idea, while war-time films including Camrades, Poppies of Flanders, and Shoulder Arms indicate that the most contemporary views of the war were overall far more positive. Likewise, the art of Paul Nash, John Nash, Christopher Nevinson, and Henry Tonks in Britain painted a negative view of the conflict in keeping with the growing perception, while popular war-time artists such as Muirhead Bone painted more serene and pleasant interpretations subsequently rejected as inaccurate. Several historians like John Terraine, Niall Ferguson and Gary Sheffield have challenged these interpretations as partial and polemical views:
These beliefs did not become widely shared because they offered the only accurate interpretation of wartime events. In every respect, the war was much more complicated than they suggest. In recent years, historians have argued persuasively against almost every popular cliché of World War I. It has been pointed out that, although the losses were devastating, their greatest impact was socially and geographically limited. The many emotions other than horror experienced by soldiers in and out of the front line, including comradeship, boredom, and even enjoyment, have been recognised. The war is not now seen as a 'fight about nothing', but as a war of ideals, a struggle between aggressive militarism and more or less liberal democracy. It has been acknowledged that British generals were often capable men facing difficult challenges and that it was under their command that the British army played a major part in the defeat of the Germans in 1918: a great forgotten victory.Though these views have been discounted as "myths", they are common. They have dynamically changed according to contemporary influences, reflecting in the 1950s perceptions of the war as "aimless" following the contrasting Second World War and emphasising conflict within the ranks during times of class conflict in the 1960s. The majority of additions to the contrary are often rejected.
The social trauma caused by unprecedented rates of casualties manifested itself in different ways, which have been the subject of subsequent historical debate. Over 8 million Europeans died in the war. Millions suffered permanent disabilities. The war gave birth to fascism and Bolshevism and destroyed the dynasties that had ruled the Ottoman, Habsburg, Russian and German Empires.
The optimism of la belle époque was destroyed, and those who had fought in the war were referred to as the Lost Generation. For years afterwards, people mourned the dead, the missing, and the many disabled. Many soldiers returned with severe trauma, suffering from shell shock (also called neurasthenia, a condition related to post-traumatic stress disorder). Many more returned home with few after-effects; however, their silence about the war contributed to the conflict's growing mythological status. Though many participants did not share in the experiences of combat or spend any significant time at the front, or had positive memories of their service, the images of suffering and trauma became the widely shared perception. Such historians as Dan Todman, Paul Fussell, and Samuel Heyns have all published works since the 1990s arguing that these common perceptions of the war are factually incorrect.
The rise of Nazism and fascism included a revival of the nationalist spirit and a rejection of many post-war changes. Similarly, the popularity of the stab-in-the-back legend (German: Dolchstoßlegende) was a testament to the psychological state of defeated Germany and was a rejection of responsibility for the conflict. This conspiracy theory of betrayal became common, and the German populace came to see themselves as victims. The widespread acceptance of the "stab-in-the-back" theory delegitimised the Weimar government and destabilised the system, opening it to extremes of right and left. The same occurred in Austria which did not consider itself responsible for the outbreak of the war and claimed not to have suffered a military defeat.
Communist and fascist movements around Europe drew strength from this theory and enjoyed a new level of popularity. These feelings were most pronounced in areas directly or harshly affected by the war. Adolf Hitler was able to gain popularity by using German discontent with the still controversial Treaty of Versailles. World War II was in part a continuation of the power struggle never fully resolved by World War I. Furthermore, it was common for Germans in the 1930s to justify acts of aggression due to perceived injustices imposed by the victors of World War I. American historian William Rubinstein wrote that:
The 'Age of Totalitarianism' included nearly all the infamous examples of genocide in modern history, headed by the Jewish Holocaust, but also comprising the mass murders and purges of the Communist world, other mass killings carried out by Nazi Germany and its allies, and also the Armenian Genocide of 1915. All these slaughters, it is argued here, had a common origin, the collapse of the elite structure and normal modes of government of much of central, eastern and southern Europe as a result of World War I, without which surely neither Communism nor Fascism would have existed except in the minds of unknown agitators and crackpots.One of the most dramatic effects of the war was the expansion of governmental powers and responsibilities in Britain, France, the United States, and the Dominions of the British Empire. To harness all the power of their societies, governments created new ministries and powers. New taxes were levied and laws enacted, all designed to bolster the war effort; many have lasted to the present. Similarly, the war strained the abilities of some formerly large and bureaucratised governments, such as in Austria-Hungary and Germany.
Gross domestic product (GDP) increased for three Allies (Britain, Italy, and the United States), but decreased in France and Russia, in neutral Netherlands, and in the three main Central Powers. The shrinkage in GDP in Austria, Russia, France, and the Ottoman Empire ranged between 30% and 40%. In Austria, for example, most pigs were slaughtered, so at war's end there was no meat.
In all nations, the government's share of GDP increased, surpassing 50% in both Germany and France and nearly reaching that level in Britain. To pay for purchases in the United States, Britain cashed in its extensive investments in American railroads and then began borrowing heavily from Wall Street. President Wilson was on the verge of cutting off the loans in late 1916 but allowed a great increase in US government lending to the Allies. After 1919, the US demanded repayment of these loans. The repayments were, in part, funded by German reparations that, in turn, were supported by American loans to Germany. This circular system collapsed in 1931 and some loans were never repaid. Britain still owed the United States $4.4 billion of World War I debt in 1934; the last installment was finally paid in 2015.
Macro- and micro-economic consequences devolved from the war. Families were altered by the departure of many men. With the death or absence of the primary wage earner, women were forced into the workforce in unprecedented numbers. At the same time, industry needed to replace the lost labourers sent to war. This aided the struggle for voting rights for women.
World War I further compounded the gender imbalance, adding to the phenomenon of surplus women. The deaths of nearly one million men during the war in Britain increased the gender gap by almost a million: from 670,000 to 1,700,000. The number of unmarried women seeking economic means grew dramatically. In addition, demobilisation and economic decline following the war caused high unemployment. The war increased female employment; however, the return of demobilised men displaced many from the workforce, as did the closure of many of the wartime factories.
In Britain, rationing was finally imposed in early 1918, limited to meat, sugar, and fats (butter and margarine), but not bread. The new system worked smoothly. From 1914 to 1918, trade union membership doubled, from a little over four million to a little over eight million.
Britain turned to her colonies for help in obtaining essential war materials whose supply from traditional sources had become difficult. Geologists such as Albert Kitson were called on to find new resources of precious minerals in the African colonies. Kitson discovered important new deposits of manganese, used in munitions production, in the Gold Coast.
Article 231 of the Treaty of Versailles (the so-called "war guilt" clause) stated Germany accepted responsibility for "all the loss and damage to which the Allied and Associated Governments and their nationals have been subjected as a consequence of the war imposed upon them by the aggression of Germany and her allies." It was worded as such to lay a legal basis for reparations, and a similar clause was inserted in the treaties with Austria and Hungary. However, neither of them interpreted it as an admission of war guilt." In 1921, the total reparation sum was placed at 132 billion gold marks. However, "Allied experts knew that Germany could not pay" this sum. The total sum was divided into three categories, with the third being "deliberately designed to be chimerical" and its "primary function was to mislead public opinion ... into believing the "total sum was being maintained." Thus, 50 billion gold marks (12.5 billion dollars) "represented the actual Allied assessment of German capacity to pay" and "therefore ... represented the total German reparations" figure that had to be paid.
This figure could be paid in cash or in-kind (coal, timber, chemical dyes, etc.). In addition, some of the territory lost—via the treaty of Versailles—was credited towards the reparation figure as were other acts such as helping to restore the Library of Louvain. By 1929, the Great Depression arrived, causing political chaos throughout the world. In 1932 the payment of reparations was suspended by the international community, by which point Germany had paid only the equivalent of 20.598 billion gold marks in reparations. With the rise of Adolf Hitler, all bonds and loans that had been issued and taken out during the 1920s and early 1930s were cancelled. David Andelman notes "refusing to pay doesn't make an agreement null and void. The bonds, the agreement, still exist." Thus, following the Second World War, at the London Conference in 1953, Germany agreed to resume payment on the money borrowed. On 3 October 2010, Germany made the final payment on these bonds.
The war contributed to the evolution of the wristwatch from women's jewellery to a practical everyday item, replacing the pocketwatch, which requires a free hand to operate. Military funding of advancements in radio contributed to the post-war popularity of the medium.

Archibald Montgomery Low (17 October 1888 – 13 September 1956) developed the first powered drone aircraft. He was an English consulting engineer, research physicist and inventor, and author of more than 40 books.
Low has been called the "father of radio guidance systems" due to his pioneering work on planes, torpedoes boats and guided rockets.  He was a pioneer in many fields though, often leading the way for others, but his lack of discipline meant he hardly ever saw a project through, being easily distracted by new ideas.  If it wasn't for this inability to see things to a conclusion, Low could well have been remembered as one of the great men of science.  Many of his scientific contemporaries disliked him, due in part to his using the title "professor", which he wasn't entitled to do as he didn't occupy an academic chair.  His love of the limelight and publicity probably also added to the dislike.
Archibald M. Low was working on the invention of television before World War I and promoting its development through the 1920s.
Low was born in Purley, London, the second son of John and Gertrude Low.  His father was an engineer with experience in steam boilers and Low's interest in all things mechanical and scientific was fired by visits to his father's place of work.  The family moved to Erith in Kent when Low was still a baby.  He was sent to Preparatory school at Colet Court when his father moved to Australia as a director of the Paddy Lackey Deep-level Company 
 gold mine. In 1896 Archibald was only 7 when he sailed with his mother, his elder brother and a maiden aunt on the ship Thermopylae to Sydney for a visit. He recalls being amazed to find that telephones were fitted in every house. As a young boy Low was forever experimenting at home, building homemade steam turbines or conducting chemical experiments that brought havoc to his local neighbourhood and caused his parents to receive many complaints about the bangs, smells and gases created by young Archie.
At the age of 11 he was enrolled into St Paul's School, an institution where he didn't fit in, being as he put it "too much of an individual".  One of his classmates for several years was Bernard Montgomery, whom Low recalled as being "rather dull".
Aged 16 Low entered the Central Technical College, an institution far more to his liking, here his abilities really started to show.  Under the guidance of his mentor Professor Ashcroft, Low's mercurial mind was given free rein over many of the scientific disciplines.  During his time at the CTC Low designed a drawing device which he called "The Low flexible and adjustable curve".  This device along with a dotted line pen and a self filling draughtsman's pen were marketed by Thornton's, a renowned instrument maker based in Manchester.  He also spent a year devising and making a selector mechanism which allowed a lever when moved to fall into a pre-selected slot.  It wasn't until 32 years later that pre-selected gears came in, long after Low had originally thought of them.
Low joined his uncle, Edward Low's engineering firm, Low Accessories and Ignition Company, which at the time was the second oldest engineering firm in the City of London.  Unfortunately the company was in a constant struggle for solvency.  Edward Low did what he could financially to help get his nephew's ideas off the ground, but what was really needed was a rich investor.  During this pre-war period Low was constantly coming up with big new ideas, such as his forced induction engine (which in modern parlance would be called a gasoline direct injection engine), which was further developed with the large engine builder F.E. Baker Ltd and exhibited on their stand at the Olympia Motor Cycle Show in November 1912. He also invented gadgets like the whistling egg-boiler which he christened "The Chanticleer".  It went on to sell very well, earning him some much-needed money.  He also experimented with gas turbines, but the alloys available at that time wouldn't stand up to the required heat. A prototype of Low's unique invention, the "petrol direct injection engine", was produced and displayed in 1912.
In May 1914 Low gave the first demonstrations of what was to become television, he called it TeleVista.  The first of these demonstration was given to the Institute of Automobile Engineers. Harry Gordon Selfridge then arranged to included one at the famous Selfridge Store Exhibitions. Reports of these were entitled "Seeing By Wireless".  Low's invention was crude and under-developed but the idea was there. The main deficiency was the selenium cell used for converting light waves into electric impulses, which responded too slowly thus spoiling the effect. The demonstration certainly garnered a lot of media interest with The Times reporting on 30 May;
An inventor, Dr. A. M. Low, has discovered a means of transmitting visual images by wire.  If all goes well with this invention, we shall soon be able, it seems, to see people at a distance.
On 29 May the Daily Chronicle reported;
Dr. Low gave a demonstration for the first time in public, with a new apparatus that he has invented, for seeing, he claims by electricity, by which it is possible for persons using a telephone to see each other at the same timeLow, of course failed to follow up this early promising work, due in part to his temperamental failings and also of course the outbreak of World War I later that year. Nature commented that the work was overblown in "Sensational paragraphs on seeing by wire". However, a US consular report from London by Deputy Consul General Carl Raymond Loop provided a different story and considerable detail about Low's system.  Low finally applied for his "Televista" Patent No. 191,405 for "Improved Apparatus for the Electrical Transmission of Optical Images" in 1917 but its release was delayed (possibly for security reasons). It was finally published in 1923. In 1927 Ronald Frank Tiltman asked Low to write the introduction to his book in which he acknowledged Low's work, referring to Low's various related patents with an apology that they were of "too technical a nature for inclusion". Although it employed an electro-mechanical scanning mechanism, with its matrix detector (camera) and mosaic screen(receiver) it is unlike all of the later intervening systems of the 20th century. In these respects, Low had a digital TV system 80 years before the advent of today's digital TV and deserves his place in the history of television.
When war broke out, Low joined the military and received officer training.  After a few months he was promoted to captain and seconded to the Royal Flying Corps, the precursor of the RAF.  His brief was to use his civilian research on Televista to remotely control the RFC drone weapons proposed by the Royal Aircraft Factory, so it could be used as a guided missile.  With two other officers (Captain Poole and Lieutenant Bowen) under him, they set to work to see if it were possible.  This project was called "Aerial Target" or AT a misnomer to fool the Germans into thinking it was about building a drone plane to test anti-aircraft capabilities.  After they built a prototype, General Sir David Henderson (director-general of Directorate of Military Aeronautics) ordered that an Experimental Works at Feltham should be created to build the first proper "Aerial Target" complete with explosive warhead.  As head of the Experimental Works, Low was given about 30 picked men, including jewellers, carpenters and aircraftsmen in order to get the pilotless plane built as quickly as possible.  The AT planes were from manufacturers such as Airco, Sopwith Aviation Company and the Royal Aircraft Factory. The de Havilland-designed Airco ATs had their first trial on 21 March 1917 at Upavon Central Flying School near Salisbury Plain, attended by 30–40 allied generals.  The AT was launched from the back of a lorry using compressed air (another first).  Low and his team successfully demonstrated their ability to control the craft before engine failure led to its crash landing. A subsequent trial of the RAF ATs on 6 July 1917 was cut short as an AT had been lost at takeoff. At a later date an electrically driven gyrocompass (yet another first) was added to the plane. In 1918 Low's Feltham Works developed the airborne controlled Royal Navy Distance Control Boats (DCB), a variant of the Coastal Motor Boat.
In 1917 Low and his team also invented the first electrically steered rocket (the world's first wireless, or wire-guided rocket), almost an exact counterpart of the one used by the Germans in 1942 against merchant shipping.  Low's inventions during the war were to a large extent before their time and hence were under-appreciated by the government of the day, although the Germans were well aware of how dangerous his inventions might be. In 1915 two attempts were made to assassinate him; the first involved shots being fired through his laboratory window in Paul Street; the second attempt was from a visitor with a German accent who came to Low's office and offered him a cigarette, which upon analysis contained enough strychnine chloride to kill.
In 1917 the priority for Low's control system changed; the new imperative being to counter the submarine threat. Low was transferred into the Royal Navy along with Lieutenant Ernest Windsor Bowen to adapt the AT system to control the DCBs but Low still commanded the RFC works at Feltham where the work was carried out.
The pre-dreadnought battleship HMS Agamemnon was converted into a remote control target ship in 1920 and the Feltham "Aerial Target" project was taken up by Royal Aircraft Establishment who tested a series of Royal Aircraft Factory 1917 type AT with a 45 h.p Armstrong Siddeley engines in 1921. Low's principles were adopted by the Air Ministry for the RAE Larynx (from "Long Range Gun with Lynx Engine"), and explosive-laden autopiloted aircraft which was developed by the Royal Aircraft Establishment from 1925 and this drone development work culminated in the fleet of Queen Bee aerial target variants of the de Havilland Tiger Moth of the 1930s.  Further developments continued by the British before and during the Second World War.
During World War II the Germans also made good use of Low's 1918 rocket guidance system and used it as one of the foundations for their guided weapons, the Henschel Hs 293, and Fritz X and for their V1 Doodlebug.
Low could have made a considerable amount of money from these inventions, but his patents couldn't stay in force for the statutory period, as he was in the employment of the War Department everything he invented was as a part of his duties so he couldn't benefit financially from them.
Low was commended for this work by a number of senior officers including Sir David Henderson and Admiral Edward Stafford Fitzherbert. Sir Henry Norman, 1st Baronet a technically competent radio engineer and distinguished politician wrote to Low in March 1918 saying "I know of no man who has more extensive and more profound scientific knowledge, combined with a greater gift on imaginative invention than yourself". res.
Not long after the war Low started the Low Engineering Company Ltd in association with the Hon. C. N. Bruce (later Lord Aberdare).  The company offices were on Kensington High Street, and Low spent much of his time trying to bring his inventions to fruition.  As usual though he was easily distracted by gadgets that he devised, taking his attention away from the more important work.  One of the better gadgets was a motor scooter that Low invented and manufactured in conjunction with Sir Henry Norman.
Despite his best efforts, business wasn't his strong point.  An example of this is the magazine he started up with his friend Lord Brabazon and others.  It was entitled Armchair Science, Low helped edit it, and at one point the sales figures were 80,000 a month, yet it never seemed to make a profit and was sold off.  Another of Low's delights was speed, especially racing cars or motorbikes.  He was a regular attendee at Brooklands and at one point invented a rocket propelled bike and numerous other gadgets and improvements for the internal combustion engine.  An example of Low's prescience is that he was worried about the number of road traffic accidents that were occurring and believed speed in cities should be restricted to 25 mph using modern radio methods to enforce it.  One of Low's peeves was excess noise, to this end he invented an audiometer to measure and record noise in a visual form.  He conducted experiments on the London Underground and achieved some success in pinpointing trouble spots and reducing their impact by use of shields over the wheels and padding of the interior panels.
In 1938 Low had lunch with a gentleman called William Joyce.  Joyce wanted Low to contribute an article to a paper he helped run.  Low declined the offer being too busy; it was only a couple of years later that Joyce gained infamy as Lord Haw-Haw.
A few of Low's inventions from this period are:
At the outbreak of the Second World War Low initially joined the Air Ministry in a civil capacity.  His job was to examine captured German aircraft and prepare reports for British pilots to enable them to identify the weak points of the enemy aircraft.  Later on he joined the Royal Pioneer Corps and was promoted to major.  Between experiments in his back garden laboratory, he gave frequent talks to service personnel on scientific matters.  Low was frequently in bad health from the late 1930s onwards, having never fully recovered from a bout of pneumonia he suffered a few years earlier.  Although nothing that he experimented with during the war ultimately came to fruition, he did work on some interesting projects:
The telephone may develop to a stage where it is unnecessary to enter a special call-box.  We shall think no more of telephoning to our office from our cars or railway-carriages than we do today of telephoning from our homesThe second stage in the development of space-ships could be the launching of what have been called "space-platforms" ... The rocket or space-station will travel round the earth in twenty four hours at most.  The value of such stations might be very great; they might enable world-wide television broadcasts to be made; they would transmit data about cosmic rays; or solar radiation; and they might have incalculable military valueNo team ever invents anything, they only develop one man's flash of geniusI always say that the greatest discovery is that we know practically nothing about anythingLow died at his London home in 1956 aged 68. The cause of his death was a malignant tumour on his lung.  He is buried in Brompton Cemetery, London.
In 1976 Low was inducted into the International Space Hall of Fame
Low was a prolific author of science books.  He aimed several of his books at the layman to try to nurture interest in science and engineering.  Quite a few of his books contained predictions on scientific advancements.
As well as these non-fiction books he wrote four science fiction novels for the younger reader.
In 1976 Low was included in the 35 pioneers listed by the International Academy of Astronautics and inducted into the International Space Hall of Fame in 1976. He has been called the "Father of Radio Guidance Systems" and the "founder of the field of radio guidance systems".  Alternatively,  it was suggest A. M. Low was the "Father of Remotely Piloted Vehicles".

Soon after its re-purposing from the Army Balloon Factory to the Royal Aircraft Factory in 1912, designers at this Farnborough base turned their thoughts to flying an unmanned aircraft. During the First World War this pioneering work resulted in trials of remotely controlled aircraft for the Royal Flying Corps and unmanned boats for the Royal Navy that were controlled from 'mother' aircraft. By the end of the war in 1918 Britain had successfully flown and controlled a drone aircraft and a number of fast unmanned motor boats operating in close flotilla formation had been individually controlled by radio from operators flying in "mother" aircraft. This work then continued in the inter war years.
There is a Royal Aircraft Factory engineering drawing dated October 1914 of an unmanned powered monoplane 9 ft 3 in (2.82 m) long with a 10 ft (3.0 m) wingspan. This was developed as a possible defence to counter the threat of aerial bombing from German dirigible airships. This new potential weapon was called "Aerial Target" (AT), a misnomer to fool the Germans into thinking it was a drone plane to test anti-aircraft capabilities.
Henry Folland, who had been at Royal Aircraft Factory since 1912, designed an AT powered by an ABC Gnat engine which was built by Ruston Proctor of Lincoln in 1916/1917.
With Harry Hawker, Sopwith at Kingston upon Thames built a single-bay biplane AT with a wingspan of about 14 ft (4.3 m) which was to carry a 50 lb (23 kg) explosive charge. Stability came from pronounced dihedral and there was a four-wheel undercarriage. The aircraft was damaged during erection at Feltham and was never tested. The design was later reworked into the Sopwith Sparrow.
The history of UAV target drones started when the Royal Flying Corps developed their prototype remote controlled aircraft and gave it the cover name "Aerial Target" (AT). All the 1917 "Aerial Target" aircraft from the various designers used the radio control system devised by Archibald Low at the RFC's Experimental Works in Feltham. One of Geoffrey de Havilland's "AT" aircraft powered by a Gnat engine that was launched from a pneumatically powered ramp in the RFC trials at Upavon on 21 March 1917 became the world's first powered drone aircraft to fly under radio control. The engine driven actuator applied progressively increasing deflection of the selected control (elevators or rudder) up to its limit until the selection lever was released by the ground operator. With no control demanded the control surface was returned to its trim position by springs. The mechanism was later exhibited by the IWM as "The original model receiving set installed in the radio controlled monoplane used in the trial flight." along with the Selective Transmitter which the operator on the ground used to send control the control signals.
Low's system's encoded the command transmissions as a countermeasure to prevent enemy intervention. These codes could be changed daily.
By July 1917 six Aerial Targets designed by the Factory had been built and were tested at Northholt. Attempts were made to launch the first three from rails laid on the ground but they all crashed in various ways during the launching process and these trials were terminated. Nevertheless, the Aerial Target was later acknowledged as a viable weapon, stating "aircraft carrying high explosive charges are capable of being controlled by wireless."
The "AT" project was transferred to Biggin Hill, to what became the Wireless Experimental Establishment. By 1922 this work had all transferred to the R.A.E., back at Farnborough where it had all begun in 1914.
Archibald Low stated "in 1917 the Experimental Works designed an electrically steered rocket...  Rocket experiments were conducted under my own patents with the help of Cdr. Brock" Like Low, Brock was an experimental officer. Brock commanded the Royal Navy Experimental Station at Stratford. Pertinent to these rocket experiments, Brock was also a Director of the C.T. Brock &amp; Co. fireworks manufacturers.  The patent "Improvements in Rockets" was raised in July 1918 referring by then to the Royal Air Force. It was not published until February 1923 for security reasons.  Firing and guidance controls could be either wire or wireless. The propulsion and guidance rocket eflux emerged from the deflecting cowl at the nose. The 1950s IWM exhibition label states "Later in 1917, an electrically steered rocket was designed…. with the designed purpose of pursuing a hostile airman." A model of this dirigible rocket was included in this exhibition. The model was accompanied by a note: "Exhibit that is part of Professor AM Low's exhibits. Model of the wireless controlled dirigible rocket missile designed to pursue a hostile airman."
During World War One, work started on radio guided weapons at various establishments, such as the experiments of Capt. Cyril Percy Ryan at Hawkcraig Experimental Station (H.M.S. Tarlair). However, as control by the Munitions Inventions Department over military research was introduced, a centre for the Royal Flying Corps radio guided weapons was established. This was the secret Experimental Works in Feltham. The focus of their work was radio guided systems but the unit also assisted with other tasks. They were involved in testing the Pomeroy bullet and George Constantinescu's synchronization gear. They provided ‘distractions’ for the Zeebrugge raid and its Commanding Officer Archibald Low travelled to France and into neutral Spain during the war to debunk reports of ‘fantastic’ weapons. Low had at least 30 specialists under his command at Feltham supported by their contractors and suppliers. They had motor transport assets and military and police security. Their balloon facility was used to conducted radio reception experiments and they tested their equipment using aircraft with trailing aerials. Low was a qualified RFC Observer. His officers included his second in command Henry Jeffrey Poole, his radio engineer George William Mahoney Whitton, the talented inventor Ernest Windsor Bowen and the carburation specialist Louis Mantell.
Low was commended for this work by a number of senior officers including Sir David Henderson (the wartime commander of the RFC) and Admiral Edward Stafford Fitzherbert (Director of Mines and Torpedoes). Sir Henry Norman, 1st Baronet (Chairman of the War Office Committee on Wireless Telegraphy and at this time the Munitions Inventions Department's permanent attaché to the French Ministry of Inventions.) wrote to Low in March 1918, saying "I know of no man who has more extensive and more profound scientific knowledge, combined with a greater gift on imaginative invention than yourself."
Their work had started in 1915 the commercial motor garage business owned by Henry Poole. This was in Chiswick. During 1916 the development showed such promise that the RFC established their secret Experimental Works in premises commandeered from the Davis Paraffin Carburettor Company and the Duval Composition Company which were situated in the old Ivory Works in Feltham. Later these Experimental Works were moved to Archibald Low's own premises at 86 High Street, Feltham where all the Navy work was also carried out in 1918.
Details of the Feltham Experimental Works have survived in the records of a legal claim against Archibald Low. On 5 December 1917 he was accused of plagiarism and abuse of office by a civilian inventor Clifton West.
On 26 January 1918 colonel Ernest Swinton provided his friend Morgan with an assessment of Clifton West as "...a clever man and very ingenious, but tends towards the type of inventions "crank". He is also the most perfect mug in the world, as I have told him and is like a bit of toasted cheese to all the rats and crooks within a hundred miles : they smell him coming and get out their Bowie knives." The case against Archibald Low was not pursued.
Clifton's plagiarism case involved his Land Torpedo, a rolling cable drum device to snag and destroy barbed wire defences, similar to that patented under instruction from his superiors on behalf of the RFC by Archibald Low.
In 1917 the priority for Low's control system changed; the new imperative being to counter the submarine threat. Low and Ernest Bowen transferred into the Royal Navy to adapt the AT system to the airborne control of Royal Navy Distance Control Boats (DCB), a variant of the Coastal Motor Boat to be filled with an explosive charge. Thornycroft were contracted to design these new DCB’s (and the conversions of some of the existing CMBs) to carry this large and heavy explosive payload in the bow. The resulting craft was considered to be fragile though seaworthy (but only in fair weather). The AT work was documented and transferred to Royal Flying Corps radio unit at Biggin Hill. The Feltham Works were still under Low's command and this is where the redevelopment and production of equipment was carried out, clock-driven impulse senders for DCBs being ordered on 13 March 1918. The port/starboard demand from the controller's sender units in the aircraft caused a gyroscope on the boat to change the direction of its axis by "precession" to the "new" required heading. Any "difference" between the boats current heading and the required heading (i.e. the gyroscopes alignment) started an electric motor driving a worm gear in the appropriate direction to turn the rudder. This reduced any "difference" as the boat responded and acquired the new required heading. Thus any difference caused the boat to manoeuvre to keep it on the gyroscope's "required" heading, whether that difference occurred due to wave, wind or tide deflecting the boat or to control signal demands from the "mother" aircraft precessing the gyroscope.
Conversions of the 40-ft CMBs Number 3, 9 and 13 were three of the five DCBs built.
The extensive trials were successful and the DCB weapon was acknowledged to be "capable of control up to the moment of hitting." Admiral Edward Stafford Fitzherbert (Director of Mines and Torpedoes) stated on 18 March 1918 in a letter concerning Archibald Low's achievements during his Navy tour of duty that "Captain Low was gazetted as Lieut. Commander as from 2 October 1917 recommended by Sir David Henderson, Brig. General Caddell, Brig. General Pitcher and Major, Sir Henry Norman, M.P., P.C.", ... "He has assigned about 14 complete Patent to Services", ... "He has voluntarily lent his entire laboratory and staff to Admiralty etc. where manufacturing is now carried out." and "Three distinct inventions have now been accepted into service after being tested, namely...1. Complete sending control gear for D.C.B. 2. Electrical Gun Timing Apparatus 3. Gun Silencer audiometer Measuring Device".
The secret Distantly Controlled Boat (D.C.B.) Section of the Royal Navy's Signals School, Portsmouth was setup to develop aerial radio systems for the control of unmanned naval vessels from 'mother' aircraft. This D.C.B. Section was based at Calshot under the command of Eric Gascoigne Robinson VC. On 1 September 1917 George Callaghan Commander-in-Chief, The Nore was informed that significant shore, &amp; mooring facilities were to be made available for D.C.B. trials and rehearsals in the Thames Estuary. Airfield facilities were also requested in the area for the ‘mother’ aircraft. On 9 October 1917 the Deputy Director of Naval Construction, William Henry Gard assessed HMS Carron for use as a D.C.B. blocking ship but it was considered more suitable as a parent ship and floating repair depot for the D.C.B. Section. By then this D.C.B. Section had access to many vessels including a submarine and to the necessary support of aircraft, pilots and the trained radio control operators. They had conducted trials guiding unmanned boats into the busy waters around Portsmouth Harbour. Then between 28 May and 31 May 1918 Trials were undertaken by the Royal Navy Dover Command, using operators in Armstrong Whitworth aircraft Nos. 5082, 5117, in charge of Captain Tate, R.A.F. to control the boats. During these trials Acting Vice-Admiral Sir Roger J. B. Keyes and Rear-Admiral Cecil Frederick Dampier were on board one of these DCBs while it was remotely controlled. Trials included steering them through 'gates' created by motor launches anchored 60 yards apart. A significant number of high ranking and senior Admiralty, Naval and political officials are referenced in the surviving records.
Harbour blocking  and shipping attacks were considered prime targets for this new weapon. Considerable resources would have been required (and put at risk) to get DCBs within range to launch attacks and this had to be balanced against the chances of success. The launch range was based upon running for 2 hours at 30 knots, the time that the DBC engines could be operated without attention. 
Capt. Dudley Pound’s Admiralty Plans Division report of 6 April 1918 on operations with D.C.B’s controlled from aircraft began “It is considered that the time is now ripe to formulate concrete plans. It is assumed that 60 - 80 miles should be considered the maximum range possible at present under normal conditions. Owing to limitations of wave-lengths, four D.C.B's would be used at a time, but further relays of four boats could be sent at intervals of not less than 5 miles. The D.C.B"s. will contain an explosive charge considerably heavier than any modern Torpedo.” The targets he evaluated in detail were enemy vessels at Emden. Zeebrugge. Ostend, at enemy harbours in Adriatic, at Constantinople and its vicinity and at sea. The report states “As regards lock-gates, wharfs, piers, etc. These can be found at Emden, Zeebrugge, Ostend, etc. Targets on the Elbe are, at present, at rather long range unless it is feasible to employ aircraft in relays.” He states “These boats, with their heavy load of explosive, will tide over the time until suitable aircraft are produced which can carry a torpedo with a head capable of creating a decisive effect on capital ships.” and “If 3 or 4 Flotillas (of four each) of these boats were prepared, a continued attack might be made on Ostend” Following a request from The Commander-In-Chief Grand Fleet on 22 July 1918 the report of the Dover Trials assessed the employment of these boats in the Bight or for fleet operations and this report of the 27 September 1918 began with the declaration that stated "Wireless controlling gear for steering a vessel from an aircraft, ship or shore station, is an accomplished fact, and can probably be fitted to any type of vessel. Successful experiments have already been carried out with submarines, motor launches, and 40 foot coastal motor boats."  Their main sources of radio control developments were Captain Ryan at the Hawkcraig Experimental Station and Captain Low in the Feltham Experimental Works. However, the DCB Section accessed the work of others such as the Birmingham inventor George Joseph Dallison and the Russian Air Force officer Sergey Alekseevich Oulianine who was based in Paris at this time.
As an indication of the extent and urgency of the D.C.B. Section's work, Captain Low recorded the supply of aircraft radio control sender units for trials with DCB No's, 20, 21, 22, and 24 and in one letter stated "...it has meant a very large amount of over time and night work I think it will be necessary to give my men at least two days rest when once this complete device has been delivered to you."
Charles Penrose Rushton Coode, The Director of Operations Division (Foreign) suggested that operations would be impeded in Northern areas during the coming winter season. Commanders of the areas covering the targets assessed in the Plans Division report were advised of the capabilities of Distantly Controlled Boats including, on the 7 October 1918, Admiral of the Fleet, Sir Somerset Arthur Gough-Calthorpe, C-in-C, Mediterranean. No D.C.B. operations were mounted before the Armistice.
Before the Feltham Experimental Works were closed John Knowles Im Thurn who was at this time the Assistant Director of Electrical Torpedo and Mining wrote to Archibald Low on 19 May 1919 stating "It is a matter of great regret to me that the Armistice and consequent demobilisation came too soon for your enlarged establishment to fill the important place we had assigned to it, as an experimental offshoot of the Signal School, Portsmouth.....Your extraordinary ability and originality as a designer, combined with your sound scientific training will be a great loss to us.." The Works closed on 13 October 1919.
The Final Report of the Post-War Questions Committee, dated 27 March 1920, stated: "We have heard evidence that aircraft carrying high explosive charges are capable of being controlled by wireless as are the Distant Control Boats, but we do not consider that they will be a real menace to Capital Ships." The Questions Committee said on the subject of the DCBs that "it is difficult, if not impossible, for an enemy to interfere with the control by wireless jambing, since each boat works on a different wave length and the discovery of the wave length is a delicate operation" and "these weapons are already capable of being handled in numbers: two of them can be controlled by one aircraft, three of them have been manoeuvred close to one another simultaneously without mutual interference, and probably as many as eight can be handled in a group if the groups are not within about four miles of one another." The committee concluded the DCB weapon "is in a different category from all others in that it is capable of control up to the moment of hitting, and this fact alone justifies close attention to development" into ultimate form as "into a shallow or surface-running torpedo of great size". While they thought that "In its present state of development...that it is not a great menace to the Capital Ship", they said it merited "uninterrupted research both in the perfection of the weapon itself and in the preparation of counter measures".
In 1921 the R.A.E. resumed unmanned aircraft development, setting up the Radio Controlled Aircraft Committee.  Initially they used their ‘1917 Type Aerial Target’ aircraft refitted with a more powerful 45 h.p. Armstrong Siddeley Ounce engine. In 1925 they developed the `Larynx’. By January 1933 a Fairey Queen IIIF drone target survived unscathed through a major RN gunnery trial. Following further demonstrations using the Queen IIIF ('Faerie Queen') aircraft, the world's first fleet of drones was developed and these entered service in 1935. They were the de Havilland DH.82 Queen Bees. Their control system came out of the same First World War / R.A.E. stable as the original de Havilland 1917 Aerial Target and they were also launched from a pneumatically powered ramp. Over 400 of these were in service before WWII. They were used to test anti-aircraft defences. A 1939 article on the Queen Bee concluded "Twenty years is a long time, but the men who have designed and developed the radio-controlled target aircraft have made full use of that time. Furthermore, the experiments of those twenty years cannot be imitated in a matter of weeks. Not only is Great Britain many years in advance of every other country in this sphere, but she is also likely to remain ahead."
The next major development were the first US fleets of target drones during the Second World War. Four veterans of the RFC (and its successor, the Royal Air Force) link the 1917 Aerial Target to these subsequent US drone developments. Archibald Low's commanding officer on the RFC Aerial Target project was Duncan Pitcher. In 1921 he was Robert Loraine's best man. Loraine had a great deal in common with Reginald Denny who founded the Radioplane Company in California. Denny and Loraine were both British actors who had successful careers in the USA. They had been in a West End production together in 1902 in London. They were both veterans of the RFC, they both visited close relative living on the boundaries of Richmond Park in London and they were both flying and making films in Hollywood in the 1930s when Denny became interested in radio controlled aircraft. Denny's Radioplane, the 1940s company that made the first mass-produced drones for the US Army and Navy was eventually acquired by Northrop Grumman who make the RQ-4 Global Hawk drone.
The Royal Navy also continued to develop their remote radio control assets. The pre-dreadnought battleship HMS Agamemnon was converted into a remote control target ship in 1920.
On 29 June 1955 Low and Lord Brabazon presented a model of the AT and the various artefacts from the Feltham Unit to the Imperial War Museum for their planned exhibition. These included the control system that flew in March 1917.
The Royal Flying Corps' Aerial Target was the world's first drone unmanned aircraft (UAV) to fly under control from the ground. A photograph of this 1917 22 foot (6.7 metre) wingspan Aerial Target aircraft exists. Parts of it were saved by Low and these still exist as well as contemporary photographs although they are not on public display. One of the 1918 Distance Control Boats CMB9/DCB1 has been saved and carefully restored.
Until 2016 the RFC Aerial Target project was deemed by most sources to have failed and been terminated. The on-line images of the Imperial War Museum Feltham artefacts were not presented as a collection. Prior to 2019 no known source had published details of the Royal Flying Corps secret patents or demonstrated that they matched and described the items in this IWM collection. The Feltham Works re-application of their system to control the Royal Navy DCBs had not been established. Details of the mysterious Feltham Works were in the National Archives but not published. References to the post war influence of the Feltham Works success as it passed via Biggin Hill to the Royal Aircraft Establishment have now been researched. The suspected influence of Pitcher and Loraine on Denny's involvement with UAVs was recognised in 2019.  The Imperial War Museum now state... "The Aerial Target... became the first drone to fly under control when it was tested in March 1917. The pilot (in control of the flight from the ground) on this occasion was the future world speed record holder Henry Segrave".
During the First World War the Aerial Targets and subsequent DCBs were developed as ripostes to the Central Powers aerial bombing and naval blockade of Britain. The ATs involved the industrial efforts of at least three of the countries major aircraft companies along with the novel engine development of the "Gnat" engine by ABC Motors, the control system development by the Feltham Works and the integration and trials facilities of other RFC bases. The project was sustained over the worst years of the war when continued Munitions Inventions Department approval was required for such projects. The unit also provided radio controls for floating mines. The Feltham Works were one of the precursors of the R.A.E. who inherited the Feltham patents and AT hardware. They resumed development of Remote Piloted Vehicles through the interwar years, leading to the fleet of Queen Bee RPVs. In 1976 Low was inducted into the International Space Hall of Fame and has been called the "Father of Radio Guidance Systems".

Wing Commander Frank Arthur Brock OBE (29 June 1884 – 23 April 1918) was a British officer commissioned into the Royal Artillery, the Royal Naval Volunteer Reserve, the Royal Naval Air Service (RNAS) and finally, when the RNAS merged with the RFC, the Royal Air Force.  He invented the explosive bullet that destroyed the German Zeppelins and he devised and executed the smoke screen used during the Zeebrugge Raid on 23 April 1918, in the British Royal Navy's attempt to neutralize the key Belgian port of Bruges-Zeebrugge during the First World War.
Brock was born in South Norwood, Surrey, the son of Arthur Brock of Haredon, Sutton, Surrey, of the famous C.T. Brock &amp; Co. fireworks manufacturers. He was educated at Dulwich College where he blew up a stove in his form room. Brock joined the family business in 1901 (later becoming a director) where he remained until the outbreak of the First World War.
He originally joined the Royal Artillery, being commissioned as a temporary lieutenant on 10 October 1914, but within a month was loaned to the Navy, to which he transferred, becoming a temporary sub-lieutenant in the Royal Naval Volunteer Reserve on 27 October 1914. He was promoted to lieutenant on 31 December 1914, becoming a flight lieutenant of the Royal Naval Air Service on 1 January 1915. Brock was a member of the Admiralty Board of Invention and Research and founded, organized and commanded the Royal Navy Experimental Station at Stratford.
Among his many developments were:
By the time the Royal Naval Air Service merged with the Royal Flying Corps to form the Royal Air Force on 1 April 1918, Brock had risen to the rank of wing commander, and in January 1918 had been made an Officer of the Order of the British Empire (OBE) in the 1918 New Year Honours.
On the night of 22–23 April 1918, the Zeebrugge Raid began when an armada of British sailors and marines led by the old cruiser, HMS Vindictive, attacked the Mole at Zeebrugge, Belgium, in order to negate the serious threat to Allied shipping, that was being posed by the port being used by the Imperial German Navy as a base for their U-boats and light shipping. Brock brought on board with him a box marked 'Highly Explosive, Do Not Open' which actually contained bottles of vintage port which were drunk by his men. For the attack, Brock was in charge of the massive smoke screens that were to cover the approach of the raiding party:
At Zeebrugge, Brock, anxious to discover the secret of the German system of sound-ranging, begged permission to go ashore, not content to watch the action from an observation ship. He joined a storming party on the Mole and was killed in action.
There is an account of German sailor Hermann Künne being involved in a fight with an English officer. Künne attacked a British officer armed with a revolver and a cutlass. Künne was similarly armed with a cutlass. He slashed his opponent across the neck and grabbed the revolver. The British officer, desperately wounded, stabbed Künne as he fell. Given that the Victoria Cross citation for Lieutenant Commander Harrison makes no mention of a sword fight, there are those who believe that Brock was the British officer killed by Künne.
Brock received a mention in despatches from Vice-Admiral Sir Roger Keyes, for his distinguished services on the night of 22–23 April 1918, He is commemorated on the Zeebrugge Memorial, which stands in Zeebrugge Churchyard. The Zeebrugge Memorial commemorates Brock, one mechanic from Brock's group, and two other officers of the Royal Navy who died on the mole at Zeebrugge and have no known grave. His wife erected a memorial at Brookwood Cemetery, which commemorates him and her sisters two deceased husbands, all three of whom had served in the Royal Navy as officers.
Henry Major Tomlinson wrote of Wing Commander Brock: "A first-rate pilot and excellent shot, Commander Brock was a typical English sportsman; and his subsequent death during the operations, for whose success he had been so largely responsible, was a loss of the gravest description to both the Navy and the empire."
Gunpowder &amp; Glory is the first biography of Frank Brock.  Co-authored by his grandson Harry Smee and the established writer Henry Macrory, the book was published in 2020 by Casemate UK Ltd and explains the centuries of the Brock family, which began its firework enterprise in the 17th century and from which Frank Brock emerged in 1884.
Other books in which Brock appears include:

Robert Hutchings Goddard (October 5, 1882 – August 10, 1945) was an American engineer, professor, physicist, and inventor who is credited with creating and building the world's first liquid-fueled rocket. Goddard successfully launched his rocket on March 16, 1926, which ushered in an era of space flight and innovation. He and his team launched 34 rockets between 1926 and 1941, achieving altitudes as high as 2.6 km (1.6 mi) and speeds as fast as 885 km/h (550 mph).
Goddard's work as both theorist and engineer anticipated many of the developments that would make spaceflight possible. He has been called the man who ushered in the Space Age.: xiii  Two of Goddard's 214 patented inventions, a multi-stage rocket (1914), and a liquid-fuel rocket (1914), were important milestones toward spaceflight. His 1919 monograph A Method of Reaching Extreme Altitudes is considered one of the classic texts of 20th-century rocket science. Goddard successfully pioneered modern methods such as two-axis control (gyroscopes and steerable thrust) to allow rockets to control their flight effectively.
Although his work in the field was revolutionary, Goddard received little public support, moral or monetary, for his research and development work.: 92, 93  He was a shy person, and rocket research was not considered a suitable pursuit for a physics professor.: 12  The press and other scientists ridiculed his theories of spaceflight. As a result, he became protective of his privacy and his work.
Years after his death, at the dawn of the Space Age, Goddard came to be recognized as one of the founding fathers of modern rocketry, along with Robert Esnault-Pelterie, Konstantin Tsiolkovsky, and Hermann Oberth. He not only recognized early on the potential of rockets for atmospheric research, ballistic missiles and space travel but also was the first to scientifically study, design, construct and fly the precursory rockets needed to eventually implement those ideas.
NASA's Goddard Space Flight Center was named in Goddard's honor in 1959. He was also inducted into the International Aerospace Hall of Fame in 1966, and the International Space Hall of Fame in 1976.
Goddard was born in Worcester, Massachusetts, to Nahum Danford Goddard (1859–1928) and Fannie Louise Hoyt (1864–1920). Robert was their only child to survive; a younger son, Richard Henry, was born with a spinal deformity and died before his first birthday. Nahum  was employed by  manufacturers, and he invented several useful tools. Goddard had English paternal family roots in New England with William Goddard (1628–91) a London grocer who settled in Watertown, Massachusetts in 1666. On his maternal side he was descended from John Hoyt and other settlers of Massachusetts in the late 1600s.
Shortly after his birth, the family moved to Boston. With a curiosity about nature, he studied the heavens using a telescope from his father and observed the birds flying. Essentially a country boy, he loved the outdoors and hiking with his father on trips to Worcester and became an excellent marksman with a rifle.: 63, 64  In 1898, his mother contracted tuberculosis and they moved back to Worcester for the clear air. On Sundays, the family attended the Episcopal church, and Robert sang in the choir.: 16 
With the electrification of American cities in the 1880s, the young Goddard became interested in science—specifically, engineering and technology. When his father showed him how to generate static electricity on the family's carpet, the five-year-old's imagination was sparked. Robert experimented, believing he could jump higher if the zinc from a battery could be charged by scuffing his feet on the gravel walk. But, holding the zinc, he could jump no higher than usual.: 15  Goddard halted the experiments after a warning from his mother that if he succeeded, he could "go sailing away and might not be able to come back.": 9 
He experimented with chemicals and created a cloud of smoke and an explosion in the house.: 64 
Goddard's father further encouraged Robert's scientific interest by providing him with a telescope, a microscope, and a subscription to Scientific American.: 10  Robert developed a fascination with flight, first with kites and then with balloons. He became a thorough diarist and documenter of his work—a skill that would greatly benefit his later career. These interests merged at age 16, when Goddard attempted to construct a balloon out of aluminum, shaping the raw metal in his home workshop, and filling it with hydrogen. After nearly five weeks of methodical, documented efforts, he finally abandoned the project, remarking, "... balloon will not go up. ... Aluminum is too heavy. Failior   crowns enterprise." However, the lesson of this failure did not restrain Goddard's growing determination and confidence in his work.: 21  He wrote in 1927, "I imagine an innate interest in mechanical things was inherited from a number of ancestors  who were machinists.": 7 
He became interested in space when he read H. G. Wells' science fiction classic The War of the Worlds at 16 years old. His dedication to pursuing space flight became fixed on October 19, 1899. The 17-year-old Goddard climbed a cherry tree to cut off dead limbs. He was transfixed by the sky, and his imagination grew. He later wrote:
On this day I climbed a tall cherry tree at the back of the barn ... and as I looked toward the fields at the east, I imagined how wonderful it would be to make some device which had even the possibility of ascending to Mars, and how it would look on a small scale, if sent up from the meadow at my feet. I have several photographs of the tree, taken since, with the little ladder I made to climb it, leaning against it.
It seemed to me then that a weight whirling around a horizontal shaft, moving more rapidly above than below, could furnish lift by virtue of the greater centrifugal force at the top of the path.

I was a different boy when I descended the tree from when I ascended. Existence at last seemed very purposive.: 26 For the rest of his life, he observed October 19 as "Anniversary Day", a private commemoration of the day of his greatest inspiration.
The young Goddard was a thin and frail boy, almost always in fragile health. He suffered from stomach problems, pleurisy, colds, and bronchitis, and he fell two years behind his classmates. He became a voracious reader, regularly visiting the local public library to borrow books on the physical sciences.: 16, 19 
Goddard's interest in aerodynamics led him to study some of Samuel Langley's scientific papers in the periodical Smithsonian. In these papers, Langley wrote that birds flap their wings with different force on each side to turn in the air. Inspired by these articles, the teenage Goddard watched swallows and chimney swifts from the porch of his home, noting how subtly the birds moved their wings to control their flight. He noted how remarkably the birds controlled their flight with their tail feathers, which he called the birds' equivalent of ailerons. He took exception to some of Langley's conclusions and in 1901 wrote a letter to St. Nicholas magazine: 5  with his own ideas. The editor of  St. Nicholas declined to publish Goddard's letter, remarking that birds fly with a certain amount of intelligence and that "machines will not act with such intelligence.": 31  Goddard disagreed, believing that a man could control a flying machine with his own intelligence.
Around this time, Goddard read Newton's Principia Mathematica, and found that Newton's Third Law of Motion applied to motion in space. He wrote later about his own tests of the Law:
I began to realize that there might be something after all to Newton's Laws. The Third Law was accordingly tested, both with devices suspended by rubber bands and by devices on floats, in the little brook back of the barn, and the said law was verified conclusively. It made me realize that if a way to navigate space were to be discovered, or invented, it would be the result of a knowledge of physics and mathematics.: 32 As his health improved, Goddard continued his formal schooling as a 19-year-old sophomore at South High Community School in Worcester in 1901. He excelled in his coursework, and his peers twice elected him class president. Making up for  lost time, he studied books on mathematics, astronomy, mechanics and composition from the school library.: 32  At his graduation ceremony in 1904, he gave his class oration as valedictorian. In his speech, entitled "On Taking Things for Granted", Goddard included a section that would become emblematic of his life:
 ust as in the sciences we have learned that we are too ignorant to safely pronounce anything impossible, so for the individual, since we cannot know just what are his limitations, we can hardly say with certainty that anything is necessarily within or beyond his grasp. Each must remember that no one can predict to what heights of wealth, fame, or usefulness he may rise until he has honestly endeavored, and he should derive courage from the fact that all sciences have been, at some time, in the same condition as he, and that it has often proved true that the dream of yesterday is the hope of today and the reality of tomorrow.: 19  Goddard enrolled at Worcester Polytechnic Institute in 1904.: 41  He quickly impressed the head of the physics department, A. Wilmer Duff, with his thirst for knowledge, and Duff took him on as a laboratory assistant and tutor.: 42  At WPI, Goddard joined the Sigma Alpha Epsilon fraternity and began a long courtship with high school classmate Miriam Olmstead, an honor student who had graduated with him as salutatorian. Eventually, she and Goddard were engaged, but they drifted apart and ended the engagement around 1909.: 51 
Goddard received his B.S. degree in physics from Worcester Polytechnic in 1908,: 50  and after serving there for a year as an instructor in physics, he began his graduate studies at Clark University in Worcester in the fall of 1909. Goddard received his M.A. degree in physics from Clark University in 1910, and then stayed at Clark to complete his Ph.D. in physics in 1911. He spent another year at Clark as an honorary fellow in physics, and in 1912 he accepted a research fellowship at Princeton University's Palmer Physical Laboratory.: 56–58 
The high school student summed up his ideas on space travel in a proposed article, "The Navigation of Space," which he submitted to the Popular Science News. The journal's editor returned it, saying that they could not use it "in the near future.": 34 
While still an undergraduate, Goddard wrote a paper proposing a method for balancing airplanes using gyro-stabilization. He submitted the idea to Scientific American, which published the paper in 1907. Goddard later wrote in his diaries that he believed his paper was the first proposal of a way to automatically stabilize aircraft in flight.: 50  His proposal came around the same time as other scientists were making breakthroughs in developing functional gyroscopes.
While studying physics at WPI, ideas came to Goddard's mind that sometimes seemed impossible, but he was compelled to record them for future investigation. He wrote that "there was something inside which simply would not stop working." He purchased some cloth-covered notebooks and began filling them with a variety of thoughts, mostly concerning his dream of space travel.: 11–13  He considered centrifugal force, radio waves, magnetic reaction, solar energy, atomic energy, ion or electrostatic propulsion and other methods to reach space. After experimenting with solid fuel rockets he was convinced by 1909 that chemical-propellant engines were the answer.: 11–12  A particularly complex concept was set down in June 1908: Sending a camera around distant planets, guided by measurements of gravity along the trajectory, and returning to earth.: 14 
His first writing on the possibility of a liquid-fueled rocket came on February 2, 1909. Goddard had begun to study ways of increasing a rocket's efficiency using methods differing from conventional solid-fuel rockets. He wrote in his notebook about using liquid hydrogen as a fuel with liquid oxygen as the oxidizer. He believed that 50 percent efficiency could be achieved with these liquid propellants (i.e., half of the heat energy of combustion converted to the kinetic energy of the exhaust gases).: 55 
In the decades around 1910, radio was a new technology, fertile for innovation. In 1912, while working at Princeton University, Goddard investigated the effects of radio waves on insulators. In order to generate radio-frequency power, he invented a vacuum tube with a beam deflection that operated like a cathode-ray oscillator tube. His patent on this tube, which predated that of Lee De Forest, became central in the suit between Arthur A. Collins, whose small company made radio transmitter tubes, and AT&amp;T and RCA over his use of vacuum tube technology.  Goddard accepted only a consultant's fee from Collins when the suit was dropped. Eventually, the two big companies allowed the country's growing electronics industry to use the De Forest patents freely.
By 1912 he had in his spare time, using calculus, developed the mathematics which allowed him to calculate the position and velocity of a rocket in vertical flight, given the weight of the rocket and weight of the propellant and the velocity (with respect to the rocket frame) of the exhaust gases. In effect he had independently developed the Tsiolkovsky rocket equation published a decade earlier in Russia. Tsiolkovsky, however,  did not account for gravity nor drag. For vertical flight from the surface of Earth Goddard included in his differential equation the effects of gravity and aerodynamic drag.: 136  He wrote: "An approximate method was found necessary ... in order to avoid an unsolved problem in the calculus of variations. The solution that was obtained revealed the fact that surprisingly small initial masses would be necessary ... provided the gases were ejected from the rocket at a high velocity, and also provided that most of the rocket consisted of propellant material.": 338–9 
His first goal was to build a sounding rocket with which to study the atmosphere. Not only would such investigation aid meteorology, but it was necessary to determine temperature, density and wind speed as functions of altitude in order to design efficient space launch vehicles. He was very reluctant to admit that his ultimate goal was, in fact, to develop a vehicle for flights into space, since most scientists, especially in the United States, did not consider such a goal to be a realistic or practical scientific pursuit, nor was the public yet ready to seriously consider such ideas. Later, in 1933, Goddard said that "n no case must we allow ourselves to be deterred from the achievement of space travel, test by test and step by step, until one day we succeed, cost what it may.": 65, 67, 74, 101 
In early 1913, Goddard became seriously ill with tuberculosis and had to leave his position at Princeton. He then returned to Worcester, where he began a prolonged process of recovery at home. His doctors did not expect him to live. He decided he should spend time outside in the fresh air and walk for exercise, and he gradually improved.: 61–64  When his nurse discovered some of his notes in his bed, he kept them, arguing, "I have to live to do this work.": 66 
It was during this period of recuperation, however, that Goddard began to produce some of his most important work. As his symptoms subsided, he allowed himself to work an hour per day with his notes made at Princeton. He was afraid that nobody would be able to read his scribbling should he
succumb.: 63 
In the technological and manufacturing atmosphere of Worcester, patents were considered essential, not only to protect original work but as documentation of first discovery. He began to see the importance of his ideas as intellectual property, and thus began to secure those ideas before someone else did—and he would have to pay to use them. In May 1913, he wrote descriptions concerning his first rocket patent applications. His father brought them to a patent lawyer in  Worcester who helped him to refine his ideas for consideration. Goddard's first patent application was submitted in October 1913.: 63–70 
In 1914, his first two landmark patents were accepted and registered. The first, U.S. Patent 1,102,653, described a multi-stage rocket fueled with a solid "explosive material." The second, U.S. Patent 1,103,503, described a rocket fueled with a solid fuel (explosive material) or with liquid propellants (gasoline and liquid nitrous oxide). The two patents would eventually become important milestones in the history of rocketry. Overall, 214 patents were published, some posthumously by his wife.
In the fall of 1914 Goddard's health had improved, and he accepted a part-time position as an instructor and research fellow at Clark University.: 73  His position at Clark allowed him to further his rocketry research. He ordered numerous supplies that could be used to build rocket prototypes for launch and spent much of 1915 in preparation for his first tests. Goddard's first test launch of a powder rocket came on an early evening in 1915 following his daytime classes at Clark.: 74  The launch was loud and bright enough to arouse the alarm of the campus janitor, and Goddard had to reassure him that his experiments, while being serious study, were also quite harmless. After this incident Goddard took his experiments inside the physics lab in order to limit any disturbance.
At the Clark physics lab Goddard conducted static tests of powder rockets to measure their thrust and efficiency. He found his earlier estimates to be verified; powder rockets were converting only about two percent of the thermal energy in their fuel into thrust and kinetic energy. At this point he applied de Laval nozzles, which were generally used with steam turbine engines, and these greatly improved efficiency. (Of the several definitions of rocket efficiency, Goddard measured in his laboratory what is today called the internal efficiency of the engine: the ratio of the kinetic energy of the exhaust gases to the available thermal energy of combustion, expressed as a percentage.): 130  By mid-summer of 1915 Goddard had obtained an average efficiency of 40 percent with a nozzle exit velocity of 6,728 feet (2,051 meters) per second.: 75  Connecting a combustion chamber full of gunpowder to various converging-diverging expansion (de Laval) nozzles, Goddard was able in static tests to achieve engine efficiencies of more than 63% and exhaust velocities of over 7,000 feet (2,134 meters) per second.: 78 
Few would recognize it at the time, but this little engine was a major breakthrough. These experiments suggested that rockets could be made powerful enough to escape Earth and travel into space. This engine and subsequent experiments sponsored by the Smithsonian Institution were the beginning of modern rocketry and, ultimately, space exploration. Goddard realized, however, that it would take the more efficient liquid propellants to reach space.
Later that year, Goddard designed an elaborate experiment at the Clark physics lab and proved that a rocket would perform in a vacuum such as that in space. He believed it would, but many other scientists were not yet convinced. His experiment demonstrated that a rocket's performance actually decreases under atmospheric pressure.
In September 1906 he wrote in his notebook about using the repulsion of electrically charged particles (ions) to produce thrust.: 13  From 1916 to 1917, Goddard built and tested the first known experimental ion thrusters, which he thought might be used for propulsion in the near-vacuum conditions of outer space. The small glass engines he built were tested at atmospheric pressure, where they generated a stream of ionized air.
By 1916, the cost of Goddard's rocket research had become too great for his modest teaching salary to bear.: 76  He began to solicit potential sponsors for financial assistance, beginning with the Smithsonian Institution, the National Geographic Society, and the Aero Club of America.
In his letter to the Smithsonian in September 1916, Goddard claimed he had achieved a 63% efficiency and a nozzle velocity of almost 2438 meters per second. With these performance levels, he believed a rocket could vertically lift a weight of 1 lb (0.45 kg) to a height of  232 miles (373 km) with an initial launch weight of only 89.6 lbs (40.64 kg). (Earth's atmosphere can be considered to end at 80 to 100 miles (130 to 160 km) altitude, where its drag effect on orbiting satellites becomes minimal.)
The Smithsonian was interested and asked Goddard to elaborate upon his initial inquiry. Goddard responded with a detailed manuscript he had already prepared, entitled A Method of Reaching Extreme Altitudes.: 79 
In January 1917, the Smithsonian agreed to provide Goddard with a five-year grant totaling US$5000.: 84  Afterward, Clark was able to contribute US$3500  and the use of their physics lab to the project. Worcester Polytechnic Institute also allowed him to use its abandoned Magnetics Laboratory on the edge of campus during this time, as a safe place for testing.: 85  WPI also made some parts in their machine shop.
Goddard's fellow Clark scientists were astonished at the unusually large Smithsonian grant for rocket research, which they thought was not real science.: 85  Decades later, rocket scientists who knew how much it cost to research and develop rockets said that he had received little financial support.
Two years later, at the insistence of Dr. Arthur G. Webster, the world-renowned head of Clark's physics department,  Goddard arranged for the Smithsonian to publish the paper,  A Method..., which documented his work.: 102 
While at Clark University, Goddard did research into solar power using a parabolic dish to concentrate the Sun's rays on a machined piece of quartz, that was sprayed with mercury, which then heated water and drove an electric generator. Goddard believed his invention had overcome all the obstacles that had previously defeated other scientists and inventors, and he had his findings published in the November 1929 issue of  Popular Science.
Not all of Goddard's early work was geared toward space travel. As the United States entered World War I in 1917, the country's universities began to lend their services to the war effort. Goddard believed his rocket research could be applied to many different military applications, including mobile artillery, field weapons and naval torpedoes. He made proposals to the Navy and Army. No record exists in his papers of any interest by the Navy to Goddard's inquiry. However, Army Ordnance was quite interested, and Goddard met several times with Army personnel.: 89 
During this time, Goddard was also contacted, in early 1918, by a civilian industrialist in Worcester about the possibility of manufacturing rockets for the military. However, as the businessman's enthusiasm grew, so did Goddard's suspicion. Talks eventually broke down as Goddard began to fear his work might be appropriated by the business. However, an Army Signal Corps officer tried to make Goddard cooperate, but he was called off by General George Squier of the Signal Corps who had been contacted by Secretary of the Smithsonian Institution, Charles Walcott.: 89–91  Goddard became leery of working with corporations and was careful to secure patents to "protect his ideas.": 152  These events led to the Signal Corps sponsoring Goddard's work during World War I.: 91 
Goddard proposed to the Army an idea for a tube-based rocket launcher as a light infantry weapon. The launcher concept became the precursor to the bazooka.: 92  The rocket-powered, recoil-free weapon was the brainchild of Goddard as a side project (under Army contract) of his work on rocket propulsion. Goddard, during his tenure at Clark University, and working at Mount Wilson Observatory for security reasons, designed the tube-fired rocket for military use during World War I. He and his co-worker Dr. Clarence N. Hickman successfully demonstrated his rocket to the U.S. Army Signal Corps at Aberdeen Proving Ground, Maryland, on November 6, 1918, using two music stands for a launch platform. The Army was impressed, but the Compiègne Armistice was signed only five days later, and further development was discontinued as World War I ended.
The delay in the development of the bazooka and other weapons was a result of the long recovery period required from Goddard's serious bout with tuberculosis. Goddard continued to be a part-time consultant to the U.S. Government at Indian Head, Maryland,: 121  until 1923, but his focus had turned to other research involving rocket propulsion, including work with liquid fuels and liquid oxygen.
Later, the former Clark University researcher Dr. Clarence N. Hickman and Army officers Col. Leslie Skinner and Lt. Edward Uhl continued Goddard's work on the bazooka. A shaped-charge warhead was attached to the rocket, leading to the tank-killing weapon used in World War II and to many other powerful rocket weapons.: 305 
In 1919 Goddard thought that it would be premature to disclose the results of his experiments because his engine was not sufficiently developed. Dr. Webster realized that Goddard had accomplished a good deal of fine work and insisted that Goddard publish his progress so far or he would take care of it himself, so Goddard asked the Smithsonian Institution if it would publish the report, updated with notes, that he had submitted in late 1916.: 102 
In late 1919, the Smithsonian published Goddard's groundbreaking work, A Method of Reaching Extreme Altitudes. The report describes Goddard's mathematical theories of rocket flight, his experiments with solid-fuel rockets, and the possibilities he saw of exploring Earth's atmosphere and beyond. Along with Konstantin Tsiolkovsky's earlier work, The Exploration of Cosmic Space by Means of Reaction Devices, which was not widely disseminated outside Russia, Goddard's report is regarded as one of the pioneering works of the science of rocketry, and 1750 copies were distributed worldwide. Goddard also sent a copy to individuals who requested one, until his personal supply was exhausted. Smithsonian aerospace historian Frank Winter said that this paper was "one of the key catalysts behind the international rocket movement of the 1920s and 30s."
Goddard described extensive experiments with solid-fuel rocket engines burning high-grade nitrocellulose smokeless powder. A critical breakthrough was the use of the steam turbine nozzle invented by the Swedish inventor Gustaf de Laval. The de Laval nozzle allows the most efficient (isentropic) conversion of the energy of hot gases into forward motion. By means of this nozzle, Goddard increased the efficiency of his rocket engines from two percent to 64 percent and obtained supersonic exhaust velocities of over Mach 7.: 44 
Though most of this work dealt with the theoretical and experimental relations between propellant, rocket mass, thrust, and velocity, a final section, entitled "Calculation of minimum mass required to raise one pound to an 'infinite' altitude," discussed the possible uses of rockets, not only to reach the upper atmosphere but to escape from Earth's gravitation altogether. He determined, using an approximate method to solve his differential equation of motion for vertical flight, that a rocket with an effective exhaust velocity (see specific impulse) of 7000 feet per second and an initial weight of 602 pounds would be able to send a one-pound payload to an infinite height. Included as a thought experiment was the idea of launching a rocket to the Moon and igniting a mass of flash powder on its surface, so as to be visible through a telescope. He discussed the matter seriously, down to an estimate of the amount of powder required. Goddard's conclusion was that a rocket with starting mass of 3.21 tons could produce a flash "just visible" from Earth, assuming a final payload weight of 10.7 pounds.
Goddard eschewed publicity, because he did not have time to reply to criticism of his work, and his imaginative ideas about space travel were shared only with private groups he trusted. He did, though, publish and talk about the rocket principle and sounding rockets, since these subjects were not too "far out." In a letter to the Smithsonian, dated March 1920, he discussed: photographing the Moon and planets from rocket-powered fly-by probes, sending messages to distant civilizations on inscribed metal plates, the use of solar energy in space, and the idea of high-velocity ion propulsion. In that same letter, Goddard clearly describes the concept of the ablative heat shield, suggesting the landing apparatus be covered with "layers of a very infusible hard substance with layers of a poor heat conductor between" designed to erode in the same way as the surface of a meteor.
Every vision is a joke until the first man accomplishes it; once realized, it becomes commonplace.
–Response to a reporter's question following criticism in The New York Times, 1920.
The publication of Goddard's document gained him national attention from U.S. newspapers, most of it negative. Although Goddard's discussion of targeting the moon was only a small part of the work as a whole (eight lines on the next to last page of 69 pages), and was intended as an illustration of the possibilities rather than a declaration of intent, the papers sensationalized his ideas to the point of misrepresentation and ridicule. Even the Smithsonian had to abstain from publicity because of the amount of ridiculous correspondence received from the general public.: 113  David Lasser, who co-founded the American Rocket Society (ARS), wrote in 1931 that Goddard was subjected in the press to the "most violent attacks."
On January 12, 1920, a front-page story in The New York Times, "Believes Rocket Can Reach Moon", reported a Smithsonian press release about a "multiple-charge, high-efficiency rocket." The chief application envisaged was "the possibility of sending recording apparatus to moderate and extreme altitudes within the Earth's atmosphere", the advantage over balloon-carried instruments being ease of recovery, since "the new rocket apparatus would go straight up and come straight down." But it also mentioned a proposal "to  to the dark part of the new moon a sufficiently large amount of the most brilliant flash powder which, in being ignited on impact, would be plainly visible in a powerful telescope. This would be the only way of proving that the rocket had really left the attraction of the earth, as the apparatus would never come back, once it had escaped that attraction."
On January 13, 1920, the day after its front-page story about Goddard's rocket, an unsigned New York Times editorial, in a section entitled "Topics of the Times", scoffed at the proposal. The article, which bore the title "A Severe Strain on Credulity", began with apparent approval, but soon went on to cast serious doubt:
As a method of sending a missile to the higher, and even highest, part of the earth's atmospheric envelope, Professor Goddard's multiple-charge rocket is a practicable, and therefore promising device. Such a rocket, too, might carry self-recording instruments, to be released at the limit of its flight, and conceivable parachutes would bring them safely to the ground. It is not obvious, however, that the instruments would return to the point of departure; indeed, it is obvious that they would not, for parachutes drift exactly as balloons do.The article pressed further on Goddard's proposal to launch rockets beyond the atmosphere:
fter the rocket quits our air and really starts on its longer journey, its flight would be neither accelerated nor maintained by the explosion of the charges it then might have left. To claim that it would be is to deny a fundamental law of dynamics, and only Dr. Einstein and his chosen dozen, so few and fit, are licensed to do that. ... Of course,  only seems to lack the knowledge ladled out daily in high schools.The basis of that criticism was the then-common belief that thrust was produced by the rocket exhaust pushing against the atmosphere; Goddard realized that Newton's third law (reaction) was the actual principle and that thrust was possible in a vacuum.
A week after the New York Times editorial, Goddard released a signed statement to the Associated Press, attempting to restore reason to what had become a sensational story:
Too much attention has been concentrated on the proposed flash power experiment, and too little on the exploration of the atmosphere. ... Whatever interesting possibilities there may be of the method that has been proposed, other than the purpose for which it was intended, no one of them could be undertaken without first exploring the atmosphere.In 1924, Goddard published an article, "How my speed rocket can propel itself in vacuum", in Popular Science, in which he explained the physics and gave details of the vacuum experiments he had performed to prove the theory. But, no matter how he tried to explain his results, he was not understood by the majority. After one of Goddard's experiments in 1929, a local Worcester newspaper carried the mocking headline "Moon rocket misses target by 238,799.mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}1⁄2 miles."
Though the unimaginative public chuckled at the "moon man," his groundbreaking paper was read seriously by many rocketeers in America, Europe, and Russia who were stirred to build their own rockets. This work was his most important contribution to the quest to "aim for the stars.": 50 
Goddard worked alone with just his team of mechanics and machinists for many years.  This was a result of the harsh criticism from the media and other scientists, and his understanding of the military applications which foreign powers might use. Goddard became increasingly suspicious of others and often worked alone, except during the two World Wars, which limited the impact of much of his work. Another limiting factor was the lack of support from the American government, military and academia, all failing to understand the value of the rocket to study the atmosphere and near space, and for military applications. 

Nevertheless, Goddard had some influence on European rocketry pioneers like Hermann Oberth and his student Max Valier, at least as proponent of the idea of space rocketry and source of inspiration, although each side developed their technology and its scientific basis independently.  Eventually Fritz von Opel was instrumental in popularizing rockets as means of propulsion for vehicles. In the 1920s, he initiated together with Max Valier, co-founder of the "Verein für Raumschiffahrt", the world's first rocket program, Opel-RAK, leading to speed records for automobiles, rail vehicles and the first manned rocket-powered flight in September of 1929. Months earlier in 1928, one of his rocket-powered prototypes, the Opel RAK2, reached piloted by von Opel himself at the AVUS speedway in Berlin a record speed of 238 km/h, watched by 3000 spectators and world media, among them Fritz Lang, director of Metropolis and Woman in the Moon, world boxing champion Max Schmeling and many more sports and show business celebrities. A world record for rail vehicles was reached with RAK3 and a top speed of 256 km/h. After these successes, von Opel piloted the world's first public rocket-powered flight using Opel RAK.1, a rocket plane designed by Julius Hatry. World media reported on these efforts, including UNIVERSAL Newsreel of the US, causing as "Raketen-Rummel" or "Rocket Rumble" immense global public excitement, and in particular in Germany, where inter alia Wernher von Braun was highly influenced. The Great Depression led to an end of the Opel-RAK program, but Max Valier continued the efforts. After switching from solid-fuel to liquid-fuel rockets, he died while testing and is considered the first fatality of the dawning space age. As an 18-year-old von Braun also became a student of Oberth and eventually the head of the Nazi era rocket program.
As Germany became ever more war-like, Goddard  refused to communicate with German rocket experimenters, though he received more and more of their correspondence.: 131  Via Wernher von Braun and his team joining the US post-war programs there is nevertheless an indirect line of scientific and technology tradition from NASA back to Goddard.
Forty-nine years after its editorial mocking Goddard, on July 17, 1969—the day after the launch of Apollo 11—The New York Times published a short item under the headline "A Correction." The three-paragraph statement summarized its 1920 editorial and concluded:
Further investigation and experimentation have confirmed the findings of Isaac Newton in the 17th Century and it is now definitely established that a rocket can function in a vacuum as well as in an atmosphere. The Times regrets the error.Goddard began considering liquid propellants, including hydrogen and oxygen, as early as 1909. He knew that hydrogen and oxygen was the most efficient fuel/oxidizer combination. Liquid hydrogen was not readily available in 1921, however, and he selected gasoline as the safest fuel to handle.: 13 
Goddard began experimenting with liquid oxidizer, liquid fuel rockets in September 1921, and successfully tested the first liquid propellant engine in November 1923.: 520  It had a cylindrical combustion chamber, using impinging jets to mix and atomize liquid oxygen and gasoline.: 499–500 
In 1924–25, Goddard had problems developing a high-pressure piston pump to send fuel to the combustion chamber. He wanted to scale up the experiments, but his funding would not allow such growth. He decided to forego the pumps and use a pressurized fuel feed system applying pressure to the fuel tank from a tank of inert gas, a technique that is still used today. The liquid oxygen, some of which evaporated,  provided its own pressure.
On December 6, 1925, he tested the simpler pressure feed system. He conducted a static test on the firing stand at the Clark University physics laboratory. The engine successfully lifted its own weight in a 27-second test in the static rack. It was a major success for Goddard, proving that a liquid fuel rocket was possible.: 140  The test moved Goddard an important step closer to launching a rocket with liquid fuel.
Goddard conducted an additional test in December, and two more in January 1926. After that, he began preparing for a possible launch of the rocket system.
Goddard launched the world's first liquid-fueled (gasoline and liquid oxygen) rocket on March 16, 1926, in Auburn, Massachusetts. Present at the launch were his crew chief Henry Sachs, Esther Goddard, and Percy Roope, who was Clark's assistant professor in the physics department. Goddard's diary entry of the event was notable for its understatement:
March 16. Went to Auburn with S in am. E and Mr. Roope came out at 1 p.m. Tried rocket at 2.30. It rose 41 feet &amp; went 184 feet, in 2.5 secs., after the lower half of the nozzle burned off. Brought materials to lab. ...: 143 His diary entry the next day elaborated:
March 17, 1926. The first flight with a rocket using liquid propellants was made yesterday at Aunt Effie's farm in Auburn. ...
Even though the release was pulled, the rocket did not rise at first, but the flame came out, and there was a steady roar. After a number of seconds it rose, slowly until it cleared the frame, and then at express train speed, curving over to the left, and striking the ice and snow, still going at a rapid rate.: 143 The rocket, which was later dubbed "Nell", rose just 41 feet during a 2.5-second flight that ended 184 feet away in a cabbage field, but it was an important demonstration that liquid fuels and oxidizers were possible propellants for larger rockets. The launch site is now a National Historic Landmark, the Goddard Rocket Launching Site.
Viewers familiar with more modern rocket designs may find it difficult to distinguish the rocket from its launching apparatus in the well-known picture of "Nell". The complete rocket is significantly taller than Goddard but does not include the pyramidal support structure which he is grasping. The rocket's combustion chamber is the small cylinder at the top; the nozzle is visible beneath it. The fuel tank, which is also part of the rocket, is the larger cylinder opposite Goddard's torso. The fuel tank is directly beneath the nozzle and is protected from the motor's exhaust by an asbestos cone. Asbestos-wrapped aluminum tubes connect the motor to the tanks, providing both support and fuel transport. This layout is no longer used, since the experiment showed that this was no more stable than placing the combustion chamber and nozzle at the base. By May, after a series of modifications to simplify the plumbing, the combustion chamber and nozzle were placed in the now classic position, at the lower end of the rocket.: 259 
Goddard determined early that fins alone were not sufficient to stabilize the rocket in flight and keep it on the desired trajectory in the face of winds aloft and other disturbing forces. He added movable vanes in the exhaust, controlled by a gyroscope, to control and steer his rocket. (The Germans used this technique in their V-2.) He also introduced the more efficient swiveling engine in several rockets, basically the method used to steer large liquid-propellant missiles and launchers today.: 263–6 
After launch of one of Goddard's rockets in July 1929 again gained the attention of the newspapers, Charles Lindbergh learned of his work in a New York Times article. At the time, Lindbergh had begun to wonder what would become of aviation (even space flight) in the distant future and had settled on jet propulsion and rocket flight as a probable next step. After checking with the Massachusetts Institute of Technology (MIT) and being assured that Goddard was a bona fide physicist and not a crackpot, he phoned Goddard in November 1929.: 141  Professor Goddard met the aviator soon after in his office at Clark University. Upon meeting Goddard, Lindbergh was immediately impressed by his research, and Goddard was similarly impressed by the flier's interest. He discussed his work openly with Lindbergh, forming an alliance that would last for the rest of his life. While having long since become reluctant to share his ideas, Goddard showed complete openness with those few who shared his dream, and whom he felt he could trust.
By late 1929, Goddard had been attracting additional notoriety with each rocket launch. He was finding it increasingly difficult to conduct his research without unwanted distractions. Lindbergh discussed finding additional financing for Goddard's work and lent his famous name to Goddard's work. In 1930 Lindbergh made several proposals to industry and private investors for funding, which proved all but impossible to find following the recent U.S. stock market crash in October 1929.
In the spring of 1930, Lindbergh finally found an ally in the Guggenheim family. Financier Daniel Guggenheim agreed to fund Goddard's research over the next four years for a total of $100,000 (~$2 million today). The Guggenheim family, especially Harry Guggenheim, would continue to support Goddard's work in the years to come. The Goddards soon moved to Roswell, New Mexico
Because of the military potential of the rocket, Goddard, Lindbergh, Harry Guggenheim, the Smithsonian Institution and others tried in 1940, before the U.S. entered World War II, to convince the Army and Navy of its value. Goddard's services were offered, but there was no interest, initially. Two young, imaginative military officers eventually got the services to attempt to contract with Goddard just prior to the war. The Navy beat the Army to the punch and secured his services to build variable-thrust, liquid-fueled rocket engines for jet-assisted take-off (JATO) of aircraft.: 293–297  These rocket engines were the precursors to the larger throttlable rocket plane engines that helped launch the space age.
Astronaut Buzz Aldrin wrote that his father, Edwin Aldrin Sr. "was an early supporter of Robert Goddard."  The elder Aldrin was a student of physics under Goddard at Clark, and worked with Lindbergh to obtain the help of the Guggenheims. Buzz believed that if Goddard had received military support as Wernher von Braun's team had enjoyed in Germany, American rocket technology would have developed much more rapidly in World War II.
Before World War II there was a lack of vision and serious interest in the United States concerning the potential of rocketry, especially in Washington. Although the Weather Bureau was interested beginning in 1929 in Goddard's rocket for atmospheric research, the Bureau could not secure governmental funding.: 719, 746  Between the World Wars, the Guggenheim Foundation was the main source of funding for Goddard's research.: 46, 59, 60  Goddard's liquid-fueled rocket was neglected by his country, according to aerospace historian Eugene Emme, but was noticed and advanced by other nations, especially the Germans.: 63  Goddard showed remarkable prescience in 1923 in a letter to the Smithsonian. He knew that the Germans were very interested in rocketry and said he "would not be surprised if the research would become something in the nature of a race," and he wondered how soon the European "theorists" would begin to build rockets.: 136 
In 1936, the U.S. military attaché in Berlin asked Charles Lindbergh to visit Germany and learn what he could of their progress in aviation. Although the Luftwaffe showed him their factories and were open concerning their growing airpower, they were silent on the subject of rocketry. When Lindbergh told Goddard of this behavior, Goddard said, "Yes, they must have plans for the rocket. When will our own people in Washington listen to reason?": 272 
Most of the U.S.'s largest universities were also slow to realize rocketry's potential. Just before World War II, the head of the aeronautics department at MIT, at a meeting held by the Army Air Corps to discuss project funding, said that the California Institute of Technology (Caltech) "can take the Buck Rogers Job ." In 1941, Goddard tried to recruit an engineer for his team from MIT but couldn't find one who was interested.: 326   There were some exceptions: MIT was at least teaching basic rocketry,: 264  and Caltech had courses in rocketry and aerodynamics. After the war, Dr. Jerome Hunsaker of MIT, having studied Goddard's patents, stated that "Every liquid-fuel rocket that flies is a Goddard rocket.": 363 
While away in Roswell, Goddard was still head of the physics department at Clark University, and Clark allowed him to devote most of his time to rocket research. Likewise, the University of California, Los Angeles (UCLA) permitted astronomer Samuel Herrick to pursue research in space vehicle guidance and control, and shortly after the war to teach courses in spacecraft guidance and orbit determination. Herrick began corresponding with Goddard in 1931 and asked if he should work in this new field, which he named astrodynamics. Herrick said that Goddard had the vision to advise and encourage him in his use of celestial mechanics "to anticipate the basic problem of space navigation." Herrick's work contributed substantially to America's readiness to control flight of Earth satellites and send men to the Moon and back.
With new financial backing, Goddard eventually relocated to Roswell, New Mexico, in summer of 1930,: 46  where he worked with his team of technicians in near-isolation and relative secrecy for years. He had consulted a meteorologist as to the best area to do his work, and Roswell seemed ideal. Here they would not endanger anyone, would not be bothered by the curious and would experience a more moderate climate (which was also better for Goddard's health).: 177  The locals valued personal privacy, knew Goddard desired his, and when travelers asked where Goddard's facilities were located, they would likely be misdirected.: 261 
By September 1931, his rockets had the now familiar appearance of a smooth casing with tail-fins. He began experimenting with gyroscopic guidance and made a flight test of such a system in April 1932. A gyroscope mounted on gimbals electrically controlled steering vanes in the exhaust, similar to the system used by the German V-2 over 10 years later. Though the rocket crashed after a short ascent, the guidance system had worked, and Goddard considered the test a success.: 193–5 
A temporary loss of funding from the Guggenheims, as a result of the depression, forced Goddard in spring of 1932 to return to his much-loathed professorial responsibilities at Clark University. He remained at the university until the autumn of 1934, when funding resumed. Because of the death of the senior Daniel Guggenheim, the management of funding was taken on by his son, Harry Guggenheim. Upon his return to Roswell, he began work on his A series of rockets, 4 to 4.5 meters long, and powered by gasoline and liquid oxygen pressurized with nitrogen. The gyroscopic control system was housed in the middle of the rocket, between the propellant tanks.: xv, 15–46 
The A-4 used a simpler pendulum system for guidance, as the gyroscopic system was being repaired. On March 8, 1935, it flew up to 1,000 feet, then turned into the wind and, Goddard reported, "roared in a powerful descent across the prairie, at close to, or at, the speed of sound." On March 28, 1935, the A-5 successfully flew vertically to an altitude of (0.91 mi; 4,800 ft) using his gyroscopic guidance system. It then turned to a nearly horizontal path, flew 13,000 feet and achieved a maximum speed of 550 miles per hour. Goddard was elated because the guidance system kept the rocket on a vertical path so well.: 208 : 978–9 
In 1936–1939, Goddard began work on the K and L series rockets, which were much more massive and designed to reach very high altitude. The K series consisted of static bench tests of a more powerful engine, achieving a thrust of 624 lbs in February 1936.  This work was plagued by trouble with chamber burn-through. In 1923, Goddard had built a regeneratively cooled engine, which circulated liquid oxygen around the outside of the combustion chamber, but he deemed the idea too complicated. He then used a curtain cooling method that involved spraying excess gasoline, which evaporated around the inside wall of the combustion chamber, but this scheme did not work well, and the larger rockets failed. Goddard returned to a smaller design, and his L-13 reached an altitude of 2.7 kilometers (1.7 mi; 8,900 ft), the highest of any of his rockets. Weight was reduced by using thin-walled fuel tanks wound with high-tensile-strength wire.: 71–148 
Goddard experimented with many of the features of today's large rockets, such as multiple combustion chambers and nozzles. In November 1936, he flew the world's first rocket (L-7) with multiple chambers, hoping to increase thrust without increasing the size of a single chamber. It had four combustion chambers, reached a height of 200 feet, and corrected its vertical path using blast vanes until one chamber burned through. This flight demonstrated that a rocket with multiple combustion chambers could fly stably and be easily guided.: 96  In July 1937 he replaced the guidance vanes with a movable tail section containing a single combustion chamber, as if on gimbals (thrust vectoring). The flight was of low altitude, but a large disturbance, probably caused by a change in the wind velocity, was corrected back to vertical. In an August test the flight path was corrected seven times by the movable tail and was captured on film by Mrs Goddard.: 113–116 

From 1940 to 1941, Goddard worked on the P series of rockets, which used propellant turbopumps (also powered by gasoline and liquid oxygen). The lightweight pumps produced higher propellant pressures, permitting a more powerful engine (greater thrust) and a lighter structure (lighter tanks and no pressurization tank), but two launches both ended in crashes after reaching an altitude of only a few hundred feet. The turbopumps worked well, however, and Goddard was pleased.: 187–215 
When Goddard mentioned the need for turbopumps, Harry Guggenheim suggested that he contact pump manufacturers to aid him. None were interested, as the development cost of these miniature pumps was prohibitive. Goddard's team was therefore left on its own and from September 1938 to June 1940 designed and tested the small turbopumps and gas generators to operate the turbines. Esther later said that the pump tests were "the most trying and disheartening phase of the research.": 274–5 
Goddard was able to flight-test many of his rockets, but many resulted in what the uninitiated would call failures, usually resulting from engine malfunction or loss of control. Goddard did not consider them failures, however, because he felt that he always learned something from a test.: 45  Most of his work involved static tests, which are a standard procedure today, before a flight test. He wrote to a correspondent:  "It is not a simple matter to differentiate unsuccessful from successful experiments. ...  work that is finally successful is the result of a series of unsuccessful tests in which difficulties are gradually eliminated.": 274 
Jimmy Doolittle was introduced to the field of space science at an early point in its history. He recalls in his autobiography, "I became interested in rocket development in the 1930s when I met Robert H. Goddard, who laid the foundation. ... While with Shell Oil I worked with him on the development of a type of fuel. ... " Harry Guggenheim and Charles Lindbergh arranged for (then Major) Doolittle to discuss with Goddard a special blend of gasoline. Doolittle flew himself to Roswell in October 1938 and was given a tour of Goddard's shop and a "short course" in rocketry.  He then wrote a memo, including a rather detailed description of Goddard's rocket. In closing he said, "interplanetary transportation is probably a dream of the very distant future, but with the moon only a quarter of a million miles away—who knows!" In July 1941, he wrote Goddard that he was still interested in his rocket propulsion research. The Army was interested only in JATO at this point. However, Doolittle and Lindbergh were concerned about the state of rocketry in the US, and Doolittle remained in touch with Goddard.: 1208–16, 1334, 1443 
Shortly after World War II, Doolittle spoke concerning Goddard to an American Rocket Society (ARS) conference at which a  large number interested in rocketry attended. He later stated that at that time "we  had not given much credence to the tremendous potential of rocketry." In 1956, he was appointed chairman of the National Advisory Committee for Aeronautics (NACA) because the previous chairman, Jerome C. Hunsaker, thought Doolittle to be more sympathetic than other scientists and engineers to the rocket, which was increasing in importance as a scientific tool as well as a weapon.: 516  Doolittle was instrumental in the successful transition of the NACA to the National Aeronautics and Space Administration (NASA) in 1958. He was offered the position as first administrator of NASA, but he turned it down.
Between 1926 and 1941, the following 35 rockets were launched:
steering         
As an instrument for reaching extreme altitudes, Goddard's rockets were not very successful; they did not achieve an altitude greater than 2.7 km in 1937, while a balloon sonde had already reached 35 km in 1921.: 456  By contrast, German rocket scientists had achieved an altitude of 2.4 km with the A-2 rocket in 1934,: 138  8 km by 1939 with the A-5,: 39  and 176 km in 1942 with the A-4 (V-2) launched vertically, reaching the outer limits of the atmosphere and into space.: 221 
Goddard's pace was slower than the Germans' because he did not have the resources they did. Simply reaching high altitudes was not his primary goal; he was trying, with a methodical approach, to perfect his liquid fuel engine and subsystems such as guidance and control so that his rocket could eventually achieve high altitudes without tumbling in the rare atmosphere, providing a stable vehicle for the experiments it would eventually carry. He had built the necessary turbopumps and was on the verge of building larger, lighter, more reliable rockets to reach extreme altitudes carrying scientific instruments when World War II intervened and changed the path of American history. He hoped to return to his experiments in Roswell after the war.: 206, 230, 330–1 : 923–4 
Though by the end of the Roswell years much of his technology had been replicated independently by others, he introduced new developments to rocketry that were used in this new enterprise: lightweight turbopumps, variable-thrust engine (in U.S.), engine with multiple combustion chambers and nozzles, and curtain cooling of combustion chamber.
Although Goddard had brought his work in rocketry to the attention of the United States Army, between World Wars, he was rebuffed, since the Army largely failed to grasp the military application of large rockets and said there was no money for new experimental weapons.: 297  German military intelligence, by contrast, had paid attention to Goddard's work. The Goddards noticed that some mail had been opened, and some mailed reports had gone missing. An accredited military attaché to the US, Friedrich von Boetticher, sent a four-page report to the Abwehr in 1936, and the spy Gustav Guellich sent a mixture of facts and made-up information, claiming to have visited Roswell and witnessed a launch. The Abwehr was very interested and responded with more questions about Goddard's work.: 77 : 227–8  Guellich's reports did include information about fuel mixtures and the important concept of fuel-curtain cooling,: 39–41  but thereafter the Germans received very little information about Goddard.
The Soviet Union had a spy in the U.S. Navy Bureau of Aeronautics. In 1935, she gave them a report Goddard had written for the Navy in 1933. It contained results of tests and flights and suggestions for military uses of his rockets. The Soviets considered this to be very valuable information. It provided few design details, but gave them the direction and knowledge about Goddard's progress.: 386–7 
Navy Lieutenant Charles F. Fischer, who had visited Goddard in Roswell earlier and gained his confidence, believed Goddard was doing valuable work and was able to convince the Bureau of Aeronautics in September 1941 that Goddard could build the JATO unit the Navy desired. While still in Roswell, and before the Navy contract took effect, Goddard began in September to apply his technology to build a variable-thrust engine to be attached to a PBY seaplane. By May 1942, he had a unit that could meet the Navy's requirements and be able to launch a heavily loaded aircraft from a short runway. In February, he received part of a PBY with bullet holes apparently acquired in the Pearl Harbor attack. Goddard wrote to Guggenheim that "I can think of nothing that would give me greater satisfaction than to have it contribute to the inevitable retaliation.": 322, 328–9, 331, 335, 337 
In April, Fischer notified Goddard that the Navy wanted to do all its rocket work at the Engineering Experiment Station at Annapolis. Esther, worried that a move to the climate of Maryland would cause Robert's health to deteriorate faster, objected. But the patriotic Goddard replied, "Esther, don't you know there's a war on?" Fischer also questioned the move, as Goddard could work just as well in Roswell. Goddard simply answered, "I was wondering when you would ask me." Fischer had wanted to offer him something bigger—a long range missile—but JATO was all he could manage, hoping for a greater project later.: 338, 9  It was a case of a square peg in a round hole, according to a disappointed Goddard.: 209 
Goddard and his team had already been in Annapolis a month and had tested his constant-thrust JATO engine when he received a Navy telegram, forwarded from Roswell, ordering him to Annapolis. Lt. Fischer asked for a crash effort. By August, his engine was producing 800 lbs of thrust for 20 seconds, and Fischer was anxious to try it on a PBY. On the sixth test run, with all bugs worked out, the PBY, piloted by Fischer, was pushed into the air from the Severn River. Fischer landed and prepared to launch again. Goddard had wanted to check the unit, but radio contact with the PBY had been lost. On the seventh try, the engine caught fire. The plane was 150 feet up when flight was aborted. Because Goddard had installed a safety feature at the last minute, there was no explosion and no lives were lost. The problem's cause was traced to hasty installation and rough handling. Cheaper, safer solid fuel JATO engines were eventually selected by the armed forces. An engineer later said, "Putting  rocket on a seaplane was like hitching an eagle to a plow.": 344–50 
Goddard's first biographer Milton Lehman notes:
In its 1942 crash effort to perfect an aircraft booster, the Navy was beginning to learn its way in rocketry. In similar efforts, the Army Air Corps was also exploring the field . Compared to Germany's massive program, these beginnings were small, yet essential to later progress. They helped develop a nucleus of trained American rocket engineers, the first of the new breed who would follow the professor into the Age of Space.: 350 In August 1943, President Atwood at Clark wrote to Goddard that the university was losing the acting head of the Physics Department, was taking on "emergency work" for the Army, and he was to "report for duty or declare the position vacant." Goddard replied that he believed he was needed by the Navy, was nearing retirement age, and was unable to lecture because of his throat problem, which did not allow him to talk above a whisper. He regretfully resigned as Professor of Physics and expressed his deepest appreciation for all Atwood and the Trustees had done for him and indirectly for the war effort.: 1509–11  In June he had gone to see a throat specialist in Baltimore, who recommended that he not talk at all, to give his throat a rest.: 1503 
The station, under Lt Commander Robert Truax, was developing another JATO engine in 1942 that used hypergolic propellants, eliminating the need for an ignition system. Chemist Ensign Ray Stiff had discovered in the literature in February that aniline and nitric acid burned fiercely immediately when mixed.: 1488 : 172  Goddard's team built the pumps for the aniline fuel and the nitric acid oxidizer and participated in the static testing.: 1520, 1531  The Navy delivered the pumps to Reaction Motors (RMI) to use in developing a gas generator for the pump turbines. Goddard went to RMI to observe testing of the pump system and would eat lunch with the RMI engineers.: 1583   (RMI was  the first firm formed to build rocket engines and built engines for the Bell X-1 rocket plane: 1  and Viking (rocket).: 169  RMI offered Goddard one-fifth interest in the company and a partnership after the war.: 1583 ) Goddard went with Navy people in December 1944 to confer with RMI on division of labor, and his team was to provide the propellant pump system for a rocket-powered interceptor because they had more experience with pumps.: 100  He consulted with RMI from 1942 through 1945.: 311  Though previously competitors, Goddard had a good working relationship with RMI, according to historian Frank H. Winter.
The Navy had Goddard build a pump system for Caltech's use with acid-aniline propellants. The team built a 3000-lb thrust engine using a cluster of four 750-lb thrust motors.: 1574, 1592  They also developed 750-lb engines for the Navy's Gorgon guided interceptor missile (experimental Project Gorgon). Goddard continued to develop the variable-thrust engine with gasoline and lox because of the hazards involved with the hypergolics.: 1592 : 355, 371 
Despite Goddard's efforts to convince the Navy that liquid-fueled rockets had greater potential, he said that the Navy had no interest in long-range missiles.: 1554  However, the Navy asked him to perfect the throttleable JATO engine. Goddard made improvements to the engine, and in November it was demonstrated to the Navy and some officials from Washington. Fischer invited the spectators to operate the controls; the engine blasted out over the Severn at full throttle with no hesitation, idled, and roared again at various thrust levels. The test was perfect, exceeding the Navy's requirements. The unit was able to be stopped and restarted, and it produced a medium thrust of 600 pounds for 15 seconds and a full thrust of 1,000 pounds for over 15 seconds. A Navy Commander commented that "It was like being Thor, playing with thunderbolts." Goddard had produced the essential propulsion control system of the rocket plane. The Goddards celebrated by attending the Army-Navy football game and attending the Fischers' cocktail party.: 350–1 
This engine was the basis of the Curtiss-Wright XLR25-CW-1 two-chamber, 15,000-pound variable-thrust engine that powered the Bell X-2 research rocket plane. After World War II, Goddard's team and some patents went to Curtiss-Wright Corporation. "Although his death in August 1945 prevented him from participating in the actual development of this engine, it was a direct descendent of his design.": 1606  Clark University and the Guggenheim Foundation received the royalties from the use of the patents.  In September 1956, the X-2 was the first plane to reach 126,000 feet altitude and in its last flight exceeded Mach 3 (3.2) before losing control and crashing. The X-2 program advanced technology in areas such as steel alloys and aerodynamics at high Mach numbers.
Don't you know about your own rocket pioneer? Dr. Goddard was ahead of us all.
–Wernher von Braun, when asked about his work, following World War II
In the spring of 1945, Goddard saw a captured German V-2 ballistic missile, in the naval laboratory in Annapolis, Maryland, where he had been working under contract. The unlaunched rocket had been captured by the US Army from the Mittelwerk factory in the Harz mountains and samples began to be shipped by Special Mission V-2 on 22 May 1945.
After a thorough inspection, Goddard was convinced that the Germans had "stolen" his work. Though the design details were not exactly the same, the basic design of the V-2 was similar to one of Goddard's rockets. The V-2, however, was technically far more advanced than the most successful of the rockets designed and tested by Goddard. The Peenemünde rocket group led by Wernher von Braun may have benefited from the pre-1939 contacts to a limited extent,: 387–8  but had also started from the work of their own space pioneer, Hermann Oberth; they also had the benefit of intensive state funding, large-scale production facilities (using slave labor), and repeated flight-testing that allowed them to refine their designs. Oberth was a theorist and had never built a rocket, but he tested small liquid propellant thrust chambers in 1929-30 which were not advancements in the "state of the art.": 273, 275  In 1922 Oberth asked Goddard for a copy of his 1919 paper and was sent one.: 96 
Nevertheless, in 1963, von Braun, reflecting on the history of rocketry, said of Goddard: "His rockets ... may have been rather crude by present-day standards, but they blazed the trail and incorporated many features used in our most modern rockets and space vehicles". He once recalled that "Goddard's experiments in liquid fuel saved us years of work, and enabled us to perfect the V-2 years before it would have been possible." After  World War II von Braun reviewed Goddard's patents and believed they contained enough technical information to build a large missile.
Three features developed by Goddard appeared in the V-2: (1) turbopumps were used to inject fuel into the combustion chamber; (2) gyroscopically controlled vanes in the nozzle stabilized the rocket until external vanes in the air could do so; and (3) excess alcohol was fed in around the combustion chamber walls, so that a blanket of evaporating gas protected the engine walls from the combustion heat.

The Germans had been watching Goddard's progress before the war and became convinced that large, liquid fuel rockets were feasible. General Walter Dornberger, head of the V-2 project, used the idea that they were in a race with the U.S. and that Goddard had "disappeared" (to work with the Navy) as a way to persuade Hitler to raise the priority of the V-2.
Goddard avoided sharing details of his work with other scientists and preferred to work alone with his technicians. Frank Malina, who was then studying rocketry at the California Institute of Technology, visited Goddard in August 1936. Goddard hesitated to discuss any of his research, other than that which had already been published in Liquid-Propellant Rocket Development. Theodore von Kármán, Malina's mentor at the time, was unhappy with Goddard's attitude and later wrote, "Naturally we at Caltech wanted as much information as we could get from Goddard for our mutual benefit. But Goddard believed in secrecy. ... The trouble with secrecy is that one can easily go in the wrong direction and never know it.": 90  However, at an earlier point, von Kármán said that Malina was "highly enthusiastic" after his visit and that Caltech made changes to their liquid-propellant rocket, based on Goddard's work and patents. Malina remembered his visit as friendly and that he saw all but a few components in Goddard's shop.: 178 
Goddard's concerns about secrecy led to criticism for failure to cooperate with other scientists and engineers. His approach at that time was that independent development of his ideas without interference would bring quicker results even though he received less technical support. George Sutton, who became a rocket scientist working with von Braun's team in the late 1940s, said that he and his fellow workers had not heard of Goddard or his contributions and that they would have saved time if they had known the details of his work. Sutton admits that it may have been their fault for not looking for Goddard's patents and depending on the German team for knowledge and guidance; he wrote that information about the patents was not well distributed in the U.S. at that early period after World War II, though Germany and the Soviet Union had copies of some of them. (The Patent Office did not release rocket patents during World War II.) However, the Aerojet Engineering Corporation, an offshoot of the Guggenheim Aeronautical Laboratory at Caltech (GALCIT), filed two patent applications in Sep 1943 referencing Goddard's U.S. Patent 1,102,653 for the multistage rocket.
By 1939, von Kármán's GALCIT had received Army Air Corps funding to develop rockets to assist in aircraft take-off. Goddard learned of this in 1940, and openly expressed his displeasure at not being considered. Malina could not understand why the Army did not arrange for an exchange of information between Goddard and Caltech since both were under government contract at the same time. Goddard did not think he could be of that much help to Caltech because they were designing rocket engines mainly with solid fuel, while he was using liquid fuel.
Goddard was concerned with avoiding the public criticism and ridicule he had faced in the 1920s, which he believed had harmed his professional reputation. He also lacked interest in discussions with people who had less understanding of rocketry than he did,: 171  feeling that his time was extremely constrained.: 23  Goddard's health was frequently poor, as a result of his earlier bout of tuberculosis, and he was uncertain about how long he had to live: 65, 190  He felt, therefore, that he hadn't the time to spare arguing with other scientists and the press about his new field of research, or helping all the amateur rocketeers who wrote to him.: 61, 71, 110–11, 114–15  In 1932 Goddard wrote to H. G. Wells:
 How many more years I shall be able to work on the problem, I do not know; I hope, as long as I live. There can be no thought of finishing, for "aiming at the stars", both literally and figuratively, is a problem to occupy generations, so that no matter how much progress one makes, there is always the thrill of just beginning.     Goddard spoke to professional groups, published articles and papers and patented his ideas; but while he discussed basic principles, he was unwilling to reveal the details of his designs until he had flown rockets to high altitudes and thus proven his theory.: 115  He tended to avoid any mention of space flight, and spoke only of high-altitude research, since he believed that other scientists regarded the subject as unscientific.: 116  GALCIT saw Goddard's publicity problems and that the word "rocket" was "of such bad repute" that they used the word "jet" in the name of JPL and the related Aerojet Engineering Corporation.
Many authors writing about Goddard mention his secrecy, but
neglect the reasons for it. Some reasons have been noted above. Much of his work was for the military and was classified.: 1541  There were some in the U.S. before World War II that called for long-range rockets, and in 1939 Major James Randolph wrote a "provocative article" advocating a 3000-mile range missile. Goddard was "annoyed" by the unclassified paper as he thought the subject of weapons should be "discussed in strict secrecy."
However, Goddard's tendency to secrecy was not absolute, nor was he totally uncooperative. In 1945 GALCIT was building the WAC Corporal for the Army.  But in 1942 they were having trouble with their liquid propellant rocket engine's performance (timely, smooth ignition and explosions). Frank Malina went  to Annapolis in February and consulted with Goddard and Stiff, and they arrived at a solution to the problem (hypergolic propellant), which resulted in the successful launch of the high-altitude research rocket in October 1945.
During the First and Second World Wars, Goddard offered his services, patents, and technology to the military, and made some significant contributions. Just before the Second World War several young Army officers and a few higher-ranking ones believed Goddard's research was important but were unable to generate funds for his work.
Toward the end of his life, Goddard, realizing he was no longer going to be able to make significant progress alone in his field, joined the American Rocket Society and became a director. He made plans to work in the budding US aerospace industry (with Curtiss-Wright), taking most of his team with him.: 382, 385 
On June 21, 1924, Goddard married Esther Christine Kisk (March 31, 1901 – June 4, 1982), a secretary in Clark University's President's office, whom he had met in 1919. She became enthusiastic about rocketry and photographed some of his work as well as aided him in his experiments and paperwork, including accounting. They enjoyed going to the movies in Roswell and participated in community organizations such as the Rotary and the Woman's Club. He painted the New Mexican scenery, sometimes with the artist Peter Hurd, and played the piano. She played bridge, while he read. Esther said Robert participated in the community and readily accepted invitations to speak to church and service groups. The couple did not have children. After his death, she sorted out Goddard's papers, and secured 131 additional patents on his work.
Concerning Goddard's religious views, he was raised as an Episcopalian, though he was not outwardly religious. The Goddards were associated with the Episcopal church in Roswell, and he attended occasionally. He once spoke to a young people's group on the relationship of science and religion.: 224 
Goddard's serious bout with tuberculosis weakened his lungs, affecting his ability to work, and was one reason he liked to work alone, in order to avoid argument and confrontation with others and use his time fruitfully. He labored with the prospect of a shorter than average life span.: 190  After arriving in Roswell, Goddard applied for life insurance, but when the company doctor examined him he said that Goddard belonged in a bed in Switzerland (where he could get the best care).: 183  Goddard's health began to deteriorate further after moving to the humid climate of Maryland to work for the Navy. He was diagnosed with throat cancer in 1945. He continued to work, able to speak only in a whisper until surgery was required, and he died in August of that year in Baltimore, Maryland.: 377, 395  He was buried in Hope Cemetery in his home town of Worcester, Massachusetts.
Goddard honored on a U.S. airmail stamp
Bronze plaque in Auburn, Massachusetts marking the town in which Goddard launched the first liquid-fueled rocket on March 16, 1926.
Insignia of the 50th Anniversary of the Goddard Space Flight Center, a NASA facility in Maryland
Robert H. Goddard High School in Roswell, New Mexico.
Goddard Hall at Worcester Polytechnic Institute
Goddard Library at Clark University
Goddard received 214 patents for his work, of which 131 were awarded after his death. Among the most influential patents were:
The Guggenheim Foundation and Goddard's estate filed suit in 1951 against the U.S. government for prior infringement of three of Goddard's patents. In 1960, the parties settled the suit, and the U.S. armed forces and NASA paid out an award of $1 million: half of the award settlement went to his wife, Esther. At that time, it was the largest government settlement ever paid in a patent case.: 404   The settlement amount exceeded the total amount of all the funding that Goddard received for his work, throughout his entire career.
Clark University is a private research university in Worcester, Massachusetts. Founded in 1887 with a large endowment from its namesake Jonas Gilman Clark, a prominent businessman, Clark was one of the first modern research universities in the United States. Originally an all-graduate institution, Clark's first undergraduates entered in 1902 and women were first enrolled in 1942.
The university now offers 46 majors, minors, and concentrations in the humanities, social sciences, natural sciences, and engineering and allows students to design specialized majors and engage in pre-professional programs. It is noted for its programs in the fields of psychology, geography, physics, biology, and entrepreneurship and is a member of the Higher Education Consortium of Central Massachusetts which enables students to cross-register to attend courses at other area institutions including Worcester Polytechnic Institute and the College of the Holy Cross. As a liberal arts–based research university, Clark makes substantial research opportunities available to its students, notably at the undergraduate level through LEEP project funding, yet is also respected for its intimate environment as the second smallest university counted among the top 66 national universities by U.S. News &amp; World Report and as one of 40 Colleges That Change Lives. It is classified among "R2: Doctoral Universities – High research activity". It was a founding member of the Association of American Universities, but departed in 1999.
Graduate and professional programs are offered through the Graduate School, the Graduate School of Management, the Graduate School of Geography, the Frances L. Hiatt School of Psychology, the Gustaf H. Carlson School of Chemistry, the Adam Institute for Urban Teaching and School Practice, the International Development, Community and Environment (IDCE), and the School of Professional Studies, and the Strassler Center for Holocaust and Genocide Studies.
The university competes intercollegiately in 17 NCAA Division III varsity sports as the Clark Cougars and is a part of the New England Women's and Men's Athletic Conference. Intramural and club sports are also offered in a wide range of activities.
Clark faculty and alumni have founded numerous companies and organizations, including Panera Bread, the American Psychological Association, and the American Physical Society, and have played leading roles in the development of modern rocketry, the wind chill factor, and the birth control pill. The university is also the alma mater of at least three living billionaires, in addition to its alumni having won three Pulitzer Prizes, multiple Suey Awards, and an Emmy Award.
On January 17, 1887, successful American businessman Jonas Gilman Clark announced his intention to found and endow a university in the city of Worcester, filing a petition in the Massachusetts Legislature requesting a charter for Clark University. An Act of Incorporation was duly enacted by the legislature and signed by the governor on March 31 of that same year. Clark, who was a friend of Leland Stanford, was probably inspired by the plans for Stanford University and founded the university with an endowment of one million dollars, and later added another million dollars because he feared the university might someday face a lack of funds. Opening on October 2, 1889, Clark was the first all-graduate university in the United States, with departments in mathematics, physics, chemistry, biology, and psychology.
G. Stanley Hall was appointed the first president of Clark University in 1888. He had been a professor of psychology and pedagogy at Johns Hopkins University, which had been founded just a few years prior and was quickly becoming a model of the modern research university. Hall spent seven months in Europe visiting other universities and recruiting faculty. He became the founder of the American Psychological Association and earned the first Ph.D. in psychology in the United States at Harvard. Clark has played a prominent role in the development of psychology as a distinguished discipline in the United States ever since. Franz Boas, founder of American cultural anthropology and adviser for the first Ph.D. in anthropology which was granted at Clark in 1891, taught at Clark from 1888 until 1892 when he resigned in a dispute with President Hall over academic freedom and joined the faculty of Columbia University. Albert A. Michelson, the first American to receive a Nobel Prize in Physics, best known for his involvement in the Michelson–Morley experiment, which measured the speed of light, was a professor from 1889 to 1892 before becoming head of the physics department at the University of Chicago.
Jonas G. Clark died in 1900, leaving gifts to the university and campus library, but reserving half of his estate for the foundation of an undergraduate college. This had been strongly opposed by President Hall in years past, but Clark College opened in 1902, managed independently of Clark University. Clark College and Clark University had different presidents until Hall's retirement in 1920. Clark University began admitting women after Clark's death, and the first female Ph.D. in psychology was awarded in 1908. Early Ph.D. students in psychology were ethnically diverse, with several early graduates being Japanese. In 1920, Francis Sumner became the first African American to earn a Ph.D. in psychology.
Clark University, along with Stanford and Johns Hopkins, was one of the fourteen founding members of the Association of American Universities, an organization of universities with the most prestigious profiles in research and graduate education, and was one of only three New England universities, along with Harvard and Yale, to be a founding member. Clark withdrew its membership in 1999, citing a conflict with its mission.
In order to celebrate the 20th anniversary of Clark's opening, President Hall invited a number of leading thinkers to the university. Among them was Sigmund Freud who, accompanied by Carl Jung, delivered his five famous "Clark Lectures" there over the course of five days in September 1909, introducing psychoanalysis to an American audience. This was Freud's only visit to the United States. Clark granted Freud an honorary degree, which hangs in the Sigmund Freud House in Vienna, Austria. It was one of the few official distinctions Freud received during his lifetime.
In the 1920s Robert Goddard, a pioneer of rocketry, considered one of the founders of space and missile technology, was chairman of the Physics Department. The Robert H. Goddard Library is named for him.
The Graduate School of Management (GSOM) was founded in 1982. In 1997, Clark announced the first PhD program in Holocaust Studies in the United States. This after the university convinced Debórah Dwork to leave Yale University and become Clark's first professor of Holocaust studies in the prior year.
The Mosakowski Institute for Public Enterprise was established in fall 2007 due to a founding gift from two Clark alumni, William '76 and Jane '75 Mosakowski.
U.S. Secretary of State and former senator and democratic presidential candidate Hillary Clinton spoke at Clark University on February 4, 2008, to an audience of approximately 3,500 in the Kneller Athletic Center.
In March 2009, Clark University convened a first-of-its-kind National Conference on Liberal Education and Effective Practice, co-sponsored by Clark's Mosakowski Institute for Public Enterprise and the Association of American Colleges and Universities.
In April 2009, then-President John Bassett denied Clark University Students for Palestinian Rights, a student group, permission to bring Norman Finkelstein to speak about the "Gaza Massacre" (2008–2009 Gaza War) because Finkelstein "would invite controversy and not dialogue or understanding". He also cited a conflict in scheduling regarding a conference on Holocaust and Genocide Studies presented by the university in the same month. However, following protests, which included a public protest in the center of campus, a petition campaign and outreach by alumni, students and faculty, Basset reversed his decision and allowed Finkelstein to speak on April 27, the last day of classes for the semester. Finkelstein spoke to around 400 students, faculty and community members in Atwood Hall.
In April 2010, Clark University received the largest gift in its 123-year history, a $14.2 million offering from the late head of Hanover Insurance, one of the nation's biggest property and casualty insurers. The gift from John Adam is intended to strengthen Clark's graduate programs in education, promote college-readiness among minority students and bolster its research profile related to urban education. This donation created the Adams Education Fund, which will enhance Clark's nationally recognized model for urban secondary education and reform, teacher-training, and community education partnerships.
On July 1, 2010, former provost David Angel became the ninth president of Clark, succeeding John Bassett, who went on to become president of Heritage University, located on the Yakama Indian Reservation in Toppenish, Washington.
Clark University has an ongoing renovation project that will cover several buildings. In the summer of 2010, overhauls occurred in Bullock and Wright Hall dormitories.
In summer 2012, Clark University underwent more renovations. The city of Worcester allowed the university to close Downing Street to unite the campus. The area was landscaped to become a pedestrian plaza. Johnson and Sanford halls were united to become the Johnson Sanford Center featuring new social, study, and multimedia spaces. The project included addition of an outdoor roof terrace and an elevator to all levels. The university has recently begun a project called LEEP to connect students and the world of academia to practical experience.
Summer 2016 saw the completion of a new Alumni and Student Engagement Center building, extending the campus across Main Street. The facility is a mixed-use building containing administrative offices, lecture halls, meeting rooms, and some retail space, and features a modern architectural look and a roof-top solar array.
In 2022, graduate students at the university organized as Clark University Graduate Workers United, a chapter of Teamsters Local 170. An NLRB-overseen election resulted in a 100-7 vote in favor of unionization, after which the union and university entered into negotiations.
The campus is located on Main Street in the Main South neighborhood about 2 miles (3.2 km) west of downtown Worcester and 40 miles (64 km) west of Boston. The campus is compact, with most of the major buildings located within the space of a single city block.
The center of campus is known as the Green. The Green is a hub for student activity, and is where most Clarkies spend their time during the warm months. It is the location of Spree Day, the welcome back BBQ, several clubs' events and graduation. The buildings surrounding The Green include Atwood Hall, Jefferson Academic Center, Higgins University Center, Jonas Clark Hall, and the Goddard Library.
Administrative offices are housed in small buildings along Woodland Street, as is the president's house. The new Shaich Family Alumni and Student Engagement Center, named in honor of a $5 million gift from the family of alumnus Ron Shaich, is across Main Street and houses meeting spaces and offices.
Jonas Clark Hall, built in 1887, was Clark University's first building. It occupies the center of campus and houses the economics, psychology and education departments. Located in the basement of Jonas Clark Hall is the university's cogeneration plant which allows the university to recycle waste heat from electrical generation into hot water, heat, and steam. It was updated in 2013 to a more efficient 2.0 kWh natural gas engine.
Estabrook Hall, located on Woodland Street, is the second oldest building on Clark's campus. Originally constructed as a dormitory, it now functions as the language center and the music center. The upper floors are primarily home to classrooms and offices for the Language, Literature, and Culture department, which includes Spanish, French, German, Latin, and Hebrew. The bottom floor and basement are practice rooms and music halls.
The Jefferson Academic Center houses various social science departments including Women's Studies, Geography, History, Geographical Information Sciences, Political Science, and Sociology.
Atwood Hall, attached to the Jefferson Academic Center, is the primary theater on campus and seats 658. Atwood Hall originally served as the chapel for the university, and in recent decades has been the scene for several notable concerts and speeches. The Grateful Dead (1967 and 1969), the Jimi Hendrix Experience (1968), Janis Joplin (1969), and Bruce Springsteen and the E Street Band (1974) have all played here. A March 15, 1968, concert by the Jimi Hendrix Experience was professionally recorded and released in 1999 as Live at Clark University. In 1963, student D'Army Bailey invited Malcolm X to speak here. Noam Chomsky spoke here on the topic of the Israeli–Palestinian conflict and the Arab Spring April 12, 2011. It was the first-ever lecture given on a Spree Day at Clark. On October 16, 2014, President Bill Clinton spoke in Atwood as a supporter of Martha Coakley's run for Governor of Massachusetts.
The Lasry Center for Bioscience (named for hedge fund manager Marc Lasry and his wife Cathy, both alumni) houses the biology department. It received a LEED Gold certification for its energy efficiency.
The Little Center is the alternate performing arts venue, with its largest room, the Michelson Theater seating 120.
The Academic Commons, also known as the AC, acts as a study area and lounge for the students, and incorporates a Sodexo coffeehouse named Jazzmans, a quiet study area, a computer room, and the Mosakowski Institute for Public Enterprises. The Goddard Library is upstairs from the Academic Commons and houses more than 375,000 volumes.
Clark University has 7 libraries.
The Kneller Athletic Center houses the basketball courts, swimming pool, racquet ball courts, handball courts, and the James and Ada B. Bickman Fitness Center which was opened in 1995 and completely renovated in 2013. Major campus events, such as International Gala, the fall concert, and first year orientation are usually held in the Kneller as the basketball courts are the largest rooms on campus and can accommodate the entire student body. The Thomas M. '62 and Joan E. '60 Dolan Field House opened in May 2003 at which time the Russ Granger Athletic Fields and Corash Tennis Courts around it were reconfigured and renovated. The Boys and Girls Club Track and Field opened in October 2016.
Students entering Clark must live on campus for the first two years unless their primary address is within 25 miles (40 km) of campus. The residence halls at Clark are organized by those who live there. The halls include the following breakdowns:
Clark owns apartments that, while outside of the main campus area, exclusively house Clark students.
The first Clark "residence halls" (Wright and Bullock) opened in 1959. Before that time, Estabrook Hall was the men's dormitory and small women's dorms stood in the current location of Little Center and Bullock Hall. Blackstone, the newest of the halls, opened in 2007.
As of fall 2007, gender blind/neutral housing is an option at Clark, meaning that students of different genders can room together.
Clark University released its Climate Action Plan December 15, 2009, detailing strategies for the university to reduce its carbon footprint while strengthening many of its existing sustainability practices. The plan sets two goals with respect to climate neutrality: First is an interim goal of reducing emissions to 20 percent below 2005-levels by 2015. The second goal is to achieve climate neutrality (net zero greenhouse gas emissions) by the year 2030.
Clark College opened in 1902 as the fulfillment of founder Jonas Clark's desire for an undergraduate liberal arts college. The administration of Clark College and Clark University was formally united in 1920 and undergraduate programs continue today under the university.
The Graduate School of Management (GSOM), founded in 1982, is led by Dean Alan Eisner. The school offers a range of master's degrees as well as undergraduate courses in Management, Marketing, and Innovation and Entrepreneurship. Notable alumni of GSOM include Libérat Mfumukeko, secretary-general of the East African Community, Matt Goldman, co-founder of the Blue Man Group
The Graduate School of Geography (GSG), founded in 1921 by Wallace Walter Atwood and led by Director James McCarthy,  offers bachelor's, master's, and doctoral degrees. Under GSG is Clark Labs, founded in 1987, which developed the IDRISI GIS and image processing software and then the TerrSet geospatial monitoring and modeling software. Alumni of the school include Paul Siple, an Antarctic explorer and inventor of the wind chill factor who attended the school on the recommendation of Admiral Richard E. Byrd. Siple named the Clark Mountains in Antarctica after Clark and several of the peaks after Clark professors in the GSG.
The School of Professional Studies (SPS) offers bachelor's degrees as well as a Master in Public Administration (MPA), Master of Science in Public Communication (MSPC), Master of Science in Information Technology (MSIT), Certificate in Community Human Services, and Certificate of Advanced Graduate Study (CAGS). Called the Evening College from its establishment in 1953 and then the College of Professional and Continuing Education (COPACE) from 1975 to 2016, the school is led by the vice provost for professional education and dean, John LaBrie. It has branch campuses in Łódź and Warsaw, Poland, with the University of Social Sciences and in Astrakhan, Russia with Astrakhan State University. There are also joint programs with Shandong University of Science and Technology and Hefei University of Technology. Alumni of SPS include Olta Xhaçka, Albanian Minister of Defense, and Keith R. Hall, former director of the National Reconnaissance Office.
The Hiatt School of Psychology, led by Chair James Córdova, offers undergraduate and doctoral degrees. Notable alumni include Francis Sumner, the father of black psychology, and Arnold Gesell, noted child psychologist. The American Psychological Association was founded at Clark in 1892 by Clark's first president, psychologist G. Stanley Hall. It was also at Clark that mazes were first used to study rat behavior by psychology Professor Edmund Sanford in his laboratory.
The Carlson School of Chemistry offers undergraduate, master's, and doctoral degrees, including a 3/2 engineering program with Columbia University's Fu Foundation School of Engineering and Applied Science. The school is led by Chair Shuanghong Huo. What was then known as Clark's chemical laboratories was once directed by Professor Charles A. Kraus, a noted chemist who was a consultant to the U.S. Chemical Warfare Service during World War I and the Manhattan Project during World War II. He also developed the anti-knock additive in gasoline.
The International Development, Community and Environment (IDCE) Department was founded in 2000. Led by Director Edward R. Carr, it is home to nearly 300 graduate and undergraduate students each year. IDCE offers an undergraduate major in International Development and Social Change and master's degrees in International Development, Environmental Science and Policy, Community Development and Planning, and Global and Community Health. It also jointly manages the Masters in Geographic Information Science with the Graduate School of Geography, and offers a dual degree program (MBA/ES&amp;P) with the Graduate School of Management.  The QS World University Rankings ranked IDCE's International Development program 15th for academic reputation in 2018.
Becker School of Design &amp; Technology was founded in March 2021 after Becker College announced its closure at the end of the Spring 2021 semester. Becker School of Design &amp; Technology offers a bachelor's degree in interactive media with multiple concentrations as well as a Design Your Own undergraduate major. A graduate MFA in interactive media is also offered. Clark University was ranked #7 by The Princeton Review in the Top 25 Game Design 2021.
Clark offers 32 undergraduate majors. It offers 57 study abroad and away programs in 34 countries. Clark has 212 full-time faculty, representing a 10:1 student-faculty ratio. Ninety-four percent of Clark's faculty have doctoral or terminal degrees. Clark University is accredited by the New England Commission of Higher Education.
In recent years, Clark has been noted especially for its geography and psychology departments, with the latter having a distinctive humanistic orientation. The School of Geography was founded by then President Wallace Walter Atwood in 1921, and is the first institution in the United States established for graduate study in this science. It has granted more doctoral degrees than any other geography program in the country. The geography department is best known for its strength in human-environment geography and for the development of the IDRISI geographic information systems software, named for the famous 12th century explorer and cartographer Muhammad al-Idrisi by Prof. Ron Eastman. It was ranked #1 for undergraduate geography by Rugg's Recommendations on Colleges and has consistently been ranked in the top 10 in the nation by other publications. The geography department also offers a graduate-level degree in GIS as part of the Fifth-Year Free program. The department's mission is ambitious: "to educate undergraduate and graduate students to be imaginative and contributing citizens of the world, and to advance the frontiers of knowledge and understanding through rigorous scholarship and creative effort."
In recent years, Clark has received widespread media coverage for its "Fifth-Year Free" program. Under Clark's BA/MA program with the fifth year free, undergraduates who maintain a B+ average are eligible for tuition-free enrollment in its one-year graduate programs, meaning that they can get a Master of Arts degree for the price of a bachelor's degree. Students apply to master's degree programs in their junior year, begin meeting requirements in their senior year and typically complete those requirements in the fifth year. Bachelor's degrees are granted en route to the master's degree.
For Fall 2019, Clark received 7,639 freshmen applications; 4,032 were admitted (52.8%) and 665 enrolled. The average high school grade point average (GPA) of the enrolled freshmen was 3.65, while the middle 50% range of SAT scores was 600–690 for evidence-based reading and writing, and 580–680 for math. The middle 50% range of the ACT Composite score was 27–31.
Admission to Clark is rated "more selective" by U.S. News &amp; World Report.
As of fall 2019, Clark's student body comprised 2,349 undergraduates and 1,149 graduate and professional students. International students make up 11.5% of undergraduates. In addition, 21% of the undergraduate student body is classified as ALANA (Asian-, Latino-, African-, and Native-American) and 61% of undergraduates are female.
The majority of the undergraduate student body, 66%, lives on campus. Clark requires undergraduates to do so for their first two years, with first-years being assigned housing based on their responses to a Housing Preferences Form. Once first-years have been assigned housing, a seniority system, whereby seniors have the first choice of spaces left, juniors have the second, and sophomores the third, ensures that seniors and juniors are usually able to live on campus if they wish to. Nonetheless, some choose to live in off-campus apartments in the immediate neighborhood of Clark, along with the graduate students outside the 1% that live on campus.
There are more than 130 student clubs and organizations at Clark. All these are headed by the Clark Undergraduate Student Council which disseminates more than $750,000 in budgets to the various clubs and their events.
The Scarlet is Clark University's student newspaper. It is published weekly and has four sections: News, Opinions, Living Arts, and Sports. Clark's literary magazine, Caesura, is published annually and features artwork, poetry, prose, essays, and creative non-fiction submitted by undergraduate and graduate students. STIR Magazine, Clark's life, culture, and style magazine was founded by Diana Levine as a student project in 2004. STIR began with a three-person staff and in black and white, and now has about 30 core students who contribute to its production in full color. The Scholarly Undergraduate Research Journal (SURJ) is Clark's student-run undergraduate research journal. It publishes undergraduate academic work and is intended to provide undergraduates with "experiences in the peer review and academic publication processes." Peer reviewers consist of undergraduate and graduate students, as well as faculty. The Freudian Slip, is a satire/humor publication founded in 2015. It publishes semi-weekly satirical articles about local and worldwide events. It is also the first university publication published exclusively online.
There is also a student-run internet radio station, Radio of Clark University (ROCU), with over 100 student DJs.
Spree Day originated in 1903 to coincide with St. Patrick's Day. It is traditional to not tell first-year students about Spree Day. Instead, the Senior class awakens the first-years by running through the dorms banging pots and pans.
While Spree Day is a day of recreation, Clark University also holds the Academic Spree Day annually during Spring semester. This academic event is when Clark undergraduates present their research and creative work.
Clark University fields 17 NCAA Division III varsity teams which compete intercollegiately as the Clark Cougars in the New England Women's and Men's Athletic Conference. Men's sports include baseball, basketball, cross country, lacrosse, soccer, swimming and diving, and tennis; women's sports include basketball, cross country, field hockey, crew, lacrosse, soccer, softball, swimming and diving, tennis, and volleyball.
The university also offers a variety of club and intramural sports such as soccer, ice hockey, ultimate frisbee, quidditch, volleyball and basketball. This contributes to Clark's 65 percent student participation rate in athletics.
In 1985, the university engaged in a partnership with community groups and business organizations to revitalize Clark neighborhoods. Its efforts in the University Park Partnership program include refurbishing dilapidated or abandoned homes, reselling them to area residents, and subsidizing mortgages for new home buyers.
In 1997, Clark opened a secondary public school, the University Park Campus School (UPCS), that is also a professional development school for Clark's teacher education program. Because of its long hours and demanding curricula, UPCS has been lauded as a model for collaboration between a university and an urban district. Students are able to attend Clark University free of charge upon graduation, provided they meet certain residency and admissions requirements. In the May 16, 2005, issue of Newsweek, UPCS was named the 68th best high school in the nation. UPCS was featured in a page-one story entitled "Town-grown triumph: In poorest part of Worcester, Clark helps put children on path to college" of the November 22, 2007, edition of The Boston Globe.
The UPCS collaborative is one of several sponsored by Clark's Jacob Hiatt Center for Urban Education focused on urban teacher education and school reform.
Clark has seven research institutes and centers.
The William and Jane Mosakowski Institute for Public Enterprise seeks to improve through the successful mobilization of use-inspired research the effectiveness of government and other institutions in addressing social concerns. The institute focuses on important social issues, including focal areas such as education reform, environmental sustainability, access to healthcare, human development, well-being and global change.
The George Perkins Marsh Institute conducts collaborative, interdisciplinary research on human-environment relationships and the human dimensions of global environmental change.
The Strassler Family Center for Holocaust and Genocide Studies an interdisciplinary center, founded in 1998, which focuses on the causes and effects of Holocausts and Genocides around the world. It is housed in Lasry House, donated by investor Marc Lasry and his wife Cathy in honor of their fathers Irwin Cohen and Moise Lasry. Debórah Dwork is the founding director and also Rose Professor of Holocaust History at Clark.
The Jacob Hiatt Center for Urban Education develops models of urban schooling, teaching and teacher education through local partnership, in order to learn from these models and expand the knowledge-base of effective practice through research.
The Center for Risk and Security (CRS) at the George Perkins Marsh Institute conducts in-depth studies of homeland security issues using a risk-analysis perspective. The center's broad range of security issues includes: terrorism; disaster management; law and human rights; resource availability; and public health.
The Center for Technology, Environment and Development (CENTED), founded in 1987, is a center for the study of natural and technological hazards in the United States. Projects include theoretical work on hazard analysis, hazard taxonomies, vulnerability, environmental equity, corporate risk management, emergency planning and hazardous waste transportation.
Clark Labs is engaged in the research and development on geospatial technologies including the development of computer software and analytical techniques for GIS and remote sensing with an emphasis on monitoring and modeling earth system dynamics. Clark Labs continues to develop and distribute TerrSet (formerly IDRISI), a geographic information system (GIS) software package that is in use at more than 40,000 sites in over 180 countries worldwide. Its chief is Dr. J. Ronald Eastman, creator of IDRISI.
The university's most famous alumnus was graduate student and professor Robert H. Goddard, a pioneering rocket scientist who conducted many experiments on campus. Clark's first president, G. Stanley Hall, founded the American Psychological Association in July 1892 at Clark. Grayson L. Kirk, a president of the Council on Foreign Relations during the Cold War and the president of Columbia University during the student protests of 1968 received his master's degree from the university, as did D'Army Bailey a prominent civil rights activist and the founder of the National Civil Rights Museum in Memphis, Tennessee. Clark is also notable for being the site of Sigmund Freud's only lectures in the United States and for being the university where Chinese poet Xu Zhimo earned his BA.
Gus Van Sant's The Sea of Trees was filmed in part on Clark University's campus. Leading actor Matthew McConaughey's character, Arthur Brennan, is a physics professor and scenes were filmed in and around Clark's Sackler Sciences Center. Clark was also a shooting location for the thriller Black Car, an independent film about a law student out for revenge, with Clark as the law school. The novel Something for Nothing, a semi-comic take on the struggles of a professor new to academia written by economist Michael W. Klein, is set at the fictional "Kester College," which like Clark University has a large main building that legend says was designed so that it could be converted into a factory should the college fail. Klein began his teaching career at Clark. Burning Annie, an independent comedy, was written and produced by two Clark alumni and is a semi-autobiographical film based on the experiences of one at Clark. One of the main characters in Annie Baker's Pulitzer Prize-winning play The Flick, an African American movie usher named Avery, is a Clarkie with a full-ride to attend the university.

Robert Hutchings Goddard (October 5, 1882 – August 10, 1945) was an American engineer, professor, physicist, and inventor who is credited with creating and building the world's first liquid-fueled rocket. Goddard successfully launched his rocket on March 16, 1926, which ushered in an era of space flight and innovation. He and his team launched 34 rockets between 1926 and 1941, achieving altitudes as high as 2.6 km (1.6 mi) and speeds as fast as 885 km/h (550 mph).
Goddard's work as both theorist and engineer anticipated many of the developments that would make spaceflight possible. He has been called the man who ushered in the Space Age.: xiii  Two of Goddard's 214 patented inventions, a multi-stage rocket (1914), and a liquid-fuel rocket (1914), were important milestones toward spaceflight. His 1919 monograph A Method of Reaching Extreme Altitudes is considered one of the classic texts of 20th-century rocket science. Goddard successfully pioneered modern methods such as two-axis control (gyroscopes and steerable thrust) to allow rockets to control their flight effectively.
Although his work in the field was revolutionary, Goddard received little public support, moral or monetary, for his research and development work.: 92, 93  He was a shy person, and rocket research was not considered a suitable pursuit for a physics professor.: 12  The press and other scientists ridiculed his theories of spaceflight. As a result, he became protective of his privacy and his work.
Years after his death, at the dawn of the Space Age, Goddard came to be recognized as one of the founding fathers of modern rocketry, along with Robert Esnault-Pelterie, Konstantin Tsiolkovsky, and Hermann Oberth. He not only recognized early on the potential of rockets for atmospheric research, ballistic missiles and space travel but also was the first to scientifically study, design, construct and fly the precursory rockets needed to eventually implement those ideas.
NASA's Goddard Space Flight Center was named in Goddard's honor in 1959. He was also inducted into the International Aerospace Hall of Fame in 1966, and the International Space Hall of Fame in 1976.
Goddard was born in Worcester, Massachusetts, to Nahum Danford Goddard (1859–1928) and Fannie Louise Hoyt (1864–1920). Robert was their only child to survive; a younger son, Richard Henry, was born with a spinal deformity and died before his first birthday. Nahum  was employed by  manufacturers, and he invented several useful tools. Goddard had English paternal family roots in New England with William Goddard (1628–91) a London grocer who settled in Watertown, Massachusetts in 1666. On his maternal side he was descended from John Hoyt and other settlers of Massachusetts in the late 1600s.
Shortly after his birth, the family moved to Boston. With a curiosity about nature, he studied the heavens using a telescope from his father and observed the birds flying. Essentially a country boy, he loved the outdoors and hiking with his father on trips to Worcester and became an excellent marksman with a rifle.: 63, 64  In 1898, his mother contracted tuberculosis and they moved back to Worcester for the clear air. On Sundays, the family attended the Episcopal church, and Robert sang in the choir.: 16 
With the electrification of American cities in the 1880s, the young Goddard became interested in science—specifically, engineering and technology. When his father showed him how to generate static electricity on the family's carpet, the five-year-old's imagination was sparked. Robert experimented, believing he could jump higher if the zinc from a battery could be charged by scuffing his feet on the gravel walk. But, holding the zinc, he could jump no higher than usual.: 15  Goddard halted the experiments after a warning from his mother that if he succeeded, he could "go sailing away and might not be able to come back.": 9 
He experimented with chemicals and created a cloud of smoke and an explosion in the house.: 64 
Goddard's father further encouraged Robert's scientific interest by providing him with a telescope, a microscope, and a subscription to Scientific American.: 10  Robert developed a fascination with flight, first with kites and then with balloons. He became a thorough diarist and documenter of his work—a skill that would greatly benefit his later career. These interests merged at age 16, when Goddard attempted to construct a balloon out of aluminum, shaping the raw metal in his home workshop, and filling it with hydrogen. After nearly five weeks of methodical, documented efforts, he finally abandoned the project, remarking, "... balloon will not go up. ... Aluminum is too heavy. Failior   crowns enterprise." However, the lesson of this failure did not restrain Goddard's growing determination and confidence in his work.: 21  He wrote in 1927, "I imagine an innate interest in mechanical things was inherited from a number of ancestors  who were machinists.": 7 
He became interested in space when he read H. G. Wells' science fiction classic The War of the Worlds at 16 years old. His dedication to pursuing space flight became fixed on October 19, 1899. The 17-year-old Goddard climbed a cherry tree to cut off dead limbs. He was transfixed by the sky, and his imagination grew. He later wrote:
On this day I climbed a tall cherry tree at the back of the barn ... and as I looked toward the fields at the east, I imagined how wonderful it would be to make some device which had even the possibility of ascending to Mars, and how it would look on a small scale, if sent up from the meadow at my feet. I have several photographs of the tree, taken since, with the little ladder I made to climb it, leaning against it.
It seemed to me then that a weight whirling around a horizontal shaft, moving more rapidly above than below, could furnish lift by virtue of the greater centrifugal force at the top of the path.

I was a different boy when I descended the tree from when I ascended. Existence at last seemed very purposive.: 26 For the rest of his life, he observed October 19 as "Anniversary Day", a private commemoration of the day of his greatest inspiration.
The young Goddard was a thin and frail boy, almost always in fragile health. He suffered from stomach problems, pleurisy, colds, and bronchitis, and he fell two years behind his classmates. He became a voracious reader, regularly visiting the local public library to borrow books on the physical sciences.: 16, 19 
Goddard's interest in aerodynamics led him to study some of Samuel Langley's scientific papers in the periodical Smithsonian. In these papers, Langley wrote that birds flap their wings with different force on each side to turn in the air. Inspired by these articles, the teenage Goddard watched swallows and chimney swifts from the porch of his home, noting how subtly the birds moved their wings to control their flight. He noted how remarkably the birds controlled their flight with their tail feathers, which he called the birds' equivalent of ailerons. He took exception to some of Langley's conclusions and in 1901 wrote a letter to St. Nicholas magazine: 5  with his own ideas. The editor of  St. Nicholas declined to publish Goddard's letter, remarking that birds fly with a certain amount of intelligence and that "machines will not act with such intelligence.": 31  Goddard disagreed, believing that a man could control a flying machine with his own intelligence.
Around this time, Goddard read Newton's Principia Mathematica, and found that Newton's Third Law of Motion applied to motion in space. He wrote later about his own tests of the Law:
I began to realize that there might be something after all to Newton's Laws. The Third Law was accordingly tested, both with devices suspended by rubber bands and by devices on floats, in the little brook back of the barn, and the said law was verified conclusively. It made me realize that if a way to navigate space were to be discovered, or invented, it would be the result of a knowledge of physics and mathematics.: 32 As his health improved, Goddard continued his formal schooling as a 19-year-old sophomore at South High Community School in Worcester in 1901. He excelled in his coursework, and his peers twice elected him class president. Making up for  lost time, he studied books on mathematics, astronomy, mechanics and composition from the school library.: 32  At his graduation ceremony in 1904, he gave his class oration as valedictorian. In his speech, entitled "On Taking Things for Granted", Goddard included a section that would become emblematic of his life:
 ust as in the sciences we have learned that we are too ignorant to safely pronounce anything impossible, so for the individual, since we cannot know just what are his limitations, we can hardly say with certainty that anything is necessarily within or beyond his grasp. Each must remember that no one can predict to what heights of wealth, fame, or usefulness he may rise until he has honestly endeavored, and he should derive courage from the fact that all sciences have been, at some time, in the same condition as he, and that it has often proved true that the dream of yesterday is the hope of today and the reality of tomorrow.: 19  Goddard enrolled at Worcester Polytechnic Institute in 1904.: 41  He quickly impressed the head of the physics department, A. Wilmer Duff, with his thirst for knowledge, and Duff took him on as a laboratory assistant and tutor.: 42  At WPI, Goddard joined the Sigma Alpha Epsilon fraternity and began a long courtship with high school classmate Miriam Olmstead, an honor student who had graduated with him as salutatorian. Eventually, she and Goddard were engaged, but they drifted apart and ended the engagement around 1909.: 51 
Goddard received his B.S. degree in physics from Worcester Polytechnic in 1908,: 50  and after serving there for a year as an instructor in physics, he began his graduate studies at Clark University in Worcester in the fall of 1909. Goddard received his M.A. degree in physics from Clark University in 1910, and then stayed at Clark to complete his Ph.D. in physics in 1911. He spent another year at Clark as an honorary fellow in physics, and in 1912 he accepted a research fellowship at Princeton University's Palmer Physical Laboratory.: 56–58 
The high school student summed up his ideas on space travel in a proposed article, "The Navigation of Space," which he submitted to the Popular Science News. The journal's editor returned it, saying that they could not use it "in the near future.": 34 
While still an undergraduate, Goddard wrote a paper proposing a method for balancing airplanes using gyro-stabilization. He submitted the idea to Scientific American, which published the paper in 1907. Goddard later wrote in his diaries that he believed his paper was the first proposal of a way to automatically stabilize aircraft in flight.: 50  His proposal came around the same time as other scientists were making breakthroughs in developing functional gyroscopes.
While studying physics at WPI, ideas came to Goddard's mind that sometimes seemed impossible, but he was compelled to record them for future investigation. He wrote that "there was something inside which simply would not stop working." He purchased some cloth-covered notebooks and began filling them with a variety of thoughts, mostly concerning his dream of space travel.: 11–13  He considered centrifugal force, radio waves, magnetic reaction, solar energy, atomic energy, ion or electrostatic propulsion and other methods to reach space. After experimenting with solid fuel rockets he was convinced by 1909 that chemical-propellant engines were the answer.: 11–12  A particularly complex concept was set down in June 1908: Sending a camera around distant planets, guided by measurements of gravity along the trajectory, and returning to earth.: 14 
His first writing on the possibility of a liquid-fueled rocket came on February 2, 1909. Goddard had begun to study ways of increasing a rocket's efficiency using methods differing from conventional solid-fuel rockets. He wrote in his notebook about using liquid hydrogen as a fuel with liquid oxygen as the oxidizer. He believed that 50 percent efficiency could be achieved with these liquid propellants (i.e., half of the heat energy of combustion converted to the kinetic energy of the exhaust gases).: 55 
In the decades around 1910, radio was a new technology, fertile for innovation. In 1912, while working at Princeton University, Goddard investigated the effects of radio waves on insulators. In order to generate radio-frequency power, he invented a vacuum tube with a beam deflection that operated like a cathode-ray oscillator tube. His patent on this tube, which predated that of Lee De Forest, became central in the suit between Arthur A. Collins, whose small company made radio transmitter tubes, and AT&amp;T and RCA over his use of vacuum tube technology.  Goddard accepted only a consultant's fee from Collins when the suit was dropped. Eventually, the two big companies allowed the country's growing electronics industry to use the De Forest patents freely.
By 1912 he had in his spare time, using calculus, developed the mathematics which allowed him to calculate the position and velocity of a rocket in vertical flight, given the weight of the rocket and weight of the propellant and the velocity (with respect to the rocket frame) of the exhaust gases. In effect he had independently developed the Tsiolkovsky rocket equation published a decade earlier in Russia. Tsiolkovsky, however,  did not account for gravity nor drag. For vertical flight from the surface of Earth Goddard included in his differential equation the effects of gravity and aerodynamic drag.: 136  He wrote: "An approximate method was found necessary ... in order to avoid an unsolved problem in the calculus of variations. The solution that was obtained revealed the fact that surprisingly small initial masses would be necessary ... provided the gases were ejected from the rocket at a high velocity, and also provided that most of the rocket consisted of propellant material.": 338–9 
His first goal was to build a sounding rocket with which to study the atmosphere. Not only would such investigation aid meteorology, but it was necessary to determine temperature, density and wind speed as functions of altitude in order to design efficient space launch vehicles. He was very reluctant to admit that his ultimate goal was, in fact, to develop a vehicle for flights into space, since most scientists, especially in the United States, did not consider such a goal to be a realistic or practical scientific pursuit, nor was the public yet ready to seriously consider such ideas. Later, in 1933, Goddard said that "n no case must we allow ourselves to be deterred from the achievement of space travel, test by test and step by step, until one day we succeed, cost what it may.": 65, 67, 74, 101 
In early 1913, Goddard became seriously ill with tuberculosis and had to leave his position at Princeton. He then returned to Worcester, where he began a prolonged process of recovery at home. His doctors did not expect him to live. He decided he should spend time outside in the fresh air and walk for exercise, and he gradually improved.: 61–64  When his nurse discovered some of his notes in his bed, he kept them, arguing, "I have to live to do this work.": 66 
It was during this period of recuperation, however, that Goddard began to produce some of his most important work. As his symptoms subsided, he allowed himself to work an hour per day with his notes made at Princeton. He was afraid that nobody would be able to read his scribbling should he
succumb.: 63 
In the technological and manufacturing atmosphere of Worcester, patents were considered essential, not only to protect original work but as documentation of first discovery. He began to see the importance of his ideas as intellectual property, and thus began to secure those ideas before someone else did—and he would have to pay to use them. In May 1913, he wrote descriptions concerning his first rocket patent applications. His father brought them to a patent lawyer in  Worcester who helped him to refine his ideas for consideration. Goddard's first patent application was submitted in October 1913.: 63–70 
In 1914, his first two landmark patents were accepted and registered. The first, U.S. Patent 1,102,653, described a multi-stage rocket fueled with a solid "explosive material." The second, U.S. Patent 1,103,503, described a rocket fueled with a solid fuel (explosive material) or with liquid propellants (gasoline and liquid nitrous oxide). The two patents would eventually become important milestones in the history of rocketry. Overall, 214 patents were published, some posthumously by his wife.
In the fall of 1914 Goddard's health had improved, and he accepted a part-time position as an instructor and research fellow at Clark University.: 73  His position at Clark allowed him to further his rocketry research. He ordered numerous supplies that could be used to build rocket prototypes for launch and spent much of 1915 in preparation for his first tests. Goddard's first test launch of a powder rocket came on an early evening in 1915 following his daytime classes at Clark.: 74  The launch was loud and bright enough to arouse the alarm of the campus janitor, and Goddard had to reassure him that his experiments, while being serious study, were also quite harmless. After this incident Goddard took his experiments inside the physics lab in order to limit any disturbance.
At the Clark physics lab Goddard conducted static tests of powder rockets to measure their thrust and efficiency. He found his earlier estimates to be verified; powder rockets were converting only about two percent of the thermal energy in their fuel into thrust and kinetic energy. At this point he applied de Laval nozzles, which were generally used with steam turbine engines, and these greatly improved efficiency. (Of the several definitions of rocket efficiency, Goddard measured in his laboratory what is today called the internal efficiency of the engine: the ratio of the kinetic energy of the exhaust gases to the available thermal energy of combustion, expressed as a percentage.): 130  By mid-summer of 1915 Goddard had obtained an average efficiency of 40 percent with a nozzle exit velocity of 6,728 feet (2,051 meters) per second.: 75  Connecting a combustion chamber full of gunpowder to various converging-diverging expansion (de Laval) nozzles, Goddard was able in static tests to achieve engine efficiencies of more than 63% and exhaust velocities of over 7,000 feet (2,134 meters) per second.: 78 
Few would recognize it at the time, but this little engine was a major breakthrough. These experiments suggested that rockets could be made powerful enough to escape Earth and travel into space. This engine and subsequent experiments sponsored by the Smithsonian Institution were the beginning of modern rocketry and, ultimately, space exploration. Goddard realized, however, that it would take the more efficient liquid propellants to reach space.
Later that year, Goddard designed an elaborate experiment at the Clark physics lab and proved that a rocket would perform in a vacuum such as that in space. He believed it would, but many other scientists were not yet convinced. His experiment demonstrated that a rocket's performance actually decreases under atmospheric pressure.
In September 1906 he wrote in his notebook about using the repulsion of electrically charged particles (ions) to produce thrust.: 13  From 1916 to 1917, Goddard built and tested the first known experimental ion thrusters, which he thought might be used for propulsion in the near-vacuum conditions of outer space. The small glass engines he built were tested at atmospheric pressure, where they generated a stream of ionized air.
By 1916, the cost of Goddard's rocket research had become too great for his modest teaching salary to bear.: 76  He began to solicit potential sponsors for financial assistance, beginning with the Smithsonian Institution, the National Geographic Society, and the Aero Club of America.
In his letter to the Smithsonian in September 1916, Goddard claimed he had achieved a 63% efficiency and a nozzle velocity of almost 2438 meters per second. With these performance levels, he believed a rocket could vertically lift a weight of 1 lb (0.45 kg) to a height of  232 miles (373 km) with an initial launch weight of only 89.6 lbs (40.64 kg). (Earth's atmosphere can be considered to end at 80 to 100 miles (130 to 160 km) altitude, where its drag effect on orbiting satellites becomes minimal.)
The Smithsonian was interested and asked Goddard to elaborate upon his initial inquiry. Goddard responded with a detailed manuscript he had already prepared, entitled A Method of Reaching Extreme Altitudes.: 79 
In January 1917, the Smithsonian agreed to provide Goddard with a five-year grant totaling US$5000.: 84  Afterward, Clark was able to contribute US$3500  and the use of their physics lab to the project. Worcester Polytechnic Institute also allowed him to use its abandoned Magnetics Laboratory on the edge of campus during this time, as a safe place for testing.: 85  WPI also made some parts in their machine shop.
Goddard's fellow Clark scientists were astonished at the unusually large Smithsonian grant for rocket research, which they thought was not real science.: 85  Decades later, rocket scientists who knew how much it cost to research and develop rockets said that he had received little financial support.
Two years later, at the insistence of Dr. Arthur G. Webster, the world-renowned head of Clark's physics department,  Goddard arranged for the Smithsonian to publish the paper,  A Method..., which documented his work.: 102 
While at Clark University, Goddard did research into solar power using a parabolic dish to concentrate the Sun's rays on a machined piece of quartz, that was sprayed with mercury, which then heated water and drove an electric generator. Goddard believed his invention had overcome all the obstacles that had previously defeated other scientists and inventors, and he had his findings published in the November 1929 issue of  Popular Science.
Not all of Goddard's early work was geared toward space travel. As the United States entered World War I in 1917, the country's universities began to lend their services to the war effort. Goddard believed his rocket research could be applied to many different military applications, including mobile artillery, field weapons and naval torpedoes. He made proposals to the Navy and Army. No record exists in his papers of any interest by the Navy to Goddard's inquiry. However, Army Ordnance was quite interested, and Goddard met several times with Army personnel.: 89 
During this time, Goddard was also contacted, in early 1918, by a civilian industrialist in Worcester about the possibility of manufacturing rockets for the military. However, as the businessman's enthusiasm grew, so did Goddard's suspicion. Talks eventually broke down as Goddard began to fear his work might be appropriated by the business. However, an Army Signal Corps officer tried to make Goddard cooperate, but he was called off by General George Squier of the Signal Corps who had been contacted by Secretary of the Smithsonian Institution, Charles Walcott.: 89–91  Goddard became leery of working with corporations and was careful to secure patents to "protect his ideas.": 152  These events led to the Signal Corps sponsoring Goddard's work during World War I.: 91 
Goddard proposed to the Army an idea for a tube-based rocket launcher as a light infantry weapon. The launcher concept became the precursor to the bazooka.: 92  The rocket-powered, recoil-free weapon was the brainchild of Goddard as a side project (under Army contract) of his work on rocket propulsion. Goddard, during his tenure at Clark University, and working at Mount Wilson Observatory for security reasons, designed the tube-fired rocket for military use during World War I. He and his co-worker Dr. Clarence N. Hickman successfully demonstrated his rocket to the U.S. Army Signal Corps at Aberdeen Proving Ground, Maryland, on November 6, 1918, using two music stands for a launch platform. The Army was impressed, but the Compiègne Armistice was signed only five days later, and further development was discontinued as World War I ended.
The delay in the development of the bazooka and other weapons was a result of the long recovery period required from Goddard's serious bout with tuberculosis. Goddard continued to be a part-time consultant to the U.S. Government at Indian Head, Maryland,: 121  until 1923, but his focus had turned to other research involving rocket propulsion, including work with liquid fuels and liquid oxygen.
Later, the former Clark University researcher Dr. Clarence N. Hickman and Army officers Col. Leslie Skinner and Lt. Edward Uhl continued Goddard's work on the bazooka. A shaped-charge warhead was attached to the rocket, leading to the tank-killing weapon used in World War II and to many other powerful rocket weapons.: 305 
In 1919 Goddard thought that it would be premature to disclose the results of his experiments because his engine was not sufficiently developed. Dr. Webster realized that Goddard had accomplished a good deal of fine work and insisted that Goddard publish his progress so far or he would take care of it himself, so Goddard asked the Smithsonian Institution if it would publish the report, updated with notes, that he had submitted in late 1916.: 102 
In late 1919, the Smithsonian published Goddard's groundbreaking work, A Method of Reaching Extreme Altitudes. The report describes Goddard's mathematical theories of rocket flight, his experiments with solid-fuel rockets, and the possibilities he saw of exploring Earth's atmosphere and beyond. Along with Konstantin Tsiolkovsky's earlier work, The Exploration of Cosmic Space by Means of Reaction Devices, which was not widely disseminated outside Russia, Goddard's report is regarded as one of the pioneering works of the science of rocketry, and 1750 copies were distributed worldwide. Goddard also sent a copy to individuals who requested one, until his personal supply was exhausted. Smithsonian aerospace historian Frank Winter said that this paper was "one of the key catalysts behind the international rocket movement of the 1920s and 30s."
Goddard described extensive experiments with solid-fuel rocket engines burning high-grade nitrocellulose smokeless powder. A critical breakthrough was the use of the steam turbine nozzle invented by the Swedish inventor Gustaf de Laval. The de Laval nozzle allows the most efficient (isentropic) conversion of the energy of hot gases into forward motion. By means of this nozzle, Goddard increased the efficiency of his rocket engines from two percent to 64 percent and obtained supersonic exhaust velocities of over Mach 7.: 44 
Though most of this work dealt with the theoretical and experimental relations between propellant, rocket mass, thrust, and velocity, a final section, entitled "Calculation of minimum mass required to raise one pound to an 'infinite' altitude," discussed the possible uses of rockets, not only to reach the upper atmosphere but to escape from Earth's gravitation altogether. He determined, using an approximate method to solve his differential equation of motion for vertical flight, that a rocket with an effective exhaust velocity (see specific impulse) of 7000 feet per second and an initial weight of 602 pounds would be able to send a one-pound payload to an infinite height. Included as a thought experiment was the idea of launching a rocket to the Moon and igniting a mass of flash powder on its surface, so as to be visible through a telescope. He discussed the matter seriously, down to an estimate of the amount of powder required. Goddard's conclusion was that a rocket with starting mass of 3.21 tons could produce a flash "just visible" from Earth, assuming a final payload weight of 10.7 pounds.
Goddard eschewed publicity, because he did not have time to reply to criticism of his work, and his imaginative ideas about space travel were shared only with private groups he trusted. He did, though, publish and talk about the rocket principle and sounding rockets, since these subjects were not too "far out." In a letter to the Smithsonian, dated March 1920, he discussed: photographing the Moon and planets from rocket-powered fly-by probes, sending messages to distant civilizations on inscribed metal plates, the use of solar energy in space, and the idea of high-velocity ion propulsion. In that same letter, Goddard clearly describes the concept of the ablative heat shield, suggesting the landing apparatus be covered with "layers of a very infusible hard substance with layers of a poor heat conductor between" designed to erode in the same way as the surface of a meteor.
Every vision is a joke until the first man accomplishes it; once realized, it becomes commonplace.
–Response to a reporter's question following criticism in The New York Times, 1920.
The publication of Goddard's document gained him national attention from U.S. newspapers, most of it negative. Although Goddard's discussion of targeting the moon was only a small part of the work as a whole (eight lines on the next to last page of 69 pages), and was intended as an illustration of the possibilities rather than a declaration of intent, the papers sensationalized his ideas to the point of misrepresentation and ridicule. Even the Smithsonian had to abstain from publicity because of the amount of ridiculous correspondence received from the general public.: 113  David Lasser, who co-founded the American Rocket Society (ARS), wrote in 1931 that Goddard was subjected in the press to the "most violent attacks."
On January 12, 1920, a front-page story in The New York Times, "Believes Rocket Can Reach Moon", reported a Smithsonian press release about a "multiple-charge, high-efficiency rocket." The chief application envisaged was "the possibility of sending recording apparatus to moderate and extreme altitudes within the Earth's atmosphere", the advantage over balloon-carried instruments being ease of recovery, since "the new rocket apparatus would go straight up and come straight down." But it also mentioned a proposal "to  to the dark part of the new moon a sufficiently large amount of the most brilliant flash powder which, in being ignited on impact, would be plainly visible in a powerful telescope. This would be the only way of proving that the rocket had really left the attraction of the earth, as the apparatus would never come back, once it had escaped that attraction."
On January 13, 1920, the day after its front-page story about Goddard's rocket, an unsigned New York Times editorial, in a section entitled "Topics of the Times", scoffed at the proposal. The article, which bore the title "A Severe Strain on Credulity", began with apparent approval, but soon went on to cast serious doubt:
As a method of sending a missile to the higher, and even highest, part of the earth's atmospheric envelope, Professor Goddard's multiple-charge rocket is a practicable, and therefore promising device. Such a rocket, too, might carry self-recording instruments, to be released at the limit of its flight, and conceivable parachutes would bring them safely to the ground. It is not obvious, however, that the instruments would return to the point of departure; indeed, it is obvious that they would not, for parachutes drift exactly as balloons do.The article pressed further on Goddard's proposal to launch rockets beyond the atmosphere:
fter the rocket quits our air and really starts on its longer journey, its flight would be neither accelerated nor maintained by the explosion of the charges it then might have left. To claim that it would be is to deny a fundamental law of dynamics, and only Dr. Einstein and his chosen dozen, so few and fit, are licensed to do that. ... Of course,  only seems to lack the knowledge ladled out daily in high schools.The basis of that criticism was the then-common belief that thrust was produced by the rocket exhaust pushing against the atmosphere; Goddard realized that Newton's third law (reaction) was the actual principle and that thrust was possible in a vacuum.
A week after the New York Times editorial, Goddard released a signed statement to the Associated Press, attempting to restore reason to what had become a sensational story:
Too much attention has been concentrated on the proposed flash power experiment, and too little on the exploration of the atmosphere. ... Whatever interesting possibilities there may be of the method that has been proposed, other than the purpose for which it was intended, no one of them could be undertaken without first exploring the atmosphere.In 1924, Goddard published an article, "How my speed rocket can propel itself in vacuum", in Popular Science, in which he explained the physics and gave details of the vacuum experiments he had performed to prove the theory. But, no matter how he tried to explain his results, he was not understood by the majority. After one of Goddard's experiments in 1929, a local Worcester newspaper carried the mocking headline "Moon rocket misses target by 238,799.mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}1⁄2 miles."
Though the unimaginative public chuckled at the "moon man," his groundbreaking paper was read seriously by many rocketeers in America, Europe, and Russia who were stirred to build their own rockets. This work was his most important contribution to the quest to "aim for the stars.": 50 
Goddard worked alone with just his team of mechanics and machinists for many years.  This was a result of the harsh criticism from the media and other scientists, and his understanding of the military applications which foreign powers might use. Goddard became increasingly suspicious of others and often worked alone, except during the two World Wars, which limited the impact of much of his work. Another limiting factor was the lack of support from the American government, military and academia, all failing to understand the value of the rocket to study the atmosphere and near space, and for military applications. 

Nevertheless, Goddard had some influence on European rocketry pioneers like Hermann Oberth and his student Max Valier, at least as proponent of the idea of space rocketry and source of inspiration, although each side developed their technology and its scientific basis independently.  Eventually Fritz von Opel was instrumental in popularizing rockets as means of propulsion for vehicles. In the 1920s, he initiated together with Max Valier, co-founder of the "Verein für Raumschiffahrt", the world's first rocket program, Opel-RAK, leading to speed records for automobiles, rail vehicles and the first manned rocket-powered flight in September of 1929. Months earlier in 1928, one of his rocket-powered prototypes, the Opel RAK2, reached piloted by von Opel himself at the AVUS speedway in Berlin a record speed of 238 km/h, watched by 3000 spectators and world media, among them Fritz Lang, director of Metropolis and Woman in the Moon, world boxing champion Max Schmeling and many more sports and show business celebrities. A world record for rail vehicles was reached with RAK3 and a top speed of 256 km/h. After these successes, von Opel piloted the world's first public rocket-powered flight using Opel RAK.1, a rocket plane designed by Julius Hatry. World media reported on these efforts, including UNIVERSAL Newsreel of the US, causing as "Raketen-Rummel" or "Rocket Rumble" immense global public excitement, and in particular in Germany, where inter alia Wernher von Braun was highly influenced. The Great Depression led to an end of the Opel-RAK program, but Max Valier continued the efforts. After switching from solid-fuel to liquid-fuel rockets, he died while testing and is considered the first fatality of the dawning space age. As an 18-year-old von Braun also became a student of Oberth and eventually the head of the Nazi era rocket program.
As Germany became ever more war-like, Goddard  refused to communicate with German rocket experimenters, though he received more and more of their correspondence.: 131  Via Wernher von Braun and his team joining the US post-war programs there is nevertheless an indirect line of scientific and technology tradition from NASA back to Goddard.
Forty-nine years after its editorial mocking Goddard, on July 17, 1969—the day after the launch of Apollo 11—The New York Times published a short item under the headline "A Correction." The three-paragraph statement summarized its 1920 editorial and concluded:
Further investigation and experimentation have confirmed the findings of Isaac Newton in the 17th Century and it is now definitely established that a rocket can function in a vacuum as well as in an atmosphere. The Times regrets the error.Goddard began considering liquid propellants, including hydrogen and oxygen, as early as 1909. He knew that hydrogen and oxygen was the most efficient fuel/oxidizer combination. Liquid hydrogen was not readily available in 1921, however, and he selected gasoline as the safest fuel to handle.: 13 
Goddard began experimenting with liquid oxidizer, liquid fuel rockets in September 1921, and successfully tested the first liquid propellant engine in November 1923.: 520  It had a cylindrical combustion chamber, using impinging jets to mix and atomize liquid oxygen and gasoline.: 499–500 
In 1924–25, Goddard had problems developing a high-pressure piston pump to send fuel to the combustion chamber. He wanted to scale up the experiments, but his funding would not allow such growth. He decided to forego the pumps and use a pressurized fuel feed system applying pressure to the fuel tank from a tank of inert gas, a technique that is still used today. The liquid oxygen, some of which evaporated,  provided its own pressure.
On December 6, 1925, he tested the simpler pressure feed system. He conducted a static test on the firing stand at the Clark University physics laboratory. The engine successfully lifted its own weight in a 27-second test in the static rack. It was a major success for Goddard, proving that a liquid fuel rocket was possible.: 140  The test moved Goddard an important step closer to launching a rocket with liquid fuel.
Goddard conducted an additional test in December, and two more in January 1926. After that, he began preparing for a possible launch of the rocket system.
Goddard launched the world's first liquid-fueled (gasoline and liquid oxygen) rocket on March 16, 1926, in Auburn, Massachusetts. Present at the launch were his crew chief Henry Sachs, Esther Goddard, and Percy Roope, who was Clark's assistant professor in the physics department. Goddard's diary entry of the event was notable for its understatement:
March 16. Went to Auburn with S in am. E and Mr. Roope came out at 1 p.m. Tried rocket at 2.30. It rose 41 feet &amp; went 184 feet, in 2.5 secs., after the lower half of the nozzle burned off. Brought materials to lab. ...: 143 His diary entry the next day elaborated:
March 17, 1926. The first flight with a rocket using liquid propellants was made yesterday at Aunt Effie's farm in Auburn. ...
Even though the release was pulled, the rocket did not rise at first, but the flame came out, and there was a steady roar. After a number of seconds it rose, slowly until it cleared the frame, and then at express train speed, curving over to the left, and striking the ice and snow, still going at a rapid rate.: 143 The rocket, which was later dubbed "Nell", rose just 41 feet during a 2.5-second flight that ended 184 feet away in a cabbage field, but it was an important demonstration that liquid fuels and oxidizers were possible propellants for larger rockets. The launch site is now a National Historic Landmark, the Goddard Rocket Launching Site.
Viewers familiar with more modern rocket designs may find it difficult to distinguish the rocket from its launching apparatus in the well-known picture of "Nell". The complete rocket is significantly taller than Goddard but does not include the pyramidal support structure which he is grasping. The rocket's combustion chamber is the small cylinder at the top; the nozzle is visible beneath it. The fuel tank, which is also part of the rocket, is the larger cylinder opposite Goddard's torso. The fuel tank is directly beneath the nozzle and is protected from the motor's exhaust by an asbestos cone. Asbestos-wrapped aluminum tubes connect the motor to the tanks, providing both support and fuel transport. This layout is no longer used, since the experiment showed that this was no more stable than placing the combustion chamber and nozzle at the base. By May, after a series of modifications to simplify the plumbing, the combustion chamber and nozzle were placed in the now classic position, at the lower end of the rocket.: 259 
Goddard determined early that fins alone were not sufficient to stabilize the rocket in flight and keep it on the desired trajectory in the face of winds aloft and other disturbing forces. He added movable vanes in the exhaust, controlled by a gyroscope, to control and steer his rocket. (The Germans used this technique in their V-2.) He also introduced the more efficient swiveling engine in several rockets, basically the method used to steer large liquid-propellant missiles and launchers today.: 263–6 
After launch of one of Goddard's rockets in July 1929 again gained the attention of the newspapers, Charles Lindbergh learned of his work in a New York Times article. At the time, Lindbergh had begun to wonder what would become of aviation (even space flight) in the distant future and had settled on jet propulsion and rocket flight as a probable next step. After checking with the Massachusetts Institute of Technology (MIT) and being assured that Goddard was a bona fide physicist and not a crackpot, he phoned Goddard in November 1929.: 141  Professor Goddard met the aviator soon after in his office at Clark University. Upon meeting Goddard, Lindbergh was immediately impressed by his research, and Goddard was similarly impressed by the flier's interest. He discussed his work openly with Lindbergh, forming an alliance that would last for the rest of his life. While having long since become reluctant to share his ideas, Goddard showed complete openness with those few who shared his dream, and whom he felt he could trust.
By late 1929, Goddard had been attracting additional notoriety with each rocket launch. He was finding it increasingly difficult to conduct his research without unwanted distractions. Lindbergh discussed finding additional financing for Goddard's work and lent his famous name to Goddard's work. In 1930 Lindbergh made several proposals to industry and private investors for funding, which proved all but impossible to find following the recent U.S. stock market crash in October 1929.
In the spring of 1930, Lindbergh finally found an ally in the Guggenheim family. Financier Daniel Guggenheim agreed to fund Goddard's research over the next four years for a total of $100,000 (~$2 million today). The Guggenheim family, especially Harry Guggenheim, would continue to support Goddard's work in the years to come. The Goddards soon moved to Roswell, New Mexico
Because of the military potential of the rocket, Goddard, Lindbergh, Harry Guggenheim, the Smithsonian Institution and others tried in 1940, before the U.S. entered World War II, to convince the Army and Navy of its value. Goddard's services were offered, but there was no interest, initially. Two young, imaginative military officers eventually got the services to attempt to contract with Goddard just prior to the war. The Navy beat the Army to the punch and secured his services to build variable-thrust, liquid-fueled rocket engines for jet-assisted take-off (JATO) of aircraft.: 293–297  These rocket engines were the precursors to the larger throttlable rocket plane engines that helped launch the space age.
Astronaut Buzz Aldrin wrote that his father, Edwin Aldrin Sr. "was an early supporter of Robert Goddard."  The elder Aldrin was a student of physics under Goddard at Clark, and worked with Lindbergh to obtain the help of the Guggenheims. Buzz believed that if Goddard had received military support as Wernher von Braun's team had enjoyed in Germany, American rocket technology would have developed much more rapidly in World War II.
Before World War II there was a lack of vision and serious interest in the United States concerning the potential of rocketry, especially in Washington. Although the Weather Bureau was interested beginning in 1929 in Goddard's rocket for atmospheric research, the Bureau could not secure governmental funding.: 719, 746  Between the World Wars, the Guggenheim Foundation was the main source of funding for Goddard's research.: 46, 59, 60  Goddard's liquid-fueled rocket was neglected by his country, according to aerospace historian Eugene Emme, but was noticed and advanced by other nations, especially the Germans.: 63  Goddard showed remarkable prescience in 1923 in a letter to the Smithsonian. He knew that the Germans were very interested in rocketry and said he "would not be surprised if the research would become something in the nature of a race," and he wondered how soon the European "theorists" would begin to build rockets.: 136 
In 1936, the U.S. military attaché in Berlin asked Charles Lindbergh to visit Germany and learn what he could of their progress in aviation. Although the Luftwaffe showed him their factories and were open concerning their growing airpower, they were silent on the subject of rocketry. When Lindbergh told Goddard of this behavior, Goddard said, "Yes, they must have plans for the rocket. When will our own people in Washington listen to reason?": 272 
Most of the U.S.'s largest universities were also slow to realize rocketry's potential. Just before World War II, the head of the aeronautics department at MIT, at a meeting held by the Army Air Corps to discuss project funding, said that the California Institute of Technology (Caltech) "can take the Buck Rogers Job ." In 1941, Goddard tried to recruit an engineer for his team from MIT but couldn't find one who was interested.: 326   There were some exceptions: MIT was at least teaching basic rocketry,: 264  and Caltech had courses in rocketry and aerodynamics. After the war, Dr. Jerome Hunsaker of MIT, having studied Goddard's patents, stated that "Every liquid-fuel rocket that flies is a Goddard rocket.": 363 
While away in Roswell, Goddard was still head of the physics department at Clark University, and Clark allowed him to devote most of his time to rocket research. Likewise, the University of California, Los Angeles (UCLA) permitted astronomer Samuel Herrick to pursue research in space vehicle guidance and control, and shortly after the war to teach courses in spacecraft guidance and orbit determination. Herrick began corresponding with Goddard in 1931 and asked if he should work in this new field, which he named astrodynamics. Herrick said that Goddard had the vision to advise and encourage him in his use of celestial mechanics "to anticipate the basic problem of space navigation." Herrick's work contributed substantially to America's readiness to control flight of Earth satellites and send men to the Moon and back.
With new financial backing, Goddard eventually relocated to Roswell, New Mexico, in summer of 1930,: 46  where he worked with his team of technicians in near-isolation and relative secrecy for years. He had consulted a meteorologist as to the best area to do his work, and Roswell seemed ideal. Here they would not endanger anyone, would not be bothered by the curious and would experience a more moderate climate (which was also better for Goddard's health).: 177  The locals valued personal privacy, knew Goddard desired his, and when travelers asked where Goddard's facilities were located, they would likely be misdirected.: 261 
By September 1931, his rockets had the now familiar appearance of a smooth casing with tail-fins. He began experimenting with gyroscopic guidance and made a flight test of such a system in April 1932. A gyroscope mounted on gimbals electrically controlled steering vanes in the exhaust, similar to the system used by the German V-2 over 10 years later. Though the rocket crashed after a short ascent, the guidance system had worked, and Goddard considered the test a success.: 193–5 
A temporary loss of funding from the Guggenheims, as a result of the depression, forced Goddard in spring of 1932 to return to his much-loathed professorial responsibilities at Clark University. He remained at the university until the autumn of 1934, when funding resumed. Because of the death of the senior Daniel Guggenheim, the management of funding was taken on by his son, Harry Guggenheim. Upon his return to Roswell, he began work on his A series of rockets, 4 to 4.5 meters long, and powered by gasoline and liquid oxygen pressurized with nitrogen. The gyroscopic control system was housed in the middle of the rocket, between the propellant tanks.: xv, 15–46 
The A-4 used a simpler pendulum system for guidance, as the gyroscopic system was being repaired. On March 8, 1935, it flew up to 1,000 feet, then turned into the wind and, Goddard reported, "roared in a powerful descent across the prairie, at close to, or at, the speed of sound." On March 28, 1935, the A-5 successfully flew vertically to an altitude of (0.91 mi; 4,800 ft) using his gyroscopic guidance system. It then turned to a nearly horizontal path, flew 13,000 feet and achieved a maximum speed of 550 miles per hour. Goddard was elated because the guidance system kept the rocket on a vertical path so well.: 208 : 978–9 
In 1936–1939, Goddard began work on the K and L series rockets, which were much more massive and designed to reach very high altitude. The K series consisted of static bench tests of a more powerful engine, achieving a thrust of 624 lbs in February 1936.  This work was plagued by trouble with chamber burn-through. In 1923, Goddard had built a regeneratively cooled engine, which circulated liquid oxygen around the outside of the combustion chamber, but he deemed the idea too complicated. He then used a curtain cooling method that involved spraying excess gasoline, which evaporated around the inside wall of the combustion chamber, but this scheme did not work well, and the larger rockets failed. Goddard returned to a smaller design, and his L-13 reached an altitude of 2.7 kilometers (1.7 mi; 8,900 ft), the highest of any of his rockets. Weight was reduced by using thin-walled fuel tanks wound with high-tensile-strength wire.: 71–148 
Goddard experimented with many of the features of today's large rockets, such as multiple combustion chambers and nozzles. In November 1936, he flew the world's first rocket (L-7) with multiple chambers, hoping to increase thrust without increasing the size of a single chamber. It had four combustion chambers, reached a height of 200 feet, and corrected its vertical path using blast vanes until one chamber burned through. This flight demonstrated that a rocket with multiple combustion chambers could fly stably and be easily guided.: 96  In July 1937 he replaced the guidance vanes with a movable tail section containing a single combustion chamber, as if on gimbals (thrust vectoring). The flight was of low altitude, but a large disturbance, probably caused by a change in the wind velocity, was corrected back to vertical. In an August test the flight path was corrected seven times by the movable tail and was captured on film by Mrs Goddard.: 113–116 

From 1940 to 1941, Goddard worked on the P series of rockets, which used propellant turbopumps (also powered by gasoline and liquid oxygen). The lightweight pumps produced higher propellant pressures, permitting a more powerful engine (greater thrust) and a lighter structure (lighter tanks and no pressurization tank), but two launches both ended in crashes after reaching an altitude of only a few hundred feet. The turbopumps worked well, however, and Goddard was pleased.: 187–215 
When Goddard mentioned the need for turbopumps, Harry Guggenheim suggested that he contact pump manufacturers to aid him. None were interested, as the development cost of these miniature pumps was prohibitive. Goddard's team was therefore left on its own and from September 1938 to June 1940 designed and tested the small turbopumps and gas generators to operate the turbines. Esther later said that the pump tests were "the most trying and disheartening phase of the research.": 274–5 
Goddard was able to flight-test many of his rockets, but many resulted in what the uninitiated would call failures, usually resulting from engine malfunction or loss of control. Goddard did not consider them failures, however, because he felt that he always learned something from a test.: 45  Most of his work involved static tests, which are a standard procedure today, before a flight test. He wrote to a correspondent:  "It is not a simple matter to differentiate unsuccessful from successful experiments. ...  work that is finally successful is the result of a series of unsuccessful tests in which difficulties are gradually eliminated.": 274 
Jimmy Doolittle was introduced to the field of space science at an early point in its history. He recalls in his autobiography, "I became interested in rocket development in the 1930s when I met Robert H. Goddard, who laid the foundation. ... While with Shell Oil I worked with him on the development of a type of fuel. ... " Harry Guggenheim and Charles Lindbergh arranged for (then Major) Doolittle to discuss with Goddard a special blend of gasoline. Doolittle flew himself to Roswell in October 1938 and was given a tour of Goddard's shop and a "short course" in rocketry.  He then wrote a memo, including a rather detailed description of Goddard's rocket. In closing he said, "interplanetary transportation is probably a dream of the very distant future, but with the moon only a quarter of a million miles away—who knows!" In July 1941, he wrote Goddard that he was still interested in his rocket propulsion research. The Army was interested only in JATO at this point. However, Doolittle and Lindbergh were concerned about the state of rocketry in the US, and Doolittle remained in touch with Goddard.: 1208–16, 1334, 1443 
Shortly after World War II, Doolittle spoke concerning Goddard to an American Rocket Society (ARS) conference at which a  large number interested in rocketry attended. He later stated that at that time "we  had not given much credence to the tremendous potential of rocketry." In 1956, he was appointed chairman of the National Advisory Committee for Aeronautics (NACA) because the previous chairman, Jerome C. Hunsaker, thought Doolittle to be more sympathetic than other scientists and engineers to the rocket, which was increasing in importance as a scientific tool as well as a weapon.: 516  Doolittle was instrumental in the successful transition of the NACA to the National Aeronautics and Space Administration (NASA) in 1958. He was offered the position as first administrator of NASA, but he turned it down.
Between 1926 and 1941, the following 35 rockets were launched:
steering         
As an instrument for reaching extreme altitudes, Goddard's rockets were not very successful; they did not achieve an altitude greater than 2.7 km in 1937, while a balloon sonde had already reached 35 km in 1921.: 456  By contrast, German rocket scientists had achieved an altitude of 2.4 km with the A-2 rocket in 1934,: 138  8 km by 1939 with the A-5,: 39  and 176 km in 1942 with the A-4 (V-2) launched vertically, reaching the outer limits of the atmosphere and into space.: 221 
Goddard's pace was slower than the Germans' because he did not have the resources they did. Simply reaching high altitudes was not his primary goal; he was trying, with a methodical approach, to perfect his liquid fuel engine and subsystems such as guidance and control so that his rocket could eventually achieve high altitudes without tumbling in the rare atmosphere, providing a stable vehicle for the experiments it would eventually carry. He had built the necessary turbopumps and was on the verge of building larger, lighter, more reliable rockets to reach extreme altitudes carrying scientific instruments when World War II intervened and changed the path of American history. He hoped to return to his experiments in Roswell after the war.: 206, 230, 330–1 : 923–4 
Though by the end of the Roswell years much of his technology had been replicated independently by others, he introduced new developments to rocketry that were used in this new enterprise: lightweight turbopumps, variable-thrust engine (in U.S.), engine with multiple combustion chambers and nozzles, and curtain cooling of combustion chamber.
Although Goddard had brought his work in rocketry to the attention of the United States Army, between World Wars, he was rebuffed, since the Army largely failed to grasp the military application of large rockets and said there was no money for new experimental weapons.: 297  German military intelligence, by contrast, had paid attention to Goddard's work. The Goddards noticed that some mail had been opened, and some mailed reports had gone missing. An accredited military attaché to the US, Friedrich von Boetticher, sent a four-page report to the Abwehr in 1936, and the spy Gustav Guellich sent a mixture of facts and made-up information, claiming to have visited Roswell and witnessed a launch. The Abwehr was very interested and responded with more questions about Goddard's work.: 77 : 227–8  Guellich's reports did include information about fuel mixtures and the important concept of fuel-curtain cooling,: 39–41  but thereafter the Germans received very little information about Goddard.
The Soviet Union had a spy in the U.S. Navy Bureau of Aeronautics. In 1935, she gave them a report Goddard had written for the Navy in 1933. It contained results of tests and flights and suggestions for military uses of his rockets. The Soviets considered this to be very valuable information. It provided few design details, but gave them the direction and knowledge about Goddard's progress.: 386–7 
Navy Lieutenant Charles F. Fischer, who had visited Goddard in Roswell earlier and gained his confidence, believed Goddard was doing valuable work and was able to convince the Bureau of Aeronautics in September 1941 that Goddard could build the JATO unit the Navy desired. While still in Roswell, and before the Navy contract took effect, Goddard began in September to apply his technology to build a variable-thrust engine to be attached to a PBY seaplane. By May 1942, he had a unit that could meet the Navy's requirements and be able to launch a heavily loaded aircraft from a short runway. In February, he received part of a PBY with bullet holes apparently acquired in the Pearl Harbor attack. Goddard wrote to Guggenheim that "I can think of nothing that would give me greater satisfaction than to have it contribute to the inevitable retaliation.": 322, 328–9, 331, 335, 337 
In April, Fischer notified Goddard that the Navy wanted to do all its rocket work at the Engineering Experiment Station at Annapolis. Esther, worried that a move to the climate of Maryland would cause Robert's health to deteriorate faster, objected. But the patriotic Goddard replied, "Esther, don't you know there's a war on?" Fischer also questioned the move, as Goddard could work just as well in Roswell. Goddard simply answered, "I was wondering when you would ask me." Fischer had wanted to offer him something bigger—a long range missile—but JATO was all he could manage, hoping for a greater project later.: 338, 9  It was a case of a square peg in a round hole, according to a disappointed Goddard.: 209 
Goddard and his team had already been in Annapolis a month and had tested his constant-thrust JATO engine when he received a Navy telegram, forwarded from Roswell, ordering him to Annapolis. Lt. Fischer asked for a crash effort. By August, his engine was producing 800 lbs of thrust for 20 seconds, and Fischer was anxious to try it on a PBY. On the sixth test run, with all bugs worked out, the PBY, piloted by Fischer, was pushed into the air from the Severn River. Fischer landed and prepared to launch again. Goddard had wanted to check the unit, but radio contact with the PBY had been lost. On the seventh try, the engine caught fire. The plane was 150 feet up when flight was aborted. Because Goddard had installed a safety feature at the last minute, there was no explosion and no lives were lost. The problem's cause was traced to hasty installation and rough handling. Cheaper, safer solid fuel JATO engines were eventually selected by the armed forces. An engineer later said, "Putting  rocket on a seaplane was like hitching an eagle to a plow.": 344–50 
Goddard's first biographer Milton Lehman notes:
In its 1942 crash effort to perfect an aircraft booster, the Navy was beginning to learn its way in rocketry. In similar efforts, the Army Air Corps was also exploring the field . Compared to Germany's massive program, these beginnings were small, yet essential to later progress. They helped develop a nucleus of trained American rocket engineers, the first of the new breed who would follow the professor into the Age of Space.: 350 In August 1943, President Atwood at Clark wrote to Goddard that the university was losing the acting head of the Physics Department, was taking on "emergency work" for the Army, and he was to "report for duty or declare the position vacant." Goddard replied that he believed he was needed by the Navy, was nearing retirement age, and was unable to lecture because of his throat problem, which did not allow him to talk above a whisper. He regretfully resigned as Professor of Physics and expressed his deepest appreciation for all Atwood and the Trustees had done for him and indirectly for the war effort.: 1509–11  In June he had gone to see a throat specialist in Baltimore, who recommended that he not talk at all, to give his throat a rest.: 1503 
The station, under Lt Commander Robert Truax, was developing another JATO engine in 1942 that used hypergolic propellants, eliminating the need for an ignition system. Chemist Ensign Ray Stiff had discovered in the literature in February that aniline and nitric acid burned fiercely immediately when mixed.: 1488 : 172  Goddard's team built the pumps for the aniline fuel and the nitric acid oxidizer and participated in the static testing.: 1520, 1531  The Navy delivered the pumps to Reaction Motors (RMI) to use in developing a gas generator for the pump turbines. Goddard went to RMI to observe testing of the pump system and would eat lunch with the RMI engineers.: 1583   (RMI was  the first firm formed to build rocket engines and built engines for the Bell X-1 rocket plane: 1  and Viking (rocket).: 169  RMI offered Goddard one-fifth interest in the company and a partnership after the war.: 1583 ) Goddard went with Navy people in December 1944 to confer with RMI on division of labor, and his team was to provide the propellant pump system for a rocket-powered interceptor because they had more experience with pumps.: 100  He consulted with RMI from 1942 through 1945.: 311  Though previously competitors, Goddard had a good working relationship with RMI, according to historian Frank H. Winter.
The Navy had Goddard build a pump system for Caltech's use with acid-aniline propellants. The team built a 3000-lb thrust engine using a cluster of four 750-lb thrust motors.: 1574, 1592  They also developed 750-lb engines for the Navy's Gorgon guided interceptor missile (experimental Project Gorgon). Goddard continued to develop the variable-thrust engine with gasoline and lox because of the hazards involved with the hypergolics.: 1592 : 355, 371 
Despite Goddard's efforts to convince the Navy that liquid-fueled rockets had greater potential, he said that the Navy had no interest in long-range missiles.: 1554  However, the Navy asked him to perfect the throttleable JATO engine. Goddard made improvements to the engine, and in November it was demonstrated to the Navy and some officials from Washington. Fischer invited the spectators to operate the controls; the engine blasted out over the Severn at full throttle with no hesitation, idled, and roared again at various thrust levels. The test was perfect, exceeding the Navy's requirements. The unit was able to be stopped and restarted, and it produced a medium thrust of 600 pounds for 15 seconds and a full thrust of 1,000 pounds for over 15 seconds. A Navy Commander commented that "It was like being Thor, playing with thunderbolts." Goddard had produced the essential propulsion control system of the rocket plane. The Goddards celebrated by attending the Army-Navy football game and attending the Fischers' cocktail party.: 350–1 
This engine was the basis of the Curtiss-Wright XLR25-CW-1 two-chamber, 15,000-pound variable-thrust engine that powered the Bell X-2 research rocket plane. After World War II, Goddard's team and some patents went to Curtiss-Wright Corporation. "Although his death in August 1945 prevented him from participating in the actual development of this engine, it was a direct descendent of his design.": 1606  Clark University and the Guggenheim Foundation received the royalties from the use of the patents.  In September 1956, the X-2 was the first plane to reach 126,000 feet altitude and in its last flight exceeded Mach 3 (3.2) before losing control and crashing. The X-2 program advanced technology in areas such as steel alloys and aerodynamics at high Mach numbers.
Don't you know about your own rocket pioneer? Dr. Goddard was ahead of us all.
–Wernher von Braun, when asked about his work, following World War II
In the spring of 1945, Goddard saw a captured German V-2 ballistic missile, in the naval laboratory in Annapolis, Maryland, where he had been working under contract. The unlaunched rocket had been captured by the US Army from the Mittelwerk factory in the Harz mountains and samples began to be shipped by Special Mission V-2 on 22 May 1945.
After a thorough inspection, Goddard was convinced that the Germans had "stolen" his work. Though the design details were not exactly the same, the basic design of the V-2 was similar to one of Goddard's rockets. The V-2, however, was technically far more advanced than the most successful of the rockets designed and tested by Goddard. The Peenemünde rocket group led by Wernher von Braun may have benefited from the pre-1939 contacts to a limited extent,: 387–8  but had also started from the work of their own space pioneer, Hermann Oberth; they also had the benefit of intensive state funding, large-scale production facilities (using slave labor), and repeated flight-testing that allowed them to refine their designs. Oberth was a theorist and had never built a rocket, but he tested small liquid propellant thrust chambers in 1929-30 which were not advancements in the "state of the art.": 273, 275  In 1922 Oberth asked Goddard for a copy of his 1919 paper and was sent one.: 96 
Nevertheless, in 1963, von Braun, reflecting on the history of rocketry, said of Goddard: "His rockets ... may have been rather crude by present-day standards, but they blazed the trail and incorporated many features used in our most modern rockets and space vehicles". He once recalled that "Goddard's experiments in liquid fuel saved us years of work, and enabled us to perfect the V-2 years before it would have been possible." After  World War II von Braun reviewed Goddard's patents and believed they contained enough technical information to build a large missile.
Three features developed by Goddard appeared in the V-2: (1) turbopumps were used to inject fuel into the combustion chamber; (2) gyroscopically controlled vanes in the nozzle stabilized the rocket until external vanes in the air could do so; and (3) excess alcohol was fed in around the combustion chamber walls, so that a blanket of evaporating gas protected the engine walls from the combustion heat.

The Germans had been watching Goddard's progress before the war and became convinced that large, liquid fuel rockets were feasible. General Walter Dornberger, head of the V-2 project, used the idea that they were in a race with the U.S. and that Goddard had "disappeared" (to work with the Navy) as a way to persuade Hitler to raise the priority of the V-2.
Goddard avoided sharing details of his work with other scientists and preferred to work alone with his technicians. Frank Malina, who was then studying rocketry at the California Institute of Technology, visited Goddard in August 1936. Goddard hesitated to discuss any of his research, other than that which had already been published in Liquid-Propellant Rocket Development. Theodore von Kármán, Malina's mentor at the time, was unhappy with Goddard's attitude and later wrote, "Naturally we at Caltech wanted as much information as we could get from Goddard for our mutual benefit. But Goddard believed in secrecy. ... The trouble with secrecy is that one can easily go in the wrong direction and never know it.": 90  However, at an earlier point, von Kármán said that Malina was "highly enthusiastic" after his visit and that Caltech made changes to their liquid-propellant rocket, based on Goddard's work and patents. Malina remembered his visit as friendly and that he saw all but a few components in Goddard's shop.: 178 
Goddard's concerns about secrecy led to criticism for failure to cooperate with other scientists and engineers. His approach at that time was that independent development of his ideas without interference would bring quicker results even though he received less technical support. George Sutton, who became a rocket scientist working with von Braun's team in the late 1940s, said that he and his fellow workers had not heard of Goddard or his contributions and that they would have saved time if they had known the details of his work. Sutton admits that it may have been their fault for not looking for Goddard's patents and depending on the German team for knowledge and guidance; he wrote that information about the patents was not well distributed in the U.S. at that early period after World War II, though Germany and the Soviet Union had copies of some of them. (The Patent Office did not release rocket patents during World War II.) However, the Aerojet Engineering Corporation, an offshoot of the Guggenheim Aeronautical Laboratory at Caltech (GALCIT), filed two patent applications in Sep 1943 referencing Goddard's U.S. Patent 1,102,653 for the multistage rocket.
By 1939, von Kármán's GALCIT had received Army Air Corps funding to develop rockets to assist in aircraft take-off. Goddard learned of this in 1940, and openly expressed his displeasure at not being considered. Malina could not understand why the Army did not arrange for an exchange of information between Goddard and Caltech since both were under government contract at the same time. Goddard did not think he could be of that much help to Caltech because they were designing rocket engines mainly with solid fuel, while he was using liquid fuel.
Goddard was concerned with avoiding the public criticism and ridicule he had faced in the 1920s, which he believed had harmed his professional reputation. He also lacked interest in discussions with people who had less understanding of rocketry than he did,: 171  feeling that his time was extremely constrained.: 23  Goddard's health was frequently poor, as a result of his earlier bout of tuberculosis, and he was uncertain about how long he had to live: 65, 190  He felt, therefore, that he hadn't the time to spare arguing with other scientists and the press about his new field of research, or helping all the amateur rocketeers who wrote to him.: 61, 71, 110–11, 114–15  In 1932 Goddard wrote to H. G. Wells:
 How many more years I shall be able to work on the problem, I do not know; I hope, as long as I live. There can be no thought of finishing, for "aiming at the stars", both literally and figuratively, is a problem to occupy generations, so that no matter how much progress one makes, there is always the thrill of just beginning.     Goddard spoke to professional groups, published articles and papers and patented his ideas; but while he discussed basic principles, he was unwilling to reveal the details of his designs until he had flown rockets to high altitudes and thus proven his theory.: 115  He tended to avoid any mention of space flight, and spoke only of high-altitude research, since he believed that other scientists regarded the subject as unscientific.: 116  GALCIT saw Goddard's publicity problems and that the word "rocket" was "of such bad repute" that they used the word "jet" in the name of JPL and the related Aerojet Engineering Corporation.
Many authors writing about Goddard mention his secrecy, but
neglect the reasons for it. Some reasons have been noted above. Much of his work was for the military and was classified.: 1541  There were some in the U.S. before World War II that called for long-range rockets, and in 1939 Major James Randolph wrote a "provocative article" advocating a 3000-mile range missile. Goddard was "annoyed" by the unclassified paper as he thought the subject of weapons should be "discussed in strict secrecy."
However, Goddard's tendency to secrecy was not absolute, nor was he totally uncooperative. In 1945 GALCIT was building the WAC Corporal for the Army.  But in 1942 they were having trouble with their liquid propellant rocket engine's performance (timely, smooth ignition and explosions). Frank Malina went  to Annapolis in February and consulted with Goddard and Stiff, and they arrived at a solution to the problem (hypergolic propellant), which resulted in the successful launch of the high-altitude research rocket in October 1945.
During the First and Second World Wars, Goddard offered his services, patents, and technology to the military, and made some significant contributions. Just before the Second World War several young Army officers and a few higher-ranking ones believed Goddard's research was important but were unable to generate funds for his work.
Toward the end of his life, Goddard, realizing he was no longer going to be able to make significant progress alone in his field, joined the American Rocket Society and became a director. He made plans to work in the budding US aerospace industry (with Curtiss-Wright), taking most of his team with him.: 382, 385 
On June 21, 1924, Goddard married Esther Christine Kisk (March 31, 1901 – June 4, 1982), a secretary in Clark University's President's office, whom he had met in 1919. She became enthusiastic about rocketry and photographed some of his work as well as aided him in his experiments and paperwork, including accounting. They enjoyed going to the movies in Roswell and participated in community organizations such as the Rotary and the Woman's Club. He painted the New Mexican scenery, sometimes with the artist Peter Hurd, and played the piano. She played bridge, while he read. Esther said Robert participated in the community and readily accepted invitations to speak to church and service groups. The couple did not have children. After his death, she sorted out Goddard's papers, and secured 131 additional patents on his work.
Concerning Goddard's religious views, he was raised as an Episcopalian, though he was not outwardly religious. The Goddards were associated with the Episcopal church in Roswell, and he attended occasionally. He once spoke to a young people's group on the relationship of science and religion.: 224 
Goddard's serious bout with tuberculosis weakened his lungs, affecting his ability to work, and was one reason he liked to work alone, in order to avoid argument and confrontation with others and use his time fruitfully. He labored with the prospect of a shorter than average life span.: 190  After arriving in Roswell, Goddard applied for life insurance, but when the company doctor examined him he said that Goddard belonged in a bed in Switzerland (where he could get the best care).: 183  Goddard's health began to deteriorate further after moving to the humid climate of Maryland to work for the Navy. He was diagnosed with throat cancer in 1945. He continued to work, able to speak only in a whisper until surgery was required, and he died in August of that year in Baltimore, Maryland.: 377, 395  He was buried in Hope Cemetery in his home town of Worcester, Massachusetts.
Goddard honored on a U.S. airmail stamp
Bronze plaque in Auburn, Massachusetts marking the town in which Goddard launched the first liquid-fueled rocket on March 16, 1926.
Insignia of the 50th Anniversary of the Goddard Space Flight Center, a NASA facility in Maryland
Robert H. Goddard High School in Roswell, New Mexico.
Goddard Hall at Worcester Polytechnic Institute
Goddard Library at Clark University
Goddard received 214 patents for his work, of which 131 were awarded after his death. Among the most influential patents were:
The Guggenheim Foundation and Goddard's estate filed suit in 1951 against the U.S. government for prior infringement of three of Goddard's patents. In 1960, the parties settled the suit, and the U.S. armed forces and NASA paid out an award of $1 million: half of the award settlement went to his wife, Esther. At that time, it was the largest government settlement ever paid in a patent case.: 404   The settlement amount exceeded the total amount of all the funding that Goddard received for his work, throughout his entire career.

Hermann Julius Oberth (German: ; 25 June 1894 – 28 December 1989) was an Austro-Hungarian-born German physicist and engineer. He is considered one of the founding fathers of rocketry and astronautics, along with the French Robert Esnault-Pelterie, the Russian Konstantin Tsiolkovsky, the American Robert Goddard and the Austro-Hungarian born Slovenian Herman Potočnik.
Oberth was born to a Transylvanian Saxon family in Nagyszeben (Hermannstadt), Kingdom of Hungary (today Sibiu in Romania). He was fluent in Romanian language.  At the age of 11 years, Oberth's interest in rocketry was set off by the novels of Jules Verne, especially From the Earth to the Moon and Around the Moon. He was fond of reading over them up until they were engraved on his memory. As a result, Oberth constructed his first model rocket as a school student at the age of 14. In his youthful experiments, he arrived independently at the concept of the multistage rocket. However, during this time, he lacked the resources to put his ideas into practice.
In 1912, Oberth began the study of medicine in Munich, Germany, but at the outbreak of World War I, he was drafted into the Imperial German Army, assigned to an infantry battalion, and sent to the Eastern Front against Russia. In 1915, Oberth was moved into a medical unit at a hospital in Segesvár (German Schäßburg, Romanian Sighișoara), Transylvania, in Austria-Hungary (today Romania). There he found the spare time to conduct a series of experiments concerning weightlessness, and later resumed his rocketry designs. By 1917, he showed designs of a missile using liquid propellant with a range of 290 km to Hermann von Stein, the Prussian Minister of War.
On 6 July 1918, Oberth married Mathilde Hummel, with whom he had four children. Among these were a son who died as a soldier in World War II, and a daughter named Ilse Oberth (1924-1944) who was a rocket technician at the Redl-Zipf (code name Schlier) V-2 rocket engine test facility and liquid oxygen plant and was killed when there was an accidental explosion on August 28, 1944. In 1919, Oberth once again moved to Germany, this time to study physics, initially in Munich and later at the University of Göttingen.
In 1922, Oberth's proposed doctoral dissertation on rocket science was rejected as "utopian". However, professor Augustin Maior of the University of Cluj, Romania offered Oberth to defend his original dissertation there in order to receive the doctorate degree. He did successfully on 23 May 1923. He next had his 92-page work published privately in June 1923 as the somewhat controversial book, Die Rakete zu den Planetenräumen ("The Rocket into Planetary Space"). By 1929, Oberth had expanded this work to a 429-page book titled Wege zur Raumschiffahrt ("Ways to Spaceflight"). Oberth commented later that he made the deliberate choice not to write another doctoral dissertation. He wrote, "I refrained from writing another one, thinking to myself: Never mind, I will prove that I am able to become a greater scientist than some of you, even without the title of Doctor." Oberth criticized the German system of education, saying "Our educational system is like an automobile which has strong rear lights, brightly illuminating the past. But looking forward, things are barely discernible."
Oberth became a member of the Verein für Raumschiffahrt (VfR) – the "Spaceflight Society" – an amateur rocketry group that had taken great inspiration from his book, and Oberth acted as something of a mentor to the enthusiasts who joined the Society, which included persons such as Wernher von Braun, Rolf Engel, Rudolf Nebel or Paul Ehmayr. Oberth lacked the opportunities to work or to teach at the college or university level, as did many well-educated experts in the physical sciences and engineering in the time period of the 1920s through the 1930s – with the situation becoming much worse during the worldwide Great Depression that started in 1929. Therefore, from 1924 through 1938, Oberth supported himself and his family by teaching physics and mathematics at the Stephan Ludwig Roth High School in Mediaș, Romania.
In parts of 1928 and 1929, Oberth also worked in Berlin as a scientific consultant on the film, Frau im Mond ("The Woman in the Moon"), which was directed and produced by the great film pioneer Fritz Lang at the Universum Film AG company. This film was of enormous value in popularizing the ideas of rocketry and space exploration. One of Oberth's main assignments was to build and launch a rocket as a publicity event just before the film's premiere. He also designed the model of the Friede, the main rocket portrayed in the film.
On 5 June 1929, Oberth won the first (Robert Esnault-Pelterie - André-Louis Hirsch) Prix REP-Hirsch of the French Astronomical Society for the encouragement of astronautics in his book Wege zur Raumschiffahrt ("Ways to Spaceflight") that had expanded Die Rakete zu den Planetenräumen to a full-length book. The book  is dedicated to Fritz Lang and Thea von Harbou.
Oberth's student Max Valier joined forces with Fritz von Opel to create the world's first large-scale experimental rocket program Opel-RAK, leading to speed records for ground and rail vehicles and the world's first rocket plane. Opel RAK.1, a purpose-built design by Julius Hatry, was demonstrated to the public and world media on September 30, 1929, piloted by von Opel. Valier's and von Opel's demonstrations had a strong and long-lasting impact on later spaceflight pioneers, in particular on another of Oberth's students, Wernher von Braun.
Shortly after the Opel RAK team's successful liquid-fuel rocket launches of April 10 and 12, 1929 by Friedrich Wilhelm Sander at Opel Rennbahn in Rüsselsheim, Oberth conducted in the autumn of 1929 a static firing of his first liquid-fueled rocket motor, which he named the Kegeldüse. The engine was built by Klaus Riedel in a workshop space provided by the Reich Institution of Chemical Technology, and although it lacked a cooling system, it did run briefly. He was helped in this experiment by an 18-year-old student Wernher von Braun, who would later become a giant in both German and American rocket engineering from the 1940s onward, culminating with the gigantic Saturn V rockets that made it possible for man to land on the Moon in 1969 and in several following years. Indeed, Von Braun said of him:
Hermann Oberth was the first, who when thinking about the possibility of spaceships grabbed a slide-rule and presented mathematically analyzed concepts and designs.... I, myself, owe to him not only the guiding-star of my life, but also my first contact with the theoretical and practical aspects of rocketry and space travel. A place of honor should be reserved in the history of science and technology for his ground-breaking contributions in the field of astronautics.In 1938, the Oberth family left Sibiu, Romania, for good, to first settle in Austria, then in Nazi Germany, then in the United States, and finally back to a democratic West Germany. Oberth himself moved on first to the Technische Hochschule in Vienna, Austria, then to the Technische Hochschule in Dresden, Germany. (A Technische Hochschule at that time was a technical college offering advanced professional training in selected fields, rather than an institution also engaged in basic research as a university.)
Oberth moved to Peenemünde, Germany, in 1941 to work on the Aggregat rocket program. Around September 1943, he was awarded the Kriegsverdienstkreuz I Klasse mit Schwertern (War Merit Cross 1st Class, with Swords) for his "outstanding, courageous behavior ... during the attack" on Peenemünde by Operation Hydra, part of Allied operations against the German rocket programme.
Later he worked on solid-propellant anti-aircraft rockets at the German WASAG military organization near Wittenberg. Around the end of World War II in Europe, the Oberth family moved to the town of Feucht, near the regional capital of Nuremberg, which became part of the American Zone of occupied Germany, and also the location of the high-level war-crimes trials of the surviving Nazi leaders. Oberth was allowed to leave Nuremberg to move to Switzerland in 1948, where he worked as an independent consultant and a writer.
In 1950, Oberth moved on to Italy, where he completed some of the work that he had begun at the WASAG organization for the new Italian Navy. In 1953, Oberth returned to Feucht, Germany, to publish his book Menschen im Weltraum (Mankind into Space), in which he described his ideas for space-based reflecting telescopes, space stations, electric-powered spaceships, and space suits.
During the 1950s and 1960s, Oberth offered his opinions regarding unidentified flying objects (UFOs). He was a supporter of the extraterrestrial hypothesis for the origin of the UFOs that were seen from Earth. For example, in an article in The American Weekly magazine of 24 October 1954, Oberth stated, "It is my thesis that flying saucers are real, and that they are space ships from another solar system. I think that they possibly are manned by intelligent observers who are members of a race that may have been investigating our earth for centuries..." He also wrote an article in the second edition of Flying Saucer Review titled "They Come From Outer Space". He discussed the history of reports of "strange luminous objects" in the sky, mentioning that the earliest historical case is of "Shining Shields" reported by Pliny the Elder. He wrote, "Having weighed all the pros and cons, I find the explanation of flying discs from outer space the most likely one. I call this the "Uraniden" hypothesis, because from our viewpoint the hypothetical beings appear to come from the sky (Greek – 'Uranos')."
Oberth eventually came to work for his former student, Wernher von Braun, who was developing space rockets for NASA in Huntsville, Alabama. (See also List of German rocket scientists in the United States.) Among other things, Oberth  was involved in writing the study, The Development of Space Technology in the Next Ten Years. In 1958, Oberth was back in Feucht, Germany, where he published his ideas on a lunar exploration vehicle, a "lunar catapult", and on "muffled" helicopters and airplanes. In 1960, back in the United States again, Oberth went to work for the Convair Corporation as a technical consultant on the Atlas rocket program.
Oberth retired in 1962 at the age of 68. From 1964 to 1967 he was a member of the National Democratic Party of Germany, which was considered to be far right. In July 1969, Oberth returned to the United States to witness the launch of the Apollo project Saturn V rocket from the Kennedy Space Center in Florida that carried the Apollo 11 crew on the first landing mission to the Moon.
The 1973 oil crisis inspired Oberth to look into alternative energy sources, including a plan for a wind power station that could utilize the jet stream. However, his primary interest during his retirement years was to turn to more abstract philosophical questions. Most notable among his several books from this period is Primer For Those Who Would Govern.
Oberth returned to the United States to view the launch of STS-61-A, the Space Shuttle Challenger launched 30 October 1985.
Oberth died in Nuremberg, West Germany, on 28 December 1989, just shortly after the fall of the Iron Curtain.
According to an obituary by Stille Hilfe, Oberth was "a loyal supporter and donor" of this Nazi support organization.
Hermann Oberth is memorialized by the Hermann Oberth Space Travel Museum in Feucht, Germany, and by the Hermann Oberth Society. The museum brings together scientists, researchers, engineers, and astronauts from the East and the West to carry on his work in rocketry and space exploration.
He discovered the Oberth effect, in which a rocket engine when traveling at high speed generates more useful energy than one travelling at low speed.
In 1980, Oberth was inducted into the International Air &amp; Space Hall of Fame at the San Diego Air &amp; Space Museum.
There is also a crater on the Moon and asteroid 9253 Oberth named after him.
The Danish Astronautical Society has named Hermann Oberth an honorary member.
The Faculty of Engineering of Lucian Blaga University of Sibiu is named after him.
In Star Trek III: The Search for Spock, the USS Grissom was classified as an Oberth-class starship. Several other Oberth-class starships also appeared in subsequent Star Trek films and television series.

Supersonic speed is the speed of an object that exceeds the speed of sound (Mach 1). For objects traveling in dry air of a temperature of 20 °C (68 °F) at sea level, this speed is approximately 343.2 m/s (1,126 ft/s; 768 mph; 667.1 kn; 1,236 km/h). Speeds greater than five times the speed of sound (Mach 5) are often referred to as hypersonic. Flights during which only some parts of the air surrounding an object, such as the ends of rotor blades, reach supersonic speeds are called transonic. This occurs typically somewhere between Mach 0.8 and Mach 1.2.
Sounds are traveling vibrations in the form of pressure waves in an elastic medium. Objects move at supersonic speed when the objects move faster than the speed at which sound propagates through the medium. In gases, sound travels longitudinally at different speeds, mostly depending on the molecular mass and temperature of the gas, and pressure has little effect. Since air temperature and composition varies significantly with altitude, the speed of sound, and Mach numbers for a steadily moving object may change. In water at room temperature supersonic speed can be considered as any speed greater than 1,440 m/s (4,724 ft/s). In solids, sound waves can be polarized longitudinally or transversely and have even higher velocities.
Supersonic fracture is crack motion faster than the speed of sound in a brittle material.
At the beginning of the 20th century, the term "supersonic" was used as an adjective to describe sound whose frequency is above the range of normal human hearing.  The modern term for this meaning is "ultrasonic".
Etymology: The word supersonic comes from two Latin derived words; 1) super: above and 2) sonus: sound, which together mean above sound, or faster than sound.
The tip of a bullwhip is thought to be the first object designed to break the sound barrier, resulting in the telltale "crack" (actually a small sonic boom). The wave motion travelling through the bullwhip is what makes it capable of achieving supersonic speeds. However, the first man-made supersonic boom was likely caused by a piece of cloth, spurring the whip's eventual development.
Most modern firearm bullets are supersonic, with rifle projectiles often travelling at speeds approaching and in some cases well exceeding Mach 3.
Most spacecraft, most notably the Space Shuttle are supersonic at least during portions of their reentry, though the effects on the spacecraft are reduced by low air densities. During ascent, launch vehicles generally avoid going supersonic below 30 km (~98,400 feet) to reduce air drag.
Note that the speed of sound decreases somewhat with altitude, due to lower temperatures found there (typically up to 25 km). At even higher altitudes the temperature starts increasing, with the corresponding increase in the speed of sound.
When an inflated balloon is burst, the torn pieces of latex contract at supersonic speed, which contributes to the sharp and loud popping noise.
To date, only one land vehicle has officially travelled at supersonic speed, the ThrustSSC. The vehicle, driven by Andy Green, holds the world land speed record, having achieved an average speed on its bi-directional run of 1,228 km/h (763 mph) in the Black Rock Desert on 15 October 1997.
The Bloodhound LSR project planned an attempt on the record in 2020 at Hakskeenpan in South Africa with a combination jet and hybrid rocket propelled car. The aim was to break the existing record, then make further attempts during which  the team hope to reach speeds of up to 1,600 km/h (1,000 mph). The effort was originally run by Richard Noble who was the leader of the ThrustSSC project, however following funding issues in 2018, the team was bought by Ian Warhurst and renamed Bloodhound LSR. Later the project was indefinitely delayed due to the Covid-19 pandemic and the vehicle was put up for sale.
Most modern fighter aircraft are supersonic aircraft, but there have been supersonic passenger aircraft, namely Concorde and the Tupolev Tu-144. Both of these passenger aircraft and some modern fighters are also capable of supercruise, a condition of sustained supersonic flight without the use of an afterburner. Due to its ability to supercruise for several hours and the relatively high frequency of flight over several decades, Concorde spent more time flying supersonically than all other aircraft combined by a considerable margin. Since Concorde's final retirement flight on November 26, 2003, there are no supersonic passenger aircraft left in service. Some large bombers, such as the Tupolev Tu-160 and Rockwell B-1 Lancer are also supersonic-capable.
The aerodynamics of supersonic aircraft is simpler than subsonic aerodynamics because the airsheets at different points along the plane often cannot affect each other. Supersonic jets and rocket vehicles require several times greater thrust to push through the extra aerodynamic drag experienced within the transonic region (around Mach 0.85–1.2). At these speeds aerospace engineers can gently guide air around the fuselage of the aircraft without producing new shock waves, but any change in cross area farther down the vehicle leads to shock waves along the body. Designers use the Supersonic area rule and the Whitcomb area rule to minimize sudden changes in size.
However, in practical applications, a supersonic aircraft must operate stably in both subsonic and supersonic profiles, hence aerodynamic design is more complex.
The main key to having low supersonic drag is to properly shape the overall aircraft to be long and thin, and close to a "perfect" shape, the von Karman ogive or Sears-Haack body. This has led to almost every supersonic cruising aircraft looking very similar to every other, with a very long and slender fuselage and large delta wings, cf. SR-71, Concorde, etc. Although not ideal for passenger aircraft, this shaping is quite adaptable for bomber use.
A de Laval nozzle (or convergent-divergent nozzle, CD nozzle or con-di nozzle) is a tube which is pinched in the middle, making a carefully balanced, asymmetric hourglass shape. It is used to accelerate a compressible fluid to supersonic speeds  in the axial (thrust) direction, by converting the thermal energy of the flow into kinetic energy. De Laval nozzles are widely used in some types of steam turbines and rocket engine nozzles. It also sees use in supersonic jet engines.
Similar flow properties have been applied to jet streams within astrophysics.
Giovanni Battista Venturi designed converging-diverging tubes known as Venturi tubes to experiment the effects in fluid pressure reduction while flowing through chokes (Venturi effect). German engineer and inventor Ernst Körting supposedly switched to a converging-diverging nozzle in his steam jet pumps by 1878 after using convergent nozzles but these nozzles remained a company secret. Later, Swedish engineer Gustaf De Laval applied his own converging diverging nozzle design for use on his impulse turbine in the year 1888.
Laval's Convergent-Divergent nozzle was first applied in a rocket engine by Robert Goddard. Most modern rocket engines that employ hot gas combustion use de Laval nozzles.
Its operation relies on the different properties of gases flowing at subsonic, sonic, and supersonic speeds. The speed of a subsonic flow of gas will increase if the pipe carrying it narrows because the mass flow rate is constant. The gas flow through a de Laval nozzle is isentropic (gas entropy is nearly constant). In a subsonic flow sound will propagate through the gas. At the "throat", where the cross-sectional area is at its minimum, the gas velocity locally becomes sonic (Mach number = 1.0), a condition called choked flow. As the nozzle cross-sectional area increases, the gas begins to expand and the gas flow increases to supersonic velocities where a sound wave will not propagate backward through the gas as viewed in the frame of reference of the nozzle (Mach number &gt; 1.0).
As the gas exits the throat the increase in area allows for it to undergo a Joule-Thompson expansion wherein the gas expands at supersonic speeds from high to low pressure pushing the velocity of the mass flow beyond sonic speed.
When comparing the general geometric shape of the nozzle between the rocket and the jet engine, it only looks different at first glance, when in fact is about the same essential facts are noticeable on the same geometric cross-sections - that the combustion chamber in the jet engine must have the same "throat" (narrowing) in the direction of the outlet of the gas jet, so that the turbine wheel of the first stage of the jet turbine is always positioned immediately behind that narrowing, while any on the further stages of the turbine are located at the larger outlet cross section of the nozzle, where the flow accelerates.
A de Laval nozzle will only choke at the throat if the pressure and mass flow through the nozzle is sufficient to reach sonic speeds, otherwise no supersonic flow is achieved, and it will act as a Venturi tube; this requires the entry pressure to the nozzle to be significantly above ambient at all times (equivalently, the stagnation pressure of the jet must be above ambient).
In addition, the pressure of the gas at the exit of the expansion portion of the exhaust of a nozzle must not be too low. Because pressure cannot travel upstream through the supersonic flow, the exit pressure can be significantly below the ambient pressure into which it exhausts, but if it is too far below ambient, then the flow will cease to be supersonic, or the flow will separate within the expansion portion of the nozzle, forming an unstable jet that may "flop" around within the nozzle, producing a lateral thrust and possibly damaging it.
In practice, ambient pressure must be no higher than roughly 2–3 times the pressure in the supersonic gas at the exit for supersonic flow to leave the nozzle.
The analysis of gas flow through de Laval nozzles involves a number of concepts and assumptions:
As the gas enters a nozzle, it is moving at subsonic velocities. As the cross-sectional area contracts the gas is forced to accelerate until the axial velocity becomes sonic at the nozzle throat, where the cross-sectional area is the smallest. From the throat the cross-sectional area then increases, allowing the gas to expand and the axial velocity to become progressively more supersonic.
The linear velocity of the exiting exhaust gases can be calculated using the following equation:
Some typical values of the exhaust gas velocity ve for rocket engines burning various propellants are:
As a note of interest, ve is sometimes referred to as the ideal exhaust gas velocity because it is based on the assumption that the exhaust gas behaves as an ideal gas.
As an example calculation using the above equation, assume that the propellant combustion gases are: at an absolute pressure entering the nozzle p = 7.0 MPa and exit the rocket exhaust at an absolute pressure pe = 0.1 MPa; at an absolute temperature of T = 3500 K; with an isentropic expansion factor γ = 1.22 and a molar mass M = 22 kg/kmol. Using those values in the above equation yields an exhaust velocity ve = 2802 m/s, or 2.80 km/s, which is consistent with above typical values.
Technical literature often interchanges without note the universal gas law constant R, which applies to any ideal gas, with the gas law constant Rs, which only applies to a specific individual gas of molar mass M.  The relationship between the two constants is Rs = R/M.
In accordance with conservation of mass the mass flow rate of the gas throughout the nozzle is the same regardless of the cross-sectional area.







m
˙



=



A

p

t





T

t





⋅




γ
R


M


⋅
(
1
+



γ
−
1

2




M
a


2



)

−



γ
+
1


2
(
γ
−
1
)







{\displaystyle {\dot {m}}={\frac {Ap_{t}}{\sqrt {T_{t}}}}\cdot {\sqrt {{\frac {\gamma }{R}}M}}\cdot (1+{\frac {\gamma -1}{2}}\mathrm {Ma} ^{2})^{-{\frac {\gamma +1}{2(\gamma -1)}}}}


When the throat is at sonic speed Ma = 1 where the equation simplifies to:







m
˙



=



A

p

t





T

t





⋅




γ
M

R



⋅
(



γ
+
1

2



)

−



γ
+
1


2
(
γ
−
1
)







{\displaystyle {\dot {m}}={\frac {Ap_{t}}{\sqrt {T_{t}}}}\cdot {\sqrt {\frac {\gamma M}{R}}}\cdot ({\frac {\gamma +1}{2}})^{-{\frac {\gamma +1}{2(\gamma -1)}}}}


By Newton's third law of motion the mass flow rate can be used to determine the force exerted by the expelled gas by:




F
=



m
˙



⋅

v

e




{\displaystyle F={\dot {m}}\cdot v_{e}}


In aerodynamics, the force exerted by the nozzle is defined as the thrust.
A combustion chamber is part of an internal combustion engine in which the fuel/air mix is burned. For steam engines, the term has also been used for an extension of the firebox which is used to allow a more complete combustion process.
In an internal combustion engine, the pressure caused by the burning air/fuel mixture applies direct force to part of the engine (e.g. for a piston engine, the force is applied to the top of the piston), which converts the gas pressure into mechanical energy (often in the form of a rotating output shaft). This contrasts an external combustion engine, where the combustion takes place in a separate part of the engine to where the gas pressure is converted into mechanical energy. 
In spark ignition engines, such as petrol (gasoline) engines, the combustion chamber is usually located in the cylinder head. The engines are often designed such that the bottom of combustion chamber is roughly in line with the top of the engine block.
Modern engines with overhead valves or overhead camshaft(s) use the top of the piston (when it is near top dead centre) as the bottom of the combustion chamber. Above this, the sides and roof of the combustion chamber include the intake valves, exhaust valves and spark plug. This forms a relatively compact combustion chamber without any protrusions to the side (i.e. all of the chamber is located directly above the piston). Common shapes for the combustion chamber are typically similar to one or more half-spheres (such as the hemi, pent-roof, wedge or kidney-shaped chambers).
The older flathead engine design uses a "bathtub"-shaped combustion chamber, with an elongated shape that sits above both the piston and the valves (which are located beside the piston). IOE engines combine elements of overhead valve and flathead engines; the intake valve is located above the combustion chamber, while the exhaust valve is located below it.
The shape of the combustion chamber, intake ports and exhaust ports are key to achieving efficient combustion and maximising power output. Cylinder heads are often designed to achieve a certain "swirl" pattern (rotational component to the gas flow) and turbulence, which improves the mixing and increases the flow rate of gasses. The shape of the piston top also affects the amount of swirl.
Another design feature to promote turbulence for good fuel/air mixing is squish, where the fuel/air mix is "squished" at high pressure by the rising piston.
The location of the spark plug is also an important factor, since this is the starting point of the flame front (the leading edge of the burning gasses) which then travels downwards towards the piston.  Good design should avoid narrow crevices where stagnant "end gas" can become trapped, reducing the power output of the engine and potentially leading to engine knocking. Most engines use a single spark plug per cylinder, however some (such as the 1986-2009 Alfa Romeo Twin Spark engine) use two spark plugs per cylinder.
Compression-ignition engines, such as diesel engines, are typically classified as either:
Direct injection engines usually give better fuel economy but indirect injection engines can use a lower grade of fuel.
Harry Ricardo was prominent in developing combustion chambers for diesel engines, the best known being the Ricardo Comet.
In a continuous flow system, for example a jet engine combustor, the pressure is controlled and the combustion creates an increase in volume. The combustion chamber in gas turbines and jet engines (including ramjets and scramjets) is called the combustor.
The combustor is fed with high pressure air by the compression system, adds fuel and burns the mix and feeds the hot, high pressure exhaust into the turbine components of the engine or out the exhaust nozzle.
Different types of combustors exist, mainly:
If the gas velocity changes, thrust is produced, such as in the nozzle of a rocket engine.
Considering the definition of combustion chamber used for internal combustion engines, the equivalent part of a steam engine would be the firebox, since this is where the fuel is burned. However, in the context of a steam engine, the term "combustion chamber" has also been used for a specific area between the firebox and the boiler. This extension of the firebox is designed to allow a more complete combustion of the fuel, improving fuel efficiency and reducing build-up of soot and scale. The use of this type of combustion chamber is large steam locomotive engines, allows the use of shorter firetubes.
Micro combustion chambers are the devices in which combustion happens at a very small volume, due to which surface to volume ratio increases which plays a vital role in stabilizing the flame.
Constant volume combustion chambers (CVCC) are the research devices that are usually equipped with spark plugs, injectors, fuel/air inlet and outlet lines, pressure transducers, thermocouples, etc. Depending on the applications, they can be provided with or without optical access using quartz windows. The constant volume combustion chambers have been extensively utilized with the aim of studying a wide range of fundamental aspects of combustion science. Principal characteristics of combustion phenomena like premixed flames, ignition, autoignition, laminar burning velocity, flame speed, diffusion flames, sprays, emission production, fuel and combustion characteristics, and chemical kinetics can be investigated using CVCCs.

In aerodynamics, a hypersonic speed is one that exceeds 5 times the speed of sound, often stated as starting at speeds of Mach 5 and above.
The precise Mach number at which a craft can be said to be flying at hypersonic speed varies, since individual physical changes in the airflow (like molecular dissociation and ionization) occur at different speeds; these effects collectively become important around Mach 5-10. The hypersonic regime can also be alternatively defined as speeds where specific heat capacity changes with the temperature of the flow as kinetic energy of the moving object is converted into heat.
While the definition of hypersonic flow can be quite vague and is generally debatable (especially due to the absence of discontinuity between supersonic and hypersonic flows), a hypersonic flow may be characterized by certain physical phenomena that can no longer be analytically discounted as in supersonic flow. The peculiarity in hypersonic flows are as follows:
As a body's Mach number increases, the density behind a bow shock generated by the body also increases, which corresponds to a decrease in volume behind the shock due to conservation of mass. Consequently, the distance between the bow shock and the body decreases at higher Mach numbers.
As Mach numbers increase, the entropy change across the shock also increases, which results in a strong entropy gradient and highly vortical flow that mixes with the boundary layer.
A portion of the large kinetic energy associated with flow at high Mach numbers transforms into internal energy in the fluid due to viscous effects. The increase in internal energy is realized as an increase in temperature. Since the pressure gradient normal to the flow within a boundary layer is approximately zero for low to moderate hypersonic Mach numbers, the increase of temperature through the boundary layer coincides with a decrease in density. This causes the bottom of the boundary layer to expand, so that the boundary layer over the body grows thicker and can often merge with the shock wave near the body leading edge.
High temperatures due to a manifestation of viscous dissipation cause non-equilibrium chemical flow properties such as vibrational excitation and dissociation and ionization of molecules resulting in convective and radiative heat-flux.
Although "subsonic" and "supersonic" usually refer to speeds below and above the local speed of sound respectively, aerodynamicists often use these terms to refer to particular ranges of Mach values. This occurs because a "transonic regime" exists around M=1 where approximations of the Navier–Stokes equations used for subsonic design no longer apply, partly because the flow locally exceeds M=1 even when the freestream Mach number is below this value.
The "supersonic regime" usually refers to the set of Mach numbers for which linearised theory may be used; for example, where the (air) flow is not chemically reacting and where heat transfer between air and vehicle may be reasonably neglected in calculations. Generally, NASA defines "high" hypersonic as any Mach number from 10 to 25, and re-entry speeds as anything greater than Mach 25. Among the spacecraft operating in these regimes are returning Soyuz and Dragon space capsules; the previously-operated Space Shuttle; various reusable spacecraft in development such as SpaceX Starship and Rocket Lab Electron; as well as (theoretical) spaceplanes.
In the following table, the "regimes" or "ranges of Mach values" are referenced instead of the usual meanings of "subsonic" and "supersonic".
The categorization of airflow relies on a number of similarity parameters, which allow the simplification of a nearly infinite number of test cases into groups of similarity. For transonic and compressible flow, the Mach and Reynolds numbers alone allow good categorization of many flow cases.
Hypersonic flows, however, require other similarity parameters. First, the analytic equations for the oblique shock angle become nearly independent of Mach number at high (~&gt;10) Mach numbers. Second, the formation of strong shocks around aerodynamic bodies means that the freestream Reynolds number is less useful as an estimate of the behavior of the boundary layer over a body (although it is still important). Finally, the increased temperature of hypersonic flows mean that real gas effects become important. For this reason, research in hypersonics is often referred to as aerothermodynamics, rather than aerodynamics.
The introduction of real gas effects means that more variables are required to describe the full state of a gas. Whereas a stationary gas can be described by three variables (pressure, temperature, adiabatic index), and a moving gas by four (flow velocity), a hot gas in chemical equilibrium also requires state equations for the chemical components of the gas, and a gas in nonequilibrium solves those state equations using time as an extra variable. This means that for a nonequilibrium flow, something between 10 and 100 variables may be required to describe the state of the gas at any given time. Additionally, rarefied hypersonic flows (usually defined as those with a Knudsen number above 0.1) do not follow the Navier–Stokes equations.
Hypersonic flows are typically categorized by their total energy, expressed as total enthalpy (MJ/kg), total pressure (kPa-MPa), stagnation pressure (kPa-MPa), stagnation temperature (K), or flow velocity (km/s).
Wallace D. Hayes developed a similarity parameter, similar to the Whitcomb area rule, which allowed similar configurations to be compared.
Hypersonic flow can be approximately separated into a number of regimes. The selection of these regimes is rough, due to the blurring of the boundaries where a particular effect can be found.
In this regime, the gas can be regarded as an ideal gas. Flow in this regime is still Mach number dependent. Simulations start to depend on the use of a constant-temperature wall, rather than the adiabatic wall typically used at lower speeds. The lower border of this region is around Mach 5, where ramjets become inefficient, and the upper border around Mach 10-12.
This is a subset of the perfect gas regime, where the gas can be considered chemically perfect, but the rotational and vibrational temperatures of the gas must be considered separately, leading to two temperature models. See particularly the modeling of supersonic nozzles, where vibrational freezing becomes important.
In this regime, diatomic or polyatomic gases (the gases found in most atmospheres) begin to dissociate as they come into contact with the bow shock generated by the body. Surface catalysis plays a role in the calculation of surface heating, meaning that the type of surface material also has an effect on the flow. The lower border of this regime is where any component of a gas mixture first begins to dissociate in the stagnation point of a flow (which for nitrogen is around 2000 K). At the upper border of this regime, the effects of ionization start to have an effect on the flow.
In this regime the ionized electron population of the stagnated flow becomes significant, and the electrons must be modeled separately. Often the electron temperature is handled separately from the temperature of the remaining gas components. This region occurs for freestream flow velocities around 3-4 km/s. Gases in this region are modeled as non-radiating plasmas.
Above around 12 km/s, the heat transfer to a vehicle changes from being conductively dominated to radiatively dominated. The modeling of gases in this regime is split into two classes:
The modeling of optically thick gases is extremely difficult, since, due to the calculation of the radiation at each point, the computation load theoretically expands exponentially as the number of points considered increases.

The highest specific impulse chemical rockets use liquid propellants (liquid-propellant rockets). They can consist of a single chemical (a monopropellant) or a mix of two chemicals, called bipropellants. Bipropellants can further be divided into two categories; hypergolic propellants, which ignite when the fuel and oxidizer make contact, and non-hypergolic propellants which require an ignition source.
About 170 different propellants made of liquid fuel have been tested, excluding minor changes to a specific propellant such as propellant additives, corrosion inhibitors, or stabilizers. In the U.S. alone at least 25 different propellant combinations have been flown. As of 2020, no completely new propellant has been used since the mid-1970s.
Many factors go into choosing a propellant for a liquid-propellant rocket engine. The primary factors include ease of operation, cost, hazards/environment and performance.
Konstantin Tsiolkovsky proposed the use of liquid propellants in 1903, in his article Exploration of Outer Space by Means of Rocket Devices.On March 16, 1926, Robert H. Goddard used liquid oxygen (LOX) and gasoline as rocket fuels for his first partially successful liquid-propellant rocket launch. Both propellants are readily available, cheap and highly energetic. Oxygen is a moderate cryogen as air will not liquefy against a liquid oxygen tank, so it is possible to store LOX briefly in a rocket without excessive insulation.
 In Germany, engineers and scientists became enthralled with liquid propulsion, building and testing them in the late 1920s within Opel RAK in Rüsselsheim. According to Max Valier's account, Opel RAK rocket designer, Friedrich Wilhelm Sander launched two liquid-fuel rockets at Opel Rennbahn in Rüsselsheim on April 10 and April 12, 1929. These Opel RAK rockets have been the first European, and after Goddard the world's second, liquid-fuel rockets in history. In his book “Raketenfahrt” Valier describes the size of the rockets as of 21 cm in diameter and with a length of 74 cm, weighing 7 kg empty and 16 kg with fuel. The maximum thrust was 45 to 50 kp, with a total burning time of 132 seconds. These properties indicate a gas pressure pumping. The first missile rose so quickly that Sander lost sight of it. Two days later, a second unit was ready to go, Sander tied a 4,000-meter-long rope to the rocket. After 2000 m or rope had been unwound, the line broke and this rocket also disappeared in the area, probably near the Opel proving ground and racetrack in Rüsselsheim, the "Rennbahn". The main purpose of these tests was to develop the propulsion system for the aircraft for crossing the English channel. Also spaceflight historian Frank H. Winter, curator at National Air and Space Museum in Washington, DC, confirms the Opel group was working, in addition to their solid-fuel rockets used for land-speed records and the world's first manned rocket-plane flights, on liquid-fuel rockets (SPACEFLIGHT, Vol. 21,2, Feb. 1979): In a cabled exclusive to The New York Times on 30 September 1929, Fritz von Opel is quoted as saying: "Sander and I now want to transfer the liquid rocket from the laboratory to practical use. With the liquid rocket I hope to be the first man to thus fly across the English Channel. I will not rest until I have accomplished that." At a speech on the donation of a RAK 2 replica to the Deutsches Museum, von Opel mentioned also Opel engineer Josef Schaberger as a key collaborator. "He belonged," von Opel said, "with the same enthusiasm as Sander to our small secret group, one of the tasks of which was to hide all the preparations from my father, because his paternal apprehensions led him to believe that I was cut out for something better than being a rocket researchist. Schaberger supervised all the details involved in construction and assembly (of rocket cars), and every time I sat behind the wheel with a few hundred pounds of explosives in my rear, and made the first contact, I did so with a feeling of total security  As early as 1928, Mr. Schaberger and I developed a liquid rocket, which was definitely the first permanently operating rocket in which the explosive was injected into the combustion chamber and simultaneously cooled using pumps.  We used benzol as the fuel," von Opel continued, "and nitrogen tetroxide as the oxidizer. This rocket was installed in a Mueller-Griessheim aircraft and developed a thrust of 70 kg (154 lb.)." By May 1929, the engine produced a thrust of 200 kg (440 lb.) "for longer than fifteen minutes and in July 1929, the Opel RAK collaborators were able to attain powered phases of more than thirty minutes for thrusts of 300 kg (660-lb.) at Opel's works in Rüsselsheim," again according to Max Valier's account. The Great Depression brought an end to the Opel RAK activities. Valier's, who died while experimenting in 1930, and Sander's work on liquid-fuel rockets was confiscated by the German military, the Heereswaffenamt and integrated into the activities under General Walter Dornberger in the early and mid-1930s in a field near Berlin. An amateur rocket group, the VfR, co-founded by Max Valier, included Wernher von Braun, who eventually became the head of the army research station that designed the V-2 rocket weapon for the Nazis. Sander was arrested by Gestapo in 1935, when private rocket-engineering became forbidden in Germany, was convicted of treason to 5 years in prison and forced to sell his company, he died in 1938.
Germany had very active rocket development before and during World War II, both for the strategic V-2 rocket and other missiles. The V-2 used an alcohol/LOX liquid-propellant engine, with hydrogen peroxide to drive the fuel pumps. The alcohol was mixed with water for engine cooling. Both Germany and the United States developed reusable liquid-propellant rocket engines that used a storeable liquid oxidizer with much greater density than LOX and a liquid fuel that ignited spontaneously on contact with the high density oxidizer. The major manufacturer of German rocket engines for military use, the HWK firm, manufactured the RLM-numbered 109-500-designation series of rocket engine systems, and either used hydrogen peroxide as a monopropellant for Starthilfe rocket-propulsive assisted takeoff needs; or as a form of thrust for MCLOS-guided air-sea glide bombs; and used in a bipropellant combination of the same oxidizer with a fuel mixture of hydrazine hydrate and methyl alcohol for rocket engine systems intended for manned combat aircraft propulsion purposes. The U.S. engine designs were fueled with the bipropellant combination of nitric acid as the oxidizer; and aniline as the fuel. Both engines were used to power aircraft, the Me 163 Komet interceptor in the case of the Walter 509-series German engine designs, and RATO units from both nations (as with the Starthilfe system for the Luftwaffe) to assist take-off of aircraft, which comprised the primary purpose for the case of the U.S. liquid-fueled rocket engine technology - much of it coming from the mind of U.S. Navy officer Robert Truax.
During the 1950s and 1960s there was a great burst of activity by propellant chemists to find high-energy liquid and solid propellants better suited to the military. Large strategic missiles need to sit in land-based or submarine-based silos for many years, able to launch at a moment's notice. Propellants requiring continuous refrigeration, which cause their rockets to grow ever-thicker blankets of ice, were not practical. As the military was willing to handle and use hazardous materials, a great number of dangerous chemicals were brewed up in large batches, most of which wound up being deemed unsuitable for operational systems.  In the case of nitric acid, the acid itself (HNO3) was unstable, and corroded most metals, making it difficult to store. The addition of a modest amount of nitrogen tetroxide, N2O4, turned the mixture red and kept it from changing composition, but left the problem that nitric acid corrodes containers it is placed in, releasing gases that can build up pressure in the process. The breakthrough was the addition of a little hydrogen fluoride (HF), which forms a self-sealing metal fluoride on the interior of tank walls that Inhibited Red Fuming Nitric Acid. This made "IRFNA" storeable. Propellant combinations based on IRFNA or pure N2O4 as oxidizer and kerosene or hypergolic (self igniting) aniline, hydrazine or unsymmetrical dimethylhydrazine (UDMH) as fuel were then adopted in the United States and the Soviet Union for use in strategic and tactical missiles. The self-igniting storeable liquid bi-propellants have somewhat lower specific impulse than LOX/kerosene but have higher density so a greater mass of propellant can be placed in the same sized tanks. Gasoline was replaced by different hydrocarbon fuels, for example RP-1 –  a highly refined grade of kerosene. This combination is quite practical for rockets that need not be stored.
The V-2 rockets developed by Nazi Germany used LOX and ethyl alcohol. One of the main advantages of alcohol was its water content which provided cooling in larger rocket engines. Petroleum-based fuels offered more power than alcohol, but standard gasoline and kerosene left too much silt and combustion by-products that could clog engine plumbing. In addition they lacked the cooling properties of ethyl alcohol.
During the early 1950s, the chemical industry in the US was assigned the task of formulating an improved petroleum-based rocket propellant which would not leave residue behind and also ensure that the engines would remain cool. The result was RP-1, the specifications of which were finalized by 1954. A highly refined form of jet fuel, RP-1 burned much more cleanly than conventional petroleum fuels and also posed less of a danger to ground personnel from explosive vapours. It became the propellant for most of the early American rockets and ballistic missiles such as the Atlas, Titan I, and Thor. The Soviets quickly adopted RP-1 for their R-7 missile, but the majority of Soviet launch vehicles ultimately used storable hypergolic propellants. As of 2017, it is used in the first stages of many orbital launchers.
Many early rocket theorists believed that hydrogen would be a marvelous propellant, since it gives the highest specific impulse. It is also considered the cleanest when oxidized with oxygen because the only by-product is water. Steam reforming of natural gas is the most common method of producing commercial bulk hydrogen at about 95% of the world production of 500 billion m3 in 1998. At high temperatures (700–1100 °C) and in the presence of a metal-based catalyst (nickel), steam reacts with methane to yield carbon monoxide and hydrogen.
Hydrogen in any state is very bulky; it is typically stored as a deeply cryogenic liquid, a technique mastered in the early 1950s as part of the hydrogen bomb development program at Los Alamos. Liquid hydrogen is stored and transported without boil-off, because helium, which has a lower boiling point than hydrogen, acts as cooling refrigerant. Only when hydrogen is loaded on a launch vehicle, where no refrigeration exists, it vents to the atmosphere.
In the late 1950s and early 1960s it was adopted for hydrogen-fuelled stages such as Centaur and Saturn upper stages. Even as a liquid, hydrogen has low density, requiring large tanks and pumps, and the extreme cold requires tank insulation. This extra weight reduces the mass fraction of the stage or requires extraordinary measures such as pressure stabilization of the tanks to reduce weight. Pressure stabilized tanks support most of the loads with internal pressure rather than with solid structures, employing primarily the tensile strength of the tank material.
The Soviet rocket programme, in part due to a lack of technical capabilities, did not use LH2 as a propellant until the 1980s when it was used for the Energia core stage.
The liquid-rocket engine propellant combination of liquid oxygen and hydrogen offers the highest specific impulse of currently used conventional rockets. This extra performance largely offsets the disadvantage of low density. Low density of a propellant leads to larger fuel tanks. However, a small increase in specific impulse in an upper stage application can have a significant increase in payload to orbit capability.
Launch pad fires due to spilled kerosene are more damaging than hydrogen fires, primarily for two reasons. First, kerosene burns about 20% hotter in absolute temperature than hydrogen. The second reason is its buoyancy. Since hydrogen is a deep cryogen it boils quickly and rises due to its very low density as a gas. Even when hydrogen burns, the gaseous H2O that is formed has a molecular weight of only 18 u compared to 29.9 u for air, so it rises quickly as well. Kerosene on the other hand falls to the ground and burns for hours when spilled in large quantities, unavoidably causing extensive heat damage that requires time-consuming repairs and rebuilding. This is a lesson most frequently experienced by test stand crews involved with firings of large, unproven rocket engines. Hydrogen-fuelled engines have special design requirements such as running propellant lines horizontally, so traps do not form in the lines and cause ruptures due to boiling in confined spaces. These considerations apply to all cryogens, such as liquid oxygen and liquid natural gas (LNG) as well. Use of liquid hydrogen fuel has an excellent safety record and superb performance that is well above that of all other practical chemical rocket propellants.
The highest specific impulse chemistry ever test-fired in a rocket engine was lithium and fluorine, with hydrogen added to improve the exhaust thermodynamics (all propellants had to be kept in their own tanks, making this a tripropellant). The combination delivered 542 s specific impulse in a vacuum, equivalent to an exhaust velocity of 5320 m/s. The impracticality of this chemistry highlights why exotic propellants are not actually used: to make all three components liquids, the hydrogen must be kept below –252 °C (just 21 K) and the lithium must be kept above 180 °C (453 K). Lithium and fluorine are both extremely corrosive, lithium ignites on contact with air, fluorine ignites on contact with most fuels, including hydrogen. Fluorine and the hydrogen fluoride (HF) in the exhaust are very toxic, which makes working around the launch pad difficult, damages the environment, and makes getting a launch license that much more difficult. Both lithium and fluorine are expensive compared to most rocket propellants. This combination has therefore never flown.
During the 1950s, the Department of Defense initially proposed lithium/fluorine as ballistic missile propellants. A 1954 accident at a chemical works where a cloud of fluorine was released into the atmosphere convinced them to instead use LOX/RP-1.
In NASA's Design Reference Mission 5.0 documents (between 2009 and 2012), liquid methane/LOX is the chosen propellant mixture for the lander module.
As of July 2022, SpaceX uses Raptor methalox bipropellant rocket engines in test flights for its Starship super-heavy-lift launch vehicle. In November 2012, CEO Elon Musk announced plans to develop liquid methane/LOX rocket engines. SpaceX had previously used only RP-1/LOX in SpaceX rocket engines. 
Although it has a lower specific impulse than liquid hydrogen, liquid methane can be produced on Mars via the Sabatier reaction and is easier to store than liquid hydrogen due to its higher boiling point and density, as well as its lack of hydrogen embrittlement. It also leaves less residue in the engines compared to kerosene, which is beneficial for reusability.
In July 2014, Firefly Space Systems announced their plans to use methane fuel for their small satellite launch vehicle, Firefly Alpha with an aerospike engine design.
In September 2014, Blue Origin and United Launch Alliance announced the joint development of the BE-4 LOX/LNG engine. The BE-4 will provide 2,400 kN (550,000 lbf) of thrust.
As of 2018, liquid fuel combinations in common use:
The table uses data from the JANNAF thermochemical tables (Joint Army-Navy-NASA-Air Force (JANNAF) Interagency Propulsion Committee) throughout, with best-possible specific impulse calculated by Rocketdyne under the assumptions of adiabatic combustion, isentropic expansion, one-dimensional expansion and shifting equilibrium Some units have been converted to metric, but pressures have not.
Definitions of some of the mixtures:
Has not all data for CO/O2, purposed for NASA for Martian-based rockets, only a specific impulse about 250 s.

The Soviet Union, officially the Union of Soviet Socialist Republics (USSR), was a transcontinental country that spanned much of Eurasia from 1922 to 1991. A flagship communist state, it was nominally a federal union of fifteen national republics; in practice, both its government and its economy were highly centralized until its final years. It was a one-party state governed by the Communist Party of the Soviet Union, with the city of Moscow serving as its capital within its largest and most populous republic: the Russian SFSR. Other major cities included Leningrad (Russian SFSR), Kiev (Ukrainian SSR), Minsk (Byelorussian SSR), Tashkent (Uzbek SSR), Alma-Ata (Kazakh SSR), and Novosibirsk (Russian SFSR). It was the largest country in the world, covering over 22,402,200 square kilometres (8,649,500 sq mi) and spanning eleven time zones.
The country's roots lay in the October Revolution of 1917, when the Bolsheviks, under the leadership of Vladimir Lenin, overthrew the Russian Provisional Government that had earlier replaced the House of Romanov of the Russian Empire. The Bolshevik victory established the Russian Soviet Republic, the world's first constitutionally guaranteed socialist state. Persisting internal tensions escalated into the Russian Civil War, which saw fighting between the Bolshevik Red Army and many anti-Bolshevik forces across the former Russian Empire, among whom the largest faction was the White Guard. The anti-communist White Guard violently repressed the Bolsheviks as well as suspected "worker and peasant" Bolsheviks during the White Terror. However, the Red Army expanded and helped local Bolsheviks take power, establishing soviets and repressing their political opponents as well as rebellious peasants during the Red Terror. By 1922, the balance of power had shifted and the Bolsheviks had emerged victorious, forming the Soviet Union with the unification of the Russian, Transcaucasian, Ukrainian, and Byelorussian republics. Upon the conclusion of the Russian Civil War, Lenin's government introduced the New Economic Policy, which led to the partial return of a free market and private property; this resulted in a period of economic recovery.
Following Lenin's death in 1924, Joseph Stalin came to power. Stalin suppressed all political opposition to his rule inside the Communist Party and inaugurated a command economy. As a result, the country underwent a period of rapid industrialization and forced collectivization that led to significant economic growth, but also contributed to a man-made famine in 1930–1933. Additionally, the labour camp system of the Gulag was also expanded in this period. Stalin fomented political paranoia and conducted the Great Purge to remove his actual and perceived opponents from the Communist Party through mass arrests of military leaders, party members, and ordinary citizens alike; all of whom were then sent to correctional labour camps or sentenced to death.
On 23 August 1939, the Soviets signed the Molotov–Ribbentrop Pact with Nazi Germany, which established an understanding of neutrality and non-aggression between the two sides. With the outbreak of World War II following the German invasion of Poland, the formally neutral Soviet Union invaded and annexed the territories of several states in Eastern Europe, including the eastern regions of Poland, and Lithuania, Latvia, and Estonia. In June 1941, Germany broke the bilateral non-aggression pact and launched a large-scale invasion of the Soviet Union, opening the Eastern Front of the global conflict. Despite initial German successes, the Soviets gained the upper hand over Axis forces at the Battle of Stalingrad and eventually captured Berlin, declaring victory over Germany on 9 May 1945. The combined Soviet civilian and military casualty count—estimated to be around 27 million people—accounted for the majority of losses on the side of the Allied forces. In the aftermath of World War II, the territory taken by the Red Army formed various Soviet satellite states under the Eastern Bloc. The subsequent beginning of the Cold War in 1947 saw the Eastern Bloc of the Soviet Union confront the Western Bloc of the United States, with the latter grouping becoming largely united in 1949 under the North Atlantic Treaty Organization and the former grouping becoming largely united in 1955 under the Warsaw Pact.
Following Stalin's death in 1953, a period known as de-Stalinization and the Khrushchev Thaw occurred under the leadership of Nikita Khrushchev. The Soviet Union developed rapidly, as millions of peasants were moved into industrialized cities. As part of the Cold War, the Soviets took an early lead in the Space Race with the first artificial satellite, the first human spaceflight, and the first probe to land on another planet (Venus). In the 1970s, there was a brief détente in the Soviet Union's relationship with the United States, but tensions resumed following the Soviet invasion of Afghanistan in 1979. Lasting until 1989, the Soviet–Afghan War drained Soviet economic resources and was matched by an escalation of American military aid to the Afghan mujahideen.
In the mid-1980s, the last Soviet leader, Mikhail Gorbachev, sought to further reform and liberalize the economy through his policies of glasnost and perestroika. The goal was to preserve the Communist Party while reversing the Era of Stagnation. In 1989, during the closing stages of the Cold War, various countries of the Warsaw Pact overthrew their Marxist–Leninist regimes, which was accompanied by the outbreak of strong nationalist and separatist movements across the entire Soviet Union. In 1991, Gorbachev initiated a national referendum—boycotted by the Soviet republics of Lithuania, Latvia, Estonia, Armenia, Georgia, and Moldova—that resulted in the majority of participating citizens voting in favour of preserving the country as a renewed federation. In August 1991, hardline members of the Communist Party staged a coup d'état against Gorbachev; the attempt failed, with Boris Yeltsin playing a high-profile role in facing down the unrest, and the Communist Party was subsequently banned. The Soviet republics, led by Russia and Ukraine, formally declared independence. On 25 December 1991, Gorbachev resigned from his presidency. All of the republics emerged from the dissolution of the Soviet Union as fully independent post-Soviet states. Above the other former republics, the Russian Federation (formerly the Russian SFSR) assumed the Soviet Union's rights and obligations and has since remained recognized as its successor legal personality in international affairs.
The Soviet Union produced many significant social and technological achievements and innovations, particularly with regard to military power. It boasted the world's second-largest economy, and the Soviet Armed Forces comprised the largest standing military in the world. An NPT-designated state, it possessed the largest arsenal of nuclear weapons in the world. It was a founding member of the United Nations as well as one of the five permanent members of the United Nations Security Council; it was also a member of the OSCE and the WFTU, and the leading member of the Council for Mutual Economic Assistance.
Between the end of World War II in 1945 and its dissolution in 1991, the Soviet Union had maintained its status as one of two superpowers vis-à-vis the United States. It was sometimes referred to informally as the "Soviet Empire" in relation to its exercising of hegemony across Europe as well as worldwide with a combination of military and economic strength; proxy conflicts and influence in the Third World; and funding of scientific research, especially in space technology and weaponry.
The word soviet is derived from the Russian word sovet (Russian: совет), meaning 'council', 'assembly', 'advice', ultimately deriving from the proto-Slavic verbal stem of *vět-iti ('to inform'), related to Slavic věst ('news'), English wise, the root in ad-vis-or (which came to English through French), or the Dutch weten ('to know'; compare wetenschap meaning 'science'). The word sovietnik means 'councillor'.
Some organizations in Russian history were called council (Russian: совет). In the Russian Empire, the State Council which functioned from 1810 to 1917 was referred to as a Council of Ministers after the revolt of 1905.
During the Georgian Affair, Vladimir Lenin envisioned an expression of Great Russian ethnic chauvinism by Joseph Stalin and his supporters, calling for these nation-states to join Russia as semi-independent parts of a greater union which he initially named as the Union of Soviet Republics of Europe and Asia (Russian: Союз Советских Республик Европы и Азии, tr. Soyuz Sovetskikh Respublik Evropy i Azii). Stalin initially resisted the proposal but ultimately accepted it, although with Lenin's agreement changed the name to the Union of Soviet Socialist Republics (USSR), although all the republics began as socialist soviet and did not change to the other order until 1936. In addition, in the national languages of several republics, the word council or conciliar in the respective language was only quite late changed to an adaptation of the Russian soviet and never in others, e.g. Ukrainian SSR.
СССР (in the Latin alphabet: SSSR) is the abbreviation of the Russian language cognate of USSR, as written in Cyrillic letters. The Soviets used this abbreviation so frequently that audiences worldwide became familiar with its meaning. After this, the most common Russian initialization is Союз ССР (transliteration: Soyuz SSR) which, after compensating for grammatical differences, essentially translates to Union of SSRs in English. In addition, the Russian short form name Советский Союз (transliteration: Sovetskiy Soyuz, which literally means Soviet Union) is also commonly used, but only in its unabbreviated form. Since the start of the Great Patriotic War at the latest, abbreviating the Russian name of the Soviet Union as СС (in the same way as, for example, United States is abbreviated into US) has been complete taboo, the reason being that СС as a Russian Cyrillic abbreviation is instead associated with the infamous Schutzstaffel of Nazi Germany, just as SS is in English.
In English language media, the state was referred to as the Soviet Union or the USSR. In other European languages, the locally translated short forms and abbreviations are usually used such as Union soviétique and URSS in French, or Sowjetunion and UdSSR in German. In the English-speaking world, the Soviet Union was also informally called Russia and its citizens Russians, although that was technically incorrect since Russia was only one of the republics of the USSR. Such misapplications of the linguistic equivalents to the term Russia and its derivatives were frequent in other languages as well.
The Soviet Union covered an area of over 22,402,200 square kilometres (8,649,500 sq mi), and was the world's largest country, a status that is retained by its successor state, Russia. It covered a sixth of Earth's land surface, and its size was comparable to the continent of North America. Its western part in Europe accounted for a quarter of the country's area and was the cultural and economic center. The eastern part in Asia extended to the Pacific Ocean to the east and Afghanistan to the south, and, except some areas in Central Asia, was much less populous. It spanned over 10,000 kilometres (6,200 mi) east to west across eleven time zones, and over 7,200 kilometres (4,500 mi) north to south. It had five climate zones: tundra, taiga, steppes, desert and mountains.
The Soviet Union, similarly to Russia, had the world's longest border, measuring over 60,000 kilometres (37,000 mi), or .mw-parser-output .frac{white-space:nowrap}.mw-parser-output .frac .num,.mw-parser-output .frac .den{font-size:80%;line-height:0;vertical-align:super}.mw-parser-output .frac .den{vertical-align:sub}.mw-parser-output .sr-only{border:0;clip:rect(0,0,0,0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}1+1⁄2 circumferences of Earth. Two-thirds of it was a coastline. The country bordered (from 1945 to 1991): Norway, Finland, the Baltic Sea, Poland, Czechoslovakia, Hungary, Romania, the Black Sea, Turkey, Iran, the Caspian Sea, Afghanistan, China, Mongolia, and North Korea. The Bering Strait separated the country from the United States, while the La Pérouse Strait separated it from Japan.
The Soviet Union's highest mountain was Communism Peak (now Ismoil Somoni Peak) in Tajik SSR, at 7,495 metres (24,590 ft). It also included most of the world's largest lakes; the Caspian Sea (shared with Iran), and Lake Baikal in Russia, the world's largest and deepest freshwater lake.
Modern revolutionary activity in the Russian Empire began with the 1825 Decembrist revolt. Although serfdom was abolished in 1861, it was done on terms unfavorable to the peasants and served to encourage revolutionaries. A parliament—the State Duma—was established in 1906 after the Russian Revolution of 1905, but Tsar Nicholas II resisted attempts to move from absolute to a constitutional monarchy. Social unrest continued and was aggravated during World War I by military defeat and food shortages in major cities.
A spontaneous popular uprising in Petrograd, in response to the wartime decay of Russia's economy and morale, culminated in the February Revolution and the toppling of Nicholas II and the imperial government in March 1917. The tsarist autocracy was replaced by the Russian Provisional Government, which intended to conduct elections to the Russian Constituent Assembly and to continue fighting on the side of the Entente in World War I.
At the same time, workers' councils, known in Russian as "Soviets", sprang up across the country. The Bolsheviks, led by Vladimir Lenin, pushed for socialist revolution in the Soviets and on the streets. On 7 November 1917, the Red Guards stormed the Winter Palace in Petrograd, ending the rule of the Provisional Government and leaving all political power to the Soviets. This event would later be officially known in Soviet bibliographies as the Great October Socialist Revolution. In December, the Bolsheviks signed an armistice with the Central Powers, though by February 1918, fighting had resumed. In March, the Soviets ended involvement in the war and signed the Treaty of Brest-Litovsk.
A long and bloody Civil War ensued between the Reds and the Whites, starting in 1917 and ending in 1923 with the Reds' victory. It included foreign intervention, the execution of the former tsar and his family, and the famine of 1921, which killed about five million people. In March 1921, during a related conflict with Poland, the Peace of Riga was signed, splitting disputed territories in Belarus and Ukraine between the Republic of Poland and Soviet Russia. Soviet Russia had to resolve similar conflicts with the newly established republics of Estonia, Finland, Latvia, and Lithuania.
On 28 December 1922, a conference of plenipotentiary delegations from the Russian SFSR, the Transcaucasian SFSR, the Ukrainian SSR and the Byelorussian SSR approved the Treaty on the Creation of the USSR and the Declaration of the Creation of the USSR, forming the Union of Soviet Socialist Republics. These two documents were confirmed by the first Congress of Soviets of the USSR and signed by the heads of the delegations, Mikhail Kalinin, Mikhail Tskhakaya, Mikhail Frunze, Grigory Petrovsky, and Alexander Chervyakov, on 30 December 1922. The formal proclamation was made from the stage of the Bolshoi Theatre.
An intensive restructuring of the economy, industry and politics of the country began in the early days of Soviet power in 1917. A large part of this was done according to the Bolshevik Initial Decrees, government documents signed by Vladimir Lenin. One of the most prominent breakthroughs was the GOELRO plan, which envisioned a major restructuring of the Soviet economy based on total electrification of the country. The plan became the prototype for subsequent Five-Year Plans and was fulfilled by 1931. After the economic policy of "War communism" during the Russian Civil War, as a prelude to fully developing socialism in the country, the Soviet government permitted some private enterprise to coexist alongside nationalized industry in the 1920s, and total food requisition in the countryside was replaced by a food tax.
From its creation, the government in the Soviet Union was based on the one-party rule of the Communist Party (Bolsheviks). The stated purpose was to prevent the return of capitalist exploitation, and that the principles of democratic centralism would be the most effective in representing the people's will in a practical manner. The debate over the future of the economy provided the background for a power struggle in the years after Lenin's death in 1924. Initially, Lenin was to be replaced by a "troika" consisting of Grigory Zinoviev of the Ukrainian SSR, Lev Kamenev of the Russian SFSR, and Joseph Stalin of the Transcaucasian SFSR.
On 1 February 1924, the USSR was recognized by the United Kingdom. The same year, a Soviet Constitution was approved, legitimizing the December 1922 union.
According to Archie Brown the constitution was never an accurate guide to political reality in the USSR. For example the fact that the Party played the leading role in making and enforcing policy was not mentioned in it until 1977. The USSR was a federative entity of many constituent republics, each with its own political and administrative entities. However, the term "Soviet Russia" – strictly applicable only to the Russian Federative Socialist Republic – was often applied to the entire country by non-Soviet writers.
On 3 April 1922, Stalin was named the General Secretary of the Communist Party of the Soviet Union. Lenin had appointed Stalin the head of the Workers' and Peasants' Inspectorate, which gave Stalin considerable power. By gradually consolidating his influence and isolating and outmaneuvering his rivals within the party, Stalin became the undisputed leader of the country and, by the end of the 1920s, established a totalitarian rule. In October 1927, Zinoviev and Leon Trotsky were expelled from the Central Committee and forced into exile.
In 1928, Stalin introduced the first five-year plan for building a socialist economy. In place of the internationalism expressed by Lenin throughout the Revolution, it aimed to build Socialism in One Country. In industry, the state assumed control over all existing enterprises and undertook an intensive program of industrialization. In agriculture, rather than adhering to the "lead by example" policy advocated by Lenin, forced collectivization of farms was implemented all over the country.
Famines ensued as a result, causing deaths estimated at three to seven million; surviving kulaks were persecuted, and many were sent to Gulags to do forced labor. Social upheaval continued in the mid-1930s. Despite the turmoil of the mid-to-late 1930s, the country developed a robust industrial economy in the years preceding World War II.
Closer cooperation between the USSR and the West developed in the early 1930s. From 1932 to 1934, the country participated in the World Disarmament Conference. In 1933, diplomatic relations between the United States and the USSR were established when in November, the newly elected President of the United States, Franklin D. Roosevelt, chose to recognize Stalin's Communist government formally and negotiated a new trade agreement between the two countries. In September 1934, the country joined the League of Nations. After the Spanish Civil War broke out in 1936, the USSR actively supported the Republican forces against the Nationalists, who were supported by Fascist Italy and Nazi Germany.
In December 1936, Stalin unveiled a new constitution that was praised by supporters around the world as the most democratic constitution imaginable, though there was some skepticism. Stalin's Great Purge resulted in the detainment or execution of many "Old Bolsheviks" who had participated in the October Revolution with Lenin. According to declassified Soviet archives, the NKVD arrested more than one and a half million people in 1937 and 1938, of whom 681,692 were shot. Over those two years, there were an average of over one thousand executions a day.
In 1939, after attempts to form a military alliance with Britain and France against Germany failed, the Soviet Union made a dramatic shift towards Nazi Germany. Almost a year after Britain and France had concluded the Munich Agreement with Germany, the Soviet Union made agreements with Germany as well, both militarily and economically during extensive talks. The two countries concluded the Molotov–Ribbentrop Pact and the German–Soviet Commercial Agreement in August 1939. The former made possible the Soviet occupation of Lithuania, Latvia, Estonia, Bessarabia, northern Bukovina, and eastern Poland, while the Soviets remained formally neutral. In late November, unable to coerce the Republic of Finland by diplomatic means into moving its border 25 kilometres (16 mi) back from Leningrad, Stalin ordered the invasion of Finland. On 14 December 1939 the Soviet Union was expelled from the League of Nations for invading Finland. In the east, the Soviet military won several decisive victories during border clashes with the Empire of Japan in 1938 and 1939. However, in April 1941, the USSR signed the Soviet–Japanese Neutrality Pact with Japan, recognizing the territorial integrity of Manchukuo, a Japanese puppet state.
Germany broke the Molotov–Ribbentrop Pact and invaded the Soviet Union on 22 June 1941 starting what was known in the USSR as the Great Patriotic War. The Red Army stopped the seemingly invincible German Army at the Battle of Moscow. The Battle of Stalingrad, which lasted from late 1942 to early 1943, dealt a severe blow to Germany from which they never fully recovered and became a turning point in the war. After Stalingrad, Soviet forces drove through Eastern Europe to Berlin before Germany surrendered in 1945. The German Army suffered 80% of its military deaths in the Eastern Front. Harry Hopkins, a close foreign policy advisor to Franklin D. Roosevelt, spoke on 10 August 1943 of the USSR's decisive role in the war.
In the same year, the USSR, in fulfilment of its agreement with the Allies at the Yalta Conference, denounced the Soviet–Japanese Neutrality Pact in April 1945 and invaded Manchukuo and other Japan-controlled territories on 9 August 1945. This conflict ended with a decisive Soviet victory, contributing to the unconditional surrender of Japan and the end of World War II.
The USSR suffered greatly in the war, losing around 27 million people. Approximately 2.8 million Soviet POWs died of starvation, mistreatment, or executions in just eight months of 1941–42. During the war, the country together with the United States, the United Kingdom and China were considered the Big Four Allied powers, and later became the Four Policemen that formed the basis of the United Nations Security Council. It emerged as a superpower in the post-war period. Once denied diplomatic recognition by the Western world, the USSR had official relations with practically every country by the late 1940s. A member of the United Nations at its foundation in 1945, the country became one of the five permanent members of the United Nations Security Council, which gave it the right to veto any of its resolutions.
During the immediate post-war period, the Soviet Union rebuilt and expanded its economy, while maintaining its strictly centralized control. It took effective control over most of the countries of Eastern Europe (except Yugoslavia and later Albania), turning them into satellite states. The USSR bound its satellite states in a military alliance, the Warsaw Pact, in 1955, and an economic organization, Council for Mutual Economic Assistance or Comecon, a counterpart to the European Economic Community (EEC), from 1949 to 1991. The USSR concentrated on its own recovery, seizing and transferring most of Germany's industrial plants, and it exacted war reparations from East Germany, Hungary, Romania, and Bulgaria using Soviet-dominated joint enterprises. It also instituted trading arrangements deliberately designed to favor the country. Moscow controlled the Communist parties that ruled the satellite states, and they followed orders from the Kremlin. Later, the Comecon supplied aid to the eventually victorious Chinese Communist Party, and its influence grew elsewhere in the world. Fearing its ambitions, the Soviet Union's wartime allies, the United Kingdom and the United States, became its enemies. In the ensuing Cold War, the two sides clashed indirectly in proxy wars.
Stalin died on 5 March 1953. Without a mutually agreeable successor, the highest Communist Party officials initially opted to rule the Soviet Union jointly through a troika headed by Georgy Malenkov. This did not last, however, and Nikita Khrushchev eventually won the ensuing power struggle by the mid-1950s. In 1956, he denounced Joseph Stalin and proceeded to ease controls over the party and society. This was known as de-Stalinization.
Moscow considered Eastern Europe to be a critically vital buffer zone for the forward defence of its western borders, in case of another major invasion such as the German invasion of 1941. For this reason, the USSR sought to cement its control of the region by transforming the Eastern European countries into satellite states, dependent upon and subservient to its leadership. As a result, Soviet military forces were used to suppress an anti-communist uprising in Hungary in 1956.
In the late 1950s, a confrontation with China regarding the Soviet rapprochement with the West, and what Mao Zedong perceived as Khrushchev's revisionism, led to the Sino–Soviet split. This resulted in a break throughout the global Marxist–Leninist movement, with the governments in Albania, Cambodia and Somalia choosing to ally with China.
During this period of the late 1950s and early 1960s, the USSR continued to realize scientific and technological exploits in the Space Race, rivaling the United States: launching the first artificial satellite, Sputnik 1 in 1957; a living dog named Laika in 1957; the first human being, Yuri Gagarin in 1961; the first woman in space, Valentina Tereshkova in 1963; Alexei Leonov, the first person to walk in space in 1965; the first soft landing on the Moon by spacecraft Luna 9 in 1966; and the first Moon rovers, Lunokhod 1 and Lunokhod 2.
Khrushchev initiated "The Thaw", a complex shift in political, cultural and economic life in the country. This included some openness and contact with other nations and new social and economic policies with more emphasis on commodity goods, allowing a dramatic rise in living standards while maintaining high levels of economic growth. Censorship was relaxed as well. Khrushchev's reforms in agriculture and administration, however, were generally unproductive. In 1962, he precipitated a crisis with the United States over the Soviet deployment of nuclear missiles in Cuba. An agreement was made with the United States to remove nuclear missiles from both Cuba and Turkey, concluding the crisis. This event caused Khrushchev much embarrassment and loss of prestige, resulting in his removal from power in 1964.
Following the ousting of Khrushchev, another period of collective leadership ensued, consisting of Leonid Brezhnev as general secretary, Alexei Kosygin as Premier and Nikolai Podgorny as Chairman of the Presidium, lasting until Brezhnev established himself in the early 1970s as the preeminent Soviet leader.
In 1968, the Soviet Union and Warsaw Pact allies invaded Czechoslovakia to halt the Prague Spring reforms. In the aftermath, Brezhnev justified the invasion and previous military interventions as well as any potential military interventions in the future by introducing the Brezhnev Doctrine, which proclaimed any threat to socialist rule in a Warsaw Pact state as a threat to all Warsaw Pact states, therefore justifying military intervention.
Brezhnev presided throughout détente with the West that resulted in treaties on armament control (SALT I, SALT II, Anti-Ballistic Missile Treaty) while at the same time building up Soviet military might.
In October 1977, the third Soviet Constitution was unanimously adopted. The prevailing mood of the Soviet leadership at the time of Brezhnev's death in 1982 was one of aversion to change. The long period of Brezhnev's rule had come to be dubbed one of "standstill", with an ageing and ossified top political leadership. This period is also known as the Era of Stagnation, a period of adverse economic, political, and social effects in the country, which began during the rule of Brezhnev and continued under his successors Yuri Andropov and Konstantin Chernenko.
In late 1979, the Soviet Union's military intervened in the ongoing civil war in neighboring Afghanistan, effectively ending a détente with the West.
Two developments dominated the decade that followed: the increasingly apparent crumbling of the Soviet Union's economic and political structures, and the patchwork attempts at reforms to reverse that process. Kenneth S. Deffeyes argued in Beyond Oil that the Reagan administration encouraged Saudi Arabia to lower the price of oil to the point where the Soviets could not make a profit selling their oil, and resulted in the depletion of the country's hard currency reserves.
Brezhnev's next two successors, transitional figures with deep roots in his tradition, did not last long. Yuri Andropov was 68 years old and Konstantin Chernenko 72 when they assumed power; both died in less than two years. In an attempt to avoid a third short-lived leader, in 1985, the Soviets turned to the next generation and selected Mikhail Gorbachev. He made significant changes in the economy and party leadership, called perestroika. His policy of glasnost freed public access to information after decades of heavy government censorship. Gorbachev also moved to end the Cold War. In 1988, the USSR abandoned its war in Afghanistan and began to withdraw its forces. In the following year, Gorbachev refused to interfere in the internal affairs of the Soviet satellite states, which paved the way for the Revolutions of 1989. In particular, the standstill of the Soviet Union at the Pan-European Picnic in August 1989 then set a peaceful chain reaction in motion at the end of which the Eastern Bloc collapsed. With the tearing down of the Berlin Wall and with East and West Germany pursuing unification, the Iron Curtain between the West and Soviet-controlled regions came down.
At the same time, the Soviet republics started legal moves towards potentially declaring sovereignty over their territories, citing the freedom to secede in Article 72 of the USSR constitution. On 7 April 1990, a law was passed allowing a republic to secede if more than two-thirds of its residents voted for it in a referendum. Many held their first free elections in the Soviet era for their own national legislatures in 1990. Many of these legislatures proceeded to produce legislation contradicting the Union laws in what was known as the "War of Laws". In 1989, the Russian SFSR convened a newly elected Congress of People's Deputies. Boris Yeltsin was elected its chairman. On 12 June 1990, the Congress declared Russia's sovereignty over its territory and proceeded to pass laws that attempted to supersede some of the Soviet laws. After a landslide victory of Sąjūdis in Lithuania, that country declared its independence restored on 11 March 1990.
A referendum for the preservation of the USSR was held on 17 March 1991 in nine republics (the remainder having boycotted the vote), with the majority of the population in those republics voting for preservation of the Union. The referendum gave Gorbachev a minor boost. In the summer of 1991, the New Union Treaty, which would have turned the country into a much looser Union, was agreed upon by eight republics. The signing of the treaty, however, was interrupted by the August Coup—an attempted coup d'état by hardline members of the government and the KGB who sought to reverse Gorbachev's reforms and reassert the central government's control over the republics. After the coup collapsed, Yeltsin was seen as a hero for his decisive actions, while Gorbachev's power was effectively ended. The balance of power tipped significantly towards the republics. In August 1991, Latvia and Estonia immediately declared the restoration of their full independence (following Lithuania's 1990 example). Gorbachev resigned as general secretary in late August, and soon afterwards, the party's activities were indefinitely suspended—effectively ending its rule. By the fall, Gorbachev could no longer influence events outside Moscow, and he was being challenged even there by Yeltsin, who had been elected President of Russia in July 1991.
The remaining 12 republics continued discussing new, increasingly looser, models of the Union. However, by December all except Russia and Kazakhstan had formally declared independence. During this time, Yeltsin took over what remained of the Soviet government, including the Moscow Kremlin. The final blow was struck on 1 December when Ukraine, the second-most powerful republic, voted overwhelmingly for independence. Ukraine's secession ended any realistic chance of the country staying together even on a limited scale.
On 8 December 1991, the presidents of Russia, Ukraine and Belarus (formerly Byelorussia), signed the Belavezha Accords, which declared the Soviet Union dissolved and established the Commonwealth of Independent States (CIS) in its place. While doubts remained over the authority of the accords to do this, on 21 December 1991, the representatives of all Soviet republics except Georgia signed the Alma-Ata Protocol, which confirmed the accords. On 25 December 1991, Gorbachev resigned as the President of the USSR, declaring the office extinct. He turned the powers that had been vested in the presidency over to Yeltsin. That night, the Soviet flag was lowered for the last time, and the Russian tricolor was raised in its place.
The following day, the Supreme Soviet, the highest governmental body, voted both itself and the country out of existence. This is generally recognized as marking the official, final dissolution of the Soviet Union as a functioning state, and the end of the Cold War. The Soviet Army initially remained under overall CIS command but was soon absorbed into the different military forces of the newly independent states. The few remaining Soviet institutions that had not been taken over by Russia ceased to function by the end of 1991.
Following the dissolution, Russia was internationally recognized as its legal successor on the international stage. To that end, Russia voluntarily accepted all Soviet foreign debt and claimed Soviet overseas properties as its own. Under the 1992 Lisbon Protocol, Russia also agreed to receive all nuclear weapons remaining in the territory of other former Soviet republics. Since then, the Russian Federation has assumed the Soviet Union's rights and obligations. Ukraine has refused to recognize exclusive Russian claims to succession of the USSR and claimed such status for Ukraine as well, which was codified in Articles 7 and 8 of its 1991 law On Legal Succession of Ukraine. Since its independence in 1991, Ukraine has continued to pursue claims against Russia in foreign courts, seeking to recover its share of the foreign property that was owned by the USSR.
The dissolution was followed by a severe drop in economic and social conditions in post-Soviet states, including a rapid increase in poverty, crime, corruption, unemployment, homelessness, rates of disease, infant mortality and domestic violence, as well as demographic losses, income inequality and the rise of an oligarchical class, along with decreases in calorie intake, life expectancy, adult literacy, and income. Between 1988 and 1989 and 1993–1995, the Gini ratio increased by an average of 9 points for all former socialist countries. The economic shocks that accompanied wholesale privatization were associated with sharp increases in mortality. Data shows Russia, Kazakhstan, Latvia, Lithuania and Estonia saw a tripling of unemployment and a 42% increase in male death rates between 1991 and 1994. In the following decades, only five or six of the post-communist states are on a path to joining the wealthy capitalist West while most are falling behind, some to such an extent that it will take over fifty years to catch up to where they were before the fall of the Soviet Bloc.
In summing up the international ramifications of these events, Vladislav Zubok stated: "The collapse of the Soviet empire was an event of epochal geopolitical, military, ideological, and economic significance." Before the dissolution, the country had maintained its status as one of the world's two superpowers for four decades after World War II through its hegemony in Eastern Europe, military strength, economic strength, aid to developing countries, and scientific research, especially in space technology and weaponry.
The analysis of the succession of states for the 15 post-Soviet states is complex. The Russian Federation is seen as the legal continuator state and is for most purposes the heir to the Soviet Union. It retained ownership of all former Soviet embassy properties, and also inherited the Soviet Union's UN membership, with its permanent seat on the Security Council.
Of the two other co-founding states of the USSR at the time of the dissolution, Ukraine was the only one that had passed laws, similar to Russia, that it is a state-successor of both the Ukrainian SSR and the USSR. Soviet treaties laid groundwork for Ukraine's future foreign agreements as well as they led to Ukraine agreeing to undertake 16.37% of debts of the Soviet Union for which it was going to receive its share of USSR's foreign property. Although it had a tough position at the time, due to Russia's position as a "single continuation of the USSR" that became widely accepted in the West as well as a constant pressure from the Western countries, allowed Russia to dispose state property of USSR abroad and conceal information about it. Due to that Ukraine never ratified "zero option" agreement that Russian Federation had signed with other former Soviet republics, as it denied disclosing of information about Soviet Gold Reserves and its Diamond Fund. The dispute over former Soviet property and assets between the two former republics is still ongoing:
The conflict is unsolvable. We can continue to poke Kiev handouts in the calculation of "solve the problem", only it won't be solved. Going to a trial is also pointless: for a number of European countries this is a political issue, and they will make a decision clearly in whose favor. What to do in this situation is an open question. Search for non-trivial solutions. But we must remember that in 2014, with the filing of the then Ukrainian Prime Minister Yatsenyuk, litigation with Russia resumed in 32 countries.Similar situation occurred with restitution of cultural property. Although on 14 February 1992 Russia and other former Soviet republics signed agreement "On the return of cultural and historic property to the origin states" in Minsk, it was halted by Russian State Duma that had eventually passed "Federal Law on Cultural Valuables Displaced to the USSR as a Result of the Second World War and Located on the Territory of the Russian Federation" which made restitution currently impossible.
Estonia, Latvia, and Lithuania consider themselves as revivals of the three independent countries that existed prior to their occupation and annexation by the Soviet Union in 1940. They maintain that the process by which they were incorporated into the Soviet Union violated both international law and their own law, and that in 1990–1991 they were reasserting an independence that still legally existed.
There are additionally six states that claim independence from the other internationally recognised post-Soviet states but possess limited international recognition: Abkhazia, Artsakh, Donetsk, Luhansk, South Ossetia and Transnistria. The Chechen separatist movement of the Chechen Republic of Ichkeria, the Gagauz separatist movement of the Gagauz Republic and the Talysh separatist movement of the Talysh-Mughan Republic lack any international recognition.
During his rule, Stalin always made the final policy decisions. Otherwise, Soviet foreign policy was set by the commission on the Foreign Policy of the Central Committee of the Communist Party of the Soviet Union, or by the party's highest body the Politburo. Operations were handled by the separate Ministry of Foreign Affairs. It was known as the People's Commissariat for Foreign Affairs (or Narkomindel), until 1946. The most influential spokesmen were Georgy Chicherin (1872–1936), Maxim Litvinov (1876–1951), Vyacheslav Molotov (1890–1986), Andrey Vyshinsky (1883–1954) and Andrei Gromyko (1909–1989). Intellectuals were based in the Moscow State Institute of International Relations.
The Marxist-Leninist leadership of the Soviet Union intensely debated foreign policy issues and changed directions several times. Even after Stalin assumed dictatorial control in the late 1920s, there were debates, and he frequently changed positions.
During the country's early period, it was assumed that Communist revolutions would break out soon in every major industrial country, and it was the Soviet responsibility to assist them. The Comintern was the weapon of choice. A few revolutions did break out, but they were quickly suppressed (the longest lasting one was in Hungary)—the Hungarian Soviet Republic—lasted only from 21 March 1919 to 1 August 1919. The Russian Bolsheviks were in no position to give any help.
By 1921, Lenin, Trotsky, and Stalin realized that capitalism had stabilized itself in Europe and there would not be any widespread revolutions anytime soon. It became the duty of the Russian Bolsheviks to protect what they had in Russia, and avoid military confrontations that might destroy their bridgehead. Russia was now a pariah state, along with Germany. The two came to terms in 1922 with the Treaty of Rapallo that settled long-standing grievances. At the same time, the two countries secretly set up training programs for the illegal German army and air force operations at hidden camps in the USSR.
Moscow eventually stopped threatening other states, and instead worked to open peaceful relationships in terms of trade, and diplomatic recognition. The United Kingdom dismissed the warnings of Winston Churchill and a few others about a continuing Marxist-Leninist threat, and opened trade relations and de facto diplomatic recognition in 1922. There was hope for a settlement of the pre-war Tsarist debts, but it was repeatedly postponed. Formal recognition came when the new Labour Party came to power in 1924. All the other countries followed suit in opening trade relations. Henry Ford opened large-scale business relations with the Soviets in the late 1920s, hoping that it would lead to long-term peace. Finally, in 1933, the United States officially recognized the USSR, a decision backed by the public opinion and especially by US business interests that expected an opening of a new profitable market.
In the late 1920s and early 1930s, Stalin ordered Marxist-Leninist parties across the world to strongly oppose non-Marxist political parties, labor unions or other organizations on the left. Stalin reversed himself in 1934 with the Popular Front program that called on all Marxist parties to join together with all anti-Fascist political, labor, and organizational forces that were opposed to fascism, especially of the Nazi variety.
In 1939, half a year after the Munich Agreement, the USSR attempted to form an anti-Nazi alliance with France and Britain. Adolf Hitler proposed a better deal, which would give the USSR control over much of Eastern Europe through the Molotov–Ribbentrop Pact. In September, Germany invaded Poland, and the USSR also invaded later that month, resulting in the partition of Poland. In response, Britain and France declared war on Germany, marking the beginning of World War II.
Up until his death in 1953, Joseph Stalin controlled all foreign relations of the Soviet Union during the interwar period. Despite the increasing build-up of Germany's war machine and the outbreak of the Second Sino-Japanese War, the Soviet Union did not cooperate with any other nation, choosing to follow its own path. However, after Operation Barbarossa, the Soviet Union's priorities changed. Despite previous conflict with the United Kingdom, Vyacheslav Molotov dropped his post war border demands.
The Cold War was a period of geopolitical tension between the United States and the Soviet Union and their respective allies, the Western Bloc and the Eastern Bloc, which began following World War II in 1945. The term cold war is used because there was no large-scale fighting directly between the two superpowers, but they each supported major regional conflicts known as proxy wars. The conflict was based around the ideological and geopolitical struggle for global influence by these two superpowers, following their temporary alliance and victory against Nazi Germany in 1945. Aside from the nuclear arsenal development and conventional military deployment, the struggle for dominance was expressed via indirect means such as psychological warfare, propaganda campaigns, espionage, far-reaching embargoes, rivalry at sports events and technological competitions such as the Space Race.
There were three power hierarchies in the Soviet Union: the legislature represented by the Supreme Soviet of the Soviet Union, the government represented by the Council of Ministers, and the Communist Party of the Soviet Union (CPSU), the only legal party and the final policymaker in the country.
At the top of the Communist Party was the Central Committee, elected at Party Congresses and Conferences. In turn, the Central Committee voted for a Politburo (called the Presidium between 1952 and 1966), Secretariat and the general secretary (First Secretary from 1953 to 1966), the de facto highest office in the Soviet Union. Depending on the degree of power consolidation, it was either the Politburo as a collective body or the General Secretary, who always was one of the Politburo members, that effectively led the party and the country (except for the period of the highly personalized authority of Stalin, exercised directly through his position in the Council of Ministers rather than the Politburo after 1941). They were not controlled by the general party membership, as the key principle of the party organization was democratic centralism, demanding strict subordination to higher bodies, and elections went uncontested, endorsing the candidates proposed from above.
The Communist Party maintained its dominance over the state mainly through its control over the system of appointments. All senior government officials and most deputies of the Supreme Soviet were members of the CPSU. Of the party heads themselves, Stalin (1941–1953) and Khrushchev (1958–1964) were Premiers. Upon the forced retirement of Khrushchev, the party leader was prohibited from this kind of double membership, but the later General Secretaries for at least some part of their tenure occupied the mostly ceremonial position of Chairman of the Presidium of the Supreme Soviet, the nominal head of state. The institutions at lower levels were overseen and at times supplanted by primary party organizations.
However, in practice the degree of control the party was able to exercise over the state bureaucracy, particularly after the death of Stalin, was far from total, with the bureaucracy pursuing different interests that were at times in conflict with the party. Nor was the party itself monolithic from top to bottom, although factions were officially banned.
The Supreme Soviet (successor of the Congress of Soviets) was nominally the highest state body for most of the Soviet history, at first acting as a rubber stamp institution, approving and implementing all decisions made by the party. However, its powers and functions were extended in the late 1950s, 1960s and 1970s, including the creation of new state commissions and committees. It gained additional powers relating to the approval of the Five-Year Plans and the government budget. The Supreme Soviet elected a Presidium (successor of the Central Executive Committee) to wield its power between plenary sessions, ordinarily held twice a year, and appointed the Supreme Court, the Procurator General and the Council of Ministers (known before 1946 as the Council of People's Commissars), headed by the Chairman (Premier) and managing an enormous bureaucracy responsible for the administration of the economy and society. State and party structures of the constituent republics largely emulated the structure of the central institutions, although the Russian SFSR, unlike the other constituent republics, for most of its history had no republican branch of the CPSU, being ruled directly by the union-wide party until 1990. Local authorities were organized likewise into party committees, local Soviets and executive committees. While the state system was nominally federal, the party was unitary.
The state security police (the KGB and its predecessor agencies) played an important role in Soviet politics. It was instrumental in the Great Purge, but was brought under strict party control after Stalin's death. Under Yuri Andropov, the KGB engaged in the suppression of political dissent and maintained an extensive network of informers, reasserting itself as a political actor to some extent independent of the party-state structure, culminating in the anti-corruption campaign targeting high-ranking party officials in the late 1970s and early 1980s.
The constitution, which was promulgated in 1924, 1936 and 1977, did not limit state power. No formal separation of powers existed between the Party, Supreme Soviet and Council of Ministers that represented executive and legislative branches of the government. The system was governed less by statute than by informal conventions, and no settled mechanism of leadership succession existed. Bitter and at times deadly power struggles took place in the Politburo after the deaths of Lenin and Stalin, as well as after Khrushchev's dismissal, itself due to a decision by both the Politburo and the Central Committee. All leaders of the Communist Party before Gorbachev died in office, except Georgy Malenkov and Khrushchev, both dismissed from the party leadership amid internal struggle within the party.
Between 1988 and 1990, facing considerable opposition, Mikhail Gorbachev enacted reforms shifting power away from the highest bodies of the party and making the Supreme Soviet less dependent on them. The Congress of People's Deputies was established, the majority of whose members were directly elected in competitive elections held in March 1989. The Congress now elected the Supreme Soviet, which became a full-time parliament, and much stronger than before. For the first time since the 1920s, it refused to rubber stamp proposals from the party and Council of Ministers. In 1990, Gorbachev introduced and assumed the position of the President of the Soviet Union, concentrated power in his executive office, independent of the party, and subordinated the government, now renamed the Cabinet of Ministers of the USSR, to himself.
Tensions grew between the Union-wide authorities under Gorbachev, reformists led in Russia by Boris Yeltsin and controlling the newly elected Supreme Soviet of the Russian SFSR, and communist hardliners. On 19–21 August 1991, a group of hardliners staged a coup attempt. The coup failed, and the State Council of the Soviet Union became the highest organ of state power "in the period of transition". Gorbachev resigned as General Secretary, only remaining President for the final months of the existence of the USSR.
The judiciary was not independent of the other branches of government. The Supreme Court supervised the lower courts (People's Court) and applied the law as established by the constitution or as interpreted by the Supreme Soviet. The Constitutional Oversight Committee reviewed the constitutionality of laws and acts. The Soviet Union used the inquisitorial system of Roman law, where the judge, procurator, and defence attorney collaborate to establish the truth.
Constitutionally, the USSR was a federation of constituent Union Republics, which were either unitary states, such as Ukraine or Byelorussia (SSRs), or federations, such as Russia or Transcaucasia (SFSRs), all four being the founding republics who signed the Treaty on the Creation of the USSR in December 1922. In 1924, during the national delimitation in Central Asia, Uzbekistan and Turkmenistan were formed from parts of Russia's Turkestan ASSR and two Soviet dependencies, the Khorezm and Bukharan SSRs. In 1929, Tajikistan was split off from the Uzbekistan SSR. With the constitution of 1936, the Transcaucasian SFSR was dissolved, resulting in its constituent republics of Armenia, Georgia and Azerbaijan being elevated to Union Republics, while Kazakhstan and Kirghizia were split off from Russian SFSR, resulting in the same status. In August 1940, Moldavia was formed from parts of Ukraine and Bessarabia and Ukrainian SSR. Estonia, Latvia and Lithuania (SSRs) were also admitted into the union which was not recognized by most of the international community and was considered an illegal occupation. Karelia was split off from Russia as a Union Republic in March 1940 and was reabsorbed in 1956. Between July 1956 and September 1991, there were 15 union republics (see map below).
While nominally a union of equals, in practice the Soviet Union was dominated by Russians. The domination was so absolute that for most of its existence, the country was commonly (but incorrectly) referred to as "Russia". While the RSFSR was technically only one republic within the larger union, it was by far the largest (both in terms of population and area), most powerful, and most highly developed. The RSFSR was also the industrial center of the Soviet Union. Historian Matthew White wrote that it was an open secret that the country's federal structure was "window dressing" for Russian dominance. For that reason, the people of the USSR were usually called "Russians", not "Soviets", since "everyone knew who really ran the show".
Under the Military Law of September 1925, the Soviet Armed Forces consisted of the Land Forces, the Air Force, the Navy, Joint State Political Directorate (OGPU), and the Internal Troops. The OGPU later became independent and in 1934 joined the NKVD, and so its internal troops were under the joint leadership of the defense and internal commissariats. After World War II, Strategic Missile Forces (1959), Air Defense Forces (1948) and National Civil Defense Forces (1970) were formed, which ranked first, third, and sixth in the official Soviet system of importance (ground forces were second, Air Force Fourth, and Navy Fifth).
The army had the greatest political influence. In 1989, there served two million soldiers divided between 150 motorized and 52 armored divisions. Until the early 1960s, the Soviet navy was a rather small military branch, but after the Caribbean crisis, under the leadership of Sergei Gorshkov, it expanded significantly. It became known for battlecruisers and submarines. In 1989 there served 500 000 men. The Soviet Air Force focused on a fleet of strategic bombers and during war situation was to eradicate enemy infrastructure and nuclear capacity. The air force also had a number of fighters and tactical bombers to support the army in the war. Strategic missile forces had more than 1,400 intercontinental ballistic missiles (ICBMs), deployed between 28 bases and 300 command centers.
In the post-war period, the Soviet Army was directly involved in several military operations abroad. These included the suppression of the uprising in East Germany (1953), Hungarian revolution (1956) and the invasion of Czechoslovakia (1968). The Soviet Union also participated in the war in Afghanistan between 1979 and 1989.
In the Soviet Union, general conscription applied.
At the end of the 1950s, with the help of engineers and technologies captured and imported from defeated Nazi Germany, the Soviets constructed the first satellite – Sputnik 1 and thus overtook the United States in terms of using space. This was followed by other successful satellites, where test dogs flight was sent. On 12 April 1961, the first cosmonaut, Yuri Gagarin, was sent to space. He flew once around the Earth and successfully landed in the Kazakh steppe. At that time, the first plans for space shuttles and orbital stations were drawn up in Soviet design offices, but in the end personal disputes between designers and management prevented this.
As for Lunar space program; USSR only had a program on automated spacecraft launches; with no crewed spacecraft used; passing on the "Moon Race" part of Space Race.

In the 1970s, specific proposals for the design of the space shuttle began to emerge, but shortcomings, especially in the electronics industry (rapid overheating of electronics), postponed the program until the end of the 1980s. The first shuttle, the Buran, flew in 1988, but without a human crew. Another shuttle, Ptichka, eventually ended up under construction, as the shuttle project was canceled in 1991. For their launch into space, there is today an unused superpower rocket, Energia, which is the most powerful in the world.
In the late 1980s, the Soviet Union managed to build the Mir orbital station. It was built on the construction of Salyut stations and its only role was civilian-grade research tasks.
The Soviet Union adopted a command economy, whereby production and distribution of goods were centralized and directed by the government. The first Bolshevik experience with a command economy was the policy of War communism, which involved the nationalization of industry, centralized distribution of output, coercive requisition of agricultural production, and attempts to eliminate money circulation, private enterprises and free trade. After the severe economic collapse, Lenin replaced war communism by the New Economic Policy (NEP) in 1921, legalizing free trade and private ownership of small businesses. The economy quickly recovered as a result.
After a long debate among the members of the Politburo about the course of economic development, by 1928–1929, upon gaining control of the country, Stalin abandoned the NEP and pushed for full central planning, starting forced collectivization of agriculture and enacting draconian labor legislation. Resources were mobilized for rapid industrialization, which significantly expanded Soviet capacity in heavy industry and capital goods during the 1930s. The primary motivation for industrialization was preparation for war, mostly due to distrust of the outside capitalist world. As a result, the USSR was transformed from a largely agrarian economy into a great industrial power, leading the way for its emergence as a superpower after World War II. The war caused extensive devastation of the Soviet economy and infrastructure, which required massive reconstruction.
By the early 1940s, the Soviet economy had become relatively self-sufficient; for most of the period until the creation of Comecon, only a tiny share of domestic products was traded internationally. After the creation of the Eastern Bloc, external trade rose rapidly. However, the influence of the world economy on the USSR was limited by fixed domestic prices and a state monopoly on foreign trade. Grain and sophisticated consumer manufactures became major import articles from around the 1960s. During the arms race of the Cold War, the Soviet economy was burdened by military expenditures, heavily lobbied for by a powerful bureaucracy dependent on the arms industry. At the same time, the USSR became the largest arms exporter to the Third World. Significant amounts of Soviet resources during the Cold War were allocated in aid to the other socialist states.
From the 1930s until its dissolution in late 1991, the way the Soviet economy operated remained essentially unchanged. The economy was formally directed by central planning, carried out by Gosplan and organized in five-year plans. However, in practice, the plans were highly aggregated and provisional, subject to ad hoc intervention by superiors. All critical economic decisions were taken by the political leadership. Allocated resources and plan targets were usually denominated in rubles rather than in physical goods. Credit was discouraged, but widespread. The final allocation of output was achieved through relatively decentralized, unplanned contracting. Although in theory prices were legally set from above, in practice they were often negotiated, and informal horizontal links (e.g. between producer factories) were widespread.
A number of basic services were state-funded, such as education and health care. In the manufacturing sector, heavy industry and defence were prioritized over consumer goods. Consumer goods, particularly outside large cities, were often scarce, of poor quality and limited variety. Under the command economy, consumers had almost no influence on production, and the changing demands of a population with growing incomes could not be satisfied by supplies at rigidly fixed prices. A massive unplanned second economy grew up at low levels alongside the planned one, providing some of the goods and services that the planners could not. The legalization of some elements of the decentralized economy was attempted with the reform of 1965.
Although statistics of the Soviet economy are notoriously unreliable and its economic growth difficult to estimate precisely, by most accounts, the economy continued to expand until the mid-1980s. During the 1950s and 1960s, it had comparatively high growth and was catching up to the West. However, after 1970, the growth, while still positive, steadily declined much more quickly and consistently than in other countries, despite a rapid increase in the capital stock (the rate of capital increase was only surpassed by Japan).
Overall, the growth rate of per capita income in the Soviet Union between 1960 and 1989 was slightly above the world average (based on 102 countries). A 1986 study published in the American Journal of Public Health claimed that, citing World Bank data, the Soviet model provided a better quality of life and human development than market economies at the same level of economic development in most cases. According to Stanley Fischer and William Easterly, growth could have been faster. By their calculation, per capita income in 1989 should have been twice higher than it was, considering the amount of investment, education and population. The authors attribute this poor performance to the low productivity of capital. Steven Rosefielde states that the standard of living declined due to Stalin's despotism. While there was a brief improvement after his death, it lapsed into stagnation.
In 1987, Mikhail Gorbachev attempted to reform and revitalize the economy with his program of perestroika. His policies relaxed state control over enterprises but did not replace it by market incentives, resulting in a sharp decline in output. The economy, already suffering from reduced petroleum export revenues, started to collapse. Prices were still fixed, and the property was still largely state-owned until after the country's dissolution. For most of the period after World War II until its collapse, Soviet GDP (PPP) was the second-largest in the world, and third during the second half of the 1980s, although on a per-capita basis, it was behind that of First World countries. Compared to countries with similar per-capita GDP in 1928, the Soviet Union experienced significant growth.
In 1990, the country had a Human Development Index of 0.920, placing it in the "high" category of human development. It was the third-highest in the Eastern Bloc, behind Czechoslovakia and East Germany, and the 25th in the world of 130 countries.
The need for fuel declined in the Soviet Union from the 1970s to the 1980s, both per ruble of gross social product and per ruble of industrial product. At the start, this decline grew very rapidly but gradually slowed down between 1970 and 1975. From 1975 and 1980, it grew even slower, only 2.6%. David Wilson, a historian, believed that the gas industry would account for 40% of Soviet fuel production by the end of the century. His theory did not come to fruition because of the USSR's collapse. The USSR, in theory, would have continued to have an economic growth rate of 2–2.5% during the 1990s because of Soviet energy fields. However, the energy sector faced many difficulties, among them the country's high military expenditure and hostile relations with the First World.
In 1991, the Soviet Union had a pipeline network of 82,000 kilometres (51,000 mi) for crude oil and another 206,500 kilometres (128,300 mi) for natural gas. Petroleum and petroleum-based products, natural gas, metals, wood, agricultural products, and a variety of manufactured goods, primarily machinery, arms and military equipment, were exported. In the 1970s and 1980s, the USSR heavily relied on fossil fuel exports to earn hard currency. At its peak in 1988, it was the largest producer and second-largest exporter of crude oil, surpassed only by Saudi Arabia.
The Soviet Union placed great emphasis on science and technology within its economy, however, the most remarkable Soviet successes in technology, such as producing the world's first space satellite, typically were the responsibility of the military. Lenin believed that the USSR would never overtake the developed world if it remained as technologically backward as it was upon its founding. Soviet authorities proved their commitment to Lenin's belief by developing massive networks, research and development organizations. In the early 1960s, the Soviets awarded 40% of chemistry PhDs to women, compared to only 5% in the United States. By 1989, Soviet scientists were among the world's best-trained specialists in several areas, such as Energy physics, selected areas of medicine, mathematics, welding and military technologies. Due to rigid state planning and bureaucracy, the Soviets remained far behind technologically in chemistry, biology, and computers when compared to the First World. The Soviet government opposed and persecuted geneticists in favour of Lysenkoism, a pseudoscience rejected by the scientific community in the Soviet Union and abroad but supported by Stalin's inner circles. Implemented in the USSR and China, it resulted in reduced crop yields and is widely believed to have contributed to the Great Chinese Famine.
Under the Reagan administration, Project Socrates determined that the Soviet Union addressed the acquisition of science and technology in a manner that was radically different from what the US was using. In the case of the US, economic prioritization was being used for indigenous research and development as the means to acquire science and technology in both the private and public sectors. In contrast, the USSR was offensively and defensively maneuvering in the acquisition and use of the worldwide technology, to increase the competitive advantage that they acquired from the technology while preventing the US from acquiring a competitive advantage. However, technology-based planning was executed in a centralized, government-centric manner that greatly hindered its flexibility. This was exploited by the US to undermine the strength of the Soviet Union and thus foster its reform.
Transport was a vital component of the country's economy. The economic centralization of the late 1920s and 1930s led to the development of infrastructure on a massive scale, most notably the establishment of Aeroflot, an aviation enterprise. The country had a wide variety of modes of transport by land, water and air. However, due to inadequate maintenance, much of the road, water and Soviet civil aviation transport were outdated and technologically backward compared to the First World.
Soviet rail transport was the largest and most intensively used in the world; it was also better developed than most of its Western counterparts. By the late 1970s and early 1980s, Soviet economists were calling for the construction of more roads to alleviate some of the burdens from the railways and to improve the Soviet government budget. The street network and automotive industry remained underdeveloped, and dirt roads were common outside major cities. Soviet maintenance projects proved unable to take care of even the few roads the country had. By the early-to-mid-1980s, the Soviet authorities tried to solve the road problem by ordering the construction of new ones. Meanwhile, the automobile industry was growing at a faster rate than road construction. The underdeveloped road network led to a growing demand for public transport.
Despite improvements, several aspects of the transport sector were still riddled with problems due to outdated infrastructure, lack of investment, corruption and bad decision-making. Soviet authorities were unable to meet the growing demand for transport infrastructure and services.
The Soviet merchant navy was one of the largest in the world.
Excess deaths throughout World War I and the Russian Civil War (including the postwar famine) amounted to a combined total of 18 million, some 10 million in the 1930s, and more than 26 million in 1941–1945. The postwar Soviet population was 45 to 50 million smaller than it would have been if pre-war demographic growth had continued. According to Catherine Merridale, "... reasonable estimate would place the total number of excess deaths for the whole period somewhere around 60 million."
The birth rate of the USSR decreased from 44.0 per thousand in 1926 to 18.0 in 1974, mainly due to increasing urbanization and the rising average age of marriages. The mortality rate demonstrated a gradual decrease as well – from 23.7 per thousand in 1926 to 8.7 in 1974. In general, the birth rates of the southern republics in Transcaucasia and Central Asia were considerably higher than those in the northern parts of the Soviet Union, and in some cases even increased in the post–World War II period, a phenomenon partly attributed to slower rates of urbanistion and traditionally earlier marriages in the southern republics. Soviet Europe moved towards sub-replacement fertility, while Soviet Central Asia continued to exhibit population growth well above replacement-level fertility.
The late 1960s and the 1970s witnessed a reversal of the declining trajectory of the rate of mortality in the USSR, and was especially notable among men of working age, but was also prevalent in Russia and other predominantly Slavic areas of the country. An analysis of the official data from the late 1980s showed that after worsening in the late-1970s and the early 1980s, adult mortality began to improve again. The infant mortality rate increased from 24.7 in 1970 to 27.9 in 1974. Some researchers regarded the rise as mostly real, a consequence of worsening health conditions and services. The rises in both adult and infant mortality were not explained or defended by Soviet officials, and the Soviet government stopped publishing all mortality statistics for ten years. Soviet demographers and health specialists remained silent about the mortality increases until the late-1980s, when the publication of mortality data resumed, and researchers could delve into the real causes.
Under Lenin, the state made explicit commitments to promote the equality of men and women. Many early Russian feminists and ordinary Russian working women actively participated in the Revolution, and many more were affected by the events of that period and the new policies. Beginning in October 1918, Lenin's government liberalized divorce and abortion laws, decriminalized homosexuality (re-criminalized in the 1930s), permitted cohabitation, and ushered in a host of reforms. However, without birth control, the new system produced many broken marriages, as well as countless out-of-wedlock children. The epidemic of divorces and extramarital affairs created social hardships when Soviet leaders wanted people to concentrate their efforts on growing the economy. Giving women control over their fertility also led to a precipitous decline in the birth rate, perceived as a threat to their country's military power. By 1936, Stalin reversed most of the liberal laws, ushering in a pronatalist era that lasted for decades.
By 1917, Russia became the first great power to grant women the right to vote. After heavy casualties in World War I and II, women outnumbered men in Russia by a 4:3 ratio. This contributed to the larger role women played in Russian society compared to other great powers at the time.
Anatoly Lunacharsky became the first People's Commissar for Education of Soviet Russia. In the beginning, the Soviet authorities placed great emphasis on the elimination of illiteracy. All left-handed children were forced to write with their right hand in the Soviet school system. Literate people were automatically hired as teachers. For a short period, quality was sacrificed for quantity. By 1940, Stalin could announce that illiteracy had been eliminated. Throughout the 1930s, social mobility rose sharply, which has been attributed to reforms in education. In the aftermath of World War II, the country's educational system expanded dramatically, which had a tremendous effect. In the 1960s, nearly all children had access to education, the only exception being those living in remote areas. Nikita Khrushchev tried to make education more accessible, making it clear to children that education was closely linked to the needs of society. Education also became important in giving rise to the New Man. Citizens directly entering the workforce had the constitutional right to a job and to free vocational training.
The education system was highly centralized and universally accessible to all citizens, with affirmative action for applicants from nations associated with cultural backwardness. However, as part of the general antisemitic policy, an unofficial Jewish quota was applied in the leading institutions of higher education by subjecting Jewish applicants to harsher entrance examinations. The Brezhnev era also introduced a rule that required all university applicants to present a reference from the local Komsomol party secretary. According to statistics from 1986, the number of higher education students per the population of 10,000 was 181 for the USSR, compared to 517 for the US.
The Soviet Union was an ethnically diverse country, with more than 100 distinct ethnic groups. The total population of the country was estimated at 293 million in 1991. According to a 1990 estimate, the majority of the population were Russians (50.78%), followed by Ukrainians (15.45%) and Uzbeks (5.84%). Overall, in 1989 the ethnic demography of the country showed that 69.8% was East Slavic, 17.5% was Turkic, 1.6% were Armenians, 1.6% were Balts, 1.5% were Finnic, 1.5% were Tajik, 1.4% were Georgian, 1.2% were Moldovan and 4.1% were of other various ethnic groups.
All citizens of the USSR had their own ethnic affiliation. The ethnicity of a person was chosen at the age of sixteen by the child's parents. If the parents did not agree, the child was automatically assigned the ethnicity of the father. Partly due to Soviet policies, some of the smaller minority ethnic groups were considered part of larger ones, such as the Mingrelians of Georgia, who were classified with the linguistically related Georgians. Some ethnic groups voluntarily assimilated, while others were brought in by force. Russians, Belarusians, and Ukrainians, who were all East Slavic and Orthodox, shared close cultural, ethnic, and religious ties, while other groups did not. With multiple nationalities living in the same territory, ethnic antagonisms developed over the years.
Members of various ethnicities participated in legislative bodies. Organs of power like the Politburo, the Secretariat of the Central Committee etc., were formally ethnically neutral, but in reality, ethnic Russians were overrepresented, although there were also non-Russian leaders in the Soviet leadership, such as Joseph Stalin, Grigory Zinoviev, Nikolai Podgorny or Andrei Gromyko. During the Soviet era, a significant number of ethnic Russians and Ukrainians migrated to other Soviet republics, and many of them settled there. According to the last census in 1989, the Russian "diaspora" in the Soviet republics had reached 25 million.
Ethnographic map of the Soviet Union, 1941
Ethnographic map of the Soviet Union, 1970
In 1917, before the revolution, health conditions were significantly behind those of developed countries. As Lenin later noted, "Either the lice will defeat socialism, or socialism will defeat the lice". The Soviet principle of health care was conceived by the People's Commissariat for Health in 1918. Health care was to be controlled by the state and would be provided to its citizens free of charge, a revolutionary concept at the time. Article 42 of the 1977 Soviet Constitution gave all citizens the right to health protection and free access to any health institutions in the USSR. Before Leonid Brezhnev became general secretary, the Soviet healthcare system was held in high esteem by many foreign specialists. This changed, however, from Brezhnev's accession and Mikhail Gorbachev's tenure as leader, during which the health care system was heavily criticized for many basic faults, such as the quality of service and the unevenness in its provision. Minister of Health Yevgeniy Chazov, during the 19th Congress of the Communist Party of the Soviet Union, while highlighting such successes as having the most doctors and hospitals in the world, recognized the system's areas for improvement and felt that billions of Soviet rubles were squandered.
After the revolution, life expectancy for all age groups went up. This statistic in itself was seen by some that the socialist system was superior to the capitalist system. These improvements continued into the 1960s when statistics indicated that the life expectancy briefly surpassed that of the United States. Life expectancy started to decline in the 1970s, possibly because of alcohol abuse. At the same time, infant mortality began to rise. After 1974, the government stopped publishing statistics on the matter. This trend can be partly explained by the number of pregnancies rising drastically in the Asian part of the country where infant mortality was the highest while declining markedly in the more developed European part of the Soviet Union.
Soviet dental technology and dental health were considered notoriously bad. In 1991, the average 35-year-old had 12 to 14 cavities, fillings or missing teeth. Toothpaste was often not available, and toothbrushes did not conform to standards of modern dentistry.
Under Lenin, the government gave small language groups their own writing systems. The development of these writing systems was highly successful, even though some flaws were detected. During the later days of the USSR, countries with the same multilingual situation implemented similar policies. A serious problem when creating these writing systems was that the languages differed dialectally greatly from each other. When a language had been given a writing system and appeared in a notable publication, it would attain "official language" status. There were many minority languages which never received their own writing system; therefore, their speakers were forced to have a second language. There are examples where the government retreated from this policy, most notably under Stalin where education was discontinued in languages that were not widespread. These languages were then assimilated into another language, mostly Russian. During World War II, some minority languages were banned, and their speakers accused of collaborating with the enemy.
As the most widely spoken of the Soviet Union's many languages, Russian de facto functioned as an official language, as the "language of interethnic communication" (Russian: язык межнационального общения), but only assumed the de jure status as the official national language in 1990.
Christianity and Islam had the highest number of adherents among the religious citizens. Eastern Christianity predominated among Christians, with Russia's traditional Russian Orthodox Church being the largest Christian denomination. About 90% of the Soviet Union's Muslims were Sunnis, with Shias being concentrated in the Azerbaijan SSR. Smaller groups included Roman Catholics, Jews, Buddhists, and a variety of Protestant denominations (especially Baptists and Lutherans).
Religious influence had been strong in the Russian Empire. The Russian Orthodox Church enjoyed a privileged status as the church of the monarchy and took part in carrying out official state functions. The immediate period following the establishment of the Soviet state included a struggle against the Orthodox Church, which the revolutionaries considered an ally of the former ruling classes.
In Soviet law, the "freedom to hold religious services" was constitutionally guaranteed, although the ruling Communist Party regarded religion as incompatible with the Marxist spirit of scientific materialism. In practice, the Soviet system subscribed to a narrow interpretation of this right, and in fact used a range of official measures to discourage religion and curb the activities of religious groups.
The 1918 Council of People's Commissars decree establishing the Russian SFSR as a secular state also decreed that "the teaching of religion in all  where subjects of general instruction are taught, is forbidden. Citizens may teach and may be taught religion privately." Among further restrictions, those adopted in 1929 included express prohibitions on a range of church activities, including meetings for organized Bible study. Both Christian and non-Christian establishments were shut down by the thousands in the 1920s and 1930s. By 1940, as many as 90% of the churches, synagogues, and mosques that had been operating in 1917 were closed.
The Soviet Union was officially a secular state, but a "government-sponsored program of forced conversion to atheism" was conducted under the doctrine of state atheism. The government targeted religions based on state interests, and while most organized religions were never outlawed, religious property was confiscated, believers were harassed, and religion was ridiculed while atheism was propagated in schools. In 1925, the government founded the League of Militant Atheists to intensify the propaganda campaign. Accordingly, although personal expressions of religious faith were not explicitly banned, a strong sense of social stigma was imposed on them by the formal structures and mass media, and it was generally considered unacceptable for members of certain professions (teachers, state bureaucrats, soldiers) to be openly religious. While persecution accelerated following Stalin's rise to power, a revival of Orthodoxy was fostered by the government during World War II and the Soviet authorities sought to control the Russian Orthodox Church rather than liquidate it. During the first five years of Soviet power, the Bolsheviks executed 28 Russian Orthodox bishops and over 1,200 Russian Orthodox priests. Many others were imprisoned or exiled. Believers were harassed and persecuted. Most seminaries were closed, and the publication of most religious material was prohibited. By 1941, only 500 churches remained open out of about 54,000 in existence before World War I.
Convinced that religious anti-Sovietism had become a thing of the past, and with the looming threat of war, the Stalin regime began shifting to a more moderate religion policy in the late 1930s. Soviet religious establishments overwhelmingly rallied to support the war effort during World War II. Amid other accommodations to religious faith after the German invasion, churches were reopened. Radio Moscow began broadcasting a religious hour, and a historic meeting between Stalin and Orthodox Church leader Patriarch Sergius of Moscow was held in 1943. Stalin had the support of the majority of the religious people in the USSR even through the late 1980s. The general tendency of this period was an increase in religious activity among believers of all faiths.
Under Nikita Khrushchev, the state leadership clashed with the churches in 1958–1964, a period when atheism was emphasized in the educational curriculum, and numerous state publications promoted atheistic views. During this period, the number of churches fell from 20,000 to 10,000 from 1959 to 1965, and the number of synagogues dropped from 500 to 97. The number of working mosques also declined, falling from 1,500 to 500 within a decade.
Religious institutions remained monitored by the Soviet government, but churches, synagogues, temples, and mosques were all given more leeway in the Brezhnev era. Official relations between the Orthodox Church and the government again warmed to the point that the Brezhnev government twice honored Orthodox Patriarch Alexy I with the Order of the Red Banner of Labour. A poll conducted by Soviet authorities in 1982 recorded 20% of the Soviet population as "active religious believers."
The legacy of the USSR remains a controversial topic. The socio-economic nature of communist states such as the USSR, especially under Stalin, has also been much debated, varyingly being labelled a form of bureaucratic collectivism, state capitalism, state socialism, or a totally unique mode of production.
The USSR implemented a broad range of policies over a long period of time, with a large amount of conflicting policies being implemented by different leaders. Some have a positive view of it whilst others are critical towards the country, calling it a repressive oligarchy. The opinions on the USSR are complex and have changed over time, with different generations having different views on the matter as well as on Soviet policies corresponding to separate time periods during its history. Leftists have largely varying views on the USSR. Whilst some leftists such as anarchists and other libertarian socialists, agree it did not give the workers control over the means of production and was a centralized oligarchy, others have more positive opinions as to the Bolshevik policies and Vladimir Lenin. Many anti-Stalinist leftists such as anarchists are extremely critical of Soviet authoritarianism and repression. Much of the criticism it receives is centered around massacres in the Soviet Union, the centralized hierarchy present in the USSR and mass political repression as well as violence towards government critics and political dissidents such as other leftists. Critics also point towards its failure to implement any substantial worker cooperatives or implementing worker liberation as well as corruption and the Soviet authoritarian nature.
Many Russians and other former Soviet citizens have nostalgia for the USSR, pointing towards most infrastructure being built during Soviet times, increased job security, increased literacy rate, increased caloric intake and supposed ethnic pluralism enacted in the Soviet Union as well as political stability. The Russian Revolution is also seen in a positive light as well as the leadership of Lenin, Nikita Khrushchev and the later USSR, although many view Joseph Stalin's rule as positive for the country. In Armenia, 12% of respondents said the USSR collapse did good, while 66% said it did harm. In Kyrgyzstan, 16% of respondents said the collapse of the USSR did good, while 61% said it did harm. In a 2018 Rating Sociological Group poll, 47% of Ukrainian respondents had a positive opinion of Soviet leader Leonid Brezhnev, who ruled the Soviet Union from 1964 to 1982. A 2021 poll conducted by the Levada Center found that 49% of Russians prefer the USSR's political system, while 18% prefer the current political system and 16% would prefer a Western Democracy. A further 62% of people polled preferred the Soviet system of central planning, while 24% prefer a market-based system. Much of the admiration of the USSR comes from the failings of the modern post-Soviet governments such as the control by oligarchs, corruption and outdated Soviet-era infrastructure as well as the rise and dominance of organised crime after the dissolution of the Soviet Union all directly leading into nostalgia for it.
The 1941–1945 period of World War II is still known in Russia as the "Great Patriotic War". The war became a topic of great importance in cinema, literature, history lessons at school, the mass media, and the arts. As a result of the massive losses suffered by the military and civilians during the conflict, Victory Day celebrated on 9 May is still one of the most important and emotional dates in Russia.
In some post Soviet republics, there is a more negative view of the USSR, although there is no unanimity on the matter. In large part due to the Holodomor, ethnic Ukrainians have a negative view of it. Russian-speaking Ukrainians of Ukraine's southern and eastern regions have a more positive view of the USSR. In some countries with internal conflict, there is also nostalgia for the USSR, especially for refugees of the post-Soviet conflicts who have been forced to flee their homes and have been displaced. This nostalgia is less an admiration for the country or its policies than it is a longing to return to their homes and not to live in poverty. The many Russian enclaves in the former USSR republics such as Transnistria have in a general a positive remembrance of it.
The left's view of the USSR is complex. While some leftists regard the USSR as an example of state capitalism or that it was an oligarchical state, other leftists admire Vladimir Lenin and the Russian Revolution. Council communists generally view the USSR as failing to create class consciousness, turning into a corrupt state in which the elite controlled society. 
Anarchists are also critical of the country, labeling the Soviet system as red fascism. Factors contributing to the anarchist animosity towards the USSR included the Soviet invasion of the anarchist Free Territory after an initial alliance, the suppression of the anarchist Kronstadt rebellion, and the defeat of the rival anarchist factions by the Soviet-supported Communist faction during the Spanish Civil War.
Maoists also have a mixed opinion on the USSR, viewing it negatively during the Sino-Soviet Split and denouncing it as revisionist and reverted to capitalism. The Chinese government in 1963 articulated its criticism of the USSR's system and promoted China's ideological line as an alternative
The culture of the Soviet Union passed through several stages during the USSR's existence. During the first decade following the revolution, there was relative freedom and artists experimented with several different styles to find a distinctive Soviet style of art. Lenin wanted art to be accessible to the Russian people. On the other hand, hundreds of intellectuals, writers, and artists were exiled or executed, and their work banned, such as Nikolay Gumilyov who was shot for alleged conspiring against the Bolshevik regime, and Yevgeny Zamyatin.
The government encouraged a variety of trends. In art and literature, numerous schools, some traditional and others radically experimental, proliferated. Communist writers Maxim Gorky and Vladimir Mayakovsky were active during this time. As a means of influencing a largely illiterate society, films received encouragement from the state, and much of director Sergei Eisenstein's best work dates from this period.
During Stalin's rule, the Soviet culture was characterized by the rise and domination of the government-imposed style of socialist realism, with all other trends being severely repressed, with rare exceptions, such as Mikhail Bulgakov's works. Many writers were imprisoned and killed.
Following the Khrushchev Thaw, censorship was diminished. During this time, a distinctive period of Soviet culture developed, characterized by conformist public life and an intense focus on personal life. Greater experimentation in art forms was again permissible, resulting in the production of more sophisticated and subtly critical work. The regime loosened its emphasis on socialist realism; thus, for instance, many protagonists of the novels of author Yury Trifonov concerned themselves with problems of daily life rather than with building socialism. Underground dissident literature, known as samizdat, developed during this late period. In architecture, the Khrushchev era mostly focused on functional design as opposed to the highly decorated style of Stalin's epoch. In music, in response to the increasing popularity of forms of popular music like jazz in the West, many jazz orchestras were permitted throughout the USSR, notably the Melodiya Ensemble, named after the principle record label in the USSR.
In the second half of the 1980s, Gorbachev's policies of perestroika and glasnost significantly expanded freedom of expression throughout the country in the media and the press.
Founded on 20 July 1924 in Moscow, Sovetsky Sport was the first sports newspaper of the Soviet Union.
The Soviet Olympic Committee formed on 21 April 1951, and the IOC recognized the new body in its 45th session. In the same year, when the Soviet representative Konstantin Andrianov became an IOC member, the USSR officially joined the Olympic Movement. The 1952 Summer Olympics in Helsinki thus became first Olympic Games for Soviet athletes. The Soviet Union was the biggest rival to the United States at the Summer Olympics, winning six of its nine appearances at the games and also topping the medal tally at the Winter Olympics six times. The Soviet Union's Olympics success has been attributed to its large investment in sports to demonstrate its superpower image and political influence on a global stage.
The Soviet Union national ice hockey team won nearly every world championship and Olympic tournament between 1954 and 1991 and never failed to medal in any International Ice Hockey Federation (IIHF) tournament in which they competed.
The advent of the state-sponsored "full-time amateur athlete" of the Eastern Bloc countries further eroded the ideology of the pure amateur, as it put the self-financed amateurs of the Western countries at a disadvantage. The Soviet Union entered teams of athletes who were all nominally students, soldiers, or working in a profession – in reality, the state paid many of these competitors to train on a full-time basis. Nevertheless, the IOC held to the traditional rules regarding amateurism.
A 1989 report by a committee of the Australian Senate claimed that "there is hardly a medal winner at the Moscow Games, certainly not a gold medal winner ... who is not on one sort of drug or another: usually several kinds. The Moscow Games might well have been called the Chemists' Games".
A member of the IOC Medical Commission, Manfred Donike, privately ran additional tests with a new technique for identifying abnormal levels of testosterone by measuring its ratio to epitestosterone in urine. Twenty percent of the specimens he tested, including those from sixteen gold medalists, would have resulted in disciplinary proceedings had the tests been official. The results of Donike's unofficial tests later convinced the IOC to add his new technique to their testing protocols. The first documented case of "blood doping" occurred at the 1980 Summer Olympics when a runner was transfused with two pints of blood before winning medals in the 5000 m and 10,000 m.
Documentation obtained in 2016 revealed the Soviet Union's plans for a statewide doping system in track and field in preparation for the 1984 Summer Olympics in Los Angeles. Dated before the decision to boycott the 1984 Games, the document detailed the existing steroids operations of the program, along with suggestions for further enhancements. Dr. Sergei Portugalov of the Institute for Physical Culture prepared the communication, directed to the Soviet Union's head of track and field. Portugalov later became one of the leading figures involved in the implementation of Russian doping before the 2016 Summer Olympics.
Official Soviet environmental policy has always attached great importance to actions in which human beings actively improve nature. Lenin's quote "Communism is Soviet power and electrification of the country!" in many respects summarizes the focus on modernization and industrial development. During the first five-year plan in 1928, Stalin proceeded to industrialize the country at all costs. Values such as environmental and nature protection have been completely ignored in the struggle to create a modern industrial society. After Stalin's death, they focused more on environmental issues, but the basic perception of the value of environmental protection remained the same.
The Soviet media has always focused on the vast expanse of land and the virtually indestructible natural resources. This made it feel that contamination and uncontrolled exploitation of nature were not a problem. The Soviet state also firmly believed that scientific and technological progress would solve all the problems. Official ideology said that under socialism environmental problems could easily be overcome, unlike capitalist countries, where they seemingly could not be solved. The Soviet authorities had an almost unwavering belief that man could transcend nature. However, when the authorities had to admit that there were environmental problems in the USSR in the 1980s, they explained the problems in such a way that socialism had not yet been fully developed; pollution in a socialist society was only a temporary anomaly that would have been resolved if socialism had developed.
The Chernobyl disaster in 1986 was the first major accident at a civilian nuclear power plant. Unparalleled in the world, it resulted in a large number of radioactive isotopes being released into the atmosphere. Radioactive doses have scattered relatively far. 4,000 new cases of thyroid cancer were reported after the incident, but this led to a relatively low number of deaths (WHO data, 2005). However, the long-term effects of the accident are unknown. Another major accident is the Kyshtym disaster.
After the dissolution of the Soviet Union, it was discovered that the environmental problems were greater than what the Soviet authorities admitted. The Kola Peninsula was one of the places with clear problems. Around the industrial cities of Monchegorsk and Norilsk, where nickel, for example, is mined, all forests have been destroyed by contamination, while the northern and other parts of Russia have been affected by emissions. During the 1990s, people in the West were also interested in the radioactive hazards of nuclear facilities, decommissioned nuclear submarines, and the processing of nuclear waste or spent nuclear fuel. It was also known in the early 1990s that the USSR had transported radioactive material to the Barents Sea and Kara Sea, which was later confirmed by the Russian parliament. The crash of the K-141 Kursk submarine in 2000 in the west further raised concerns. In the past, there were accidents involving submarines K-19, K-8, a K-129, K-27, K-219 and K-278 Komsomolets.
1918–1924  Turkestan3
1918–1941  Volga German4
1919–1990  Bashkir
1920–1925  Kirghiz2
1920–1990  Tatar
1921–1990  Adjarian
1921–1945  Crimean
1921–1991  Dagestan
1921–1924  Mountain
1921–1990  Nakhichevan
1922–1991  Yakut
1923–1990  Buryat1
1923–1940  Karelian
1924–1940  Moldavian
1924–1929  Tajik
1925–1992  Chuvash5
1925–1936  Kazakh2
1926–1936  Kirghiz
1931–1991  Abkhaz
1932–1992  Karakalpak
1934–1990  Mordovian
1934–1990  Udmurt6
1935–1943  Kalmyk
1936–1944  Checheno-Ingush
1936–1944  Kabardino-Balkarian
1936–1990  Komi
1936–1990  Mari
1936–1990  North Ossetian
1944–1957  Kabardin
1956–1991  Karelian
1957–1990  Checheno-Ingush
1957–1989  Kabardino-Balkarian
1958–1990  Kalmyk
1961–1992  Tuvan
1990–1991  Gorno-Altai
1991–1992  Crimean
Gas Dynamics Laboratory (GDL) (Russian: Газодинамическая лаборатория) was the first Soviet research and development laboratory to focus on rocket technology. Its activities were initially devoted to the development of solid propellant rockets, which became the prototypes of missiles in the Katyusha rocket launcher, as well as liquid propellant rockets, which became the prototypes of Soviet rockets and spacecraft. At the end of 1933 it became part of the Reactive Scientific Research Institute (RNII). A number of craters  on the far side of the Moon are named after GDL employees.
Nikolai Tikhomirov (1921—1930);
Boris Sergeevich Petropavlovsky  (1930-1931);
Nikolai Yakovlevich Ilyin  (1931—1932);
Ivan Kleymyonov (12.1932 - 9.1933, then head of the RNII).
The GDL utilised smokeless (TNT) gunpowder on a non-volatile solvent for solid propellant rockets. The first test-firing of a solid fuel rocket was carried out in March 1928, which flew for about 1,300 meters In 1931 the world's first successful use of rockets to assist take-off of aircraft were carried out on a U-1, the Soviet designation for a Avro 504 trainer, which achieved about one hundred successful assisted takeoffs. Successful assisted takeoffs were also achieved on the Tupolev TB-1 (Russian 'ТБ-1') and Tupolev TB-3 aircraft. Further developments in the early 1930s were led by Georgy Langemak, including firing rockets from aircraft and the ground. In 1932 in-air test firings of RS-82 missiles from an Tupolev I-4 aircraft armed with six launchers successfully took place. RNII then modified these rockets for the famous Katyusha rocket launcher, which were used during World War II. In these works,  the main design contribution was made by GDL employees Nikolai Tikhomirov, Vladimir Artemyev, Boris Petropavlovsky, Georgy Langemak, Ivan Isidorovich and others.
On 15 May 1929 a section was created to develop electric rocket engines, headed by 23 year old Valentin Glushko,  Glushko proposed to use energy in electric explosion of metals to create rocket propulsion. In the early 1930s the world's first example of an electrothermal rocket engine was created. This early work by GDL has been steadily carried on and electric rocket engines were used in the 1960s onboard the Voskhod 1 spacecraft and Zond-2 Venus probe.
In 1931 Glushko was redirected to work on liquid propellant rocket engines. This resulted in the creation of ORM (from "Experimental Rocket Motor" in Russian) engines ORM-1  to ORM-52 .  To increase the resource,  various technical solutions were used: the jet nozzle had a spirally finned wall and was cooled by  fuel components, curtain cooling was used for the combustion chamber and ceramic thermal insulation of the combustion chamber using zirconium dioxide. Nitric acid, solutions of nitric acid with froholic nitrogen, tetranitromethane, hypochloric acid and hydrogen peroxide were first proposed as an oxidizing agent. As a result of experiments, by the end of 1933, a high-boiling fuel from kerosene and nitric acid was selected as the most convenient in operation and industrial production. In 1931 self-igniting combustible  and chemical ignition  of fuel with gimbal engine suspension were proposed. For fuel supply in 1931-1932 fuel pumps operating from combustion chamber gases were developed. In 1933 a centrifugal turbopump unit for a  rocket engine with a thrust of 3000 N was developed. A total of 100 bench tests of liquid-propellant rockets were conducted using various types of fuel, both low and high-boiling and thrust up to 300 kg was achieved.
Experimental liquid propellant rockets were constructed, the first two rockets with a planned lifting height of 2–4 km were manufactured and testing was continued by RNII.
The work on the creation of engines under the leadership of Glushko was carried out by employees of the  ERD and liquid-propellant engine section, including the active involvement of A. L.  Maly, V. I.  Serov, E. N.  Kuzmin, I. I.  Kulagin, E. S.  Petrov, P. I.  Minaev, B. A.  Kutkin, V. P.  Yukov, N. G.  Chernyshev and others.
In 1966, the Commission of the  USSR  Academy of Sciences on Lunar Names assigned craters on the far side of the  Moon names in honor of the following workers of the GDL; Nikolai Tikhomirov, N. P.  Alyokhina, Vladimir Artemyev, Artamonova, A. I.  Gavrilova, A. D.  Gracheva, Zhiritsky, A. L.  Maly, Y. B.  Mezentseva, E. S. Petropavlovsky, B.S. Petrova, G. F.  Firsova, N. G.  Chernysheva. In 1962 the names GDL, GIRD and RNII were assigned to crater chains on the far side of the Moon.
The V. P. Glushko Museum of Cosmonautics and Rocket Technology  is a memorial museum telling about the beginning of the domestic space engine industry, including the history of GDL. The museum is located in the Peter and Paul Fortress, which in the 1930s housed GDL stands for testing rocket engines. It was opened on April 12, 1973.

A solid-propellant rocket or solid rocket is a rocket with a rocket engine that uses solid propellants (fuel/oxidizer). The earliest rockets were solid-fuel rockets powered by gunpowder; they were used in warfare by the Chinese, Persians, Mongols, and Indians as early as the 13th century.
All rockets used some form of solid or powdered propellant up until the 20th century, when liquid-propellant rockets offered more efficient and controllable alternatives. Solid rockets are still used today in military armaments worldwide, model rockets, solid rocket boosters and on larger applications for their simplicity and reliability.
Since solid-fuel rockets can remain in storage for a long time without much propellant degradation and because they almost always launch reliably, they have been frequently used in military applications such as missiles. The lower performance of solid propellants (as compared to liquids) does not favor their use as primary propulsion in modern medium-to-large launch vehicles customarily used to orbit commercial satellites and launch major space probes. Solids are, however, frequently used as strap-on boosters to increase payload capacity or as spin-stabilized add-on upper stages when higher-than-normal velocities are required. Solid rockets are used as light launch vehicles for low Earth orbit (LEO) payloads under 2 tons or escape payloads up to 500 kilograms (1,100 lb).
A simple solid rocket motor consists of a casing, nozzle, grain (propellant charge), and igniter.
The solid grain mass burns in a predictable fashion to produce exhaust gases, the flow of which is described by Taylor–Culick flow. The nozzle dimensions are calculated to maintain a design chamber pressure, while producing thrust from the exhaust gases.
Once ignited, a simple solid rocket motor cannot be shut off, because it contains all the ingredients necessary for combustion within the chamber in which they are burned. More advanced solid rocket motors can be throttled, and also be extinguished, and then re-ignited by control of the nozzle geometry, or through the use of vent ports. Further, pulsed rocket motors that burn in segments, and that can be ignited upon command are available.
Modern designs may also include a steerable nozzle for guidance, avionics, recovery hardware (parachutes), self-destruct mechanisms, APUs, controllable tactical motors, controllable divert and attitude control motors, and thermal management materials.
The medieval Song dynasty Chinese invented a very primitive form of solid-propellant rocket. Illustrations and descriptions in the 14th century Chinese military treatise Huolongjing by the Ming dynasty military writer and philosopher Jiao Yu confirm that the Chinese in 1232 used proto solid propellant rockets then known as "fire arrows" to drive back the Mongols during the Mongol siege of Kaifeng. Each arrow took a primitive form of a simple, solid-propellant rocket tube that was filled with gunpowder. One open end allowed the gas to escape and was attached to a long stick that acted as a guidance system for flight direction control.
The first rockets with tubes of cast iron were used by the Kingdom of Mysore under Hyder Ali and Tipu Sultan in the 1750s. These rockets had a reach of targets up to a mile and a half away. These were extremely effective in the Second Anglo-Mysore War that ended in a humiliating defeat for the British Empire. Word of the success of the Mysore rockets against the British Imperial power triggered research in England, France, Ireland and elsewhere. When the British finally conquered the fort of Srirangapatana in 1799, hundreds of rockets were shipped off to the Royal Arsenal near London to be reverse-engineered. This led to the first industrial manufacture of military rockets with the Congreve rocket in 1804.
In 1921 the Soviet research and development laboratory Gas Dynamics Laboratory began developing solid-propellant rockets, which resulted in the first launch in 1928, that flew for approximately 1,300 metres. These rockets were used in 1931 for the world's first successful use of rockets to assist take-off of aircraft.  The research continued from 1933 by the Reactive Scientific Research Institute (RNII) with the development of the RS-82 and RS-132 rockets, including designing several variations for ground-to-air, ground-to-ground, air-to-ground and air-to-air combat. The earliest known use by the Soviet Air Force of aircraft-launched unguided anti-aircraft rockets in combat against heavier-than-air aircraft took place in August 1939, during the Battle of Khalkhin Gol.  In June 1938, the RNII began developing a multiple rocket launcher based on the RS-132 rocket. In August 1939, the completed product was the BM-13 / Katyusha rocket launcher. Towards the end of 1938 the first significant large scale testing of the rocket launchers took place, 233 rockets of various types were used. A salvo of rockets could completely straddle a target at a range of 5,500 metres (3.4 mi). By the end of World War II total production of rocket launchers reached about 10,000. with 12 million rockets of the RS type produced for the Soviet armed forces.
In the United States modern castable composite solid rocket motors were invented by the American aerospace engineer Jack Parsons at Caltech in 1942 when he replaced double base propellant with roofing asphalt and potassium perchlorate. This made possible slow-burning rocket motors of adequate size and with sufficient shelf-life for jet-assisted take off applications. Charles Bartley, employed at JPL (Caltech), substituted curable synthetic rubber for the gooey asphalt, creating a flexible but geometrically stable load-bearing propellant grain that bonded securely to the motor casing. This made possible much larger solid rocket motors. Atlantic Research Corporation significantly boosted composite propellant Isp in 1954 by increasing the amount of powdered aluminium in the propellant to as much as 20%.
Solid-propellant rocket technology got its largest boost in technical innovation, size and capability with the various mid-20th century government initiatives to develop increasingly capable military missiles. After initial designs of ballistic missile military technology designed with liquid-propellant rockets in the 1940s and 1950s, both the Soviet Union and the United States embarked on major initiatives to develop solid-propellant local, regional, and intercontinental ballistic missiles, including solid-propellant missiles that could be launched from air or sea. Many other governments also developed these military technologies over the next 50 years.
By the later 1980s and continuing to 2020, these government-developed highly-capable solid rocket technologies have been applied to orbital spaceflight by many government-directed programs, most often as booster rockets to add extra thrust during the early ascent of their primarily liquid rocket launch vehicles.  Some designs have had solid rocket upper stages as well.  Examples flying in the 2010s include the European Ariane 5, US Atlas V and Space Shuttle, and Japan's H-II.
The largest solid rocket motors ever built were Aerojet's three 6.60-meter (260 in) monolithic solid motors cast in Florida. Motors 260 SL-1 and SL-2 were 6.63 meters (261 in) in diameter, 24.59 meters (80 ft 8 in) long, weighed 842,900 kilograms (1,858,300 lb), and had a maximum thrust of 16 MN (3,500,000 lbf). Burn duration was two minutes. The nozzle throat was large enough to walk through standing up. The motor was capable of serving as a 1-to-1 replacement for the 8-engine Saturn I liquid-propellant first stage but was never used as such. Motor 260 SL-3 was of similar length and weight but had a maximum thrust of 24 MN (5,400,000 lbf) and a shorter duration.
Design begins with the total impulse required, which determines the fuel and oxidizer mass. Grain geometry and chemistry are then chosen to satisfy the required motor characteristics.
The following are chosen or solved simultaneously. The results are exact dimensions for grain, nozzle, and case geometries:
The grain may or may not be bonded to the casing. Case-bonded motors are more difficult to design, since the deformation of the case and the grain under flight must be compatible.
Common modes of failure in solid rocket motors include fracture of the grain, failure of case bonding, and air pockets in the grain. All of these produce an instantaneous increase in burn surface area and a corresponding increase in exhaust gas production rate and pressure, which may rupture the casing.
Another failure mode is casing seal failure. Seals are required in casings that have to be opened to load the grain. Once a seal fails, hot gas will erode the escape path and result in failure. This was the cause of the Space Shuttle Challenger disaster.
Solid rocket fuel deflagrates from the surface of exposed propellant in the combustion chamber. In this fashion, the geometry of the propellant inside the rocket motor plays an important role in the overall motor performance. As the surface of the propellant burns, the shape evolves (a subject of study in internal ballistics), most often changing the propellant surface area exposed to the combustion gases. Since the propellant volume is equal to the cross sectional area 




A

s




{\displaystyle A_{s}}

 times the fuel length, the volumetric propellant consumption rate is the cross section area times the linear burn rate 






b
˙





{\displaystyle {\dot {b}}}

, and the instantaneous mass flow rate of combustion gases generated is equal to the volumetric rate times the fuel density 



ρ


{\displaystyle \rho }

:
Several geometric configurations are often used depending on the application and desired thrust curve:
Circular bore simulation
C-slot simulation
Moon burner simulation
5-point finocyl simulation
The casing may be constructed from a range of materials. Cardboard is used for small black powder model motors, whereas aluminium is used for larger composite-fuel hobby motors. Steel was used for the space shuttle boosters. Filament-wound graphite epoxy casings are used for high-performance motors.
The casing must be designed to withstand the pressure and resulting stresses of the rocket motor, possibly at elevated temperature. For design, the casing is considered a pressure vessel.
To protect the casing from corrosive hot gases, a sacrificial thermal liner on the inside of the casing is often implemented, which ablates to prolong the life of the motor casing.
A convergent-divergent design accelerates the exhaust gas out of the nozzle to produce thrust. The nozzle must be constructed from a material that can withstand the heat of the combustion gas flow. Often, heat-resistant carbon-based materials are used, such as amorphous graphite or carbon-carbon.
Some designs include directional control of the exhaust. This can be accomplished by gimballing the nozzle, as in the Space Shuttle SRBs, by the use of jet vanes in the exhaust as in the V-2 rocket, or by liquid injection thrust vectoring (LITV).
LITV consists of injecting a liquid into the exhaust stream after the nozzle throat. The liquid then vaporizes, and in most cases chemically reacts, adding mass flow to one side of the exhaust stream and thus providing a control moment. For example, the Titan IIIC solid boosters injected nitrogen tetroxide for LITV; the tanks can be seen on the sides of the rocket between the main center stage and the boosters.
An early Minuteman first stage used a single motor with four gimballed nozzles to provide pitch, yaw, and roll control.
A typical, well-designed ammonium perchlorate composite propellant (APCP) first-stage motor may have a vacuum specific impulse (Isp) as high as 285.6 seconds (2.801 km/s) (Titan IVB SRMU). This compares to 339.3 s (3.327 km/s) for RP1/LOX (RD-180) and 452.3 s (4.436 km/s) for LH2/LOX (Block II RS-25) bipropellant engines. Upper stage specific impulses are somewhat greater: as much as 303.8 s (2.979 km/s) for APCP (Orbus 6E), 359 s (3.52 km/s) for RP1/LOX (RD-0124) and 465.5 s (4.565 km/s) for LH2/LOX (RL10B-2). Propellant fractions are usually somewhat higher for (non-segmented) solid propellant first stages than for upper stages. The 53,000-kilogram (117,000 lb) Castor 120 first stage has a propellant mass fraction of 92.23% while the 14,000-kilogram (31,000 lb) Castor 30 upper stage developed for Orbital Science's Taurus II COTS(Commercial Off The Shelf) (International Space Station resupply) launch vehicle has a 91.3% propellant fraction with 2.9% graphite epoxy motor casing, 2.4% nozzle, igniter and thrust vector actuator, and 3.4% non-motor hardware including such things as payload mount, interstage adapter, cable raceway, instrumentation, etc. Castor 120 and Castor 30 are 2.36 and 2.34 meters (93 and 92 in) in diameter, respectively, and serve as stages on the Athena IC and IIC commercial launch vehicles. A four-stage Athena II using Castor 120s as both first and second stages became the first commercially developed launch vehicle to launch a lunar probe (Lunar Prospector) in 1998.
Solid rockets can provide high thrust for relatively low cost. For this reason, solids have been used as initial stages in rockets (for example the Space Shuttle), while reserving high specific impulse engines, especially less massive hydrogen-fueled engines, for higher stages. In addition, solid rockets have a long history as the final boost stage for satellites due to their simplicity, reliability, compactness and reasonably high mass fraction. A spin-stabilized solid rocket motor is sometimes added when extra velocity is required, such as for a mission to a comet or the outer solar system, because a spinner does not require a guidance system (on the newly added stage). Thiokol's extensive family of mostly titanium-cased Star space motors has been widely used, especially on Delta launch vehicles and as spin-stabilized upper stages to launch satellites from the cargo bay of the Space Shuttle. Star motors have propellant fractions as high as 94.6% but add-on structures and equipment reduce the operating mass fraction by 2% or more.
Higher performing solid rocket propellants are used in large strategic missiles (as opposed to commercial launch vehicles). HMX, C4H8N4(NO2)4, a nitramine with greater energy than ammonium perchlorate, was used in the propellant of the Peacekeeper ICBM and is the main ingredient in NEPE-75 propellant used in the Trident II D-5 Fleet Ballistic Missile. It is because of explosive hazard that the higher energy military solid propellants containing HMX are not used in commercial launch vehicles except when the LV is an adapted ballistic missile already containing HMX propellant (Minotaur IV and V based on the retired Peacekeeper ICBMs). The Naval Air Weapons Station at China Lake, California, developed a new compound, C6H6N6(NO2)6, called simply CL-20 (China Lake compound #20). Compared to HMX, CL-20 has 14% more energy per mass, 20% more energy per volume, and a higher oxygen-to-fuel ratio. One of the motivations for development of these very high energy density military solid propellants is to achieve mid-course exo-atmospheric ABM capability from missiles small enough to fit in existing ship-based below-deck vertical launch tubes and air-mobile truck-mounted launch tubes. CL-20 propellant compliant with Congress' 2004 insensitive munitions (IM) law has been demonstrated and may, as its cost comes down, be suitable for use in commercial launch vehicles, with a very significant increase in performance compared with the currently favored APCP solid propellants. With a specific impulse of 309 s already demonstrated by Peacekeeper's second stage using HMX propellant, the higher energy of CL-20 propellant can be expected to increase specific impulse to around 320 s in similar ICBM or launch vehicle upper stage applications, without the explosive hazard of HMX.
An attractive attribute for military use is the ability for solid rocket propellant to remain loaded in the rocket for long durations and then be reliably launched at a moment's notice.
Black powder (gunpowder) is composed of charcoal (fuel), potassium nitrate (oxidizer), and sulfur (fuel and catalyst). It is one of the oldest pyrotechnic compositions with application to rocketry. In modern times, black powder finds use in low-power model rockets (such as Estes and Quest rockets), as it is cheap and fairly easy to produce. The fuel grain is typically a mixture of pressed fine powder (into a solid, hard slug), with a burn rate that is highly dependent upon exact composition and operating conditions. The performance or specific impulse of black powder is low, around 80 seconds. The grain is sensitive to fracture and, therefore, catastrophic failure. Black powder does not typically find use in motors above 40 newtons (9.0 pounds-force).
Composed of powdered zinc metal and powdered sulfur (oxidizer), ZS or "micrograin" is another pressed propellant that does not find any practical application outside specialized amateur rocketry circles due to its poor performance (as most ZS burns outside the combustion chamber) and fast linear burn rates on the order of 2 m/s. ZS is most often employed as a novelty propellant as the rocket accelerates extremely quickly leaving a spectacular large orange fireball behind it.
In general, rocket candy propellants are an oxidizer (typically potassium nitrate) and a sugar fuel (typically dextrose, sorbitol, or sucrose) that are cast into shape by gently melting the propellant constituents together and pouring or packing the amorphous colloid into a mold. Candy propellants generate a low-medium specific impulse of roughly 130 s and, thus, are used primarily by amateur and experimental rocketeers.
DB propellants are composed of two monopropellant fuel components where one typically acts as a high-energy (yet unstable) monopropellant and the other acts as a lower-energy stabilizing (and gelling) monopropellant. In typical circumstances, nitroglycerin is dissolved in a nitrocellulose gel and solidified with additives. DB propellants are implemented in applications where minimal smoke is required yet medium-high performance (Isp of roughly 235 s) is required. The addition of metal fuels (such as aluminium) can increase the performance (around 250 s), though metal oxide nucleation in the exhaust can turn the smoke opaque.
A powdered oxidizer and powdered metal fuel are intimately mixed and immobilized with a rubbery binder (that also acts as a fuel). Composite propellants are often either ammonium nitrate-based (ANCP) or ammonium perchlorate-based (APCP). Ammonium nitrate composite propellant often uses magnesium and/or aluminium as fuel and delivers medium performance (Isp of about 210 s) whereas ammonium perchlorate composite propellant often uses aluminium fuel and delivers high performance (vacuum Isp up to 296 s with a single piece nozzle or 304 s with a high area ratio telescoping nozzle). Aluminium is used as fuel because it has a reasonable specific energy density, a high volumetric energy density, and is difficult to ignite accidentally. Composite propellants are cast, and retain their shape after the rubber binder, such as Hydroxyl-terminated polybutadiene (HTPB), cross-links (solidifies) with the aid of a curative additive. Because of its high performance, moderate ease of manufacturing, and moderate cost, APCP finds widespread use in space rockets, military rockets, hobby and amateur rockets, whereas cheaper and less efficient ANCP finds use in amateur rocketry and gas generators. Ammonium dinitramide, NH4N(NO2)2, is being considered as a 1-to-1 chlorine-free substitute for ammonium perchlorate in composite propellants. Unlike ammonium nitrate, ADN can be substituted for AP without a loss in motor performance.
Polyurethane-bound aluminium-APCP solid fuel was used in the submarine launched Polaris missiles. APCP used in the space shuttle Solid Rocket Boosters consisted of ammonium perchlorate (oxidizer, 69.6% by weight), aluminium (fuel, 16%), iron oxide (a catalyst, 0.4%), polybutadiene acrylonitrile (PBAN) polymer (a non-urethane rubber binder that held the mixture together and acted as secondary fuel, 12.04%), and an epoxy curing agent (1.96%). It developed a specific impulse of 242 seconds (2.37 km/s) at sea level or 268 seconds (2.63 km/s) in a vacuum. The 2005-2009 Constellation Program was to use a similar PBAN-bound APCP.
In 2009, a group succeeded in creating a propellant of water and nanoaluminium (ALICE).
Typical HEC propellants start with a standard composite propellant mixture (such as APCP) and add a high-energy explosive to the mix. This extra component usually is in the form of small crystals of RDX or HMX, both of which have higher energy than ammonium perchlorate. Despite a modest increase in specific impulse, implementation is limited due to the increased hazards of the high-explosive additives.
Composite modified double base propellants start with a nitrocellulose/nitroglycerin double base propellant as a binder and add solids (typically ammonium perchlorate (AP) and powdered aluminium) normally used in composite propellants. The ammonium perchlorate makes up the oxygen deficit introduced by using nitrocellulose, improving the overall specific impulse. The aluminium improves specific impulse as well as combustion stability. High performing propellants such as NEPE-75 used to fuel the Trident II D-5, SLBM replace most of the AP with polyethylene glycol-bound HMX, further increasing specific impulse. The mixing of composite and double base propellant ingredients has become so common as to blur the functional definition of double base propellants.
One of the most active areas of solid propellant research is the development of high-energy, minimum-signature propellant using C6H6N6(NO2)6 CL-20 nitroamine (China Lake compound #20), which has 14% higher energy per mass and 20% higher energy density than HMX. The new propellant has been successfully developed and tested in tactical rocket motors. The propellant is non-polluting: acid-free, solid particulates-free, and lead-free. It is also smokeless and has only a faint shock diamond pattern that is visible in the otherwise transparent exhaust. Without the bright flame and dense smoke trail produced by the burning of aluminized propellants, these smokeless propellants all but eliminate the risk of giving away the positions from which the missiles are fired. The new CL-20 propellant is shock-insensitive (hazard class 1.3) as opposed to current HMX smokeless propellants which are highly detonable (hazard class 1.1). CL-20 is considered a major breakthrough in solid rocket propellant technology but has yet to see widespread use because costs remain high.
Electric solid propellants (ESPs) are a family of high performance plastisol solid propellants that can be ignited and throttled by the application of electric current. Unlike conventional rocket motor propellants that are difficult to control and extinguish, ESPs can be ignited reliably at precise intervals and durations. It requires no moving parts and the propellant is insensitive to flames or electrical sparks.
Solid propellant rocket motors can be bought for use in model rocketry; they are normally small cylinders of black powder fuel with an integral nozzle and optionally a small charge that is set off when the propellant is exhausted after a time delay. This charge can be used to trigger a camera, or deploy a parachute. Without this charge and delay, the motor may ignite a second stage (black powder only).
In mid- and high-power rocketry, commercially made APCP motors are widely used. They can be designed as either single-use or reloadables. These motors are available in impulse ranges from "A" (1.26Ns– 2.50Ns) to "O"(20.48KNs – 40.96KNs), from several manufacturers. They are manufactured in standardized diameters, and varying lengths depending on required impulse. Standard motor diameters are 13, 18, 24, 29, 38, 54, 75, 98, and 150 millimeters. Different propellant formulations are available to produce different thrust profiles, as well as "special effects" such as colored flames, smoke trails, or large quantities of sparks (produced by adding titanium sponge to the mix).
Almost all sounding rockets use solid motors.
Due to reliability, ease of storage and handling, solid rockets are used on missiles and ICBMs.
Solid rockets are suitable for launching small payloads to orbital velocities, especially if three or more stages are used. Many of these are based on repurposed ICBMs.
Larger liquid-fueled orbital rockets often use solid rocket boosters to gain enough initial thrust to launch the fully fueled rocket.
Solid fuel is also used for some upper stages, particularly the Star 37 (sometimes referred to as the "Burner" upper stage) and the Star 48 (sometimes referred to as the "Payload Assist Module", or PAM), both manufactured originally by Thiokol, and today by Northrop Grumman. They are used to lift large payloads to intended orbits (such as the Global Positioning System satellites), or smaller payloads to interplanetary—or even interstellar—trajectories. Another solid-fuel upper stage, used by the Space Shuttle and the Titan IV, was the Boeing-manufactured Inertial Upper Stage (IUS).
Some rockets, like the Antares (manufactured by Northrop Grumman), have mandatory solid-fuel upper stages. The Antares rocket uses the Northrop Grumman-manufactured Castor 30 as an upper stage.

The Katyusha (Russian: Катю́ша, IPA:  (listen)) is a type of rocket artillery first built and fielded by the Soviet Union in World War II. Multiple rocket launchers such as these deliver explosives to a target area more intensively than conventional artillery, but with lower accuracy and requiring a longer time to reload. They are fragile compared to artillery guns, but are cheap, easy to produce, and usable on almost any chassis. The Katyushas of World War II, the first self-propelled artillery mass-produced by the Soviet Union, were usually mounted on ordinary trucks. This mobility gave the Katyusha, and other self-propelled artillery, another advantage: being able to deliver a large blow all at once, and then move before being located and attacked with counter-battery fire.
Katyusha weapons of World War II included the BM-13 launcher, light BM-8, and heavy BM-31. Today, the nickname is also applied to newer truck-mounted post-Soviet – in addition to non-Soviet – multiple-rocket launchers, notably the common BM-21 Grad and its derivatives.
Initially, concerns for secrecy kept the military designation of the Katyushas from being known by the soldiers who operated them. They were called by code names such as Kostikov guns, after A. Kostikov, the head of the RNII, the Reactive Scientific Research Institute, and finally classed as Guards Mortars. The name BM-13 was only allowed into secret documents in 1942, and remained classified until after the war.
Because they were marked with the letter K (for Voronezh Komintern Factory), Red Army troops adopted a nickname from Mikhail Isakovsky's popular wartime song, "Katyusha", about a girl longing for her absent beloved, who has gone away on military service. Katyusha is the Russian equivalent of Katie, an endearing diminutive form of the name Katherine. Yekaterina is given the diminutive Katya, which itself is then given the affectionate diminutive Katyusha.
German troops coined the nickname "Stalin's organ" (Stalinorgel), after Soviet leader Joseph Stalin, comparing the visual resemblance of the launch array to a pipe organ, and the sound of the weapon's rocket motors, a distinctive howling sound which terrified the German troops, adding a psychological warfare aspect to their use. Weapons of this type are known by the same name in Denmark (Danish: Stalinorgel), Finland (Finnish: Stalinin urut), France (French: orgue de Staline), Norway (Norwegian: Stalinorgel), the Netherlands and Belgium (Dutch: Stalinorgel), Hungary (Hungarian: Sztálinorgona), Spain and other Spanish-speaking countries (Spanish: Órganos de Stalin) as well as in Sweden (Swedish: Stalinorgel).
The heavy BM-31 launcher was also referred to as Andryusha (Андрюша, an affectionate diminutive of "Andrew").
Katyusha rocket launchers, which were built in Voronezh, were mounted on many platforms during World War II, including on trucks, artillery tractors, tanks, and armoured trains, as well as on naval and riverine vessels as assault support weapons. Soviet engineers also mounted single Katyusha rockets on lengths of railway track to serve in urban combat.
The design was relatively simple, consisting of racks of parallel rails on which rockets were mounted, with a folding frame to raise the rails to launch position. Each truck had 14 to 48 launchers. The  M-13 rocket of the BM-13 system was 80 cm (2 ft 7 in) long, 13.2 cm (5.2 in) in diameter and weighed 42 kg (93 lb).
The weapon is less accurate than conventional artillery guns, but is extremely effective in saturation bombardment. A battery of four BM-13 launchers could fire a salvo in 7–10 seconds that delivered 4.35 tons of high explosives over a 400,000-square-metre (4,300,000 sq ft) impact zone, making its power roughly equivalent to that of 72 conventional artillery guns.  With an efficient crew, the launchers could redeploy to a new location immediately after firing, denying the enemy the opportunity for counterbattery fire. Katyusha batteries were often massed in very large numbers to create a shock effect on enemy forces. The weapon's disadvantage was the long time it took to reload a launcher, in contrast to conventional guns which could sustain a continuous low rate of fire.
Initial development of solid propellant rockets was carried out by Nikolai Tikhomirov at the Soviet Gas Dynamics Laboratory (GDL), with the first test-firing of a solid fuel rocket carried out in March 1928, which flew for about 1,300 meters The rockets were used to assist take-off of aircraft and were later developed into the RS-82 and RS-132 (RS for Reaktivnyy Snaryad, 'rocket-powered shell') in the early 1930s led by Georgy Langemak, including firing rockets from aircraft and the ground. In June 1938, GDL's successor Reactive Scientific Research Institute (RNII) began building several prototype launchers for the modified 132 mm M-132 rockets. Firing over the sides of ZIS-5 trucks proved unstable, and V.N. Galkovskiy proposed mounting the launch rails longitudinally. In August 1939, the result was the BM-13 (BM stands for боевая машина (translit. boyevaya mashina), 'combat vehicle' for M-13 rockets).
The first large-scale testing of the rocket launchers took place at the end of 1938, when 233 rounds of various types were used. A salvo of rockets could completely straddle a target at a range of 5,500 metres (3.4 mi). But the artillery branch was not fond of the Katyusha, because it took up to 50 minutes to load and fire 24 rounds, while a conventional howitzer could fire 95 to 150 rounds in the same time. Testing with various rockets was conducted through 1940, and the BM-13-16 with launch rails for sixteen rockets was authorized for production. Only forty launchers were built before Germany invaded the Soviet Union in June 1941.
After their success in the first month of the war, mass production was ordered and the development of other models proceeded. The Katyusha was inexpensive and could be manufactured in light industrial installations which did not have the heavy equipment to build conventional artillery gun barrels. By the end of 1942, 3,237 Katyusha launchers of all types had been built, and by the end of the war total production reached about 10,000.
The truck-mounted Katyushas were installed on ZIS-6 6×4 trucks, as well as the two-axle ZIS-5 and ZIS-5V. In 1941, a small number of BM-13 launchers were mounted on STZ-5 artillery tractors. A few were also tried on KV tank chassis as the KV-1K, but this was a needless waste of heavy armour. Starting in 1942, they were also mounted on various British, Canadian and U.S. Lend-Lease trucks, in which case they were sometimes referred to as BM-13S. The cross-country performance of the Studebaker US6 2½-ton 6×6 truck was so good that it became the GAU's standard mounting in 1943, designated BM-13N (normalizovanniy, 'standardized'), and more than 1,800 of this model were manufactured by the end of World War II. After World War II, BM-13s were based on Soviet-built ZIS-151 trucks.
The 82 mm BM-8 was approved in August 1941, and deployed as the BM-8-36 on truck beds and BM-8-24 on T-40 and T-60 light tank chassis. Later these were also installed on GAZ-67 jeeps as the BM-8-8, and on the larger Studebaker trucks as the BM-8-48. In 1942, the team of scientists Leonid Shvarts, Moisei Komissarchik and engineer Yakov Shor received the Stalin prize for the development of the BM-8-48.
Based on the M-13, the M-30 rocket was developed in 1942. Its bulbous warhead required it to be fired from a grounded frame, called the M-30 (single frame, four round; later double frame, 8 round), instead of a launch rail mounted on a truck. In 1944 it became the basis for the BM-31-12 truck-mounted launcher.
A battery of BM-13-16 launchers included four firing vehicles, two reload trucks and two technical support trucks, with each firing vehicle having a crew of six. Reloading was executed in 3–4 minutes, although the standard procedure was to switch to a new position some 10 km away due to the ease with which the battery could be identified by the enemy. Three batteries were combined into a division (company), and three divisions into a separate mine-firing regiment of rocket artillery.
Soviet World War II rocket systems were named according to the following templates:
where:
In particular, BM-8-16 is a vehicle which fires M-8 missiles and has 16 rails. BM-31-12 is a vehicle which fires M-31 missiles and has 12 launch tubes. Short names such as BM-8 or BM-13 were used too. Number of launch rails/tubes is absent here. Such names describe launchers only no matter what vehicle they are mounted on. In particular BM-8-24 had a number of variants: vehicle mounted (ZIS-5 truck), tank mounted (T-40) and tractor mounted (STZ-3). All of them had the same name: BM-8-24. Other launchers had a number of variants mounted on different vehicles too. Typical set of vehicles for soviet missile systems is the following:
Note: There was also an experimental KV-1K – Katyusha mounted on KV-1 tank which was not taken in service.
A list of some implementations of the Katyusha follows:
Rockets used in the above implementations were:
The M-8 and M-13 rocket could also be fitted with smoke warheads, although this was not common.
The Axis powers had captured Katyushas during the war. Germany considered producing a local copy, but instead created the 8 cm Raketen-Vielfachwerfer, which was based on the Katyusha.
Romania had started developing its Mareșal tank destroyer in late 1942. One of the first experimental models was equipped with a Katyusha rocket launcher and tested in the summer of 1943. The project was not continued.
The multiple rocket launchers were top secret in the beginning of World War II. A special unit of the NKVD troops was raised to operate them. On July 14, 1941, an experimental artillery battery of seven launchers was first used in battle at Rudnya in Smolensk Oblast of Russia, under the command of Captain Ivan Flyorov, destroying a concentration of German troops with tanks, armored vehicles and trucks at the marketplace, causing massive German Army casualties and its retreat from the town in panic, see also in articles by a Russian military historian Andrey Sapronov, an eyewitness of the maiden launches. Following the success, the Red Army organized new Guards mortar batteries for the support of infantry divisions. A battery's complement was standardized at four launchers. They remained under NKVD control until German Nebelwerfer rocket launchers became common later in the war.
On August 8, 1941, Stalin ordered the formation of eight special Guards mortar regiments under the direct control of the Reserve of the Supreme High Command (RVGK).  Each regiment comprised three battalions of three batteries, totalling 36 BM-13 or BM-8 launchers.  Independent Guards mortar battalions were also formed, comprising 12 launchers in three batteries of four.  By the end of 1941, there were eight regiments, 35 independent battalions, and two independent batteries in service, fielding a total of 554 launchers.
In June 1942 heavy Guards mortar battalions were formed around the new M-30 static rocket launch frames, consisting of 96 launchers in three batteries. In July, a battalion of BM-13s was added to the establishment of a tank corps. In 1944, the BM-31 was used in motorized heavy Guards mortar battalions of 48 launchers. In 1943, Guards mortar brigades, and later divisions, were formed equipped with static launchers.
By the end of 1942, 57 regiments were in service—together with the smaller independent battalions, this was the equivalent of 216 batteries: 21% BM-8 light launchers, 56% BM-13, and 23% M-30 heavy launchers. By the end of the war, the equivalent of 518 batteries were in service.
The success and economy of multiple rocket launchers (MRL) have led them to continue to be developed. In the years following WWII, the BM-13 was replaced by the 140 mm BM-14 and the BM-31 was replaced by the 240 mm BM-24. During the Cold War, the Soviet Union fielded several models of Katyusha-like MRL, notably the BM-21 Grad launchers somewhat inspired by the earlier weapon, and the larger BM-27 Uragan. Advances in artillery munitions have been applied to some Katyusha-type multiple launch rocket systems, including bomblet submunitions, remotely deployed land mines, and chemical warheads.
With the breakup of the Soviet Union, Russia inherited most of its military arsenal including its large complement of MRLs. In recent history, they have been used by the Russian Armed Forces during the First and Second Chechen Wars and by the Armenian and Azerbaijani Armed Forces during the First Nagorno-Karabakh War. The Georgian Defense Forces are reported to have used BM-21 Grad or similar rocket artillery in fighting in the 2008 South Ossetia war.
Katyusha-like launchers were exported to Afghanistan, Angola, Czechoslovakia, Khmer Republic, Egypt, East Germany, Hungary, Iran, Iraq, Mongolia, North Korea, Poland, Syria, Yemen and Vietnam.  They were also built in Czechoslovakia, the People's Republic of China, North Korea, and Iran.
Proper Katyushas (BM-13s) also saw action in the Korean War, used by the Chinese People's Volunteer Army and Korean People's Army against the South Korean and United Nations forces. Soviet BM-13s were known to have been imported to China before the Sino-Soviet split and were operational in the People's Liberation Army. The Viet Minh deployed them against the French Far East Expeditionary Corps during the Battle of Dien Bien Phu at the end of the First Indochina War.
Israel captured BM-24 MRLs during the Six-Day War (1967), used them in two battalions during the Yom Kippur War (1973) and the 1982 Lebanon War, and later developed the MAR-240 launcher for the same rockets, based on a Sherman tank chassis.
The rockets were employed by the Tanzania People's Defence Force in the Uganda-Tanzania War. Tanzanian forces called them Baba Mtakatifu (Kiswahili for "Holy Father") while the Ugandans called them Saba Saba.
During the 2006 Lebanon War, Hezbollah fired between 3,970 and 4,228 rockets, from light truck-mounts and single-rail man-portable launchers. About 95% of these were 122 mm (4.8 in) Syrian-manufactured M-21OF type artillery rockets which carried warheads up to 30 kg (66 lb) and had a range of 20 km, perhaps up to 30 km (19 mi). Most rockets fired at Israel by Hamas from the Gaza Strip are of the simpler Qassam rocket type, but Hamas has also launched 122-mm Grad-type Katyusha rockets  against several cities in Israel, although they are not reported to have truck-mounted launchers. Although Katyusha originally referred to the mobile launcher, today the rockets are often referred to as Katyushas.
Some allege that the Central Intelligence Agency bought Katyushas from the Egyptian Armed Forces and supplied them to the Mujahideen (via Pakistan's ISI) during the Soviet–Afghan War.
Katyusha-like MRLs were also allegedly used by the Rwandan Patriotic Front during its 1990 invasion of Rwanda, through the 1994 genocide. They were effective in battle, but translated into much anti-Tutsi sentiment in the local media.
It was reported that BM-21 Grad launchers were used against the U.S. Army during the 2003 invasion of Iraq. They have also been used in the Afghanistan and Iraq insurgencies. In Iraq, according to Associated Press and Agence France-Presse reports, Katyusha-like rockets were fired at the Green Zone late March 2008.
Katyusha rockets were reportedly used by both Gaddafi Loyalists and anti-Gaddafi forces during the Libyan Civil War.
In February 2013, the Defence Ministry of Yemen reported seizing an Iranian ship, and that the ship's cargo included (among its other weapons) Katyusha rockets.
On May 19, 2019, a Katyusha rocket was fired inside the Green Zone in Baghdad, Iraq, landing less than a mile from the US Embassy near the statue of the Unknown Soldier. No casualties were reported.
On January 4, 2020, four Katyusha rockets were fired in the Baghdad area. According to two Iraqi Police sources and an official Iraqi Armed Forces statement, one Katyusha rocket landed in the Green Zone in Celebration Square near the U.S. Embassy and another landed in the nearby Jadriya neighborhood. Two other Katyusha rockets landed in Balad Air Base, which houses U.S. Armed Forces troops, according to two security sources.
Participants in the creation of the Katyusha rocket launcher received official recognition only in 1991. By decree of the President of the USSR Mikhail Gorbachev dated June 21, 1991, I. T. Kleymenov, G. E. Langemak, V. N. Luzhin, B. S. Petropavlovsky, B. M. Slonimer and N. I. Tikhomirov were posthumously awarded title of Heroes of Socialist Labor for their work on the creation of the Katyusha.
