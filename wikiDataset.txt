rocket (from Italian: rocchetto, lit.uc0u8201 'bobbin/spool') is a spacecraft, aircraft, vehicle or projectile that obtains thrust from a rocket engine. Rocket engine exhaust is formed entirely from propellant carried within the rocket. Rocket engines work by action and reaction and push rockets forward simply by expelling their exhaust in the opposite direction at high speed, and can therefore work in the vacuum of space.

In fact, rockets work more efficiently in the vacuum of space than in an atmosphere. Multistage rockets are capable of attaining escape velocity from Earth and therefore can achieve unlimited maximum altitude. Compared with airbreathing engines, rockets are lightweight and powerful and capable of generating large accelerations. To control their flight, rockets rely on momentum, airfoils, auxiliary reaction engines, gimballed thrust, momentum wheels, deflection of the exhaust stream, propellant flow, spin, or gravity.

Rockets for military and recreational uses date back to at least 13th-century China. Significant scientific, interplanetary and industrial use did not occur until the 20th century, when rocketry was the enabling technology for the Space Age, including setting foot on the Moon. Rockets are now used for fireworks, missiles and other weaponry, ejection seats, launch vehicles for artificial satellites, human spaceflight, and space exploration.

Chemical rockets are the most common type of high power rocket, typically creating a high speed exhaust by the combustion of fuel with an oxidizer. The stored propellant can be a simple pressurized gas or a single liquid fuel that disassociates in the presence of a catalyst (monopropellant), two liquids that spontaneously react on contact (hypergolic propellants), two liquids that must be ignited to react (like kerosene (RP1) and liquid oxygen, used in most liquid-propellant rockets), a solid combination of fuel with oxidizer (solid fuel), or solid fuel with liquid or gaseous oxidizer (hybrid propellant system). Chemical rockets store a large amount of energy in an easily released form, and can be very dangerous. However, careful design, testing, construction and use minimizes risks.

The first gunpowder-powered rockets evolved in medieval China under the Song dynasty by the 13th century. They also developed an early form of MLRS during this time. The Mongols adopted Chinese rocket technology and the invention spread via the Mongol invasions to the Middle East and to Europe in the mid-13th century. Rockets are recorded in use by the Song navy in a military exercise dated to 1245. Internal-combustion rocket propulsion is mentioned in a reference to 1264, recording that the "ground-rat", a type of firework, had frightened the Empress-Mother Gongsheng at a feast held in her honor by her son the Emperor Lizong. Subsequently, rockets are included in the military treatise Huolongjing, also known as the Fire Drake Manual, written by the Chinese artillery officer Jiao Yu in the mid-14th century. This text mentions the first known multistage rocket, the 'fire-dragon issuing from the water' (Huo long chu shui), thought to have been used by the Chinese navy.

Medieval and early modern rockets were used militarily as incendiary weapons in sieges. Between 1270 and 1280, Hasan al-Rammah wrote al-furusiyyah wa al-manasib al-harbiyya (The Book of Military Horsemanship and Ingenious War Devices), which included 107 gunpowder recipes, 22 of them for rockets. In Europe, Konrad Kyeser described rockets in his military treatise Bellifortis around 1405.

The name "rocket" comes from the Italian rocchetta, meaning "bobbin" or "little spindle", given due to the similarity in shape to the bobbin or spool used to hold the thread from a spinning wheel. Leonhard Fronsperger and Conrad Haas adopted the Italian term into German in the mid-16th century; "rocket" appears in English by the early 17th century. Artis Magnae Artilleriae pars prima, an important early modern work on rocket artillery, by Casimir Siemienowicz, was first printed in Amsterdam in 1650.

The Mysorean rockets were the first successful iron-cased rockets, developed in the late 18th century in the Kingdom of Mysore (part of present-day India) under the rule of Hyder Ali.

The Congreve rocket was a British weapon designed and developed by Sir William Congreve in 1804. This rocket was based directly on the Mysorean rockets, used compressed powder and was fielded in the Napoleonic Wars. It was Congreve rockets to which Francis Scott Key was referring, when he wrote of the "rockets'92 red glare" while held captive on a British ship that was laying siege to Fort McHenry in 1814. Together, the Mysorean and British innovations increased the effective range of military rockets from 100 to 2,000 yards.

The first mathematical treatment of the dynamics of rocket propulsion is due to William Moore (1813). In 1814 Congreve published a book in which he discussed the use of multiple rocket launching apparatus. In 1815 Alexander Dmitrievich Zasyadko constructed rocket-launching platforms, which allowed rockets to be fired in salvos (6 rockets at a time), and gun-laying devices. William Hale in 1844 greatly increased the accuracy of rocket artillery. Edward Mounier Boxer further improved the Congreve rocket in 1865.

William Leitch first proposed the concept of using rockets to enable human spaceflight in 1861. Leitch's rocket spaceflight description was first provided in his 1861 essay "A Journey Through Space", which was later published in his book God's Glory in the Heavens (1862). Konstantin Tsiolkovsky later (in 1903) also conceived this idea, and extensively developed a body of theory that has provided the foundation for subsequent spaceflight development.

The British Royal Flying Corps designed a guided rocket during World War I. Archibald Low stated '93...in 1917 the Experimental Works designed an electrically steered rocket'85 Rocket experiments were conducted under my own patents with the help of Cdr. Brock'94 The patent '93Improvements in Rockets'94 was raised in July 1918 but not published until February 1923 for security reasons. Firing and Guidance controls could be either wire or wireless. The propulsion and guidance rocket eflux emerged from the deflecting cowl at the nose.

In 1920, Professor Robert Goddard of Clark University published proposed improvements to rocket technology in A Method of Reaching Extreme Altitudes. In 1923, Hermann Oberth (1894'961989) published Die Rakete zu den Planetenr'e4umen ("The Rocket into Planetary Space"). Modern rockets originated in 1926 when Goddard attached a supersonic (de Laval) nozzle to a high pressure combustion chamber. These nozzles turn the hot gas from the combustion chamber into a cooler, hypersonic, highly directed jet of gas, more than doubling the thrust and raising the engine efficiency from 2% to 64%. His use of liquid propellants instead of gunpowder greatly lowered the weight and increased the effectiveness of rockets.

In 1921 the Soviet research and development laboratory Gas Dynamics Laboratory began developing solid-propellant rockets, which resulted in the first launch in 1928, which flew for approximately 1,300 metres. These rockets were used in 1931 for the world's first successful use of rockets for [jet-assisted takeoff]] of aircraft and became the prototypes for the Katyusha rocket launcher, which were used during World War II.

In 1943 production of the V-2 rocket began in Germany. It was designed by the Peenem'fcnde Army Research Center with Wernher von Braun serving as the technical director. The V-2 became the first artificial object to travel into space by crossing the K'e1rm'e1n line with the vertical launch of MW 18014 on 20 June 1944. In parallel with the German guided-missile programme, rockets were also used on aircraft, either for assisting horizontal take-off (RATO), vertical take-off (Bachem Ba 349 "Natter") or for powering them (Me 163, see list of World War II guided missiles of Germany). The Allies' rocket programs were less technological, relying mostly on unguided missiles like the Soviet Katyusha rocket in the artillery role, and the American anti tank bazooka projectile. These used solid chemical propellants.

The Americans captured a large number of German rocket scientists, including Wernher von Braun, in 1945, and brought them to the United States as part of Operation Paperclip. After World War II scientists used rockets to study high-altitude conditions, by radio telemetry of temperature and pressure of the atmosphere, detection of cosmic rays, and further techniques; note too the Bell X-1, the first crewed vehicle to break the sound barrier (1947). Independently, in the Soviet Union's space program research continued under the leadership of the chief designer Sergei Korolev (1907'961966).

During the Cold War rockets became extremely important militarily with the development of modern intercontinental ballistic missiles (ICBMs). The 1960s saw rapid development of rocket technology, particularly in the Soviet Union (Vostok, Soyuz, Proton) and in the United States (e.g. the X-15). Rockets came into use for space exploration. American crewed programs (Project Mercury, Project Gemini and later the Apollo programme) culminated in 1969 with the first crewed landing on the Moon '96 using equipment launched by the Saturn V rocket.

Vehicle configurations
Rocket vehicles are often constructed in the archetypal tall thin "rocket" shape that takes off vertically, but there are actually many different types of rockets including:

tiny models such as balloon rockets, water rockets, skyrockets or small solid rockets that can be purchased at a hobby store
missiles
space rockets such as the enormous Saturn V used for the Apollo program
rocket cars
rocket bike
rocket-powered aircraft (including rocket assisted takeoff of conventional aircraft '96 RATO)
rocket sleds
rocket trains
rocket torpedoes
rocket-powered jet packs
rapid escape systems such as ejection seats and launch escape systems
space probes
A rocket design can be as simple as a cardboard tube filled with black powder, but to make an efficient, accurate rocket or missile involves overcoming a number of difficult problems. The main difficulties include cooling the combustion chamber, pumping the fuel (in the case of a liquid fuel), and controlling and correcting the direction of motion.

Rockets consist of a propellant, a place to put propellant (such as a propellant tank), and a nozzle. They may also have one or more rocket engines, directional stabilization device(s) (such as fins, vernier engines or engine gimbals for thrust vectoring, gyroscopes) and a structure (typically monocoque) to hold these components together. Rockets intended for high speed atmospheric use also have an aerodynamic fairing such as a nose cone, which usually holds the payload.

As well as these components, rockets can have any number of other components, such as wings (rocketplanes), parachutes, wheels (rocket cars), even, in a sense, a person (rocket belt). Vehicles frequently possess navigation systems and guidance systems that typically use satellite navigation and inertial navigation systems.

Rocket engines employ the principle of jet propulsion. The rocket engines powering rockets come in a great variety of different types; a comprehensive list can be found in the main article, Rocket engine. Most current rockets are chemically powered rockets (usually internal combustion engines, but some employ a decomposing monopropellant) that emit a hot exhaust gas. A rocket engine can use gas propellants, solid propellant, liquid propellant, or a hybrid mixture of both solid and liquid. Some rockets use heat or pressure that is supplied from a source other than the chemical reaction of propellant(s), such as steam rockets, solar thermal rockets, nuclear thermal rocket engines or simple pressurized rockets such as water rocket or cold gas thrusters. With combustive propellants a chemical reaction is initiated between the fuel and the oxidizer in the combustion chamber, and the resultant hot gases accelerate out of a rocket engine nozzle (or nozzles) at the rearward-facing end of the rocket. The acceleration of these gases through the engine exerts force ("thrust") on the combustion chamber and nozzle, propelling the vehicle (according to Newton's Third Law). This actually happens because the force (pressure times area) on the combustion chamber wall is unbalanced by the nozzle opening; this is not the case in any other direction. The shape of the nozzle also generates force by directing the exhaust gas along the axis of the rocket.

Rocket propellant is mass that is stored, usually in some form of propellant tank or casing, prior to being used as the propulsive mass that is ejected from a rocket engine in the form of a fluid jet to produce thrust. For chemical rockets often the propellants are a fuel such as liquid hydrogen or kerosene burned with an oxidizer such as liquid oxygen or nitric acid to produce large volumes of very hot gas. The oxidiser is either kept separate and mixed in the combustion chamber, or comes premixed, as with solid rockets.

Sometimes the propellant is not burned but still undergoes a chemical reaction, and can be a 'monopropellant' such as hydrazine, nitrous oxide or hydrogen peroxide that can be catalytically decomposed to hot gas.

Alternatively, an inert propellant can be used that can be externally heated, such as in steam rocket, solar thermal rocket or nuclear thermal rockets.

For smaller, low performance rockets such as attitude control thrusters where high performance is less necessary, a pressurised fluid is used as propellant that simply escapes the spacecraft through a propelling nozzle.

The first liquid-fuel rocket, constructed by Robert H. Goddard, differed significantly from modern rockets. The rocket engine was at the top and the fuel tank at the bottom of the rocket, based on Goddard's belief that the rocket would achieve stability by "hanging" from the engine like a pendulum in flight. However, the rocket veered off course and crashed 184 feet (56 m) away from the launch site, indicating that the rocket was no more stable than one with the rocket engine at the base.

Rockets or other similar reaction devices carrying their own propellant must be used when there is no other substance (land, water, or air) or force (gravity, magnetism, light) that a vehicle may usefully employ for propulsion, such as in space. In these circumstances, it is necessary to carry all the propellant to be used.

However, they are also useful in other situations:

Some military weapons use rockets to propel warheads to their targets. A rocket and its payload together are generally referred to as a missile when the weapon has a guidance system (not all missiles use rocket engines, some use other engines such as jets) or as a rocket if it is unguided. Anti-tank and anti-aircraft missiles use rocket engines to engage targets at high speed at a range of several miles, while intercontinental ballistic missiles can be used to deliver multiple nuclear warheads from thousands of miles, and anti-ballistic missiles try to stop them. Rockets have also been tested for reconnaissance, such as the Ping-Pong rocket, which was launched to surveil enemy targets, however, recon rockets have never come into wide use in the military.

Sounding rockets are commonly used to carry instruments that take readings from 50 kilometers (31 mi) to 1,500 kilometers (930 mi) above the surface of the Earth. The first images of Earth from space were obtained from a V-2 rocket in 1946 (flight #13).

Rocket engines are also used to propel rocket sleds along a rail at extremely high speed. The world record for this is Mach 8.5.

Larger rockets are normally launched from a launch pad that provides stable support until a few seconds after ignition. Due to their high exhaust velocity'972,500 to 4,500 m/s (9,000 to 16,200 km/h; 5,600 to 10,100 mph)'97rockets are particularly useful when very high speeds are required, such as orbital speed at approximately 7,800 m/s (28,000 km/h; 17,000 mph). Spacecraft delivered into orbital trajectories become artificial satellites, which are used for many commercial purposes. Indeed, rockets remain the only way to launch spacecraft into orbit and beyond. They are also used to rapidly accelerate spacecraft when they change orbits or de-orbit for landing. Also, a rocket may be used to soften a hard parachute landing immediately before touchdown (see retrorocket).

Rockets were used to propel a line to a stricken ship so that a Breeches buoy can be used to rescue those on board. Rockets are also used to launch emergency flares.

Some crewed rockets, notably the Saturn V and Soyuz, have launch escape systems. This is a small, usually solid rocket that is capable of pulling the crewed capsule away from the main vehicle towards safety at a moments notice. These types of systems have been operated several times, both in testing and in flight, and operated correctly each time.

This was the case when the Safety Assurance System (Soviet nomenclature) successfully pulled away the L3 capsule during three of the four failed launches of the Soviet moon rocket, N1 vehicles 3L, 5L and 7L. In all three cases the capsule, albeit uncrewed, was saved from destruction. Only the three aforementioned N1 rockets had functional Safety Assurance Systems. The outstanding vehicle, 6L, had dummy upper stages and therefore no escape system giving the N1 booster a 100% success rate for egress from a failed launch.

A successful escape of a crewed capsule occurred when Soyuz T-10, on a mission to the Salyut 7 space station, exploded on the pad.

Solid rocket propelled ejection seats are used in many military aircraft to propel crew away to safety from a vehicle when flight control is lost.

A model rocket is a small rocket designed to reach low altitudes (e.g., 100'96500 m (330'961,640 ft) for 30 g (1.1 oz) model) and be recovered by a variety of means.

According to the United States National Association of Rocketry (nar) Safety Code, model rockets are constructed of paper, wood, plastic and other lightweight materials. The code also provides guidelines for motor use, launch site selection, launch methods, launcher placement, recovery system design and deployment and more. Since the early 1960s, a copy of the Model Rocket Safety Code has been provided with most model rocket kits and motors. Despite its inherent association with extremely flammable substances and objects with a pointed tip traveling at high speeds, model rocketry historically has proven to be a very safe hobby and has been credited as a significant source of inspiration for children who eventually become scientists and engineers.

Hobbyists build and fly a wide variety of model rockets. Many companies produce model rocket kits and parts but due to their inherent simplicity some hobbyists have been known to make rockets out of almost anything. Rockets are also used in some types of consumer and professional fireworks. A water rocket is a type of model rocket using water as its reaction mass. The pressure vessel (the engine of the rocket) is usually a used plastic soft drink bottle. The water is forced out by a pressurized gas, typically compressed air. It is an example of Newton's third law of motion.

The scale of amateur rocketry can range from a small rocket launched in one's own backyard to a rocket that reached space. Amateur rocketry is split into three categories according to total engine impulse: low-power, mid-power, and high-power.

Hydrogen peroxide rockets are used to power jet packs, and have been used to power cars and a rocket car holds the all time (albeit unofficial) drag racing record.

Corpulent Stump is the most powerful non-commercial rocket ever launched on an Aerotech engine in the United Kingdom.

Launches for orbital spaceflights, or into interplanetary space, are usually from a fixed location on the ground, but would also be possible from an aircraft or ship.

Rocket launch technologies include the entire set of systems needed to successfully launch a vehicle, not just the vehicle itself, but also the firing control systems, mission control center, launch pad, ground stations, and tracking stations needed for a successful launch or recovery or both. These are often collectively referred to as the "ground segment".

Orbital launch vehicles commonly take off vertically, and then begin to progressively lean over, usually following a gravity turn trajectory.

Once above the majority of the atmosphere, the vehicle then angles the rocket jet, pointing it largely horizontally but somewhat downwards, which permits the vehicle to gain and then maintain altitude while increasing horizontal speed. As the speed grows, the vehicle will become more and more horizontal until at orbital speed, the engine will cut off.

All current vehicles stage, that is, jettison hardware on the way to orbit. Although vehicles have been proposed which would be able to reach orbit without staging, none have ever been constructed, and, if powered only by rockets, the exponentially increasing fuel requirements of such a vehicle would make its useful payload tiny or nonexistent. Most current and historical launch vehicles "expend" their jettisoned hardware, typically by allowing it to crash into the ocean, but some have recovered and reused jettisoned hardware, either by parachute or by propulsive landing.

When launching a spacecraft to orbit, a "dogleg" is a guided, powered turn during ascent phase that causes a rocket's flight path to deviate from a "straight" path. A dogleg is necessary if the desired launch azimuth, to reach a desired orbital inclination, would take the ground track over land (or over a populated area, e.g. Russia usually does launch over land, but over unpopulated areas), or if the rocket is trying to reach an orbital plane that does not reach the latitude of the launch site. Doglegs are undesirable due to extra onboard fuel required, causing heavier load, and a reduction of vehicle performance.

Rocket exhaust generates a significant amount of acoustic energy. As the supersonic exhaust collides with the ambient air, shock waves are formed. The sound intensity from these shock waves depends on the size of the rocket as well as the exhaust velocity. The sound intensity of large, high performance rockets could potentially kill at close range.

The Space Shuttle generated 180 dB of noise around its base. To combat this, NASA developed a sound suppression system which can flow water at rates up to 900,000 gallons per minute (57 m3/s) onto the launch pad. The water reduces the noise level from 180 dB down to 142 dB (the design requirement is 145 dB). Without the sound suppression system, acoustic waves would reflect off of the launch pad towards the rocket, vibrating the sensitive payload and crew. These acoustic waves can be so severe as to damage or destroy the rocket.

Noise is generally most intense when a rocket is close to the ground, since the noise from the engines radiates up away from the jet, as well as reflecting off the ground. This noise can be reduced somewhat by flame trenches with roofs, by water injection around the jet and by deflecting the jet at an angle.

For crewed rockets various methods are used to reduce the sound intensity for the passengers, and typically the placement of the astronauts far away from the rocket engines helps significantly. For the passengers and crew, when a vehicle goes supersonic the sound cuts off as the sound waves are no longer able to keep up with the vehicle.

The effect of the combustion of propellant in the rocket engine is to increase the internal energy of the resulting gases, utilizing the stored chemical energy in the fuel. As the internal energy increases, pressure increases, and a nozzle is utilized to convert this energy into a directed kinetic energy. This produces thrust against the ambient environment to which these gases are released. The ideal direction of motion of the exhaust is in the direction so as to cause thrust. At the top end of the combustion chamber the hot, energetic gas fluid cannot move forward, and so, it pushes upward against the top of the rocket engine's combustion chamber. As the combustion gases approach the exit of the combustion chamber, they increase in speed. The effect of the convergent part of the rocket engine nozzle on the high pressure fluid of combustion gases, is to cause the gases to accelerate to high speed. The higher the speed of the gases, the lower the pressure of the gas (Bernoulli's principle or conservation of energy) acting on that part of the combustion chamber. In a properly designed engine, the flow will reach Mach 1 at the throat of the nozzle. At which point the speed of the flow increases. Beyond the throat of the nozzle, a bell shaped expansion part of the engine allows the gases that are expanding to push against that part of the rocket engine. Thus, the bell part of the nozzle gives additional thrust. Simply expressed, for every action there is an equal and opposite reaction, according to Newton's third law with the result that the exiting gases produce the reaction of a force on the rocket causing it to accelerate the rocket.

In a closed chamber, the pressures are equal in each direction and no acceleration occurs. If an opening is provided in the bottom of the chamber then the pressure is no longer acting on the missing section. This opening permits the exhaust to escape. The remaining pressures give a resultant thrust on the side opposite the opening, and these pressures are what push the rocket along.

The shape of the nozzle is important. Consider a balloon propelled by air coming out of a tapering nozzle. In such a case the combination of air pressure and viscous friction is such that the nozzle does not push the balloon but is pulled by it. Using a convergent/divergent nozzle gives more force since the exhaust also presses on it as it expands outwards, roughly doubling the total force. If propellant gas is continuously added to the chamber then these pressures can be maintained for as long as propellant remains. Note that in the case of liquid propellant engines, the pumps moving the propellant into the combustion chamber must maintain a pressure larger than the combustion chamber '96 typically on the order of 100 atmospheres.

As a side effect, these pressures on the rocket also act on the exhaust in the opposite direction and accelerate this exhaust to very high speeds (according to Newton's Third Law). From the principle of conservation of momentum the speed of the exhaust of a rocket determines how much momentum increase is created for a given amount of propellant. This is called the rocket's specific impulse. Because a rocket, propellant and exhaust in flight, without any external perturbations, may be considered as a closed system, the total momentum is always constant. Therefore, the faster the net speed of the exhaust in one direction, the greater the speed of the rocket can achieve in the opposite direction. This is especially true since the rocket body's mass is typically far lower than the final total exhaust mass.

The general study of the forces on a rocket is part of the field of ballistics. Spacecraft are further studied in the subfield of astrodynamics.

Flying rockets are primarily affected by the following:

Thrust from the engine(s)
Gravity from celestial bodies
Drag if moving in atmosphere
Lift; usually relatively small effect except for rocket-powered aircraft
In addition, the inertia and centrifugal pseudo-force can be significant due to the path of the rocket around the center of a celestial body; when high enough speeds in the right direction and altitude are achieved a stable orbit or escape velocity is obtained.

These forces, with a stabilizing tail (the empennage) present will, unless deliberate control efforts are made, naturally cause the vehicle to follow a roughly parabolic trajectory termed a gravity turn, and this trajectory is often used at least during the initial part of a launch. (This is true even if the rocket engine is mounted at the nose.) Vehicles can thus maintain low or even zero angle of attack, which minimizes transverse stress on the launch vehicle, permitting a weaker, and hence lighter, launch vehicle.

Drag is a force opposite to the direction of the rocket's motion relative to any air it is moving through. This slows the speed of the vehicle and produces structural loads. The deceleration forces for fast-moving rockets are calculated using the drag equation.

Drag can be minimised by an aerodynamic nose cone and by using a shape with a high ballistic coefficient (the "classic" rocket shape'97long and thin), and by keeping the rocket's angle of attack as low as possible.

During a launch, as the vehicle speed increases, and the atmosphere thins, there is a point of maximum aerodynamic drag called max Q. This determines the minimum aerodynamic strength of the vehicle, as the rocket must avoid buckling under these forces.

A typical rocket engine can handle a significant fraction of its own mass in propellant each second, with the propellant leaving the nozzle at several kilometres per second. This means that the thrust-to-weight ratio of a rocket engine, and often the entire vehicle can be very high, in extreme cases over 100. This compares with other jet propulsion engines that can exceed 5 for some of the better engines.

It can be shown that the net thrust of a rocket is:

where:

The effective exhaust velocity is more or less the speed the exhaust leaves the vehicle, and in the vacuum of space, the effective exhaust velocity is often equal to the actual average exhaust speed along the thrust axis. However, the effective exhaust velocity allows for various losses, and notably, is reduced when operated within an atmosphere.

The rate of propellant flow through a rocket engine is often deliberately varied over a flight, to provide a way to control the thrust and thus the airspeed of the vehicle. This, for example, allows minimization of aerodynamic losses and can limit the increase of g-forces due to the reduction in propellant load.

Impulse is defined as a force acting on an object over time, which in the absence of opposing forces (gravity and aerodynamic drag), changes the momentum (integral of mass and velocity) of the object. As such, it is the best performance class (payload mass and terminal velocity capability) indicator of a rocket, rather than takeoff thrust, mass, or "power". The total impulse of a rocket (stage) burning its propellant is:

When there is fixed thrust, this is simply:

The total impulse of a multi-stage rocket is the sum of the impulses of the individual stages.

As can be seen from the thrust equation, the effective speed of the exhaust controls the amount of thrust produced from a particular quantity of fuel burnt per second.

An equivalent measure, the net impulse per weight unit of propellant expelled, is called specific Impulse, , and this is one of the most important figures that describes a rocket's performance. It is defined such that it is related to the effective exhaust velocity by:

where:

Thus, the greater the specific impulse, the greater the net thrust and performance of the engine. is determined by measurement while testing the engine. In practice the effective exhaust velocities of rockets varies but can be extremely high, ~4500 m/s, about 15 times the sea level speed of sound in air.

The delta-v capacity of a rocket is the theoretical total change in velocity that a rocket can achieve without any external interference (without air drag or gravity or other forces).

When is constant, the delta-v that a rocket vehicle can provide can be calculated from the Tsiolkovsky rocket equation:

where:

When launched from the Earth practical delta-vs for a single rockets carrying payloads can be a few km/s. Some theoretical designs have rockets with delta-vs over 9 km/s.

The required delta-v can also be calculated for a particular manoeuvre; for example the delta-v to launch from the surface of the Earth to low Earth orbit is about 9.7 km/s, which leaves the vehicle with a sideways speed of about 7.8 km/s at an altitude of around 200 km. In this manoeuvre about 1.9 km/s is lost in air drag, gravity drag and gaining altitude.

The ratio is sometimes called the mass ratio.

Almost all of a launch vehicle's mass consists of propellant. Mass ratio is, for any 'burn', the ratio between the rocket's initial mass and its final mass. Everything else being equal, a high mass ratio is desirable for good performance, since it indicates that the rocket is lightweight and hence performs better, for essentially the same reasons that low weight is desirable in sports cars.

Rockets as a group have the highest thrust-to-weight ratio of any type of engine; and this helps vehicles achieve high mass ratios, which improves the performance of flights. The higher the ratio, the less engine mass is needed to be carried. This permits the carrying of even more propellant, enormously improving the delta-v. Alternatively, some rockets such as for rescue scenarios or racing carry relatively little propellant and payload and thus need only a lightweight structure and instead achieve high accelerations. For example, the Soyuz escape system can produce 20 g.

Achievable mass ratios are highly dependent on many factors such as propellant type, the design of engine the vehicle uses, structural safety margins and construction techniques.

The highest mass ratios are generally achieved with liquid rockets, and these types are usually used for orbital launch vehicles, a situation which calls for a high delta-v. Liquid propellants generally have densities similar to water (with the notable exceptions of liquid hydrogen and liquid methane), and these types are able to use lightweight, low pressure tanks and typically run high-performance turbopumps to force the propellant into the combustion chamber.

Some notable mass fractions are found in the following table (some aircraft are included for comparison purposes):

Thus far, the required velocity (delta-v) to achieve orbit has been unattained by any single rocket because the propellant, tankage, structure, guidance, valves and engines and so on, take a particular minimum percentage of take-off mass that is too great for the propellant it carries to achieve that delta-v carrying reasonable payloads. Since Single-stage-to-orbit has so far not been achievable, orbital rockets always have more than one stage.

For example, the first stage of the Saturn V, carrying the weight of the upper stages, was able to achieve a mass ratio of about 10, and achieved a specific impulse of 263 seconds. This gives a delta-v of around 5.9 km/s whereas around 9.4 km/s delta-v is needed to achieve orbit with all losses allowed for.

This problem is frequently solved by staging'97the rocket sheds excess weight (usually empty tankage and associated engines) during launch. Staging is either serial where the rockets light after the previous stage has fallen away, or parallel, where rockets are burning together and then detach when they burn out.

The maximum speeds that can be achieved with staging is theoretically limited only by the speed of light. However the payload that can be carried goes down geometrically with each extra stage needed, while the additional delta-v for each stage is simply additive.

From Newton's second law, the acceleration, , of a vehicle is simply:

where m is the instantaneous mass of the vehicle and is the net force acting on the rocket (mostly thrust, but air drag and other forces can play a part).

As the remaining propellant decreases, rocket vehicles become lighter and their acceleration tends to increase until the propellant is exhausted. This means that much of the speed change occurs towards the end of the burn when the vehicle is much lighter. However, the thrust can be throttled to offset or vary this if needed. Discontinuities in acceleration also occur when stages burn out, often starting at a lower acceleration with each new stage firing.

Peak accelerations can be increased by designing the vehicle with a reduced mass, usually achieved by a reduction in the fuel load and tankage and associated structures, but obviously this reduces range, delta-v and burn time. Still, for some applications that rockets are used for, a high peak acceleration applied for just a short time is highly desirable.

The minimal mass of vehicle consists of a rocket engine with minimal fuel and structure to carry it. In that case the thrust-to-weight ratio of the rocket engine limits the maximum acceleration that can be designed. It turns out that rocket engines generally have truly excellent thrust to weight ratios (137 for the NK-33 engine; some solid rockets are over 1000), and nearly all really high-g vehicles employ or have employed rockets.

The high accelerations that rockets naturally possess means that rocket vehicles are often capable of vertical takeoff, and in some cases, with suitable guidance and control of the engines, also vertical landing. For these operations to be done it is necessary for a vehicle's engines to provide more than the local gravitational acceleration.

The energy density of a typical rocket propellant is often around one-third that of conventional hydrocarbon fuels; the bulk of the mass is (often relatively inexpensive) oxidizer. Nevertheless, at take-off the rocket has a great deal of energy in the fuel and oxidizer stored within the vehicle. It is of course desirable that as much of the energy of the propellant end up as kinetic or potential energy of the body of the rocket as possible.

Energy from the fuel is lost in air drag and gravity drag and is used for the rocket to gain altitude and speed. However, much of the lost energy ends up in the exhaust.

In a chemical propulsion device, the engine efficiency is simply the ratio of the kinetic power of the exhaust gases and the power available from the chemical reaction:

100% efficiency within the engine (engine efficiency ) would mean that all the heat energy of the combustion products is converted into kinetic energy of the jet. This is not possible, but the near-adiabatic high expansion ratio nozzles that can be used with rockets come surprisingly close: when the nozzle expands the gas, the gas is cooled and accelerated, and an energy efficiency of up to 70% can be achieved. Most of the rest is heat energy in the exhaust that is not recovered. The high efficiency is a consequence of the fact that rocket combustion can be performed at very high temperatures and the gas is finally released at much lower temperatures, and so giving good Carnot efficiency.

However, engine efficiency is not the whole story. In common with the other jet-based engines, but particularly in rockets due to their high and typically fixed exhaust speeds, rocket vehicles are extremely inefficient at low speeds irrespective of the engine efficiency. The problem is that at low speeds, the exhaust carries away a huge amount of kinetic energy rearward. This phenomenon is termed propulsive efficiency ().

However, as speeds rise, the resultant exhaust speed goes down, and the overall vehicle energetic efficiency rises, reaching a peak of around 100% of the engine efficiency when the vehicle is travelling exactly at the same speed that the exhaust is emitted. In this case the exhaust would ideally stop dead in space behind the moving vehicle, taking away zero energy, and from conservation of energy, all the energy would end up in the vehicle. The efficiency then drops off again at even higher speeds as the exhaust ends up traveling forwards '96 trailing behind the vehicle.

From these principles it can be shown that the propulsive efficiency for a rocket moving at speed with an exhaust velocity is:

And the overall (instantaneous) energy efficiency is:

For example, from the equation, with an of 0.7, a rocket flying at Mach 0.85 (which most aircraft cruise at) with an exhaust velocity of Mach 10, would have a predicted overall energy efficiency of 5.9%, whereas a conventional, modern, air-breathing jet engine achieves closer to 35% efficiency. Thus a rocket would need about 6x more energy; and allowing for the specific energy of rocket propellant being around one third that of conventional air fuel, roughly 18x more mass of propellant would need to be carried for the same journey. This is why rockets are rarely if ever used for general aviation.

Since the energy ultimately comes from fuel, these considerations mean that rockets are mainly useful when a very high speed is required, such as ICBMs or orbital launch. For example, NASA's Space Shuttle fired its engines for around 8.5 minutes, consuming 1,000 tonnes of solid propellant (containing 16% aluminium) and an additional 2,000,000 litres of liquid propellant (106,261 kg of liquid hydrogen fuel) to lift the 100,000 kg vehicle (including the 25,000 kg payload) to an altitude of 111 km and an orbital velocity of 30,000 km/h. At this altitude and velocity, the vehicle had a kinetic energy of about 3 TJ and a potential energy of roughly 200 GJ. Given the initial energy of 20 TJ, the Space Shuttle was about 16% energy efficient at launching the orbiter.

Thus jet engines, with a better match between speed and jet exhaust speed (such as turbofans'97in spite of their worse )'97dominate for subsonic and supersonic atmospheric use, while rockets work best at hypersonic speeds. On the other hand, rockets serve in many short-range relatively low speed military applications where their low-speed inefficiency is outweighed by their extremely high thrust and hence high accelerations.

One subtle feature of rockets relates to energy. A rocket stage, while carrying a given load, is capable of giving a particular delta-v. This delta-v means that the speed increases (or decreases) by a particular amount, independent of the initial speed. However, because kinetic energy is a square law on speed, this means that the faster the rocket is travelling before the burn the more orbital energy it gains or loses.

This fact is used in interplanetary travel. It means that the amount of delta-v to reach other planets, over and above that to reach escape velocity can be much less if the delta-v is applied when the rocket is travelling at high speeds, close to the Earth or other planetary surface; whereas waiting until the rocket has slowed at altitude multiplies up the effort required to achieve the desired trajectory.

The reliability of rockets, as for all physical systems, is dependent on the quality of engineering design and construction.

Because of the enormous chemical energy in rocket propellants (greater energy by weight than explosives, but lower than gasoline), consequences of accidents can be severe. Most space missions have some problems. In 1986, following the Space Shuttle Challenger disaster, American physicist Richard Feynman, having served on the Rogers Commission, estimated that the chance of an unsafe condition for a launch of the Shuttle was very roughly 1%; more recently the historical per person-flight risk in orbital spaceflight has been calculated to be around 2% or 4%.

In May, 2003 the astronaut office made clear its position on the need and feasibility of improving crew safety for future NASA crewed missions indicating their "consensus that an order of magnitude reduction in the risk of human life during ascent, compared to the Space Shuttle, is both achievable with current technology and consistent with NASA's focus on steadily improving rocket reliability".

The costs of rockets can be roughly divided into propellant costs, the costs of obtaining and/or producing the 'dry mass' of the rocket, and the costs of any required support equipment and facilities.

Most of the takeoff mass of a rocket is normally propellant. However propellant is seldom more than a few times more expensive than gasoline per kilogram (as of 2009 gasoline was about $1/kg [$0.45/lb] or less), and although substantial amounts are needed, for all but the very cheapest rockets, it turns out that the propellant costs are usually comparatively small, although not completely negligible. With liquid oxygen costing $0.15 per kilogram ($0.068/lb) and liquid hydrogen $2.20/kg ($1.00/lb), the Space Shuttle in 2009 had a liquid propellant expense of approximately $1.4 million for each launch that cost $450 million from other expenses (with 40% of the mass of propellants used by it being liquids in the external fuel tank, 60% solids in the SRBs).

Even though a rocket's non-propellant, dry mass is often only between 5'9620% of total mass, nevertheless this cost dominates. For hardware with the performance used in orbital launch vehicles, expenses of $2000'96$10,000+ per kilogram of dry weight are common, primarily from engineering, fabrication, and testing; raw materials amount to typically around 2% of total expense. For most rockets except reusable ones (shuttle engines) the engines need not function more than a few minutes, which simplifies design.

Extreme performance requirements for rockets reaching orbit correlate with high cost, including intensive quality control to ensure reliability despite the limited safety factors allowable for weight reasons. Components produced in small numbers if not individually machined can prevent amortization of R&D and facility costs over mass production to the degree seen in more pedestrian manufacturing. Amongst liquid-fueled rockets, complexity can be influenced by how much hardware must be lightweight, like pressure-fed engines can have two orders of magnitude lesser part count than pump-fed engines but lead to more weight by needing greater tank pressure, most often used in just small maneuvering thrusters as a consequence.

To change the preceding factors for orbital launch vehicles, proposed methods have included mass-producing simple rockets in large quantities or on large scale, or developing reusable rockets meant to fly very frequently to amortize their up-front expense over many payloads, or reducing rocket performance requirements by constructing a non-rocket spacelaunch system for part of the velocity to orbit (or all of it but with most methods involving some rocket use).

The costs of support equipment, range costs and launch pads generally scale up with the size of the rocket, but vary less with launch rate, and so may be considered to be approximately a fixed cost.

Rockets in applications other than launch to orbit (such as military rockets and rocket-assisted take off), commonly not needing comparable performance and sometimes mass-produced, are often relatively inexpensive.

Since the early 2010s, new private options for obtaining spaceflight services emerged, bringing substantial price pressure into the existing market.

A spacecraft is a vehicle or machine designed to fly in outer space. A type of artificial satellite, spacecraft are used for a variety of purposes, including communications, Earth observation, meteorology, navigation, space colonization, planetary exploration, and transportation of humans and cargo. All spacecraft except single-stage-to-orbit vehicles cannot get into space on their own, and require a launch vehicle (carrier rocket).

On a sub-orbital spaceflight, a space vehicle enters space and then returns to the surface without having gained sufficient energy or velocity to make a full Earth orbit. For orbital spaceflights, spacecraft enter closed orbits around the Earth or around other celestial bodies. Spacecraft used for human spaceflight carry people on board as crew or passengers from start or on orbit (space stations) only, whereas those used for robotic space missions operate either autonomously or telerobotically. Robotic spacecraft used to support scientific research are space probes. Robotic spacecraft that remain in orbit around a planetary body are artificial satellites. To date, only a handful of interstellar probes, such as Pioneer 10 and 11, Voyager 1 and 2, and New Horizons, are on trajectories that leave the Solar System.

Orbital spacecraft may be recoverable or not. Most are not. Recoverable spacecraft may be subdivided by a method of reentry to Earth into non-winged space capsules and winged spaceplanes. Recoverable spacecraft may be reusable (can be launched again or several times, like the SpaceX Dragon and the Space Shuttle orbiters) or expendable (like the Soyuz). In recent years, more space agencies are tending towards reusable spacecraft.

Humanity has achieved space flight, but only a few nations have the technology for orbital launches: Russia (RSA or "Roscosmos"), the United States (NASA), the member states of the European Space Agency (ESA), Japan (JAXA), China (CNSA), India (ISRO), Taiwan (National Chung-Shan Institute of Science and Technology, Taiwan National Space Organization (NSPO), Israel (ISA), Iran (ISA), and North Korea (NADA). In addition, several private companies have developed or are developing the technology for orbital launches independently from government agencies. The most prominent examples of such companies are SpaceX and Blue Origin.

A German V-2 became the first spacecraft when it reached an altitude of 189 km in June 1944 in Peenem'fcnde, Germany. Sputnik 1 was the first artificial satellite. It was launched into an elliptical low Earth orbit (LEO) by the Soviet Union on 4 October 1957. The launch ushered in new political, military, technological, and scientific developments; while the Sputnik launch was a single event, it marked the start of the Space Age. Apart from its value as a technological first, Sputnik 1 also helped to identify the upper atmospheric layer's density, through measuring the satellite's orbital changes. It also provided data on radio-signal distribution in the ionosphere. Pressurized nitrogen in the satellite's false body provided the first opportunity for meteoroid detection. Sputnik 1 was launched during the International Geophysical Year from Site No.1/5, at the 5th Tyuratam range, in Kazakh SSR (now at the Baikonur Cosmodrome). The satellite traveled at 29,000 kilometres per hour (18,000 mph), taking 96.2 minutes to complete an orbit, and emitted radio signals at 20.005 and 40.002 MHz

While Sputnik 1 was the first spacecraft to orbit the Earth, other man-made objects had previously reached an altitude of 100 km, which is the height required by the international organization F'e9d'e9ration A'e9ronautique Internationale to count as a spaceflight. This altitude is called the K'e1rm'e1n line. In particular, in the 1940s there were several test launches of the V-2 rocket, some of which reached altitudes well over 100 km.

As of 2016, only three nations have flown crewed spacecraft: USSR/Russia, USA, and China. The first crewed spacecraft was Vostok 1, which carried Soviet cosmonaut Yuri Gagarin into space in 1961, and completed a full Earth orbit. There were five other crewed missions which used a Vostok spacecraft. The second crewed spacecraft was named Freedom 7, and it performed a sub-orbital spaceflight in 1961 carrying American astronaut Alan Shepard to an altitude of just over 187 kilometers (116 mi). There were five other crewed missions using Mercury spacecraft.

Other Soviet crewed spacecraft include the Voskhod, Soyuz, flown uncrewed as Zond/L1, L3, TKS, and the Salyut and Mir crewed space stations. Other American crewed spacecraft include the Gemini spacecraft, the Apollo spacecraft including the Apollo Lunar Module, the Skylab space station, the Space Shuttle with undetached European Spacelab and private US Spacehab space stations-modules, and the SpaceX Crew Dragon configuration of their Dragon 2. US company Boeing also developed and flown a spacecraft of their own, the CST-100, commonly referred to as Starliner, but a crewed flight is yet to occur. China developed, but did not fly Shuguang, and is currently using Shenzhou (its first crewed mission was in 2003).

Except for the Space Shuttle, all of the recoverable crewed orbital spacecraft were space capsules.

The International Space Station, crewed since November 2000, is a joint venture between Russia, the United States, Canada and several other countries.

Spaceplanes are spacecraft are built in the shape of, and function as, airplanes. The first example of such was the North American X-15 spaceplane, which conducted two crewed flights which reached an altitude of over 100 km in the 1960s. This first reusable spacecraft was air-launched on a suborbital trajectory on July 19, 1963.

The first partially reusable orbital spacecraft, a winged non-capsule, the Space Shuttle, was launched by the USA on the 20th anniversary of Yuri Gagarin's flight, on April 12, 1981. During the Shuttle era, six orbiters were built, all of which have flown in the atmosphere and five of which have flown in space. Enterprise was used only for approach and landing tests, launching from the back of a Boeing 747 SCA and gliding to deadstick landings at Edwards AFB, California. The first Space Shuttle to fly into space was Columbia, followed by Challenger, Discovery, Atlantis, and Endeavour. Endeavour was built to replace Challenger when it was lost in January 1986. Columbia broke up during reentry in February 2003.

The first automatic partially reusable spacecraft was the Buran-class shuttle, launched by the USSR on November 15, 1988, although it made only one flight and this was uncrewed. This spaceplane was designed for a crew and strongly resembled the U.S. Space Shuttle, although its drop-off boosters used liquid propellants and its main engines were located at the base of what would be the external tank in the American Shuttle. Lack of funding, complicated by the dissolution of the USSR, prevented any further flights of Buran. The Space Shuttle was subsequently modified to allow for autonomous re-entry in case of necessity.

Per the Vision for Space Exploration, the Space Shuttle was retired in 2011 mainly due to its old age and high cost of program reaching over a billion dollars per flight. The Shuttle's human transport role is to be replaced by SpaceX's SpaceX Dragon 2 and Boeing's CST-100 Starliner. Dragon 2's first crewed flight occurred on May 30, 2020. The Shuttle's heavy cargo transport role is to be replaced by expendable rockets such as the Space Launch System and ULA's Vulcan rocket, as well as the commercial launch vehicles.

Scaled Composites' SpaceShipOne was a reusable suborbital spaceplane that carried pilots Mike Melvill and Brian Binnie on consecutive flights in 2004 to win the Ansari X Prize. The Spaceship Company will build its successor SpaceShipTwo. A fleet of SpaceShipTwos operated by Virgin Galactic was planned to begin reusable private spaceflight carrying paying passengers in 2014, but was delayed after the crash of VSS Enterprise.

A spacecraft astrionics system comprises different subsystems, depending on the mission profile. Spacecraft subsystems comprise the spacecraft's "bus" and may include attitude determination and control (variously called ADAC, ADC, or ACS), guidance, navigation and control (GNC or GN&C), communications (comms), command and data handling (CDH or C&DH), power (EPS), thermal control (TCS), propulsion, and structures. Attached to the bus are typically payloads.

Life support
Spacecraft intended for human spaceflight must also include a life support system for the crew.
Attitude control
A Spacecraft needs an attitude control subsystem to be correctly oriented in space and respond to external torques and forces properly. The attitude control subsystem consists of sensors and actuators, together with controlling algorithms. The attitude-control subsystem permits proper pointing for the science objective, sun pointing for power to the solar arrays and earth pointing for communications.
GNC
Guidance refers to the calculation of the commands (usually done by the CDH subsystem) needed to steer the spacecraft where it is desired to be. Navigation means determining a spacecraft's orbital elements or position. Control means adjusting the path of the spacecraft to meet mission requirements.
Command and data handling
The C&DH subsystem receives commands from the communications subsystem, performs validation and decoding of the commands, and distributes the commands to the appropriate spacecraft subsystems and components. The CDH also receives housekeeping data and science data from the other spacecraft subsystems and components, and packages the data for storage on a data recorder or transmission to the ground via the communications subsystem. Other functions of the CDH include maintaining the spacecraft clock and state-of-health monitoring.
Communications
Spacecraft, both robotic and crewed, utilize various communications systems for communication with terrestrial stations as well as for communication between spacecraft in space. Technologies utilized include RF and optical communication. In addition, some spacecraft payloads are explicitly for the purpose of ground'96ground communication using receiver/retransmitter electronic technologies.
Power
Spacecraft need an electrical power generation and distribution subsystem for powering the various spacecraft subsystems. For spacecraft near the Sun, solar panels are frequently used to generate electrical power. Spacecraft designed to operate in more distant locations, for example Jupiter, might employ a radioisotope thermoelectric generator (RTG) to generate electrical power. Electrical power is sent through power conditioning equipment before it passes through a power distribution unit over an electrical bus to other spacecraft components. Batteries are typically connected to the bus via a battery charge regulator, and the batteries are used to provide electrical power during periods when primary power is not available, for example when a low Earth orbit spacecraft is eclipsed by Earth.
Thermal control
Spacecraft must be engineered to withstand transit through Earth's atmosphere and the space environment. They must operate in a vacuum with temperatures potentially ranging across hundreds of degrees Celsius as well as (if subject to reentry) in the presence of plasmas. Material requirements are such that either high melting temperature, low density materials such as beryllium and reinforced carbon'96carbon or (possibly due to the lower thickness requirements despite its high density) tungsten or ablative carbon'96carbon composites are used. Depending on mission profile, spacecraft may also need to operate on the surface of another planetary body. The thermal control subsystem can be passive, dependent on the selection of materials with specific radiative properties. Active thermal control makes use of electrical heaters and certain actuators such as louvers to control temperature ranges of equipments within specific ranges.
Spacecraft propulsion
Spacecraft may or may not have a propulsion subsystem, depending on whether or not the mission profile calls for propulsion. The Swift spacecraft is an example of a spacecraft that does not have a propulsion subsystem. Typically though, LEO spacecraft include a propulsion subsystem for altitude adjustments (drag make-up maneuvers) and inclination adjustment maneuvers. A propulsion system is also needed for spacecraft that perform momentum management maneuvers. Components of a conventional propulsion subsystem include fuel, tankage, valves, pipes, and thrusters. The thermal control system interfaces with the propulsion subsystem by monitoring the temperature of those components, and by preheating tanks and thrusters in preparation for a spacecraft maneuver.
Structures
Spacecraft must be engineered to withstand launch loads imparted by the launch vehicle, and must have a point of attachment for all the other subsystems. Depending on mission profile, the structural subsystem might need to withstand loads imparted by entry into the atmosphere of another planetary body, and landing on the surface of another planetary body.
Payload
The payload depends on the mission of the spacecraft, and is typically regarded as the part of the spacecraft "that pays the bills". Typical payloads could include scientific instruments (cameras, telescopes, or particle detectors, for example), cargo, or a human crew.
Ground segment
The ground segment, though not technically part of the spacecraft, is vital to the operation of the spacecraft. Typical components of a ground segment in use during normal operations include a mission operations facility where the flight operations team conducts the operations of the spacecraft, a data processing and storage facility, ground stations to radiate signals to and receive signals from the spacecraft, and a voice and data communications network to connect all mission elements.
Launch vehicle
The launch vehicle propels the spacecraft from Earth's surface, through the atmosphere, and into an orbit, the exact orbit being dependent on the mission configuration. The launch vehicle may be expendable or reusable.
An aircraft is a vehicle or machine that is able to fly by gaining support from the air. It counters the force of gravity by using either static lift or by using the dynamic lift of an airfoil, or in a few cases the downward thrust from jet engines. Common examples of aircraft include airplanes, helicopters, airships (including blimps), gliders, paramotors, and hot air balloons.

The human activity that surrounds aircraft is called aviation. The science of aviation, including designing and building aircraft, is called aeronautics. Crewed aircraft are flown by an onboard pilot, but unmanned aerial vehicles may be remotely controlled or self-controlled by onboard computers. Aircraft may be classified by different criteria, such as lift type, aircraft propulsion, usage and others.

Flying model craft and stories of manned flight go back many centuries; however, the first manned ascent '97 and safe descent '97 in modern times took place by larger hot-air balloons developed in the 18th century. Each of the two World Wars led to great technical advances. Consequently, the history of aircraft can be divided into five eras:

Pioneers of flight, from the earliest experiments to 1914.
First World War, 1914 to 1918.
Aviation between the World Wars, 1918 to 1939.
Second World War, 1939 to 1945.
Postwar era, also called the Jet Age, 1945 to the present day.
Aerostats use buoyancy to float in the air in much the same way that ships float on the water. They are characterized by one or more large cells or canopies, filled with a relatively low-density gas such as helium, hydrogen, or hot air, which is less dense than the surrounding air. When the weight of this is added to the weight of the aircraft structure, it adds up to the same weight as the air that the craft displaces.

Small hot-air balloons, called sky lanterns, were first invented in ancient China prior to the 3rd century BC and used primarily in cultural celebrations, and were only the second type of aircraft to fly, the first being kites, which were first invented in ancient China over two thousand years ago. (See Han Dynasty)

A balloon was originally any aerostat, while the term airship was used for large, powered aircraft designs '97 usually fixed-wing. In 1919, Frederick Handley Page was reported as referring to "ships of the air," with smaller passenger types as "Air yachts." In the 1930s, large intercontinental flying boats were also sometimes referred to as "ships of the air" or "flying-ships". '97 though none had yet been built. The advent of powered balloons, called dirigible balloons, and later of rigid hulls allowing a great increase in size, began to change the way these words were used. Huge powered aerostats, characterized by a rigid outer framework and separate aerodynamic skin surrounding the gas bags, were produced, the Zeppelins being the largest and most famous. There were still no fixed-wing aircraft or non-rigid balloons large enough to be called airships, so "airship" came to be synonymous with these aircraft. Then several accidents, such as the Hindenburg disaster in 1937, led to the demise of these airships. Nowadays a "balloon" is an unpowered aerostat and an "airship" is a powered one.

A powered, steerable aerostat is called a dirigible. Sometimes this term is applied only to non-rigid balloons, and sometimes dirigible balloon is regarded as the definition of an airship (which may then be rigid or non-rigid). Non-rigid dirigibles are characterized by a moderately aerodynamic gasbag with stabilizing fins at the back. These soon became known as blimps. During World War II, this shape was widely adopted for tethered balloons; in windy weather, this both reduces the strain on the tether and stabilizes the balloon. The nickname blimp was adopted along with the shape. In modern times, any small dirigible or airship is called a blimp, though a blimp may be unpowered as well as powered.

Heavier-than-air aircraft, such as airplanes, must find some way to push air or gas downwards so that a reaction occurs (by Newton's laws of motion) to push the aircraft upwards. This dynamic movement through the air is the origin of the term. There are two ways to produce dynamic upthrust '97 aerodynamic lift, and powered lift in the form of engine thrust.

Aerodynamic lift involving wings is the most common, with fixed-wing aircraft being kept in the air by the forward movement of wings, and rotorcraft by spinning wing-shaped rotors sometimes called rotary wings. A wing is a flat, horizontal surface, usually shaped in cross-section as an aerofoil. To fly, air must flow over the wing and generate lift. A flexible wing is a wing made of fabric or thin sheet material, often stretched over a rigid frame. A kite is tethered to the ground and relies on the speed of the wind over its wings, which may be flexible or rigid, fixed, or rotary.

With powered lift, the aircraft directs its engine thrust vertically downward. V/STOL aircraft, such as the Harrier Jump Jet and Lockheed Martin F-35B take off and land vertically using powered lift and transfer to aerodynamic lift in steady flight.

A pure rocket is not usually regarded as an aerodyne because it does not depend on the air for its lift (and can even fly into space); however, many aerodynamic lift vehicles have been powered or assisted by rocket motors. Rocket-powered missiles that obtain aerodynamic lift at very high speed due to airflow over their bodies are a marginal case.

The forerunner of the fixed-wing aircraft is the kite. Whereas a fixed-wing aircraft relies on its forward speed to create airflow over the wings, a kite is tethered to the ground and relies on the wind blowing over its wings to provide lift. Kites were the first kind of aircraft to fly and were invented in China around 500 BC. Much aerodynamic research was done with kites before test aircraft, wind tunnels, and computer modelling programs became available.

The first heavier-than-air craft capable of controlled free-flight were gliders. A glider designed by George Cayley carried out the first true manned, controlled flight in 1853.

The practical, powered, fixed-wing aircraft (the airplane or aeroplane) was invented by Wilbur and Orville Wright. Besides the method of propulsion, fixed-wing aircraft are in general characterized by their wing configuration. The most important wing characteristics are:

Number of wings '97 monoplane, biplane, etc.
Wing support '97 Braced or cantilever, rigid, or flexible.
Wing planform '97 including aspect ratio, angle of sweep, and any variations along the span (including the important class of delta wings).
Location of the horizontal stabilizer, if any.
Dihedral angle '97 positive, zero, or negative (anhedral).
A variable geometry aircraft can change its wing configuration during flight.

A flying wing has no fuselage, though it may have small blisters or pods. The opposite of this is a lifting body, which has no wings, though it may have small stabilizing and control surfaces.

Wing-in-ground-effect vehicles are generally not considered aircraft. They "fly" efficiently close to the surface of the ground or water, like conventional aircraft during takeoff. An example is the Russian ekranoplan nicknamed the "Caspian Sea Monster". Man-powered aircraft also rely on ground effect to remain airborne with minimal pilot power, but this is only because they are so underpowered'97in fact, the airframe is capable of flying higher.

Rotorcraft, or rotary-wing aircraft, use a spinning rotor with aerofoil section blades (a rotary wing) to provide lift. Types include helicopters, autogyros, and various hybrids such as gyrodynes and compound rotorcraft.

Helicopters have a rotor turned by an engine-driven shaft. The rotor pushes air downward to create lift. By tilting the rotor forward, the downward flow is tilted backward, producing thrust for forward flight. Some helicopters have more than one rotor and a few have rotors turned by gas jets at the tips.

Autogyros have unpowered rotors, with a separate power plant to provide thrust. The rotor is tilted backward. As the autogyro moves forward, air blows upward across the rotor, making it spin. This spinning increases the speed of airflow over the rotor, to provide lift. Rotor kites are unpowered autogyros, which are towed to give them forward speed or tethered to a static anchor in high-wind for kited flight.

Cyclogyros rotate their wings about a horizontal axis.

Compound rotorcraft have wings that provide some or all of the lift in forward flight. They are nowadays classified as powered lift types and not as rotorcraft. Tiltrotor aircraft (such as the Bell Boeing V-22 Osprey), tiltwing, tail-sitter, and coleopter aircraft have their rotors/propellers horizontal for vertical flight and vertical for forward flight.

A lifting body is an aircraft body shaped to produce lift. If there are any wings, they are too small to provide significant lift and are used only for stability and control. Lifting bodies are not efficient: they suffer from high drag, and must also travel at high speed to generate enough lift to fly. Many of the research prototypes, such as the Martin Marietta X-24, which led up to the Space Shuttle, were lifting bodies, though the Space Shuttle is not, and some supersonic missiles obtain lift from the airflow over a tubular body.
Powered lift types rely on engine-derived lift for vertical takeoff and landing (VTOL). Most types transition to fixed-wing lift for horizontal flight. Classes of powered lift types include VTOL jet aircraft (such as the Harrier Jump Jet) and tiltrotors, such as the Bell Boeing V-22 Osprey, among others. A few experimental designs rely entirely on engine thrust to provide lift throughout the whole flight, including personal fan-lift hover platforms and jetpacks. VTOL research designs include the Rolls-Royce Thrust Measuring Rig.
The Flettner airplane uses a rotating cylinder in place of a fixed wing, obtaining lift from the Magnus effect.
The ornithopter obtains thrust by flapping its wings.
The smallest aircraft are toys/recreational items, and nano aircraft.

The largest aircraft by dimensions and volume (as of 2016) is the 302 ft (92 m) long British Airlander 10, a hybrid blimp, with helicopter and fixed-wing features, and reportedly capable of speeds up to 90 mph (140 km/h; 78 kn), and an airborne endurance of two weeks with a payload of up to 22,050 lb (10,000 kg).

The largest aircraft by weight and largest regular fixed-wing aircraft ever built, as of 2016, is the Antonov An-225 Mriya. That Ukrainian-built six-engine Russian transport of the 1980s is 84 m (276 ft) long, with an 88 m (289 ft) wingspan. It holds the world payload record, after transporting 428,834 lb (194,516 kg) of goods, and has recently flown 100 t (220,000 lb) loads commercially. With a maximum loaded weight of 550'96700 t (1,210,000'961,540,000 lb), it is also the heaviest aircraft built to date. It can cruise at 500 mph (800 km/h; 430 kn).

The largest military airplanes are the Ukrainian Antonov An-124 Ruslan (world's second-largest airplane, also used as a civilian transport), and American Lockheed C-5 Galaxy transport, weighing, loaded, over 380 t (840,000 lb). The 8-engine, piston/propeller Hughes H-4 Hercules "Spruce Goose" '97 an American World War II wooden flying boat transport with a greater wingspan (94m/260ft) than any current aircraft and a tail height equal to the tallest (Airbus A380-800 at 24.1m/78ft) '97 flew only one short hop in the late 1940s and never flew out of ground effect.

The largest civilian airplanes, apart from the above-noted An-225 and An-124, are the Airbus Beluga cargo transport derivative of the Airbus A300 jet airliner, the Boeing Dreamlifter cargo transport derivative of the Boeing 747 jet airliner/transport (the 747-200B was, at its creation in the 1960s, the heaviest aircraft ever built, with a maximum weight of over 400 t (880,000 lb)), and the double-decker Airbus A380 "super-jumbo" jet airliner (the world's largest passenger airliner).

The fastest recorded powered aircraft flight and fastest recorded aircraft flight of an air-breathing powered aircraft was of the NASA X-43A Pegasus, a scramjet-powered, hypersonic, lifting body experimental research aircraft, at Mach 9.6, exactly 3,292.8 m/s (11,854 km/h; 6,400.7 kn; 7,366 mph). The X-43A set that new mark, and broke its own world record of Mach 6.3, exactly 2,160.9 m/s (7,779 km/h; 4,200.5 kn; 4,834 mph), set in March 2004, on its third and final flight on 16 November 2004.

Prior to the X-43A, the fastest recorded powered airplane flight (and still the record for the fastest manned, powered airplane / fastest manned, non-spacecraft aircraft) was of the North American X-15A-2, rocket-powered airplane at Mach 6.72, or 2,304.96 m/s (8,297.9 km/h; 4,480.48 kn; 5,156.0 mph), on 3 October 1967. On one flight it reached an altitude of 354,300 ft (108,000 m).

The fastest known, production aircraft (other than rockets and missiles) currently or formerly operational (as of 2016) are:

The fastest fixed-wing aircraft, and fastest glider, is the Space Shuttle, a rocket-glider hybrid, which has re-entered the atmosphere as a fixed-wing glider at more than Mach 25, equal to 8,575 m/s (30,870 km/h; 16,668 kn; 19,180 mph).
The fastest military airplane ever built: Lockheed SR-71 Blackbird, a U.S. reconnaissance jet fixed-wing aircraft, known to fly beyond Mach 3.3, equal to 1,131.9 m/s (4,075 km/h; 2,200.2 kn; 2,532 mph). On 28 July 1976, an SR-71 set the record for the fastest and highest-flying operational aircraft with an absolute speed record of 2,193 mph (3,529 km/h; 1,906 kn; 980 m/s) and an absolute altitude record of 85,068 ft (25,929 m). At its retirement in January 1990, it was the fastest air-breathing aircraft / fastest jet aircraft in the world, a record still standing as of August 2016.
Note: Some sources refer to the above-mentioned X-15 as the "fastest military airplane" because it was partly a project of the U.S. Navy and Air Force; however, the X-15 was not used in non-experimental actual military operations.
The fastest current military aircraft are the Soviet/Russian Mikoyan-Gurevich MiG-25 '97 capable of Mach 3.2, equal to 1,097.6 m/s (3,951 km/h; 2,133.6 kn; 2,455 mph), at the expense of engine damage, or Mach 2.83, equal to 970.69 m/s (3,494.5 km/h; 1,886.87 kn; 2,171.4 mph), normally '97 and the Russian Mikoyan MiG-31E (also capable of Mach 2.83 normally). Both are fighter-interceptor jet airplanes, in active operations as of 2016.
The fastest civilian airplane ever built, and fastest passenger airliner ever built: the briefly operated Tupolev Tu-144 supersonic jet airliner (Mach 2.35, 1,600 mph, 2,587 km/h), which was believed to cruise at about Mach 2.2. The Tu-144 (officially operated from 1968 to 1978, ending after two crashes of the small fleet) was outlived by its rival, the Concorde (Mach 2.23), a French/British supersonic airliner, known to cruise at Mach 2.02 (1.450 mph, 2,333 kmh at cruising altitude), operating from 1976 until the small Concorde fleet was grounded permanently in 2003, following the crash of one in the early 2000s.
The fastest civilian airplane currently flying: the Cessna Citation X, an American business jet, capable of Mach 0.935, or 320.705 m/s (1,154.54 km/h; 623.401 kn; 717.40 mph). Its rival, the American Gulfstream G650 business jet, can reach Mach 0.925, or 317.275 m/s (1,142.19 km/h; 616.733 kn; 709.72 mph)
The fastest airliner currently flying is the Boeing 747, quoted as being capable of cruising over Mach 0.885, 303.555 m/s (1,092.80 km/h; 590.064 kn; 679.03 mph). Previously, the fastest were the troubled, short-lived Russian (Soviet Union) Tupolev Tu-144 SST (Mach 2.35; equal to 806.05 m/s (2,901.8 km/h; 1,566.84 kn; 1,803.1 mph)) and the French/British Concorde, with a maximum speed of Mach 2.23 or 686 m/s (2,470 km/h; 1,333 kn; 1,530 mph) and a normal cruising speed of Mach 2 or 320.705 m/s (1,154.54 km/h; 623.401 kn; 717.40 mph). Before them, the Convair 990 Coronado jet airliner of the 1960s flew at over 600 mph (970 km/h; 520 kn; 270 m/s).
Gliders are heavier-than-air aircraft that do not employ propulsion once airborne. Take-off may be by launching forward and downward from a high location, or by pulling into the air on a tow-line, either by a ground-based winch or vehicle, or by a powered "tug" aircraft. For a glider to maintain its forward air speed and lift, it must descend in relation to the air (but not necessarily in relation to the ground). Many gliders can "soar", i.e., gain height from updrafts such as thermal currents. The first practical, controllable example was designed and built by the British scientist and pioneer George Cayley, whom many recognise as the first aeronautical engineer. Common examples of gliders are sailplanes, hang gliders and paragliders.

Balloons drift with the wind, though normally the pilot can control the altitude, either by heating the air or by releasing ballast, giving some directional control (since the wind direction changes with altitude). A wing-shaped hybrid balloon can glide directionally when rising or falling; but a spherically shaped balloon does not have such directional control.

Kites are aircraft that are tethered to the ground or other object (fixed or mobile) that maintains tension in the tether or kite line; they rely on virtual or real wind blowing over and under them to generate lift and drag. Kytoons are balloon-kite hybrids that are shaped and tethered to obtain kiting deflections, and can be lighter-than-air, neutrally buoyant, or heavier-than-air.

Powered aircraft have one or more onboard sources of mechanical power, typically aircraft engines although rubber and manpower have also been used. Most aircraft engines are either lightweight reciprocating engines or gas turbines. Engine fuel is stored in tanks, usually in the wings but larger aircraft also have additional fuel tanks in the fuselage.

Propeller aircraft use one or more propellers (airscrews) to create thrust in a forward direction. The propeller is usually mounted in front of the power source in tractor configuration but can be mounted behind in pusher configuration. Variations of propeller layout include contra-rotating propellers and ducted fans.

Many kinds of power plant have been used to drive propellers. Early airships used man power or steam engines. The more practical internal combustion piston engine was used for virtually all fixed-wing aircraft until World War II and is still used in many smaller aircraft. Some types use turbine engines to drive a propeller in the form of a turboprop or propfan. Human-powered flight has been achieved, but has not become a practical means of transport. Unmanned aircraft and models have also used power sources such as electric motors and rubber bands.

Jet aircraft use airbreathing jet engines, which take in air, burn fuel with it in a combustion chamber, and accelerate the exhaust rearwards to provide thrust.

Different jet engine configurations include the turbojet and turbofan, sometimes with the addition of an afterburner. Those with no rotating turbomachinery include the pulsejet and ramjet. These mechanically simple engines produce no thrust when stationary, so the aircraft must be launched to flying speed using a catapult, like the V-1 flying bomb, or a rocket, for example. Other engine types include the motorjet and the dual-cycle Pratt & Whitney J58.

Compared to engines using propellers, jet engines can provide much higher thrust, higher speeds and, above about 40,000 ft (12,000 m), greater efficiency. They are also much more fuel-efficient than rockets. As a consequence nearly all large, high-speed or high-altitude aircraft use jet engines.

Some rotorcraft, such as helicopters, have a powered rotary wing or rotor, where the rotor disc can be angled slightly forward so that a proportion of its lift is directed forwards. The rotor may, like a propeller, be powered by a variety of methods such as a piston engine or turbine. Experiments have also used jet nozzles at the rotor blade tips.

Rocket-powered aircraft have occasionally been experimented with, and the Messerschmitt Me 163 Komet fighter even saw action in the Second World War. Since then, they have been restricted to research aircraft, such as the North American X-15, which traveled up into space where air-breathing engines cannot work (rockets carry their own oxidant). Rockets have more often been used as a supplement to the main power plant, typically for the rocket-assisted take off of heavily loaded aircraft, but also to provide high-speed dash capability in some hybrid designs such as the Saunders-Roe SR.53.
The ornithopter obtains thrust by flapping its wings. It has found practical use in a model hawk used to freeze prey animals into stillness so that they can be captured, and in toy birds.
Aircraft are designed according to many factors such as customer and manufacturer demand, safety protocols and physical and economic constraints. For many types of aircraft the design process is regulated by national airworthiness authorities.

The key parts of an aircraft are generally divided into three categories:

The structure comprises the main load-bearing elements and associated equipment.
The propulsion system (if it is powered) comprises the power source and associated equipment, as described above.
The avionics comprise the control, navigation and communication systems, usually electrical in nature.
The approach to structural design varies widely between different types of aircraft. Some, such as paragliders, comprise only flexible materials that act in tension and rely on aerodynamic pressure to hold their shape. A balloon similarly relies on internal gas pressure, but may have a rigid basket or gondola slung below it to carry its payload. Early aircraft, including airships, often employed flexible doped aircraft fabric covering to give a reasonably smooth aeroshell stretched over a rigid frame. Later aircraft employed semi-monocoque techniques, where the skin of the aircraft is stiff enough to share much of the flight loads. In a true monocoque design there is no internal structure left. With the recent emphasis on sustainability hemp has picked up some attention, having a way smaller carbon foot print and 10 times stronger than steel, hemp could become the standard of manufacturing in the future.

The key structural parts of an aircraft depend on what type it is.

Lighter-than-air types are characterised by one or more gasbags, typically with a supporting structure of flexible cables or a rigid framework called its hull. Other elements such as engines or a gondola may also be attached to the supporting structure.

Heavier-than-air types are characterised by one or more wings and a central fuselage. The fuselage typically also carries a tail or empennage for stability and control, and an undercarriage for takeoff and landing. Engines may be located on the fuselage or wings. On a fixed-wing aircraft the wings are rigidly attached to the fuselage, while on a rotorcraft the wings are attached to a rotating vertical shaft. Smaller designs sometimes use flexible materials for part or all of the structure, held in place either by a rigid frame or by air pressure. The fixed parts of the structure comprise the airframe.

The avionics comprise the aircraft flight control systems and related equipment, including the cockpit instrumentation, navigation, radar, monitoring, and communications systems.

The flight envelope of an aircraft refers to its approved design capabilities in terms of airspeed, load factor and altitude. The term can also refer to other assessments of aircraft performance such as maneuverability. When an aircraft is abused, for instance by diving it at too-high a speed, it is said to be flown outside the envelope, something considered foolhardy since it has been taken beyond the design limits which have been established by the manufacturer. Going beyond the envelope may have a known outcome such as flutter or entry to a non-recoverable spin (possible reasons for the boundary).

The range is the distance an aircraft can fly between takeoff and landing, as limited by the time it can remain airborne.

For a powered aircraft the time limit is determined by the fuel load and rate of consumption.

For an unpowered aircraft, the maximum flight time is limited by factors such as weather conditions and pilot endurance. Many aircraft types are restricted to daylight hours, while balloons are limited by their supply of lifting gas. The range can be seen as the average ground speed multiplied by the maximum time in the air.

The Airbus A350-900ULR is now the longest range airliner.

Flight dynamics with text.png
Flight dynamics is the science of air vehicle orientation and control in three dimensions. The three critical flight dynamics parameters are the angles of rotation around three axes which pass through the vehicle's center of gravity, known as pitch, roll, and yaw.

Roll is a rotation about the longitudinal axis (equivalent to the rolling or heeling of a ship) giving an up-down movement of the wing tips measured by the roll or bank angle.
Pitch is a rotation about the sideways horizontal axis giving an up-down movement of the aircraft nose measured by the angle of attack.
Yaw is a rotation about the vertical axis giving a side-to-side movement of the nose known as sideslip.
Flight dynamics is concerned with the stability and control of an aircraft's rotation about each of these axes.

An aircraft that is unstable tends to diverge from its intended flight path and so is difficult to fly. A very stable aircraft tends to stay on its flight path and is difficult to maneuver. Therefore, it is important for any design to achieve the desired degree of stability. Since the widespread use of digital computers, it is increasingly common for designs to be inherently unstable and rely on computerised control systems to provide artificial stability.

A fixed wing is typically unstable in pitch, roll, and yaw. Pitch and yaw stabilities of conventional fixed wing designs require horizontal and vertical stabilisers, which act similarly to the feathers on an arrow. These stabilizing surfaces allow equilibrium of aerodynamic forces and to stabilise the flight dynamics of pitch and yaw. They are usually mounted on the tail section (empennage), although in the canard layout, the main aft wing replaces the canard foreplane as pitch stabilizer. Tandem wing and tailless aircraft rely on the same general rule to achieve stability, the aft surface being the stabilising one.

A rotary wing is typically unstable in yaw, requiring a vertical stabiliser.

A balloon is typically very stable in pitch and roll due to the way the payload is slung underneath the center of lift.

Flight control surfaces enable the pilot to control an aircraft's flight attitude and are usually part of the wing or mounted on, or integral with, the associated stabilizing surface. Their development was a critical advance in the history of aircraft, which had until that point been uncontrollable in flight.

Aerospace engineers develop control systems for a vehicle's orientation (attitude) about its center of mass. The control systems include actuators, which exert forces in various directions, and generate rotational forces or moments about the aerodynamic center of the aircraft, and thus rotate the aircraft in pitch, roll, or yaw. For example, a pitching moment is a vertical force applied at a distance forward or aft from the aerodynamic center of the aircraft, causing the aircraft to pitch up or down. Control systems are also sometimes used to increase or decrease drag, for example to slow the aircraft to a safe speed for landing.

The two main aerodynamic forces acting on any aircraft are lift supporting it in the air and drag opposing its motion. Control surfaces or other techniques may also be used to affect these forces directly, without inducing any rotation.

Aircraft permit long distance, high speed travel and may be a more fuel efficient mode of transportation in some circumstances. Aircraft have environmental and climate impacts beyond fuel efficiency considerations, however. They are also relatively noisy compared to other forms of travel and high altitude aircraft generate contrails, which experimental evidence suggests may alter weather patterns.

Aircraft are produced in several different types optimized for various uses; military aircraft, which includes not just combat types but many types of supporting aircraft, and civil aircraft, which include all non-military types, experimental and model.

A military aircraft is any aircraft that is operated by a legal or insurrectionary armed service of any type. Military aircraft can be either combat or non-combat:

Combat aircraft are aircraft designed to destroy enemy equipment using its own armament. Combat aircraft divide broadly into fighters and bombers, with several in-between types, such as fighter-bombers and attack aircraft, including attack helicopters.
Non-combat aircraft are not designed for combat as their primary function, but may carry weapons for self-defense. Non-combat roles include search and rescue, reconnaissance, observation, transport, training, and aerial refueling. These aircraft are often variants of civil aircraft.
Most military aircraft are powered heavier-than-air types. Other types, such as gliders and balloons, have also been used as military aircraft; for example, balloons were used for observation during the American Civil War and World War I, and military gliders were used during World War II to land troops.

Civil aircraft divide into commercial and general types, however there are some overlaps.

Commercial aircraft include types designed for scheduled and charter airline flights, carrying passengers, mail and other cargo. The larger passenger-carrying types are the airliners, the largest of which are wide-body aircraft. Some of the smaller types are also used in general aviation, and some of the larger types are used as VIP aircraft.

General aviation is a catch-all covering other kinds of private (where the pilot is not paid for time or expenses) and commercial use, and involving a wide range of aircraft types such as business jets (bizjets), trainers, homebuilt, gliders, warbirds and hot air balloons to name a few. The vast majority of aircraft today are general aviation types.

An experimental aircraft is one that has not been fully proven in flight, or that carries a Special Airworthiness Certificate, called an Experimental Certificate in United States parlance. This often implies that the aircraft is testing new aerospace technologies, though the term also refers to amateur-built and kit-built aircraft, many of which are based on proven designs.

A model aircraft is a small unmanned type made to fly for fun, for static display, for aerodynamic research or for other purposes. A scale model is a replica of some larger design.

A vehicle (from Latin: vehiculum) is a machine that transports people or cargo. Vehicles include wagons, bicycles, motor vehicles (motorcycles, cars, trucks, buses), railed vehicles (trains, trams), watercraft (ships, boats, underwater vehicles), amphibious vehicles (screw-propelled vehicles, hovercraft), aircraft (airplanes, helicopters, aerostats) and spacecraft.

Land vehicles are classified broadly by what is used to apply steering and drive forces against the ground: wheeled, tracked, railed or skied. ISO 3833-1977 is the standard, also internationally used in legislation, for road vehicles types, terms and definitions.

The oldest boats found by archaeological excavation are logboats, with the oldest logboat found, the Pesse canoe found in a bog in the Netherlands, being carbon dated to 8040 - 7510 BC, making it 9,500'9610,000 years old,
a 7,000-year-old seagoing boat made from reeds and tar has been found in Kuwait.
Boats were used between 4000 -3000 BC in Sumer, ancient Egypt and in the Indian Ocean.
There is evidence of camel pulled wheeled vehicles about 4000'963000 BC.
The earliest evidence of a wagonway, a predecessor of the railway, found so far was the 6 to 8.5 km (4 to 5 mi) long Diolkos wagonway, which transported boats across the Isthmus of Corinth in Greece since around 600 BC. Wheeled vehicles pulled by men and animals ran in grooves in limestone, which provided the track element, preventing the wagons from leaving the intended route.
In 200 CE, Ma Jun built a south-pointing chariot, a vehicle with an early form of guidance system.
Railways began reappearing in Europe after the Dark Ages. The earliest known record of a railway in Europe from this period is a stained-glass window in the Minster of Freiburg im Breisgau dating from around 1350.
In 1515, Cardinal Matth'e4us Lang wrote a description of the Reisszug, a funicular railway at the Hohensalzburg Fortress in Austria. The line originally used wooden rails and a hemp haulage rope and was operated by human or animal power, through a treadwheel.
1769 Nicolas-Joseph Cugnot is often credited with building the first self-propelled mechanical vehicle or automobile in 1769.
In Russia, in the 1780s, Ivan Kulibin developed a human-pedalled, three-wheeled carriage with modern features such as a flywheel, brake, gear box and bearings; however, it was not developed further.
1783 Montgolfier brothers first balloon vehicle
1801 Richard Trevithick built and demonstrated his Puffing Devil road locomotive, which many believe was the first demonstration of a steam-powered road vehicle, though it could not maintain sufficient steam pressure for long periods and was of little practical use.
1817 Push bikes, draisines or hobby horses were the first human means of transport to make use of the two-wheeler principle, the draisienne (or Laufmaschine, "running machine"), invented by the German Baron Karl von Drais, is regarded as the forerunner of the modern bicycle (and motorcycle). It was introduced by Drais to the public in Mannheim in summer 1817.
1885 Karl Benz built (and subsequently patented) the first automobile, powered by his own four-stroke cycle gasoline engine in Mannheim, Germany
1885 Otto Lilienthal began experimental gliding and achieved the first sustained, controlled, reproducible flights.
1903 Wright brothers flew the first controlled, powered aircraft
1907 First helicopters Gyroplane no.1 (tethered) and Cornu helicopter (free flight)
1928 Opel RAK.1 rocket car
1929 Opel RAK.1 rocket glider
1961 Vostok vehicle carried the first human, Yuri Gagarin, into space
1969 Apollo Program first manned vehicle landed on the moon
2010 The number of road motor vehicles in operation worldwide surpassed the 1 billion mark '96 roughly one for every seven people.
There are over 1 billion bicycles in use worldwide. In 2002 there were an estimated 590 million cars and 205 million motorcycles in service in the world. At least 500 million Chinese Flying Pigeon bicycles have been made, more than any other single model of vehicle. The most-produced model of motor vehicle is the Honda Super Cub motorcycle, having passed 60 million units in 2008. The most-produced car model is the Toyota Corolla, with at least 35 million made by 2010. The most common fixed-wing airplane is the Cessna 172, with about 44,000 having been made as of 2017. The Soviet Mil Mi-8, at 17,000, is the most-produced helicopter. The top commercial jet airliner is the Boeing 737, at about 10,000 in 2018. At around 14,000 for both, the most produced trams are the KTM-5 and Tatra T3. The most common trolleybus is ZiU-9.

Locomotion consists of a means that allows displacement with little opposition, a power source to provide the required kinetic energy and a means to control the motion, such as a brake and steering system. By far, most vehicles use wheels which employ the principle of rolling to enable displacement with very little rolling friction.

It is essential that a vehicle have a source of energy to drive it. Energy can be extracted from external sources, as in the cases of a sailboat, a solar-powered car, or an electric streetcar that uses overhead lines. Energy can also be stored, provided it can be converted on demand and the storing medium's energy density and power density are sufficient to meet the vehicle's needs.

Human power is a simple source of energy that requires nothing more than humans. Despite the fact that humans cannot exceed 500 W (0.67 hp) for meaningful amounts of time, the land speed record for human-powered vehicles (unpaced) is 133 km/h (83 mph), as of 2009 on a recumbent bicycle.

The most common type of energy source is fuel. External combustion engines can use almost anything that burns as fuel, whilst internal combustion engines and rocket engines are designed to burn a specific fuel, typically gasoline, diesel or ethanol.

Another common medium for storing energy is batteries, which have the advantages of being responsive, useful in a wide range of power levels, environmentally friendly, efficient, simple to install, and easy to maintain. Batteries also facilitate the use of electric motors, which have their own advantages. On the other hand, batteries have low energy densities, short service life, poor performance at extreme temperatures, long charging times, and difficulties with disposal (although they can usually be recycled). Like fuel, batteries store chemical energy and can cause burns and poisoning in event of an accident. Batteries also lose effectiveness with time. The issue of charge time can be resolved by swapping discharged batteries with charged ones; however, this incurs additional hardware costs and may be impractical for larger batteries. Moreover, there must be standard batteries for battery swapping to work at a gas station. Fuel cells are similar to batteries in that they convert from chemical to electrical energy, but have their own advantages and disadvantages.

Electrified rails and overhead cables are a common source of electrical energy on subways, railways, trams, and trolleybuses. Solar energy is a more modern development, and several solar vehicles have been successfully built and tested, including Helios, a solar-powered aircraft.

Nuclear power is a more exclusive form of energy storage, currently limited to large ships and submarines, mostly military. Nuclear energy can be released by a nuclear reactor, nuclear battery, or repeatedly detonating nuclear bombs. There have been two experiments with nuclear-powered aircraft, the Tupolev Tu-119 and the Convair X-6.

Mechanical strain is another method of storing energy, whereby an elastic band or metal spring is deformed and releases energy as it is allowed to return to its ground state. Systems employing elastic materials suffer from hysteresis, and metal springs are too dense to be useful in many cases.

Flywheels store energy in a spinning mass. Because a light and fast rotor is energetically favorable, flywheels can pose a significant safety hazard. Moreover, flywheels leak energy fairly quickly and affect a vehicle's steering through the gyroscopic effect. They have been used experimentally in gyrobuses.

Wind energy is used by sailboats and land yachts as the primary source of energy. It is very cheap and fairly easy to use, the main issues being dependence on weather and upwind performance. Balloons also rely on the wind to move horizontally. Aircraft flying in the jet stream may get a boost from high altitude winds.

Compressed gas is currently an experimental method of storing energy. In this case, compressed gas is simply stored in a tank and released when necessary. Like elastics, they have hysteresis losses when gas heats up during compression.

Gravitational potential energy is a form of energy used in gliders, skis, bobsleds and numerous other vehicles that go down hill. Regenerative braking is an example of capturing kinetic energy where the brakes of a vehicle are augmented with a generator or other means of extracting energy.

When needed, the energy is taken from the source and consumed by one or more motors or engines. Sometimes there is an intermediate medium, such as the batteries of a diesel submarine.

Most motor vehicles have internal combustion engines. They are fairly cheap, easy to maintain, reliable, safe and small. Since these engines burn fuel, they have long ranges but pollute the environment. A related engine is the external combustion engine. An example of this is the steam engine. Aside from fuel, steam engines also need water, making them impractical for some purposes. Steam engines also need time to warm up, whereas IC engines can usually run right after being started, although this may not be recommended in cold conditions. Steam engines burning coal release sulfur into the air, causing harmful acid rain.

While intermittent internal combustion engines were once the primary means of aircraft propulsion, they have been largely superseded by continuous internal combustion engines: gas turbines. Turbine engines are light and, particularly when used on aircraft, efficient. On the other hand, they cost more and require careful maintenance. They can also be damaged by ingesting foreign objects, and they produce a hot exhaust. Trains using turbines are called gas turbine-electric locomotives. Examples of surface vehicles using turbines are M1 Abrams, MTT Turbine SUPERBIKE and the Millennium. Pulse jet engines are similar in many ways to turbojets, but have almost no moving parts. For this reason, they were very appealing to vehicle designers in the past; however their noise, heat and inefficiency has led to their abandonment. A historical example of the use of a pulse jet was the V-1 flying bomb. Pulse jets are still occasionally used in amateur experiments. With the advent of modern technology, the pulse detonation engine has become practical and was successfully tested on a Rutan VariEze. While the pulse detonation engine is much more efficient than the pulse jet and even turbine engines, it still suffers from extreme noise and vibration levels. Ramjets also have few moving parts, but they only work at high speed, so that their use is restricted to tip jet helicopters and high speed aircraft such as the Lockheed SR-71 Blackbird.

Rocket engines are primarily used on rockets, rocket sleds and experimental aircraft. Rocket engines are extremely powerful. The heaviest vehicle ever to leave the ground, the Saturn V rocket, was powered by five F-1 rocket engines generating a combined 180 million horsepower (134.2 gigawatt). Rocket engines also have no need to "push off" anything, a fact that the New York Times denied in error. Rocket engines can be particularly simple, sometimes consisting of nothing more than a catalyst, as in the case of a hydrogen peroxide rocket. This makes them an attractive option for vehicles such as jet packs. Despite their simplicity, rocket engines are often dangerous and susceptible to explosions. The fuel they run off may be flammable, poisonous, corrosive or cryogenic. They also suffer from poor efficiency. For these reasons, rocket engines are only used when absolutely necessary.

Electric motors are used in electric vehicles such as electric bicycles, electric scooters, small boats, subways, trains, trolleybuses, trams and experimental aircraft. Electric motors can be very efficient: over 90% efficiency is common. Electric motors can also be built to be powerful, reliable, low-maintenance and of any size. Electric motors can deliver a range of speeds and torques without necessarily using a gearbox (although it may be more economical to use one). Electric motors are limited in their use chiefly by the difficulty of supplying electricity.

Compressed gas motors have been used on some vehicles experimentally. They are simple, efficient, safe, cheap, reliable and operate in a variety of conditions. One of the difficulties met when using gas motors is the cooling effect of expanding gas. These engines are limited by how quickly they absorb heat from their surroundings. The cooling effect can, however, double as air conditioning. Compressed gas motors also lose effectiveness with falling gas pressure.

Ion thrusters are used on some satellites and spacecraft. They are only effective in a vacuum, which limits their use to spaceborne vehicles. Ion thrusters run primarily off electricity, but they also need a propellant such as caesium, or more recently xenon. Ion thrusters can achieve extremely high speeds and use little propellant; however they are power-hungry.

The mechanical energy that motors and engines produce must be converted to work by wheels, propellers, nozzles, or similar means. Aside from converting mechanical energy into motion, wheels allow a vehicle to roll along a surface and, with the exception of railed vehicles, to be steered. Wheels are ancient technology, with specimens being discovered from over 5000 years ago. Wheels are used in a plethora of vehicles, including motor vehicles, armoured personnel carriers, amphibious vehicles, airplanes, trains, skateboards and wheelbarrows.

Nozzles are used in conjunction with almost all reaction engines. Vehicles using nozzles include jet aircraft, rockets and personal watercraft. While most nozzles take the shape of a cone or bell, some unorthodox designs have been created such as the aerospike. Some nozzles are intangible, such as the electromagnetic field nozzle of a vectored ion thruster.

Continuous track is sometimes used instead of wheels to power land vehicles. Continuous track has the advantages of a larger contact area, easy repairs on small damage, and high maneuverability. Examples of vehicles using continuous track are tanks, snowmobiles and excavators. Two continuous tracks used together allow for steering. The largest vehicle in the world, the Bagger 288, is propelled by continuous tracks.

Propellers (as well as screws, fans and rotors) are used to move through a fluid. Propellers have been used as toys since ancient times, however it was Leonardo da Vinci who devised what was one of the earliest propeller driven vehicles, the "aerial-screw". In 1661, Toogood & Hays adopted the screw for use as a ship propeller. Since then, the propeller has been tested on many terrestrial vehicles, including the Schienenzeppelin train and numerous cars. In modern times, propellers are most prevalent on watercraft and aircraft, as well as some amphibious vehicles such as hovercraft and ground-effect vehicles. Intuitively, propellers cannot work in space as there is no working fluid, however some sources have suggested that since space is never empty, a propeller could be made to work in space.

Similarly to propeller vehicles, some vehicles use wings for propulsion. Sailboats and sailplanes are propelled by the forward component of lift generated by their sails/wings. Ornithopters also produce thrust aerodynamically. Ornithopters with large rounded leading edges produce lift by leading-edge suction forces.

Paddle wheels are used on some older watercraft and their reconstructions. These ships were known as paddle steamers. Because paddle wheels simply push against the water, their design and construction is very simple. The oldest such ship in scheduled service is the Skibladner. Many pedalo boats also use paddle wheels for propulsion.

Screw-propelled vehicles are propelled by auger-like cylinders fitted with helical flanges. Because they can produce thrust on both land and water, they are commonly used on all-terrain vehicles. The ZiL-2906 was a Soviet-designed screw-propelled vehicle designed to retrieve cosmonauts from the Siberian wilderness.

All or almost all of the useful energy produced by the engine is usually dissipated as friction; so minimising frictional losses is very important in many vehicles. The main sources of friction are rolling friction and fluid drag (air drag or water drag).

Wheels have low bearing friction and pneumatic tyres give low rolling friction. Steel wheels on steel tracks are lower still.

Aerodynamic drag can be reduced by streamlined design features.

Friction is desirable and important in supplying traction to facilitate motion on land. Most land vehicles rely on friction for accelerating, decelerating and changing direction. Sudden reductions in traction can cause loss of control and accidents.

Most vehicles, with the notable exception of railed vehicles, have at least one steering mechanism. Wheeled vehicles steer by angling their front or rear wheels. The B-52 Stratofortress has a special arrangement in which all four main wheels can be angled. Skids can also be used to steer by angling them, as in the case of a snowmobile. Ships, boats, submarines, dirigibles and aeroplanes usually have a rudder for steering. On an airplane, ailerons are used to bank the airplane for directional control, sometimes assisted by the rudder.

With no power applied, most vehicles come to a stop due to friction. But it is often required to stop a vehicle faster than by friction alone: so almost all vehicles are equipped with a braking system. Wheeled vehicles are typically equipped with friction brakes, which use the friction between brake pads (stators) and brake rotors to slow the vehicle. Many airplanes have high performance versions of the same system in their landing gear for use on the ground. A Boeing 757 brake, for example, has 3 stators and 4 rotors. The Space Shuttle also uses frictional brakes on its wheels. As well as frictional brakes, hybrid/electric cars, trolleybuses and electric bicycles can also use regenerative brakes to recycle some of the vehicle's potential energy. High-speed trains sometimes use frictionless Eddy-current brakes; however widespread application of the technology has been limited by overheating and interference issues.

Aside from landing gear brakes, most large aircraft have other ways of decelerating. In aircraft, air brakes are aerodynamic surfaces that provide braking force by increasing the frontal cross section thus aerodynamic drag of the aircraft. These are usually implemented as flaps that oppose air flow when extended and are flush with aircraft when retracted. Reverse thrust is also used in many aeroplane engines. Propeller aircraft achieve reverse thrust by reversing the pitch of the propellers, while jet aircraft do so by redirecting their engine exhaust forwards. On aircraft carriers, arresting gears are used to stop an aircraft. Pilots may even apply full forward throttle on touchdown, in case the arresting gear does not catch and a go around is needed.

Parachutes are used to slow down vehicles travelling very fast. Parachutes have been used in land, air and space vehicles such as the ThrustSSC, Eurofighter Typhoon and Apollo Command Module. Some older Soviet passenger jets had braking parachutes for emergency landings. Boats use similar devices called sea anchors to maintain stability in rough seas.

To further increase the rate of deceleration or where the brakes have failed, several mechanisms can be used to stop a vehicle. Cars and rolling stock usually have hand brakes that, while designed to secure an already parked vehicle, can provide limited braking should the primary brakes fail. A secondary procedure called forward-slip is sometimes used to slow airplanes by flying at an angle, causing more drag.

Motor vehicle and trailer categories are defined according to the following international classification:

Category M: passenger vehicles.
Category N: motor vehicles for the carriage of goods.
Category O: trailers and semi-trailers.
In the European Union the classifications for vehicle types are defined by:

Commission Directive 2001/116/EC of 20 December 2001, adapting to technical progress Council Directive 70/156/EEC on the approximation of the laws of the Member States relating to the type-approval of motor vehicles and their trailers
Directive 2002/24/EC of the European Parliament and of the Council of 18 March 2002 relating to the type-approval of two or three wheeled motor vehicles and repealing Council Directive 92/61/EEC
European Community, is based on the Community's WVTA (whole vehicle type-approval) system. Under this system, manufacturers can obtain certification for a vehicle type in one Member State if it meets the EC technical requirements and then market it EU-wide with no need for further tests. Total technical harmonization already has been achieved in three vehicle categories (passenger cars, motorcycles, and tractors) and soon will extend to other vehicle categories (coaches and utility vehicles). It is essential that European car manufacturers be ensured access to as large a market as possible.

While the Community type-approval system allows manufacturers to benefit fully from internal market opportunities, worldwide technical harmonization in the context of the United Nations Economic Commission for Europe (UNECE) offers a market beyond European borders.

In many cases, it is unlawful to operate a vehicle without a license or certification. The least strict form of regulation usually limits what passengers the driver may carry or prohibits them completely (e.g., a Canadian ultra-light license without endorsements). The next level of licensing may allow passengers, but without any form of compensation or payment. A private driver's license usually has these conditions. Commercial licenses that allow the transport of passengers and cargo are more tightly regulated. The most strict form of licensing is generally reserved for school buses, hazardous materials transports and emergency vehicles.

The driver of a motor vehicle is typically required to hold a valid driver's license while driving on public lands, whereas the pilot of an aircraft must have a license at all times, regardless of where in the jurisdiction the aircraft is flying.

Vehicles are often required to be registered. Registration may be for purely legal reasons, for insurance reasons or to help law enforcement recover stolen vehicles. Toronto Police Service, for example, offers free and optional bicycle registration online. On motor vehicles, registration often takes the form of a vehicle registration plate, which makes it easy to identify a vehicle. In Russia, trucks and buses have their licence plate numbers repeated in large black letters on the back. On aircraft, a similar system is used where a tail number is painted on various surfaces. Like motor vehicles and aircraft, watercraft also have registration numbers in most jurisdictions, however the vessel name is still the primary means of identification as has been the case since ancient times. For this reason, duplicate registration names are generally rejected. In Canada, boats with an engine power of 10 hp (7.5 kW) or greater require registration, leading to the ubiquitous "9.9 hp (7.4 kW)" engine.

Registration may be conditional on the vehicle being approved for use on public highways, as in the case of the UK and Ontario. Many US states also have requirements for vehicles operating on public highways. Aircraft have more stringent requirements, as they pose a high risk of damage to people and property in event of an accident. In the US, the FAA requires aircraft to have an airworthiness certificate. Because US aircraft must be flown for some time before they are certified, there is a provision for an experimental airworthiness certificate. FAA experimental aircraft are restricted in operation, including no overflights of populated areas, in busy airspace or with unessential passengers. Materials and parts used in FAA certified aircraft must meet the criteria set forth by the technical standard orders.

In many jurisdictions, the operator of a vehicle is legally obligated to carry safety equipment with or on them. Common examples include seat belts in cars, helmets on motorcycles and bicycles, fire extinguishers on boats, buses and airplanes and life jackets on boats and commercial aircraft. Passenger aircraft carry a great deal of safety equipment including inflatable slides are rafts, oxygen masks, oxygen tanks, life jackets, satellite beacons and first aid kits. Some equipment such as life jackets has led to debate regarding their usefulness. In the case of Ethiopian Airlines Flight 961, the life jackets saved many people but also led to many deaths when passengers inflated their vests prematurely.

There are specific real-estate arrangements made to allow vehicles to travel from one place to another. The most common arrangements are public highways, where appropriately licensed vehicles can navigate without hindrance. These highways are on public land and are maintained by the government. Similarly, toll routes are open to the public after paying a toll. These routes and the land they rest on may be government or privately owned or a combination of both. Some routes are privately owned but grant access to the public. These routes often have a warning sign stating that the government does not maintain the way. An example of this are byways in England and Wales. In Scotland, land is open to un-motorised vehicles if the land meets certain criteria. Public land is sometimes open to use by off-road vehicles. On US public land, the Bureau of Land Management (BLM) decides where vehicles may be used. Railways often pass over land not owned by the railway company. The right to this land is granted to the railway company through mechanisms such as easement. Watercraft are generally allowed to navigate public waters without restriction as long as they do not cause a disturbance. Passing through a lock, however, may require paying a toll. Despite the common law tradition Cuius est solum, eius est usque ad coelum et ad inferos of owning all the air above one's property, the US Supreme Court ruled that aircraft in the US have the right to use air above someone else's property without their consent. While the same rule generally applies in all jurisdictions, some countries such as Cuba and Russia have taken advantage of air rights on a national level to earn money. There are some areas that aircraft are barred from overflying. This is called prohibited airspace. Prohibited airspace is usually strictly enforced due to potential damage from espionage or attack. In the case of Korean Air Lines Flight 007, the airliner entered prohibited airspace over Soviet territory and was shot down as it was leaving.

For a comparison of transportation fatality rates, see: Air safety statistics.

Several different metrics used to compare and evaluate the safety of different vehicles. The main three are deaths per billion passenger-journeys, deaths per billion passenger-hours and deaths per billion passenger-kilometers.

A projectile is an object that is propelled by the application of an external force and then moves freely under the influence of gravity and air resistance. Although any objects in motion through space are projectiles, they are commonly found in warfare and sports (for example, a thrown baseball, kicked football, fired bullet, shot arrow, stone released from catapult).

In ballistics mathematical equations of motion are used to analyze projectile trajectories through launch, flight, and impact.

Blowguns and pneumatic rifles use compressed gases, while most other guns and cannons utilize expanding gases liberated by sudden chemical reactions by propellants like smokeless powder. Light-gas guns use a combination of these mechanisms.

Railguns utilize electromagnetic fields to provide a constant acceleration along the entire length of the device, greatly increasing the muzzle velocity.

Some projectiles provide propulsion during flight by means of a rocket engine or jet engine. In military terminology, a rocket is unguided, while a missile is guided. Note the two meanings of "rocket" (weapon and engine): an ICBM is a guided missile with a rocket engine.

An explosion, whether or not by a weapon, causes the debris to act as multiple high velocity projectiles. An explosive weapon or device may also be designed to produce many high velocity projectiles by the break-up of its casing; these are correctly termed fragments.

Many projectiles, e.g. shells, may carry an explosive charge or another chemical or biological substance. Aside from explosive payload, a projectile can be designed to cause special damage, e.g. fire (see also early thermal weapons), or poisoning (see also arrow poison).

In projectile motion the most important force applied to the '91projectile'92 is the propelling force, in this case the propelling forces are the muscles that act upon the ball to make it move, and the stronger the force applied, the more propelling force, which means the projectile (the ball) will travel farther. See pitching, bowling.

A projectile that does not contain an explosive charge or any other kind of payload is termed a kinetic projectile, kinetic energy weapon, kinetic energy warhead, kinetic warhead, kinetic kill vehicle or kinetic penetrator. Typical kinetic energy weapons are blunt projectiles such as rocks and round shots, pointed ones such as arrows, and somewhat pointed ones such as bullets. Among projectiles that do not contain explosives are those launched from railguns, coilguns, and mass drivers, as well as kinetic energy penetrators. All of these weapons work by attaining a high muzzle velocity, or initial velocity, generally up to hypervelocity, and collide with their targets, converting the kinetic energy associated with the relative velocity between the two objects into destructive shock waves and heat. Other types of kinetic weapons are accelerated over time by a rocket engine, or by gravity. In either case, it is this kinetic energy that destroys its target.

Some kinetic weapons for targeting objects in spaceflight are anti-satellite weapons and anti-ballistic missiles. Since in order to reach an object in orbit it is necessary to attain an extremely high velocity, their released kinetic energy alone is enough to destroy their target; explosives are not necessary. For example: the energy of TNT is 4.6 MJ/kg, and the energy of a kinetic kill vehicle with a closing speed of 10 km/s (22,000 mph) is 50 MJ/kg. For comparison, 50 MJ is equivalent to the kinetic energy of a school bus weighing 5 metric tons, traveling at 509 km/h (316 mph; 141 m/s). This saves costly weight and there is no detonation to be precisely timed. This method, however, requires direct contact with the target, which requires a more accurate trajectory. Some hit-to-kill warheads are additionally equipped with an explosive directional warhead to enhance the kill probability (e.g. Israeli Arrow missile or U.S. Patriot PAC-3).

With regard to anti-missile weapons, the Arrow missile and MIM-104 Patriot PAC-2 have explosives, while the Kinetic Energy Interceptor (KEI), Lightweight Exo-Atmospheric Projectile (LEAP, used in Aegis BMDS), and THAAD do not (see Missile Defense Agency).

A kinetic projectile can also be dropped from aircraft. This is applied by replacing the explosives of a regular bomb with a non-explosive material (e.g. concrete), for a precision hit with less collateral damage. A typical bomb has a mass of 900 kg (2,000 lb) and a speed of impact of 800 km/h (500 mph). It is also applied for training the act of dropping a bomb with explosives. This method has been used in Operation Iraqi Freedom and the subsequent military operations in Iraq by mating concrete-filled training bombs with JDAM GPS guidance kits, to attack vehicles and other relatively "soft" targets located too close to civilian structures for the use of conventional high explosive bombs.

A Prompt Global Strike may use a kinetic weapon. A kinetic bombardment may involve a projectile dropped from Earth orbit.

A hypothetical kinetic weapon that travels at a significant fraction of the speed of light, usually found in science fiction, is termed a relativistic kill vehicle (RKV).

Some projectiles stay connected by a cable to the launch equipment after launching it:

for guidance: wire-guided missile (range up to 4,000 metres or 13,000 feet)
to administer an electric shock, as in the case of a Taser (range up to 10.6 metres or 35 feet); two projectiles are shot simultaneously, each with a cable.
to make a connection with the target, either to tow it towards the launcher, as with a whaling harpoon, or to draw the launcher to the target, as a grappling hook does.
An object projected at an angle to the horizontal has both the vertical and horizontal components of velocity. The vertical component of the velocity on the y-axis is given as while the horizontal component of the velocity is . There are various calculations for projectiles at a specific angle :

1. Time to reach maximum height. It is symbolized as (), which is the time taken for the projectile to reach the maximum height from the plane of projection. Mathematically, it is given as where = acceleration due to gravity (app 9.81 m/s'b2), = initial velocity (m/s) and = angle made by the projectile with the horizontal axis.

2. Time of flight (): this is the total time taken for the projectile to fall back to the same plane from which it was projected. Mathematically it is given as .

3. Maximum Height (): this is the maximum height attained by the projectile OR the maximum displacement on the vertical axis (y-axis) covered by the projectile. It is given as .

4. Range (): The Range of a projectile is the horizontal distance covered (on the x-axis) by the projectile. Mathematically, . The Range is maximum when angle = 45'b0, i.e. .

Ballistics is the field of mechanics concerned with the launching, flight behaviour and impact effects of projectiles, especially ranged weapon munitions such as bullets, unguided bombs, rockets or the like; the science or art of designing and accelerating projectiles so as to achieve a desired performance.

A ballistic body is a free-moving body with momentum which can be subject to forces such as the forces exerted by pressurized gases from a gun barrel or a propelling nozzle, normal force by rifling, and gravity and air drag during flight.

A ballistic missile is a missile that is guided only during the relatively brief initial phase of powered flight and the trajectory is subsequently governed by the laws of classical mechanics; in contrast to (for example) a cruise missile which is aerodynamically guided in powered flight like a fixed-wing aircraft.

The earliest known ballistic projectiles were stones and spears, and the throwing stick.

The oldest evidence of stone-tipped projectiles, which may or may not have been propelled by a bow (c.f. atlatl), dating to c. 64,000 years ago, were found in Sibudu Cave, present day-South Africa. The oldest evidence of the use of bows to shoot arrows dates to about 10,000 years ago; it is based on pinewood arrows found in the Ahrensburg valley north of Hamburg. They had shallow grooves on the base, indicating that they were shot from a bow. The oldest bow so far recovered is about 8,000 years old, found in the Holmeg'e5rd swamp in Denmark.

Archery seems to have arrived in the Americas with the Arctic small tool tradition, about 4,500 years ago.

The first devices identified as guns appeared in China around 1000 AD, and by the 12th century the technology was spreading through the rest of Asia, and into Europe by the 13th century.

After millennia of empirical development, the discipline of ballistics was initially studied and developed by Italian mathematician Niccol'f2 Tartaglia in 1531, although he continued to use segments of straight-line motion, conventions established by greek philosopher aristotle and Albert of Saxony, but with the innovation that he connected the straight lines by a circular arc. Galileo established the principle of compound motion in 1638, using the principle to derive the parabolic form of the ballistic trajectory. Ballistics was put on a solid scientific and mathematical basis by Isaac Newton, with the publication of Philosophi'e6 Naturalis Principia Mathematica in 1687. This gave mathematical laws of motion and gravity which for the first time made it possible to successfully predict trajectories.

The word ballistics comes from the Greek uc0u946 u940 u955 u955 u949 u953 u957  ballein, meaning "to throw".

A projectile is any object projected into space (empty or not) by the exertion of a force. Although any object in motion through space (for example a thrown baseball) is a projectile, the term most commonly refers to a ranged weapon. Mathematical equations of motion are used to analyze projectile trajectory.

Examples of projectiles include balls, arrows, bullets, artillery shells, wingless rockets, etc.

Throwing is the launching of a projectile by hand. Although some other animals can throw, humans are unusually good throwers due to their high dexterity and good timing capabilities, and it is believed that this is an evolved trait. Evidence of human throwing dates back 2 million years. The 90 mph throwing speed found in many athletes far exceeds the speed at which chimpanzees can throw things, which is about 20 mph. This ability reflects the ability of the human shoulder muscles and tendons to store elasticity until it is needed to propel an object.

A sling is a projectile weapon typically used to throw a blunt projectile such as a stone, clay or lead "sling-bullet".

A sling has a small cradle or pouch in the middle of two lengths of cord. The sling stone is placed in the pouch. The middle finger or thumb is placed through a loop on the end of one cord, and a tab at the end of the other cord is placed between the thumb and forefinger. The sling is swung in an arc, and the tab released at a precise moment. This frees the projectile to fly to the target.

A bow is a flexible piece of material which shoots aerodynamic projectiles called arrows. A string joins the two ends and when the string is drawn back, the ends of the stick are flexed. When the string is released, the potential energy of the flexed stick is transformed into the velocity of the arrow. Archery is the art or sport of shooting arrows from bows.

A catapult is a device used to launch a projectile a great distance without the aid of explosive devices '96 particularly various types of ancient and medieval siege engines. The catapult has been used since ancient times, because it was proven to be one of the most effective mechanisms during warfare. The word "catapult" comes from the Latin catapulta, which in turn comes from the Greek uc0u954 u945 u964 u945 u960 u941 u955 u964 u951 u962  (katapeltu275 s), itself from u954 u945 u964 u940  (kata), "against'94 and u960 u940 u955 u955 u969  (pallu333 ), "to toss, to hurl". Catapults were invented by the ancient Greeks.

A gun is a normally tubular weapon or other device designed to discharge projectiles or other material. The projectile may be solid, liquid, gas, or energy and may be free, as with bullets and artillery shells, or captive as with Taser probes and whaling harpoons. The means of projection varies according to design but is usually effected by the action of gas pressure, either produced through the rapid combustion of a propellant or compressed and stored by mechanical means, operating on the projectile inside an open-ended tube in the fashion of a piston. The confined gas accelerates the movable projectile down the length of the tube imparting sufficient velocity to sustain the projectile's travel once the action of the gas ceases at the end of the tube or muzzle. Alternatively, acceleration via electromagnetic field generation may be employed in which case the tube may be dispensed with and a guide rail substituted.

A weapons engineer or armourer who applies the scientific principles of ballistics to design cartridges are often called a ballistician.

A rocket is a missile, spacecraft, aircraft or other vehicle that obtains thrust from a rocket engine. Rocket engine exhaust is formed entirely from propellants carried within the rocket before use. Rocket engines work by action and reaction. Rocket engines push rockets forward simply by throwing their exhaust backwards extremely fast.

While comparatively inefficient for low speed use, rockets are relatively lightweight and powerful, capable of generating large accelerations and of attaining extremely high speeds with reasonable efficiency. Rockets are not reliant on the atmosphere and work very well in space.

Rockets for military and recreational uses date back to at least 13th century China. Significant scientific, interplanetary and industrial use did not occur until the 20th century, when rocketry was the enabling technology for the Space Age, including setting foot on the Moon. Rockets are now used for fireworks, weaponry, ejection seats, launch vehicles for artificial satellites, human spaceflight, and space exploration.

Chemical rockets are the most common type of high performance rocket and they typically create their exhaust by the combustion of rocket propellant. Chemical rockets store a large amount of energy in an easily released form, and can be very dangerous. However, careful design, testing, construction and use minimizes risks.

Ballistics is often broken down into the following four categories:

Internal ballistics the study of the processes originally accelerating projectiles
Transition ballistics the study of projectiles as they transition to unpowered flight
External ballistics the study of the passage of the projectile (the trajectory) in flight
Terminal ballistics the study of the projectile and its effects as it ends its flight
Internal ballistics (also interior ballistics), a sub-field of ballistics, is the study of the propulsion of a projectile.

In guns internal ballistics covers the time from the propellant's ignition until the projectile exits the gun barrel. The study of internal ballistics is important to designers and users of firearms of all types, from small-bore rifles and pistols, to high-tech artillery.

For rocket propelled projectiles, internal ballistics covers the period during which a rocket engine is providing thrust.

Transitional ballistics, also known as intermediate ballistics, is the study of a projectile's behavior from the time it leaves the muzzle until the pressure behind the projectile is equalized, so it lies between internal ballistics and external ballistics.

External ballistics is the part of the science of ballistics that deals with the behaviour of a non-powered projectile in flight.

External ballistics is frequently associated with firearms, and deals with the unpowered free-flight phase of the bullet after it exits the gun barrel and before it hits the target, so it lies between transitional ballistics and terminal ballistics.

However, external ballistics is also concerned with the free-flight of rockets and other projectiles, such as balls, arrows etc.

Terminal ballistics is the study of the behavior and effects of a projectile when it hits its target.

Terminal ballistics is relevant both for small caliber projectiles as well as for large caliber projectiles (fired from artillery). The study of extremely high velocity impacts is still very new and is as yet mostly applied to spacecraft design.

Forensic ballistics involves analysis of bullets and bullet impacts to determine information of use to a court or other part of a legal system. Separately from ballistics information, firearm and tool mark examinations ("ballistic fingerprinting") involve analyzing firearm, ammunition, and tool mark evidence in order to establish whether a certain firearm or tool was used in the commission of a crime.

Astrodynamics is the application of ballistics and celestial mechanics to the practical problems concerning the motion of rockets and other spacecraft. The motion of these objects is usually calculated from Newton's laws of motion and Newton's law of universal gravitation. It is a core discipline within space mission design and control.

A bullet is a kinetic projectile, a component of firearm ammunition that is shot from a gun barrel. The term is from Middle French, originating as the diminutive of the word boulle (boullet), which means "small ball". Bullets are made of a variety of materials, such as copper, lead, steel, polymer, rubber and even wax. Bullets are made in various shapes and constructions (depending on the intended applications), including specialized functions such as hunting, target shooting, training and combat. Bullets are often tapered, making them more aerodynamic. Bullet sizes are expressed by their weights and diameters (referred to as "calibers") in both imperial and metric measurement systems. For example: 55 grain .223 caliber bullets are of the same weight and caliber as 3.56 gram 5.56mm caliber bullets. Bullets do not normally contain explosives (see Incendiary ammunition and Exploding bullet), but strike or damage the intended target by transferring kinetic energy upon impact and penetration (see terminal ballistics).

Bullets are available singly (as in muzzle-loading and cap and ball firearms), but are more often packaged with propellant as cartridges ("rounds" of ammunition). Bullets are components of paper cartridges, or (much more commonly) in the form of metallic cartridges. Although the word bullet is often used in colloquial language to refer to a cartridge round, a bullet is not a cartridge but rather a component of one. A cartridge is a combination package of the bullet (i.e., the projectile), the case (which holds everything together), the propellant (which provides the majority of the energy to launch the projectile) and the primer (which ignites the propellant). This use of the term bullet (when intending to describe a cartridge) often leads to confusion when a cartridge, and all its components, are specifically referred to. The cartridges, in turn, may be held in a magazine or a belt (for rapid-fire weapons).

The bullets used in many cartridges are fired at muzzle velocities faster than the speed of sound'97about 343 metres per second (1,130 ft/s) in dry air at 20 'b0C (68 'b0F)'97and thus can travel a substantial distance to a target before a nearby observer hears the sound of the shot. The sound of gunfire (i.e. the "muzzle report") is often accompanied with a loud bullwhip-like crack as the supersonic bullet pierces through the air creating a sonic boom. Bullet speeds at various stages of flight depend on intrinsic factors such as sectional density, aerodynamic profile and ballistic coefficient, and extrinsic factors such as barometric pressure, humidity, air temperature and wind speed. Subsonic cartridges fire bullets slower than the speed of sound, so there are no sonic booms. This means that a subsonic cartridge, such as .45 ACP, can be substantially quieter than a supersonic cartridge, such as the .223 Remington, even without the use of a suppressor.

Bullets shot by firearms can be used for target practice or to injure or kill animals, or people. Death can be by blood loss or damage to vital organs, or even asphyxiation if blood enters the lungs. Bullets are not the only projectiles shot from firearm-like equipment: BBs are shot from BB guns, airsoft pellets are shot by airsoft guns, paintballs are shot by paintball markers, and small rocks can be hurtled from slingshots. There are also flare guns, potato guns (and spud guns), rubber bullets, tasers, bean bag rounds, grenade launchers, flash bangs, tear gas, RPGs, and missile launchers.

The first true gun evolved from the fire lance (a bamboo tube that fired porcelain shrapnel) with the invention of the metal hand cannon sometime around 1288, which the Yuan Dynasty used to win a decisive victory against Mongolian rebels. The artillery cannon appeared in 1326, and the European hand cannon in 1364. Early projectiles were made of stone. Eventually it was discovered that stone would not penetrate stone fortifications, which led to the use of denser materials as projectiles. Hand cannon projectiles developed in a similar manner. The first recorded instance of a metal ball from a hand cannon penetrating armor was in 1425. Shot retrieved from the wreck of the Mary Rose (sunk in 1545, raised in 1982) are of different sizes, and some are stone while others are cast iron.

The development of the hand culverin and matchlock arquebus brought about the use of cast lead balls as projectiles. The original round musket ball was smaller than the bore of the barrel. At first it was loaded into the barrel just resting upon the powder. Later, some sort of material was used as a wadding between the ball and the powder as well as over the ball to keep it in place, it held the bullet firmly in the barrel and against the powder. (Bullets not firmly on the powder risked exploding the barrel, with the condition known as a "short start".)

The loading of muskets was, therefore, easy with the old smooth-bore Brown Bess and similar military muskets. The original muzzle-loading rifle, however, was loaded with a piece of leather or cloth wrapped around the ball, to allow the ball to engage the grooves in the barrel. Loading was a bit more difficult, particularly when the bore of the barrel was fouled from previous firings. For this reason, and because rifles were not often fitted for bayonets, early rifles were rarely used for military purposes, compared to muskets.

The first half of the nineteenth century saw a distinct change in the shape and function of the bullet. In 1826, Henri-Gustave Delvigne, a French infantry officer, invented a breech with abrupt shoulders on which a spherical bullet was rammed down until it caught the rifling grooves. Delvigne's method, however, deformed the bullet and was inaccurate.

In 1855 a detachment of 1st U.S. Dragoons, while on patrol, traded lead for gold bullets with Pima Indians along the California Arizona border. "The use of gold bullets by Indians is confirmed by Aubry in a journey across central Arizona. "The Indians use gold bullets for their guns. They are of different sizes and each Indian has a pouch of them. We saw an Indian load his gun with one large and three small gold bullets to shoot a rabbit."

Square bullets have origins that almost pre-date civilization and were used by slingers in slings. They were typically made out of copper or lead. The most notable use of square bullet designs was done by James Puckle and Kyle Tunis who patented them, where they were briefly used in one version of the Puckle gun. The early use of these in the black-powder era was soon discontinued due to irregular and unpredictable flight patterns.

Delvigne continued to develop bullet design and by 1830 had started to develop cylindro-conical bullets. His bullet designs were improved by Francois Tamisier with the addition of "ball grooves" which are known as "cannelures", which moved the resistance of air behind the center of gravity of the bullet.

Tamisier also developed progressive rifling. The rifle grooves were deeper toward the breech, becoming shallower as they progressed toward the muzzle. This causes the bullet to be progressively molded into the grooves which increases range and accuracy.

The Thouvenin rifle barrel has a forcing plug in the breech of the barrel to mold the bullet into the rifling with the use of a special ramrod. While successful in increasing accuracy it was extremely hard to clean. These improvements were the basis for the development of the Mini'e9 ball.

Among the first pointed or "conical" bullets were those designed by Captain John Norton of the British Army in 1832. Norton's bullet had a hollow base made of lotus pith that, on firing, expanded under pressure to engage with a barrel's rifling. The British Board of Ordnance rejected it because spherical bullets had been in use for the previous 300 years.

Renowned English gunsmith William Greener invented the Greener bullet in 1836. Greener fitted the hollow base of an oval bullet with a wooden plug that more reliably forced the base of the bullet to expand and catch the rifling. Tests proved that Greener's bullet was extremely effective, but the military rejected it because, being two parts, they judged it as too complicated to produce.

The soft lead Mini'e9 ball was first introduced in 1847 by Claude-'c9tienne Mini'e9, a captain in the French Army. It was another improvement of the work done by Delvigne. As designed by Mini'e9, the bullet was conical in shape with a hollow cavity in the rear, which was fitted with a small iron cap instead of a wooden plug. When fired, the iron cap forced itself into the hollow cavity at the rear of the bullet, thus expanding the sides of the bullet to grip and engage the rifling. In 1851, the British adopted the Mini'e9 ball for their 702-inch Pattern 1851 Mini'e9 rifle. In 1855, James Burton, a machinist at the US Armory at Harper's Ferry, West Virginia, improved the Mini'e9 ball further by eliminating the metal cup in the bottom of the bullet. The Mini'e9 ball first saw widespread use in the Crimean War (1853-1856). Roughly 90% of the battlefield casualties in the American Civil War (1861-1865) were caused by Mini'e9 balls fired from rifled muskets.

A similar bullet called the Nessler ball was also developed for smoothbore muskets.

Between 1854 and 1857, Sir Joseph Whitworth conducted a long series of rifle experiments, and proved, among other points, the advantages of a smaller bore and, in particular, of an elongated bullet. The Whitworth bullet was made to fit the grooves of the rifle mechanically. The Whitworth rifle was never adopted by the government, although it was used extensively for match purposes and target practice between 1857 and 1866, when it was gradually superseded by Metford's.

In 1861 W.B. Chace approached President Abraham Lincoln with an improved ball design for muskets. In firing over the Potomac river where the Chace ball and the round ball were alternated Lincoln observed that the Chace design carried a third or more farther fired at the same elevation. Although Lincoln recommended testing, it never took place.

About 1862 and later, W. E. Metford carried out an exhaustive series of experiments on bullets and rifling, and invented the important system of light rifling with increasing spiral, and a hardened bullet. The combined result was that, in December 1888, the Lee'96Metford small-bore (.303", 7.70 mm) rifle, Mark I, was finally adopted for the British army. The Lee'96Metford was the predecessor of the Lee'96Enfield.

The next important change in the history of the rifle bullet occurred in 1882, when Lt. Colonel Eduard Rubin, director of the Swiss Army Laboratory at Thun, invented the copper-jacketed bullet '97 an elongated bullet with a lead core in a copper jacket. It was also small bore (7.5mm and 8mm) and it is the precursor of the 8mm Lebel bullet adopted for the smokeless powder ammunition of the Lebel Model 1886 rifle.

The surface of lead bullets fired at high velocity may melt due to hot gases behind and friction with the bore. Because copper has a higher melting point, and greater specific heat capacity and hardness, copper-jacketed bullets allow greater muzzle velocities.

European advances in aerodynamics led to the pointed spitzer bullet. By the beginning of the twentieth century, most world armies had begun the transition to spitzer bullets. These bullets flew for greater distances more accurately and carried more energy with them. Spitzer bullets combined with machine guns greatly increased the lethality of the battlefield.

The latest advancement in bullet shape was the boat tail, a streamlined base for spitzer bullets. The vacuum created as air moving at high speed passes over the end of a bullet slows the projectile. The streamlined boat tail design reduces this form drag by allowing the air to flow along the surface of the tapering end. The resulting aerodynamic advantage is currently seen as the optimum shape for rifle technology. The first combination spitzer and boat-tail bullet, named Balle "D" from its inventor (a lieutenant-colonel Desaleux), was introduced as standard military ammunition in 1901, for the French Lebel Model 1886 rifle.

A ballistic tip bullet is a hollow-point rifle bullet that has a plastic tip on the end of the bullet itself. This improves external ballistics by streamlining the bullet, allowing it to cut through the air more easily, and improves terminal ballistics by allowing the bullet to act as a JHP on impact.

As a side effect, it also feeds better in weapons that have trouble feeding rounds that are not FMJ rounds.

Bullet designs have to solve two primary problems. In the barrel, they must first form a seal with the gun's bore. If a strong seal is not achieved, gas from the propellant charge leaks past the bullet, thus reducing efficiency and possibly accuracy. The bullet must also engage the rifling without damaging or excessively fouling the gun's bore, and without distorting the bullet, which will also reduce accuracy. Bullets must have a surface that forms this seal without excessive friction. These interactions between bullet and bore are termed internal ballistics. Bullets must be produced to a high standard, as surface imperfections can affect firing accuracy.

The physics affecting the bullet once it leaves the barrel is termed external ballistics. The primary factors affecting the aerodynamics of a bullet in flight are the bullet's shape and the rotation imparted by the rifling of the gun barrel. Rotational forces stabilize the bullet gyroscopically as well as aerodynamically. Any asymmetry in the bullet is largely canceled as it spins. However, a spin rate greater than the optimum value adds more trouble than good, by magnifying the smaller asymmetries or sometimes resulting in the bullet exploding midway in flight. With smooth-bore firearms, a spherical shape was optimum because no matter how it was oriented, it presented a uniform front. These unstable bullets tumbled erratically and provided only moderate accuracy; however, the aerodynamic shape changed little for centuries. Generally, bullet shapes are a compromise between aerodynamics, interior ballistic necessities, and terminal ballistics requirements. Another method of stabilization is to place the center of mass of the bullet as far forward as is practical, which is how the Mini'e9 ball and the shuttlecock are designed. This makes the bullet fly front-forward by means of aerodynamics.

Terminal ballistics and stopping power are aspects of bullet design that affect what happens when a bullet impacts with an object. The outcome of the impact is determined by the composition and density of the target material, the angle of incidence, and the velocity and physical characteristics of the bullet itself. Bullets are generally designed to penetrate, deform, or break apart. For a given material and bullet, the strike velocity is the primary factor that determines which outcome is achieved.

Bullet shapes are many and varied, and an array of them can be found in any reloading manual that sells bullet molds. Mold manufacturers such as RCBS, Paul Jones Moulds, and David Mos offer many different calibers and designs. With a mold, bullets can be made at home for reloading ammunition, where local laws allow. Hand-casting, however, is only time- and cost-effective for solid lead bullets. Cast and jacketed bullets are also commercially available from numerous manufacturers for hand loading and are most often more convenient than casting bullets from bulk or scrap lead.

Propulsion of the ball can happen via several methods:

by using only gunpowder (i.e. as in flintlock weapons)
by using a percussion cap and gunpowder (i.e. as in percussion weapons)
by using a cartridge (which contains primer, gunpowder and bullet in a single package)
Bullets for black powder, or muzzle-loading firearms, were classically molded from pure lead. This worked well for low-speed bullets, fired at velocities of less than 450 m/s (1,475 ft/s). For slightly higher-speed bullets fired in modern firearms, a harder alloy of lead and tin or typesetter's lead (used to mold linotype) works very well. For even higher-speed bullet use, jacketed coated lead bullets are used. The common element in all of these, lead, is widely used because it is very dense, thereby providing a high amount of mass'97and thus, kinetic energy'97for a given volume. Lead is also cheap, easy to obtain, easy to work, and melts at a low temperature, which results in comparatively easy fabrication of bullets.

Lead: Simple cast, extruded, swaged, or otherwise fabricated lead slugs are the simplest form of bullets. At speeds of greater than 300 m/s (1,000 ft/s) (common in most handguns), lead is deposited in rifled bores at an ever-increasing rate. Alloying the lead with a small percentage of tin and/or antimony serves to reduce this effect, but grows less effective as velocities are increased. A cup made of harder metal, such as copper, placed at the base of the bullet and called a gas check, is often used to decrease lead deposits by protecting the rear of the bullet against melting when fired at higher pressures, but this too does not solve the problem at higher velocities. A modern solution is to powder coat the lead projectile, encasing it in a protective skin, allowing higher velocities to be achieved without lead deposits.
Jacketed lead: Bullets intended for even higher-velocity applications generally have a lead core that is jacketed or plated with gilding metal, cupronickel, copper alloys, or steel; a thin layer of harder metal protects the softer lead core when the bullet is passing through the barrel and during flight, which allows delivering the bullet intact to the target. There, the heavy lead core delivers its kinetic energy to the target. Full metal jacket or "ball" bullets (cartridges with ball bullets, which despite the name are not spherical, are called ball ammunition) are completely encased in the harder metal jacket, except for the base. Some bullet jackets do not extend to the front of the bullet, to aid expansion and increase lethality; these are called soft point (if the exposed lead tip is solid) or hollow point bullets (if a cavity or hole is present). Steel bullets are often plated with copper or other metals for corrosion resistance during long periods of storage. Synthetic jacket materials such as nylon and Teflon have been used, with limited success, especially in rifles; however, hollow point bullets with plastic aerodynamic tips have been very successful at both improving accuracy and enhancing expansion. Newer plastic coatings for handgun bullets, such as Teflon-coated bullets, are making their way into the market.
Solid or monolithic solid: Mono-metal bullets intended for deep penetration in big game animals and slender shaped very-low-drag projectiles for long range shooting are produced out of metals like oxygen free copper and alloys like copper nickel, tellurium copper and brass, for example highly machinable UNS C36000 free-cutting brass. Often these projectiles are turned on precision CNC lathes. In the case of solids, and the ruggedness of the game animals on which they are used, e.g., the African buffalo or elephant, expansion is almost entirely relinquished for the necessary penetration. In shotgunning, "slug" loads are often solid large single lead projectiles, sometimes with a hollow point, used for deer or wild pig hunting in jurisdictions that do not allow rifles (because a missed slug shot will travel considerably less far than a rifle bullet).
Fluted: In appearance, these are solid bullets with scalloped sides (missing material). The theory is that the flutes produce hydraulic jetting when passing through tissues, creating a wound channel larger than that made by conventional expanding ammunition such as hollowpoints.
Hard cast: A hard lead alloy intended to reduce fouling of rifling grooves (especially of the polygonal rifling used in some popular pistols). Benefits include simpler manufacture than jacketed bullets and good performance against hard targets; limitations are an inability to mushroom and subsequent over-penetration of soft targets.
Blank: Wax, paper, plastic, and other materials are used to simulate live gunfire and are intended only to hold the powder in a blank cartridge and to produce noise, flame and smoke. The "bullet" may be captured in a purpose-designed device or it may be allowed to expend what little energy it has in the air. Some blank cartridges are crimped or closed at the end and do not contain any bullet; some are fully loaded cartridges (without bullets) designed to propel rifle grenades. Blank cartridges, at short ranges, can be lethal due to the force of the expanding gas '96 numerous tragic accidents have occurred with blank cartridges (e.g., the death of actor Jon-Erik Hexum).
Practice: Made from lightweight materials like rubber, wax, wood, plastic, or lightweight metal, practice bullets are intended for short-range target work, only. Because of their weight and low velocity, they have limited ranges.
Polymer: These are metal-polymer composites, generally lighter and having higher velocities than pure metal bullets of the same dimensions. They permit unusual designs that are difficult with conventional casting or lathing.
Less lethal, or less than lethal: Rubber bullets, plastic bullets, and beanbags are designed to be non-lethal, for example for use in riot control. They are generally low velocity and are fired from shotguns, grenade launchers, paint ball guns, or specially designed firearms and air gun devices.
Incendiary: These bullets are made with explosive or flammable mixtures in the tips that are designed to ignite on contact with a target. The intent is to ignite fuel or munitions in the target area, thereby adding to the destructive power of the bullet itself.
Exploding: Similar to the incendiary bullet, this type of projectile is designed to explode upon hitting a hard surface, preferably the bone of the intended target. Not to be mistaken for cannon shells or grenades with fuse devices, these bullets have only cavities filled with a small amount of low explosive depending on the velocity and deformation upon impact to detonate. Exploding bullets have been used in various aircraft machine guns and in anti-materiel rifles.
Tracer: These have hollow backs, filled with a flare material. Usually this is a mixture of magnesium, a perchlorate, and strontium salts to yield a bright red color, although other materials providing other colors have also sometimes been used. Tracer material burns out after a certain amount of time. Such ammunition is useful to the shooter as a means of learning how to point shoot moving targets with rifles. This type of round is also used by all branches of the United States military in combat environments as a signaling device to friendly forces. Normally it is loaded at a four to one ratio with ball ammunition and is intended to show where the shooter are firing so friendly forces can engage the target as well. The flight characteristics of tracer rounds differ from normal bullets due to their lighter weight.
Armor-piercing: Jacketed designs where the core material is a very hard, high-density metal such as tungsten, tungsten carbide, depleted uranium, or steel. A pointed tip is often used, but a flat tip on the penetrator portion is generally more effective.
Nontoxic shot: Steel, bismuth, tungsten, and other exotic bullet alloys prevent release of toxic lead into the environment. Regulations in several countries mandate the use of nontoxic projectiles especially when hunting waterfowl. It has been found that birds swallow small lead shot for their gizzards to grind food (as they would swallow pebbles of similar size), and the effects of lead poisoning by constant grinding of lead pellets against food means lead poisoning effects are magnified. Such concerns apply primarily to shotguns, firing pellets (shot) and not bullets, but there is evidence suggesting that consumption of spent rifle and pistol ammunition is also hazardous to wildlife. Reduction of hazardous substances (RoHS) legislation has also been applied to bullets on occasion to reduce the impact of lead on the environment at shooting ranges. The United States Environmental Protection Agency announced that the agency does not have the legal authority to regulate this type of product (lead bullets) under the Toxic Substances Control Act (TSCA), nor is the agency seeking such authority. With some nontoxic shot, e.g., steel shot, care must be taken to shoot only in shotguns (and with chokes) specifically designed and designated for steel shot; for other, particularly older, shotguns, serious damage to the barrel and chokes can occur. And, because steel is lighter and less dense than lead, larger sized pellets must be used, thus reducing the number of pellets in a given charge of shot and possibly limiting patterns on the target; other formulations, e.g. bismuth, do not present this disability.
Blended-metal: Bullets made using cores from powdered metals other than lead with binder or sometimes sintered.
Frangible: Designed to disintegrate into tiny particles upon impact to minimize their penetration for reasons of range safety, to limit environmental impact, or to limit the shoot-through danger behind the intended target. An example is the Glaser Safety Slug, usually a pistol caliber bullet made from an amalgam of lead shot and a hard (and thus frangible) plastic binder designed to penetrate a human target and release its component shot pellets without exiting the target.
Multiple impact bullet: Bullets that are made of separate slugs that fit together inside the cartridge, and act as a single projectile inside the barrel as they are fired. The projectiles part in flight, but are held in formation by tethers that keep the individual parts of the "bullet" from flying too far away from each other. The intention of such ammo is to increase hit chance by giving a shot-like spread to rifled slug firing guns, while maintaining a consistency in shot groupings. Multiple impact bullets may be less stable in flight than conventional solid bullets because of the added aerodynamic drag from the tether line holding the pieces in formation, and each projectile affects the flight of all the others. This may limit the benefit provided by the spread of each bullet at longer ranges.
Poisonous bullets were a subject to an international agreement as early as the Strasbourg Agreement (1675).

The St. Petersburg Declaration of 1868 prohibited the use of explosive projectiles weighing less than 400 grams. It was reasoned that more deadly bullets would lead to less suffering.

The Hague Convention prohibits certain kinds of ammunition for use by uniformed military personnel against the uniformed military personnel of opposing forces. These include projectiles that explode within an individual, poisoned and expanding bullets.

Protocol III of the 1983 Convention on Certain Conventional Weapons, an annexe to the Geneva Conventions, prohibits the use of incendiary ammunitions against civilians.

Some jurisdictions acting on environmental concerns have banned hunting with lead bullets and shotgun pellets.

In December 2014, a federal appeals court denied a lawsuit by environmental groups that the EPA must use the Toxic Substances Control Act to regulate lead in shells and cartridges. The groups sought to regulate "spent lead", yet EPA could not regulate spent lead without also regulating cartridges and shells, per the court.

A kinetic bombardment or a kinetic orbital strike is the hypothetical act of attacking a planetary surface with an inert projectile from orbit (orbital bombardment), where the destructive power comes from the kinetic energy of the projectile impacting at very high speeds. The concept originated during the Cold War.

Typical depictions of the tactic are of a satellite containing a magazine of tungsten rods and a directional thrust system. When a strike is ordered, the launch vehicle brakes one of the rods out of its orbit and into a suborbital trajectory that intersects the target. The rods would typically be shaped to minimize air resistance and maximize velocity upon impact.

Kinetic bombardment has the advantage of being able to deliver projectiles from a very high angle at a very high speed, making them extremely difficult to defend against. In addition, projectiles would not require explosive warheads, and'97in the simplest designs'97would consist entirely of solid metal rods, giving rise to the common nickname "rods from God". Disadvantages include the technical difficulties of ensuring accuracy and the high costs of positioning ammunition in orbit.

During the Korean and Vietnam Wars, there was limited use of the Lazy Dog bomb, a kinetic projectile shaped like a conventional bomb but only about 1.75-inch-long (44 mm) and 0.50-inch-diameter (13 mm). A piece of sheet metal was folded to make the fins and welded to the rear of the projectile. These were dumped from aircraft onto enemy troops and had the same effect as a machine gun fired vertically. Similar flechette projectiles have been used since the first World War.

Project Thor was an idea for a weapons system that launches telephone pole-sized kinetic projectiles made from tungsten from Earth's orbit to damage targets on the ground. Jerry Pournelle created the concept while working in operations research at Boeing in the 1950s before becoming a science-fiction writer. In the 1980s, a similar idea was conceptualized as a potential part of the Strategic Defense Initiative, there codenamed Brilliant Pebbles.

A system described in the 2003 United States Air Force report was that of 20-foot-long (6.1 m), 1-foot-diameter (0.30 m) tungsten rods that are satellite-controlled and have global strike capability, with impact speeds of Mach 10.

The bomb would naturally contain a large kinetic energy because it moves at orbital velocities, around 8 kilometres per second (26,000 ft/s; 8,000 m/s; Mach 24) in orbit and 3 kilometres per second (9,800 ft/s; 3,000 m/s; Mach 8.8) at impact. As the rod reenters Earth's atmosphere it would lose most of the velocity, but the remaining energy would cause considerable damage. Some systems are quoted as having the yield of a small tactical nuclear bomb. These designs are envisioned as a bunker buster. As the name suggests, the 'bunker buster' is powerful enough to destroy a nuclear bunker. With 6'968 satellites on a given orbit, a target could be hit within 12'9615 minutes from any given time, less than half the time taken by an ICBM and without the launch warning. Such a system could also be equipped with sensors to detect incoming anti-ballistic missile-type threats and relatively light protective measures to use against them (e.g. Hit-To-Kill Missiles or megawatt-class chemical laser). The time between deorbit and impact would only be a few minutes, and depending on the orbits and positions in the orbits, the system would have a worldwide range. There would be no need to deploy missiles, aircraft or other vehicles.

In the case of the system mentioned in the 2003 Air Force report above, a 6.1 by 0.3 metres (20 ft 'd7 1 ft) tungsten cylinder impacting at Mach 10 (11,200 ft/s; 3,400 m/s) has a kinetic energy equivalent to approximately 11.5 tons of TNT (48 GJ). The mass of such a cylinder is itself greater than 9 short tons (8.2 t), so the practical applications of such a system are limited to those situations where its other characteristics provide a clear and decisive advantage'97a conventional bomb/warhead of similar weight to the tungsten rod, delivered by conventional means, provides similar destructive capability and is far more practical and cost-effective.

The highly elongated shape and high mass of the projectiles are intended to enhance sectional density (and therefore minimize kinetic energy loss due to air friction) and maximize penetration of hard or buried targets. The larger device is expected to be quite effective at penetrating deeply buried bunkers and other command and control targets.

The weapon would be very hard to defend against. It has a very high closing velocity and small radar cross-section. Launch is difficult to detect. Any infrared launch signature occurs in orbit, at no fixed position. The infrared launch signature also has a much smaller magnitude compared to a ballistic missile launch. The system would also have to cope with atmospheric heating from re-entry, which could melt non-tungsten components of the weapon.

The phrase "rods from God" is also used to describe the same concept. An Air Force report called them "hypervelocity rod bundles".

In the mid-1960s, popular science interest in orbital mechanics led to a number of science fiction stories which explored their implications. Among these was The Moon Is a Harsh Mistress by Robert A. Heinlein in which the citizens of the Moon bombard the Earth with rocks wrapped in iron containers which are in turn fired from an electromagnetic launch system at Earth-based targets.

In the 1970s and 1980s this idea was refined in science fiction novels such as Footfall by Larry Niven and Jerry Pournelle (the same Pournelle that first proposed the idea for military use in a non-fiction context), in which aliens use a Thor-type system. During the 1980s and 1990s references to such weapons became a staple of science fiction roleplaying games such as Traveller, Shadowrun and Heavy Gear (the latter game naming these weapons ortillery, a portmanteau of orbital artillery), as well as visual media including Babylon 5's "mass drivers" and the film Starship Troopers, itself an adaptation of a Heinlein novel of the same name. A smaller "crowbar" variant is mentioned in David's Sling by Marc Stiegler (Baen, 1988). Set in the Cold War, the story is based on the use of (relatively inexpensive) information-based "intelligent" systems to overcome an enemy's numerical advantage. The orbital kinetic bombardment system is used first to destroy the Soviet tank armies that have invaded Europe and then to take out Soviet ICBM silos prior to a nuclear strike.

In Neal Stephenson's Anathem, a kinetic bombardment weapon is deployed from orbit to trigger the eruption of a dormant volcano.

The repurposing of space colonies for use in kinetic bombardment (referred as a "colony drop") is a frequent element of the Gundam franchise and is central to the plots of Mobile Suit Gundam: Char's Counterattack, and Mobile Suit Gundam 0083: Stardust Memory, while a more limited bombardment is key to the climax of Mobile Suit Gundam: Iron-Blooded Orphans.

From the mid-1990s, kinetic weapons as science fiction plot devices appeared in video games. Appearing in Syndicate Wars as a player-usable weapon, it also featured prominently in the plots of other video games, such as Tom Clancy's EndWar, Mass Effect 2, Call of Duty: Ghosts, and Halo.

Halo features the Magnetic Accelerator Cannon (MAC), or Mass Accelerator Cannon, as the primary weapon system employed by the United Nations Space Command (UNSC) on its warships and orbital defense platforms. Essentially large coilguns, MACs are capable of firing a variety of ammunition types varying on the model and bore, ranging from hyper-dense kinetic kill slugs to sub-caliber rounds to semi-autonomous drone missiles. Most predominantly featured in Halo Wars and Halo Wars 2, the MAC is an ability that allows the player to utilize the UNSC Spirit of Fire's point-defense MAC for pinpoint orbital bombardment, allowing the player to heavily damage or destroy enemy units. However, there are variants of the MAC platforms mounted to various ships and stations, with the most powerful being able to fire a 3,000 ton projectile at anywhere between 0.4% and 25% the speed of light.

The film G.I. Joe: Retaliation depicts the destruction of Central London with a tungsten rod dropped from a satellite system.

Orion's Arm features them as a major weapon type in the galaxy of 10,000 years in the future, where they can be used on planets at speeds up to 99.9% that of light, typically sterilizing a large portion of the target world. They are referred to as Relativistic Kinetic Kill Systems, or RKKS (pronounced "rocks").

A railgun is a linear motor device, typically designed as a weapon, that uses electromagnetic force to launch high velocity projectiles. The projectile normally does not contain explosives, instead relying on the projectile's high speed, mass, and kinetic energy to inflict damage. The railgun uses a pair of parallel conductors (rails), along which a sliding armature is accelerated by the electromagnetic effects of a current that flows down one rail, into the armature and then back along the other rail. It is based on principles similar to those of the homopolar motor.

As of 2020, railguns have been researched as weapons utilizing electromagnetic forces to impart a very high kinetic energy to a projectile (e.g. APFSDS) rather than using conventional propellants. While explosive-powered military guns cannot readily achieve a muzzle velocity of more than uc0u8776 2 km/s, railguns can readily exceed 3 km/s. For a similar projectile, the range of railguns may exceed that of conventional guns. The destructive force of a projectile depends on its kinetic energy and mass at the point of impact. Because of the potentially high velocity of a railgun-launched projectile, its destructive force may be much greater than conventionally launched projectiles of the same size. The absence of explosive propellants or warheads to store and handle, as well as the low cost of projectiles compared to conventional weaponry, are also advantages.

Railguns are still very much at the research stage after decades of R&D, and it remains to be seen whether they will ever be deployed as practical military weapons. Any trade-off analysis between electromagnetic (EM) propulsion systems and chemical propellants for weapons applications must also factor in its durability, availability and economics, as well as the novelty, bulkiness, high energy demand and complexity of the pulsed power supplies that are needed for electromagnetic launcher systems.

The railgun in its simplest form differs from a traditional electric motor in that no use is made of additional field windings (or permanent magnets). This basic configuration is formed by a single loop of current and thus requires high currents (e.g., of order one million amperes) to produce sufficient accelerations (and muzzle velocities). A relatively common variant of this configuration is the augmented railgun in which the driving current is channeled through additional pairs of parallel conductors, arranged to increase ('augment') the magnetic field experienced by the moving armature. These arrangements reduce the current required for a given acceleration. In electric motor terminology, augmented railguns are usually series-wound configurations. Some railguns also use strong neodymium magnets with the field perpendicular to the current flow to increase the force on the projectile.

The armature may be an integral part of the projectile, but it may also be configured to accelerate a separate, electrically isolated or non-conducting projectile. Solid, metallic sliding conductors are often the preferred form of railgun armature but plasma or 'hybrid' armatures can also be used. A plasma armature is formed by an arc of ionised gas that is used to push a solid, non-conducting payload in a similar manner to the propellant gas pressure in a conventional gun. A hybrid armature uses a pair of plasma contacts to interface a metallic armature to the gun rails. Solid armatures may also 'transition' into hybrid armatures, typically after a particular velocity threshold is exceeded. The high current required to power a railgun can be provided by various power supply technologies, such as capacitors, pulse generators and disc generators.

For potential military applications, railguns are usually of interest because they can achieve much greater muzzle velocities than guns powered by conventional chemical propellants. Increased muzzle velocities with better aerodynamically streamlined projectiles can convey the benefits of increased firing ranges while, in terms of target effects, increased terminal velocities can allow the use of kinetic energy rounds incorporating hit-to-kill guidance, as replacements for explosive shells. Therefore, typical military railgun designs aim for muzzle velocities in the range of 2,000'963,500 m/s (4,500'967,800 mph; 7,200'9612,600 km/h) with muzzle energies of 5'9650 megajoules (MJ). For comparison, 50 MJ is equivalent to the kinetic energy of a school bus weighing 5 metric tons, traveling at 509 km/h (316 mph; 141 m/s). For single loop railguns, these mission requirements require launch currents of a few million amperes, so a typical railgun power supply might be designed to deliver a launch current of 5 MA for a few milliseconds. As the magnetic field strengths required for such launches will typically be approximately 10 tesla (100 kilogauss), most contemporary railgun designs are effectively air-cored, i.e., they do not use ferromagnetic materials such as iron to enhance the magnetic flux. However, if the barrel is made of a magnetically permeable material, the magnetic field strength increases because the increase in permeability (uc0u956  = u956 0*u956 r, where u956  is the effective permeability, u956 0 is the permeability constant and u956 r is the relative permeability of the barrel). This increases the force on the projectile.

Railgun velocities generally fall within the range of those achievable by two-stage light-gas guns; however, the latter are generally only considered to be suitable for laboratory use, while railguns are judged to offer some potential prospects for development as military weapons. A light gas gun, the Combustion Light Gas Gun in a 155 mm prototype form was projected to achieve 2500 m/s with a 70 caliber barrel. In some hypervelocity research projects, projectiles are 'pre-injected' into railguns, to avoid the need for a standing start, and both two-stage light-gas guns and conventional powder guns have been used for this role. In principle, if railgun power supply technology can be developed to provide safe, compact, reliable, combat survivable, and lightweight units, then the total system volume and mass needed to accommodate such a power supply and its primary fuel can become less than the required total volume and mass for a mission equivalent quantity of conventional propellants and explosive ammunition. Arguably such technology has been matured with the introduction of the Electromagnetic Aircraft Launch System (EMALS) (albeit that railguns require much higher system powers, because roughly similar energies must be delivered in a few milliseconds, as opposed to a few seconds). Such a development would then convey a further military advantage in that the elimination of explosives from any military weapons platform will decrease its vulnerability to enemy fire.

The concept of the railgun was first introduced by French inventor Andr'e9 Louis Octave Fauchon-Villepl'e9e, who created a small working model in 1917 with the help of the Soci'e9t'e9 anonyme des accumulateurs Tudor (now Tudor Batteries). During World War I, the French Director of Inventions at the Ministry of Armaments, Jules-Louis Brenton, commissioned Fauchon-Villeplee to develop a 30-mm to 50-mm electric cannon on July 25, 1918, after delegates from the Commission des Inventions witnessed test trials of the working model in 1917. However, the project was abandoned once World War I ended later that year on November 11, 1918. Fauchon-Villeplee filed for a US patent on 1 April 1919, which was issued in July 1922 as patent no. 1,421,435 "Electric Apparatus for Propelling Projectiles". In his device, two parallel busbars are connected by the wings of a projectile, and the whole apparatus surrounded by a magnetic field. By passing current through busbars and projectile, a force is induced which propels the projectile along the bus-bars and into flight.

In 1923, Russian scientist A. L. Korol'92kov detailed his criticisms of Fauchon-Villeplee's design, arguing against some of the claims that Fauchon-Villeplee made about the advantages of his invention. Korol'92kov eventually concluded that while the construction of a long-range electric gun was within the realm of possibility, the practical application of Fauchon-Villeplee's railgun was hindered by its enormous electric energy consumption and its need for a special electric generator of considerable capacity to power it.

In 1944, during World War II, Joachim H'e4nsler of Germany's Ordnance Office proposed the first theoretically viable railgun. By late 1944, the theory behind his electric anti-aircraft gun had been worked out sufficiently to allow the Luftwaffe's Flak Command to issue a specification, which demanded a muzzle velocity of 2,000 m/s (4,500 mph; 7,200 km/h; 6,600 ft/s) and a projectile containing 0.5 kg (1.1 lb) of explosive. The guns were to be mounted in batteries of six firing twelve rounds per minute, and it was to fit existing 12.8 cm FlaK 40 mounts. It was never built. When details were discovered after the war it aroused much interest and a more detailed study was done, culminating with a 1947 report which concluded that it was theoretically feasible, but that each gun would need enough power to illuminate half of Chicago.

During 1950, Sir Mark Oliphant, an Australian physicist and first director of the Research School of Physical Sciences at the new Australian National University, initiated the design and construction of the world's largest (500 megajoule) homopolar generator. This machine was operational from 1962 and was later used to power a large-scale railgun that was used as a scientific experiment.

In 1980, the Ballistic Research Laboratory (later consolidated to form the U.S. Army Research Laboratory) began a long-term program of theoretical and experimental research on railguns. The work was conducted predominantly at the Aberdeen Proving Ground, and much of the early research drew inspiration from the railgun experiments performed by the Australian National University. Topics of research included plasma dynamics, electromagnetic fields, telemetry, and current and heat transport. While military research into railgun technology in the United States ensued continuously in the following decades, the direction and focus that it took shifted dramatically with major changes in funding levels and the needs of different government agencies. In 1984, the formation of the Strategic Defense Initiative Organization caused research goals to shift toward establishing a constellation of satellites to intercept intercontinental ballistic missiles. As a result, the U.S. military focused on developing small guided projectiles that could withstand the high-G launch from ultra-high velocity plasma armature railguns. But after the publication of an important Defense Science Board study in 1985, the U.S. Army, Marine Corps, and DARPA were assigned to develop anti-armor, electromagnetic launch technologies for mobile ground combat vehicles. In 1990, the U.S. Army collaborated with the University of Texas at Austin to establish the Institute for Advanced Technology (IAT), which focused on research involving solid and hybrid armatures, rail-armature interactions, and electromagnetic launcher materials. The facility became the Army's first Federally Funded Research and Development Center and housed a few of the Army's electromagnetic launchers, such as the Medium Caliber Launcher.

Since 1993 the British and American governments have collaborated on a railgun project at the Dundrennan Weapons Testing Centre that culminated in the 2010 test where BAE Systems fired a 3.2 kg (7 pound) projectile at 18.4-megajoules [3,390 m/s (7,600 mph; 12,200 km/h; 11,100 ft/s)]. In 1994, India's DRDO's Armament Research and Development Establishment developed a railgun with a 240 kJ, low inductance capacitor bank operating at 5 kV power able to launch projectiles of 3'963.5 g weight to a velocity of more than 2,000 m/s (4,500 mph; 7,200 km/h; 6,600 ft/s). In 1995, the Center for Electromagnetics at the University of Texas at Austin designed and developed a rapid-fire railgun launcher called the Cannon-Caliber Electromagnetic Gun. The launcher prototype was later tested at the U.S. Army Research Laboratory, where it demonstrated a breech efficiency over 50 percent.

In 2010, the United States Navy tested a BAE Systems-designed compact-sized railgun for ship emplacement that accelerated a 3.2 kg (7 pound) projectile to hypersonic velocities of approximately 3,390 m/s (7,600 mph; 12,200 km/h; 11,100 ft/s), or about Mach 10, with 18.4 MJ of kinetic energy. It was the first time in history that such levels of performance were reached. They gave the project the motto "Velocitas Eradico", Latin for "I, [who am] speed, eradicate"'97or in the vernacular, "Speed Kills". An earlier railgun of the same design (32-megajoules) resides at the Dundrennan Weapons Testing Centre in the United Kingdom.

Low power, small scale railguns have also made popular college and amateur projects. Several amateurs actively carry out research on railguns.

A railgun consists of two parallel metal rails (hence the name). At one end, these rails are connected to an electrical power supply, to form the breech end of the gun. Then, if a conductive projectile is inserted between the rails (e.g. by insertion into the breech), it completes the circuit. Electrons flow from the negative terminal of the power supply up the negative rail, across the projectile, and down the positive rail, back to the power supply.

This current makes the railgun behave as an electromagnet, creating a magnetic field inside the loop formed by the length of the rails up to the position of the armature. In accordance with the right-hand rule, the magnetic field circulates around each conductor. Since the current is in the opposite direction along each rail, the net magnetic field between the rails (B) is directed at right angles to the plane formed by the central axes of the rails and the armature. In combination to all with the current (I) in the armature, this produces a Lorentz force which accelerates the projectile along the rails, always out of the loop (regardless of supply polarity) and away from the power supply, toward the muzzle end of the rails. There are also Lorentz forces acting on the rails and attempting to push them apart, but since the rails are mounted firmly, they cannot move.

By definition, if a current of one ampere flows in a pair of ideal infinitely long parallel conductors that are separated by a distance of one meter, then the magnitude of the force on each meter of those conductors will be exactly 0.2 micro-newtons. Furthermore, in general, the force will be proportional to the square of the magnitude of the current and inversely proportional to the distance between the conductors. It also follows that, for railguns with projectile masses of a few kg and barrel lengths of a few m, very large currents will be required to accelerate projectiles to velocities of the order of 1000 m/s.

A very large power supply, providing on the order of one million amperes of current, will create a tremendous force on the projectile, accelerating it to a speed of many kilometers per second (km/s). Although these speeds are possible, the heat generated from the propulsion of the object is enough to erode the rails rapidly. Under high-use conditions, current railguns would require frequent replacement of the rails, or to use a heat-resistant material that would be conductive enough to produce the same effect. At this time it is generally acknowledged that it will take major breakthroughs in materials science and related disciplines to produce high-powered railguns capable of firing more than a few shots from a single set of rails. The barrel must withstand these conditions for up to several rounds per minute for thousands of shots without failure or significant degradation. These parameters are well beyond the state of the art in materials science.

This section presents some elementary analysis of the fundamental theoretical electromagnetic principles that govern the mechanics of railguns.

If a railgun were to provide a uniform magnetic field of strength , oriented at right angles to both the armature and the bore axis, then, with an armature current and an armature length , the force accelerating the projectile would be given by the formula:

Here the force, current and field are all treated as vectors, so the above vector cross product gives a force directed along the bore axis, acting on the current in the armature, as a consequence of the magnetic field.

In most simple railguns, the magnetic field is only provided by the current flowing in the rails, i.e. behind the armature. It follows that the magnetic field will neither be constant nor spatially uniform. Hence, in practice, the force must be calculated after making due allowances for the spatial variation of the magnetic field over the volume of the armature.

To illustrate the principles involved, it can be useful to consider the rails and the armature as thin wires or "filaments". With this approximation, the magnitude of the force vector can be determined from a form of the Biot'96Savart law and a result of the Lorentz force. The force can be derived mathematically in terms of the permeability constant (), the radius of the rails (which are assumed to be circular in cross section) (), the distance between the central axes of the rails () and the current () as described below.

First, it can be shown from the Biot'96Savart law that at one end of a semi-infinite current-carrying wire, the magnetic field at a given perpendicular distance () from the end of the wire is given by

Note this is if the wire runs from the location of the armature e.g. from x = 0 back to and is measured relative to the axis of the wire.

So, if the armature connects the ends of two such semi-infinite wires separated by a distance, , a fairly good approximation assuming the length of the wires is much larger than , the total field from both wires at any point on the armature is:

where is the perpendicular distance from the point on the armature to the axis of one of the wires.

Note that between the rails is assuming the rails are lying in the xy plane and run from x = 0 back to as suggested above.

Next, to evaluate the force on the armature, the above expression for the magnetic field on the armature can be used in conjunction with the Lorentz Force Law,

To give the force as

This shows that the force will be proportional to the product of and the square of the current, . Because the value of uc0u956 0 is small (4u960 'd710u8722 7 H/m) it follows that powerful railguns need large driving currents.

The above formula is based on the assumption that the distance () between the point where the force () is measured and the beginning of the rails is greater than the separation of the rails () by a factor of about 3 or 4 (). Some other simplifying assumptions have also been made; to describe the force more accurately, the geometry of the rails and the projectile must be considered.

With most practical railgun geometries, it is not easy to produce an electromagnetic expression for the railgun force that is both simple and reasonably accurate. For a more workable simple model, a useful alternative is to use a lumped circuit model, to describe the relationship between the driving current and the railgun force.

In these models the railgun is modeled on an electrical circuit and the driving force can be determined from the energy flow in the circuit. The voltage across the railgun breech is given by

So the total power flowing into the railgun is then simply the product . This power represents an energy flow into three main forms: kinetic energy in the projectile and armature, energy stored in the magnetic field, and energy lost via electrical resistance heating of the rails (and armature).

As the projectile travels along the barrel, the distance from the breech to the armature increases. Hence the resistance and inductance of the barrel also increase. For a simple model, the barrel resistance and inductance can be assumed to vary as linear functions of the projectile position, , so these quantities are modeled as

where is the resistance per unit length and is the inductance per unit length, or the inductance gradient. It follows that

where is the all-important projectile velocity, . Then

Now, if the driving current is held constant, the term will be zero. Resistive losses now correspond to a power flow , while the power flow represents the electromagnetic work done.

This simple model predicts that exactly half of the electromagnetic work will be used to store energy in the magnetic field along the barrel, , as the length of the current loop increases.

The other half of the electromagnetic work represents the more useful power flow - into the kinetic energy of the projectile. Since power can be expressed as force times speed, this shows the force on the railgun armature is given by

This equation also shows that high accelerations will require very high currents. For an ideal square bore single-turn railgun, the value of would be about 0.6 microHenries per meter (uc0u956 H/m) but most practical railgun barrels exhibit lower values of than this. Maximizing the inductance gradient is but one of the challenges faced by the designers of railgun barrels.

Since the lumped circuit model describes the railgun force in terms of fairly normal circuit equations, it becomes possible to specify a simple time domain model of a railgun. Ignoring friction and air drag, the projectile acceleration is given by

where m is the projectile mass. The motion along the barrel is given by

and the above voltage and current terms can be placed into appropriate circuit equations to determine the time variation of current and voltage.

It can also be noted that the textbook formula for the high frequency inductance per unit length of a pair of parallel round wires, of radius r and axial separation d is:

So the lumped parameter model also predicts the force for this case as:

With practical railgun geometries, much more accurate two or three dimensional models of the rail and armature current distributions (and the associated forces) can be computed, e.g., by using finite element methods to solve formulations based on either the scalar magnetic potential or the magnetic vector potential.

The power supply must be able to deliver large currents, sustained and controlled over a useful amount of time. The most important gauge of power supply effectiveness is the energy it can deliver. As of December 2010, the greatest known energy used to propel a projectile from a railgun was 33 megajoules. The most common forms of power supplies used in railguns are capacitors and compulsators which are slowly charged from other continuous energy sources.

The rails need to withstand enormous repulsive forces during shooting, and these forces will tend to push them apart and away from the projectile. As rail/projectile clearances increase, arcing develops, which causes rapid vaporization and extensive damage to the rail surfaces and the insulator surfaces. This limited some early research railguns to one shot per service interval.

The inductance and resistance of the rails and power supply limit the efficiency of a railgun design. Currently different rail shapes and railgun configurations are being tested, most notably by the U.S. Navy (Naval Research Laboratory), the Institute for Advanced Technology at the University of Texas at Austin, and BAE Systems.

The rails and projectiles must be built from strong conductive materials; the rails need to survive the violence of an accelerating projectile, and heating because of the large currents and friction involved. Some erroneous work has suggested that the recoil force in railguns can be redirected or eliminated; careful theoretical and experimental analysis reveals that the recoil force acts on the breech closure just as in a chemical firearm. The rails also repel themselves via a sideways force caused by the rails being pushed by the magnetic field, just as the projectile is. The rails need to survive this without bending and must be very securely mounted. Currently published material suggests that major advances in material science must be made before rails can be developed that allow railguns to fire more than a few full-power shots before replacement of the rails is required.

In current designs massive amounts of heat are created by the electricity flowing through the rails, as well as by the friction of the projectile leaving the device. This causes three main problems: melting of equipment, decreased safety of personnel, and detection by enemy forces owing to increased infrared signature. As briefly discussed above, the stresses involved in firing this sort of device require an extremely heat-resistant material. Otherwise the rails, barrel, and all equipment attached would melt or be irreparably damaged.

In practice, the rails used with most railgun designs are subject to erosion from each launch. Additionally, projectiles can be subject to some degree of ablation, and this can limit railgun life, in some cases severely.

Railguns have a number of potential practical applications, primarily for the military. However, there are other theoretical applications currently being researched.

Electrodynamic assistance to launch rockets has been studied. Space applications of this technology would likely involve specially formed electromagnetic coils and superconducting magnets. Composite materials would likely be used for this application.

For space launches from Earth, relatively short acceleration distances (less than a few km) would require very strong acceleration forces, higher than humans can tolerate. Other designs include a longer helical (spiral) track, or a large ring design whereby a space vehicle would circle the ring numerous times, gradually gaining speed, before being released into a launch corridor leading skyward. Nevertheless, if technically feasible and cost effective to build, imparting hyper-velocity escape velocity to a projectile launching at sea level, where the atmosphere is the most dense, may result in much of the launch velocity being lost to aerodynamic drag. In addition, the projectile might still require some form of on-board guidance and control to realize a useful orbital insertion angle that may not be achievable based simply on the launcher's upward elevation angle relative to the surface of the earth, (see practical considerations of escape velocity).

In 2003, Ian McNab outlined a plan to turn this idea into a realized technology. Because of strong acceleration, this system would launch only sturdy materials, such as food, water, and'97most importantly'97fuel. Under ideal circumstances (equator, mountain, heading east) the system would cost $528/kg, compared with $5,000/kg on the conventional rocket. The McNab railgun could make approximately 2000 launches per year, for a total of maximum 500 tons launched per year. Because the launch track would be 1.6 km long, power will be supplied by a distributed network of 100 rotating machines (compulsator) spread along the track. Each machine would have a 3.3-ton carbon fibre rotor spinning at high speeds. A machine can recharge in a matter of hours using 10 MW power. This machine could be supplied by a dedicated generator. The total launch package would weigh almost 1.4 tons. Payload per launch in these conditions is over 400 kg. There would be a peak operating magnetic field of 5 T'97half of this coming from the rails, and the other half from augmenting magnets. This halves the required current through the rails, which reduces the power fourfold.

NASA has proposed to use a railgun to launch "wedge-shaped aircraft with scramjets" to high altitude at Mach 10, where it would then launch a small payload into orbit using conventional rocket propulsion. The extreme g-forces involved with direct railgun ground-launch to space may restrict the usage to only the sturdiest of payloads. Alternatively, very long rail systems may be used to reduce the required launch acceleration.

Railguns are being researched as weapons with projectiles that do not contain explosives or propellants, but are given extremely high velocities: 2,500 m/s (8,200 ft/s) (approximately Mach 7 at sea level) or more. For comparison, the M16 rifle has a muzzle speed of 930 m/s (3,050 ft/s), and the 16-inch/50-caliber Mark 7 gun that armed World War II American battleships has a muzzle speed of 760 m/s (2,490 ft/s), which because of its much greater projectile mass (up to 2,700 pounds) generated a muzzle energy of 360 MJ and a downrange kinetic impact of energy of over 160 MJ (see also Project HARP). By firing smaller projectiles at extremely high velocities, railguns may yield kinetic energy impacts equal or superior to the destructive energy of 5"/54 caliber Mark 45 Naval guns, (which achieve up to 10MJ at the muzzle), but with greater range. This decreases ammunition size and weight, allowing more ammunition to be carried and eliminating the hazards of carrying explosives or propellants in a tank or naval weapons platform. Also, by firing more aerodynamically streamlined projectiles at greater velocities, railguns may achieve greater range, less time to target, and at shorter ranges less wind drift, bypassing the physical limitations of conventional firearms: "the limits of gas expansion prohibit launching an unassisted projectile to velocities greater than about 1.5 km/s and ranges of more than 50 miles [80 km] from a practical conventional gun system."

Current railgun technologies necessitate a long and heavy barrel, but a railgun's ballistics far outperform conventional cannons of equal barrel lengths. Railguns can also deliver area of effect damage by detonating a bursting charge in the projectile which unleashes a swarm of smaller projectiles over a large area.

Assuming that the many technical challenges facing fieldable railguns are overcome, including issues like railgun projectile guidance, rail endurance, and combat survivability and reliability of the electrical power supply, the increased launch velocities of railguns may provide advantages over more conventional guns for a variety of offensive and defensive scenarios. Railguns have limited potential to be used against both surface and airborne targets.

The first weaponized railgun planned for production, the General Atomics Blitzer system, began full system testing in September 2010. The weapon launches a streamlined discarding sabot round designed by Boeing's Phantom Works at 1,600 m/s (5,200 ft/s) (approximately Mach 5) with accelerations exceeding 60,000 gn. During one of the tests, the projectile was able to travel an additional 7 kilometres (4.3 mi) downrange after penetrating a 1uc0u8260 8 inch (3.2 mm) thick steel plate. The company hopes to have an integrated demo of the system by 2016 followed by production by 2019, pending funding. Thus far, the project is self-funded.

In October 2013, General Atomics unveiled a land based version of the Blitzer railgun. A company official claimed the gun could be ready for production in "two to three years".

Railguns are being examined for use as anti-aircraft weapons to intercept air threats, particularly anti-ship cruise missiles, in addition to land bombardment. A supersonic sea-skimming anti-ship missile can appear over the horizon 20 miles from a warship, leaving a very short reaction time for a ship to intercept it. Even if conventional defense systems react fast enough, they are expensive and only a limited number of large interceptors can be carried. A railgun projectile can reach several times the speed of sound faster than a missile; because of this, it can hit a target, such as a cruise missile, much faster and farther away from the ship. Projectiles are also typically much cheaper and smaller, allowing for many more to be carried (they have no guidance systems, and rely on the railgun to supply their kinetic energy, rather than providing it themselves). The speed, cost, and numerical advantages of railgun systems may allow them to replace several different systems in the current layered defense approach. A railgun projectile without the ability to change course can hit fast-moving missiles at a maximum range of 30 nmi (35 mi; 56 km). As is the case with the Phalanx CIWS, unguided railgun rounds will require multiple/many shots to bring down maneuvering supersonic anti-ship missiles, with the odds of hitting the missile improving dramatically the closer it gets. The Navy plans for railguns to be able to intercept endoatmospheric ballistic missiles, stealthy air threats, supersonic missiles, and swarming surface threats; a prototype system for supporting interception tasks is to be ready by 2018, and operational by 2025. This timeframe suggests the weapons are planned to be installed on the Navy's next-generation surface combatants, expected to start construction by 2028.

BAE Systems was at one point interested in installing railguns on their Future Fighting Vehicle.

India has successfully tested their own railgun. Russia, China, Turkey's ASELSANuc0u8202  and Yeteknoloji are also developing railguns.

Helical railguns are multi-turn railguns that reduce rail and brush current by a factor equal to the number of turns. Two rails are surrounded by a helical barrel and the projectile or re-usable carrier is also helical. The projectile is energized continuously by two brushes sliding along the rails, and two or more additional brushes on the projectile serve to energize and commute several windings of the helical barrel direction in front of and/or behind the projectile. The helical railgun is a cross between a railgun and a coilgun. They do not currently exist in a practical, usable form.

A helical railgun was built at MIT in 1980 and was powered by several banks of, for the time, large capacitors (approximately 4 farads). It was about 3 meters long, consisting of 2 meters of accelerating coil and 1 meter of decelerating coil. It was able to launch a glider or projectile about 500 meters.

A plasma railgun is a linear accelerator and a plasma energy weapon which, like a projectile railgun, uses two long parallel electrodes to accelerate a "sliding short" armature. However, in a plasma railgun, the armature and ejected projectile consists of plasma, or hot, ionized, gas-like particles, instead of a solid slug of material. MARAUDER (Magnetically Accelerated Ring to Achieve Ultra-high Directed Energy and Radiation) is, or was, a United States Air Force Research Laboratory project concerning the development of a coaxial plasma railgun. It is one of several United States Government efforts to develop plasma-based projectiles. The first computer simulations occurred in 1990, and its first published experiment appeared on August 1, 1993. As of 1993 the project appeared to be in the early experimental stages. The weapon was able to produce doughnut-shaped rings of plasma and balls of lightning that exploded with devastating effects when hitting their target. The project's initial success led to it becoming classified, and only a few references to MARAUDER appeared after 1993.

Full-scale models have been built and fired, including a 90 mm (3.5 in) bore, 9 megajoule kinetic energy gun developed by the US DARPA. Rail and insulator wear problems still need to be solved before railguns can start to replace conventional weapons. Probably the oldest consistently successful system was built by the UK's Defence Research Agency at Dundrennan Range in Kirkcudbright, Scotland. This system was established in 1993 and has been operated for over 10 years.

The Yugoslavian Military Technology Institute developed, within a project named EDO-0, a railgun with 7 kJ kinetic energy, in 1985. In 1987 a successor was created, project EDO-1, that used projectile with a mass of 0.7 kg (1.5 lb) and achieved speeds of 3,000 m/s (9,800 ft/s), and with a mass of 1.1 kg (2.4 lb) reached speeds of 2,400 m/s (7,900 ft/s). It used a track length of 0.7 m (2.3 ft). According to those working on it, with other modifications it was able to achieve a speed of 4,500 m/s (14,800 ft/s). The aim was to achieve projectile speed of 7,000 m/s (23,000 ft/s).

China is now one of the major players in electromagnetic launchers; in 2012 it hosted the 16th International Symposium on Electromagnetic Launch Technology (EML 2012) at Beijing. Satellite imagery in late 2010 suggested that tests were being conducted at an armor and artillery range near Baotou, in the Inner Mongolia Autonomous Region.

The United States military have expressed interest in pursuing research in electric gun technology throughout the late 20th century, since electromagnetic guns don't require propellants to fire a shot as conventional gun systems do, significantly increasing crew safety and reducing logistics costs, as well as provide a greater range. In addition, railgun systems have shown to potentially provide higher velocity of projectiles, which would increase accuracy for anti-tank, artillery, and air defense by decreasing the time it takes for the projectile to reach its target destination. During the early 1990s, the U.S. Army dedicated more than $150 million into electric gun research. At the University of Texas at Austin Center for Electromechanics, military railguns capable of delivering tungsten armor-piercing bullets with kinetic energies of nine megajoules (9 MJ) have been developed. Nine megajoules is enough energy to deliver 2 kg (4.4 lb) of projectile at 3 km/s (1.9 mi/s)'97at that velocity, a sufficiently long rod of tungsten or another dense metal could easily penetrate a tank, and potentially pass through it, (see APFSDS).

The United States Naval Surface Warfare Center Dahlgren Division demonstrated an 8 MJ railgun firing 3.2 kg (7.1 lb) projectiles in October 2006 as a prototype of a 64 MJ weapon to be deployed aboard Navy warships. The main problem the U.S. Navy has had with implementing a railgun cannon system is that the guns wear out because of the immense pressures, stresses and heat that are generated by the millions of amperes of current necessary to fire projectiles with megajoules of energy. While not nearly as powerful as a cruise missile like a BGM-109 Tomahawk, that will deliver 3,000 MJ of energy to a target, such weapons would, in theory, allow the Navy to deliver more granular firepower at a fraction of the cost of a missile, and will be much harder to shoot down versus future defensive systems. For context, another relevant comparison is the Rheinmetall 120mm gun used on main battle tanks, which generates 9 MJ of muzzle energy.

In 2007 BAE Systems delivered a 32 MJ prototype (muzzle energy) to the U.S. Navy. The same amount of energy is released by the detonation of 4.8 kg (11 lb) of C4.

On January 31, 2008, the U.S. Navy tested a railgun that fired a projectile at 10.64 MJ with a muzzle velocity of 2,520 m/s (8,270 ft/s). The power was provided by a new 9-megajoule prototype capacitor bank using solid-state switches and high-energy-density capacitors delivered in 2007 and an older 32-MJ pulse power system from the US Army's Green Farm Electric Gun Research and Development Facility developed in the late 1980s that was previously refurbished by General Atomics Electromagnetic Systems (EMS) Division. It is expected to be ready between 2020 and 2025.

A test of a railgun took place on December 10, 2010, by the U.S. Navy at the Naval Surface Warfare Center Dahlgren Division. During the test, the Office of Naval Research set a world record by conducting a 33 MJ shot from the railgun, which was built by BAE Systems.

Another test took place in February 2012, at the Naval Surface Warfare Center Dahlgren Division. While similar in energy to the aforementioned test, the railgun used was considerably more compact, with a more conventional looking barrel. A General Atomics-built prototype was delivered for testing in October 2012.

External video
video icon Additional footage
video icon February 2012 test
In 2014 the U.S. Navy had plans to integrate a railgun that has a range of over 160 km (100 mi) onto a ship by 2016. This weapon, while having a form factor more typical of a naval gun, was to utilize components largely in common with those developed and demonstrated at Dahlgren. The hyper-velocity rounds weigh 10 kg (23 lb), are 18 in (460 mm), and are fired at Mach 7.

A future goal was to develop projectiles that were self-guided '96 a necessary requirement to hit distant targets or intercept missiles. When the guided rounds are developed, the Navy is projecting each round to cost about $25,000, though developing guided projectiles for guns has a history of doubling or tripling initial cost estimates. Some high velocity projectiles developed by the Navy have command guidance, but the accuracy of the command guidance is not known, nor even if it can survive a full power shot.

The only U.S. Navy ships that can produce enough electrical power to get the desired performance are the three Zumwalt-class destroyers (DDG-1000 series); they can generate 78 megawatts of power, more than is necessary to power a railgun. However, the Zumwalt has been canceled and no further units will be built. Engineers are working to derive technologies developed for the DDG-1000 series ships into a battery system so other warships can operate a railgun. As of 2014 most destroyers can spare only nine megawatts of additional electricity, while it would require 25 megawatts to propel a projectile to the desired maximum range (i.e., to launch 32MJ projectiles at a rate of 10 shots per minute). Even if ships, such as the Arleigh Burke-class destroyer, can be upgraded with enough electrical power to operate a railgun, the space taken up on the ships by the integration of an additional weapon system may force the removal of existing weapon systems to make room available. The first shipboard tests was to be from a railgun installed on an Spearhead-class expeditionary fast transport (EPF), but this was later changed to land based testing.

Though the 23 lb projectiles have no explosives, their Mach 7 velocity gives them 32 megajoules of energy, but impact kinetic energy downrange will typically be 50 percent or less of the muzzle energy. The Navy looked into other uses for railguns, besides land bombardment, such as air defense; with the right targeting systems, projectiles could intercept aircraft, cruise missiles, and even ballistic missiles. The Navy is also developing directed-energy weapons for air defense use, but it will be years or decades before they will be effective.

The railgun would be part of a Navy fleet that envisions future offensive and defensive capabilities being provided in layers: lasers to provide close range defense, railguns to provide medium range attack and defense, and cruise missiles to provide long-range attack; though railguns will cover targets up to 100 miles away that previously needed a missile. The Navy may eventually enhance railgun technology to enable it to fire at a range of 200 nmi (230 mi; 370 km) and impact with 64 megajoules of energy. One shot would require 6 million amps of current, so it will take a long time to develop capacitors that can generate enough energy and strong enough gun materials.

The most promising near-term application for weapons-rated railguns and electromagnetic guns, in general, is probably aboard naval ships with sufficient spare electrical generating capacity and battery storage space. In exchange, ship survivability may be enhanced through a comparable reduction in the quantities of potentially dangerous chemical propellants and explosives employed. Ground combat forces, however, may find that co-locating an additional electrical power supply on the battlefield for every gun system may not be as weight and space efficient, survivable, or convenient a source of immediate projectile-launching energy as conventional propellants, which are manufactured safely behind the lines and delivered to the weapon, pre-packaged, through a robust and dispersed logistics system.

In July, 2017, Defensetech reported that the Navy wished to push the Office of Naval Research's prototype railgun from a science experiment into useful weapon territory. The goal, according to Tom Beutner, head of Naval Air Warfare and Weapons for the ONR, was ten shots per minute at 32 megajoules. A 32 megajoule railgun shot is equivalent to about 23,600,000 foot-pounds, so a single 32 MJ shot has the same muzzle energy as about 200,000 .22 rounds being fired simultaneously. In more conventional power units, a 32 MJ shot every 6 s is a net power of 5.3 MW (or 5300 kW). If the railgun is assumed to be 20% efficient at turning electrical energy into kinetic energy, the ship's electrical supplies will need to provide about 25 MW for as long as firing continues.

As of 2020 the Navy had spent $500m on rail gun development over 17 years. The Navy was focusing on firing hypersonic projectiles from existing conventional guns already available in numbers. On June 1, 2021 The Drive reported that the US navy's proposed 2022 fiscal year budget had no funding for railgun research and development. Technical challenges could not be overcome, such as the massive forces of firing wearing out the barrel after only one or two dozen shots, and a rate of fire too low to be useful for missile defense. Priorities had also changed since railgun development started, with the Navy putting more focus on longer range hypersonic missiles compared to comparatively shorter range railgun projectiles.

Research on railgun technology served as a major area of focus at the Ballistic Research Laboratory (BRL) throughout the 1980s. In addition to analyzing the performance and electrodynamic and thermodynamic properties of railguns at other institutions (like Maxwell Laboratories' CHECMATE railgun), BRL procured their own railguns for study such as their one-meter railgun and their four-meter rail gun. In 1984, BRL researchers devised a technique to analyze the residue left behind on the bore surface after a shot was fired in order to investigate the cause of the bore's progressive degradation. In 1991, they determined the properties required for developing an effective launch package as well as the design criteria necessary for a railgun to incorporate finned, long rod projectiles.

Research into railguns continued after the Ballistic Research Laboratory was consolidated with six other independent Army laboratories to form the U.S. Army Research Laboratory (ARL) in 1992. One of the major projects in railgun research that ARL was involved in was the Cannon-Caliber Electromagnetic Gun (CCEMG) program, which took place at the Center for Electromechanics at the University of Texas (UT-CEM) and was sponsored by the U.S. Marine Corps and the U.S. Army Armament Research Development and Engineering Center. As part of the CCEMG program, UT-CEM designed and developed the Cannon-Caliber Electromagnetic Launcher, a rapid-fire railgun launcher, in 1995. Featuring a 30-mm roundbore, the launcher was capable of firing three, five-round salvos of 185-g launch packages at a muzzle velocity of 1850 m/s and a firing rate of 5 Hz. Rapid-fire operation was achieved by driving the launcher with multiple 83544 peak pulses provided by the CCEMG compulsator. The CCEMG railgun included several features: ceramic sidewalls, directional preloading, and liquid cooling. ARL was responsible for assessing the performance of the launcher, which was tested at the ARL Transonic Experimental Facility in Aberdeen Proving Ground, MD.

The U.S. Army Research Laboratory also monitored electromagnetic and electrothermal gun technology development at the Institute for Advanced Technology (IAT) at the University of Texas at Austin, one of five university and industry laboratories that ARL federated to procure technical support. It housed the two electromagnetic launchers, the Leander OAT and the AugOAT, as well as the Medium Caliber Launcher. The facility also provided a power system that included thirteen 1- MJ capacitor banks, an assortment of electromagnetic launcher devices and diagnostic apparatuses. The focus of the research activity was on designs, interactions and materials required for electromagnetic launchers.

In 1999, a collaboration between ARL and IAT led to the development of a radiometric method of measuring the temperature distribution of railgun armatures during a pulsed electrical discharge without disturbing the magnetic field. In 2001, ARL became the first to obtain a set of accuracy data on electromagnetic gun-launched projectiles using jump tests. In 2004, ARL researchers published papers examining the interaction of high temperature plasmas for the purpose of developing efficient railgun igniters. Early papers describe the plasma-propellant interaction group at ARL and their attempts to understand and distinguish between the chemical, thermal, and radiation effect of plasmas on conventional solid propellants. Using scanning electron microscopy and other diagnostic techniques, they evaluated in detail the influence of plasmas on specific propellant materials.

China is developing its own railgun system. According to a CNBC report from U.S. intelligence, China's railgun system was first revealed in 2011, and ground testing began in 2014. In 2015 when the weapon system gained the ability to strike over extended ranges with increased lethality. The weapon system was successfully mounted on a Chinese Navy ship in December 2017, with sea trials happening later.

In early February 2018, pictures of what is claimed to be a Chinese railgun were published online. In the pictures the gun is mounted on the bow of a Type 072III-class landing ship Haiyangshan. Media suggests that the system is or soon will be ready for testing. In March 2018, it was reported that China confirmed it had begun testing its electromagnetic rail gun at sea.

In November 2017, India's Defence Research and Development Organisation carried out a successful test of a 12 mm square bore electromagnetic railgun. Tests of a 30 mm version are planned to be conducted. India aims to fire a one kilogram projectile at a velocity of more than 2,000 meters per second using a capacitor bank of 10 megajoules. Electromagnetic guns and directed energy weapons are among the systems which Indian Navy aims to acquire in its modernisation plan up to 2030.

Major technological and operational hurdles must be overcome before railguns can be deployed:

Railgun durability: To date, public railgun demonstrations have not shown an ability to fire multiple full power shots from the same set of rails. However, the United States Navy has claimed hundreds of shots from the same set of rails. In a March 2014 statement to the Intelligence, Emerging Threats and Capabilities Subcommittee of the House Armed Services Committee, Chief of Naval Research Admiral Matthew Klunder stated, "Barrel life has increased from tens of shots to over 400, with a program path to achieve 1000 shots." However, the Office of Naval Research (ONR) will not confirm that the 400 shots are full-power shots. Further, there is nothing published to indicate there are any high megajoule-class railguns with the capability of firing hundreds of full-power shots while staying within the strict operational parameters necessary to fire railgun shots accurately and safely. Railguns should be able to fire 6 rounds per minute with a rail life of about 3000 rounds, tolerating launch accelerations of tens of thousands of g's, extreme pressures and megaampere currents, but this is not feasible with current technology.
Projectile guidance: A future capability critical to fielding a real railgun weapon is developing a robust guidance package that will allow the railgun to fire at distant targets or to hit incoming missiles. Developing such a package is a real challenge. The U.S. Navy's RFP Navy SBIR 2012.1 '96 Topic N121-102 for developing such a package gives a good overview of just how challenging railgun projectile guidance is:
The package must fit within the mass (< 2 kg), diameter (< 40 mm outer diameter), and volume (200 cm3) constraints of the projectile and do so without altering the center of gravity. It should also be able to survive accelerations of at least 20,000 g (threshold) / 40,000 g (objective) in all axes, high electromagnetic fields (E > 5,000 V/m, B > 2 T), and surface temperatures of > 800 deg C. The package should be able to operate in the presence of any plasma that may form in the bore or at the muzzle exit and must also be radiation hardened owing to exo-atmospheric flight. Total power consumption must be less than 8 watts (threshold)/5 watts (objective) and the battery life must be at least 5 minutes (from initial launch) to enable operation during the entire engagement. In order to be affordable, the production cost per projectile must be as low as possible, with a goal of less than $1,000 per unit.
On June 22, 2015, General Atomics' Electromagnetic Systems announced that projectiles with on-board electronics survived the whole railgun launch environment and performed their intended functions in four consecutive tests on June 9 and 10 June at the U.S. Army's Dugway Proving Ground in Utah. The on-board electronics successfully measured in-bore accelerations and projectile dynamics, for several kilometers downrange, with the integral data link continuing to operate after the projectiles impacted the desert floor, which is essential for precision guidance.

A linear motor is an electric motor that has had its stator and rotor "unrolled", thus, instead of producing a torque (rotation), it produces a linear force along its length. However, linear motors are not necessarily straight. Characteristically, a linear motor's active section has ends, whereas more conventional motors are arranged as a continuous loop.

A typical mode of operation is as a Lorentz-type actuator, in which the applied force is linearly proportional to the current and the magnetic field .

Linear motors are by far most commonly found in high accuracy engineering applications. It is a thriving field of applied research with dedicated scientific conferences and engineering text books.

Many designs have been put forward for linear motors, falling into two major categories, low-acceleration and high-acceleration linear motors. Low-acceleration linear motors are suitable for maglev trains and other ground-based transportation applications. High-acceleration linear motors are normally rather short, and are designed to accelerate an object to a very high speed, for example see the coilgun.

High-acceleration linear motors are typically used in studies of hypervelocity collisions, as weapons, or as mass drivers for spacecraft propulsion. They are usually of the AC linear induction motor (LIM) design with an active three-phase winding on one side of the air-gap and a passive conductor plate on the other side. However, the direct current homopolar linear motor railgun is another high acceleration linear motor design. The low-acceleration, high speed and high power motors are usually of the linear synchronous motor (LSM) design, with an active winding on one side of the air-gap and an array of alternate-pole magnets on the other side. These magnets can be permanent magnets or electromagnets. The motor for the Shanghai maglev train, for instance, is an LSM.

Brushless linear motors are members of the Synchronous motor family. They are typically used in standard linear stages or integrated into custom, high performance positioning systems. Invented in late 1980s by Anwar Chitayat at Anorad Corporation, now Rockwell Automation, and helped improving throughput and quality of industrial manufacturing processes.

Brush (electric) linear motors were used in industrial automation applications prior to the invention of Brushless linear motors. Compared with three phase Brushless motors, which are typically being used today, brush motors operates with a single phase. Brush linear motors have lower cost since they do not need moving cables and three phase servo drives. However, they require higher maintenance since their brushes wear out.

In this design the rate of movement of the magnetic field is controlled, usually electronically, to track the motion of the rotor. For cost reasons synchronous linear motors rarely use commutators, so the rotor often contains permanent magnets, or soft iron. Examples include coilguns and the motors used on some maglev systems, as well as many other linear motors. In high precision industrial automation linear motors are typically configured with a magnet stator and a moving coil. Hall effect sensor is attached to the rotor to track the magnetic flux of the stator. The electrical current is typically provided from a stationary servo drive to the moving coil by a moving cable inside a cable carrier.

In this design, the force is produced by a moving linear magnetic field acting on conductors in the field. Any conductor, be it a loop, a coil or simply a piece of plate metal, that is placed in this field will have eddy currents induced in it thus creating an opposing magnetic field, in accordance with Lenz's law. The two opposing fields will repel each other, thus creating motion as the magnetic field sweeps through the metal.

In this design a large current is passed through a metal sabot across sliding contacts that are fed from two rails. The magnetic field this generates causes the metal to be projected along the rails.

Efficient and compact design applicable to replacement of pneumatic cylinders.

Piezoelectric drive is often used to drive small linear motors.

The history of linear electric motors can be traced back at least as far as the 1840s, to the work of Charles Wheatstone at King's College London, but Wheatstone's model was too inefficient to be practical. A feasible linear induction motor is described in the U.S. Patent 782,312 (1905 - inventor Alfred Zehden of Frankfurt-am-Main), for driving trains or lifts. The German engineer Hermann Kemper built a working model in 1935. In the late 1940s, Dr. Eric Laithwaite of Manchester University, later Professor of Heavy Electrical Engineering at Imperial College in London developed the first full-size working model. In a single sided version the magnetic repulsion forces the conductor away from the stator, levitating it, and carrying it along in the direction of the moving magnetic field. He called the later versions of it magnetic river.

Because of these properties, linear motors are often used in maglev propulsion, as in the Japanese Linimo magnetic levitation train line near Nagoya. However, linear motors have been used independently of magnetic levitation, as in the Bombardier Innovia Metro systems worldwide and a number of modern Japanese subways, including Tokyo's Toei uc0u332 edo Line.

Similar technology is also used in some roller coasters with modifications but, at present, is still impractical on street running trams, although this, in theory, could be done by burying it in a slotted conduit.

Outside of public transportation, vertical linear motors have been proposed as lifting mechanisms in deep mines, and the use of linear motors is growing in motion control applications. They are also often used on sliding doors, such as those of low floor trams such as the Alstom Citadis and the Socimi Eurotram. Dual axis linear motors also exist. These specialized devices have been used to provide direct X-Y motion for precision laser cutting of cloth and sheet metal, automated drafting, and cable forming. Most linear motors in use are LIM (linear induction motor), or LSM (linear synchronous motor). Linear DC motors are not used due to higher cost and linear SRM suffers from poor thrust. So for long run in traction LIM is mostly preferred and for short run LSM is mostly preferred.

High-acceleration linear motors have been suggested for a number of uses. They have been considered for use as weapons, since current armour-piercing ammunition tends to consist of small rounds with very high kinetic energy, for which just such motors are suitable. Many amusement park launched roller coasters now use linear induction motors to propel the train at a high speed, as an alternative to using a lift hill. The United States Navy is also using linear induction motors in the Electromagnetic Aircraft Launch System that will replace traditional steam catapults on future aircraft carriers. They have also been suggested for use in spacecraft propulsion. In this context they are usually called mass drivers. The simplest way to use mass drivers for spacecraft propulsion would be to build a large mass driver that can accelerate cargo up to escape velocity, though RLV launch assist like StarTram to low Earth orbit has also been investigated.

High-acceleration linear motors are difficult to design for a number of reasons. They require large amounts of energy in very short periods of time. One rocket launcher design calls for 300 GJ for each launch in the space of less than a second. Normal electrical generators are not designed for this kind of load, but short-term electrical energy storage methods can be used. Capacitors are bulky and expensive but can supply large amounts of energy quickly. Homopolar generators can be used to convert the kinetic energy of a flywheel into electric energy very rapidly. High-acceleration linear motors also require very strong magnetic fields; in fact, the magnetic fields are often too strong to permit the use of superconductors. However, with careful design, this need not be a major problem.

Two different basic designs have been invented for high-acceleration linear motors: railguns and coilguns.

Linear motors are commonly used for actuating high performance industrial automation equipment. Their advantage, unlike any other commonly used actuator, such as ball screw, timing belt, or rack and pinion, is that they provide any combination of high precision, high velocity, high force and long travel.

Linear motors are widely used. One of the major uses of linear motors is for propelling the shuttle in looms.

Linear motors have been used for sliding doors and various similar actuators. Also, they have been used for baggage handing and even large-scale bulk materials transport.

Linear motors are sometimes used to create rotary motion, for example, they have been used at observatories to deal with the large radius of curvature.

Linear motors may also be used as an alternative to conventional chain-run lift hills for roller coasters. The coaster Maverick at Cedar Point uses one such linear motor in place of a chain lift.

A linear motor has been used for accelerating cars for crash tests.

The combination of high precision, high velocity, high force, and long travel makes brushless linear motors attractive for driving industrial automations equipment. They serve industries and applications such as semiconductor steppers, electronics surface-mount technology, automotive cartesian coordinate robots, aerospace chemical milling, optics electron microscope, healthcare laboratory automation, food and beverage pick and place.

Synchronous linear motor actuators, used in machine tools, provide high force, high velocity, high precision and high dynamic stiffness, resulting in high smoothness of motion and low settling time. They may reach velocities of 2 m/s and micron-level accuracies, at short cycle times and a smooth surface finish.

All of the following applications are in rapid transit and have the active part of the motor in the cars.

Originally developed in the late 1970s by UTDC in Canada as the Intermediate Capacity Transit System (ICTS). A test track was constructed in Millhaven, Ontario, for extensive testing of prototype cars, after which three lines were constructed:

Line 3 Scarborough in Toronto (opened 1985)
Expo Line of the Vancouver SkyTrain (opened 1985 and extended in 1994)
Detroit People Mover in Detroit (opened 1987)
ICTS was sold to Bombardier Transportation in 1991 and later known as Advanced Rapid Transit (ART) before adopting its current branding in 2011. Since then, several more installations have been made:

Kelana Jaya Line in Kuala Lumpur (opened 1998 and extended in 2016)
Millennium Line of the Vancouver SkyTrain (opened 2002 and extended in 2016)
AirTrain JFK in New York (opened 2003)
Airport Express (Beijing Subway) (opened 2008)
Everline in Yongin, South Korea (opened 2013)
All Innovia Metro systems use third rail electrification.

One of the biggest challenges faced by Japanese railway engineers in the 1970s to the 1980s was the ever increasing construction costs of subways. In response, the Japan Subway Association began studying on the feasibility of the "mini-metro" for meeting urban traffic demand in 1979. In 1981, the Japan Railway Engineering Association studied on the use of linear induction motors for such small-profile subways and by 1984 was investigating on the practical applications of linear motors for urban rail with the Japanese Ministry of Land, Infrastructure, Transport and Tourism. In 1988, a successful demonstration was made with the Limtrain at Saitama and influenced the eventual adoption of the linear motor for the Nagahori Tsurumi-ryokuchi Line in Osaka and Toei Line 12 (present-day Toei Oedo Line) in Tokyo.

To date, the following subway lines in Japan use linear motors and use overhead lines for power collection:

Two Osaka Metro lines in Osaka:
Nagahori Tsurumi-ryokuchi Line (opened 1990)
Imazatosuji Line (opened 2006)
Toei uc0u332 edo Line in Tokyo (opened 2000)
Kaigan Line of the Kobe Municipal Subway (opened 2001)
Nanakuma Line of the Fukuoka City Subway (opened 2005)
Yokohama Municipal Subway Green Line (opened 2008)
Sendai Subway Tuc0u333 zai Line (opened 2015)
In addition, Kawasaki Heavy Industries has also exported the Linear Metro to the Guangzhou Metro in China; all of the Linear Metro lines in Guangzhou use third rail electrification:

Line 4 (opened 2005)
Line 5 (opened 2009).
Line 6 (opened 2013)
There is at least one known monorail system which is not magnetically levitated, but nonetheless uses linear motors. This is the Moscow Monorail. Originally, traditional motors and wheels were to be used. However, it was discovered during test runs that the proposed motors and wheels would fail to provide adequate traction under some conditions, for example, when ice appeared on the rail. Hence, wheels are still used, but the trains use linear motors to accelerate and slow down. This is possibly the only use of such a combination, due to the lack of such requirements for other train systems.
The TELMAGV is a prototype of a monorail system that is also not magnetically levitated but uses linear motors.
High-speed trains:
Transrapid: first commercial use in Shanghai (opened in 2004)
SCMaglev, under construction in Japan (fastest train in the world, planned to open by 2027)
Rapid transit:
Birmingham Airport, UK (opened 1984, closed 1995)
M-Bahn in Berlin, Germany (opened in 1989, closed in 1991)
Daejeon EXPO, Korea (ran only 1993)
HSST: Linimo line in Aichi Prefecture, Japan (opened 2005)
Incheon Airport Maglev (opened July 2014)
Changsha Maglev Express (opened 2016)
S1 line of Beijing Subway (opened 2017)
There are many roller coasters throughout the world that use LIMs to accelerate the ride vehicles. The first being Flight of Fear at Kings Island and Kings Dominion, both opening in 1996. Battlestar Galctica: Human VS Cylon & Revenge of the Mummy at Universal Studios Singapore opened in 2010. They both use LIMs to accelerate from certain point in the rides. Revenge of the Mummy also is located at Universal Studios Hollywood and Universal Studios Florida.The Incredible Hulk Coaster and VelociCoaster at Universal's Islands of Adventure also uses linear motors. Rock 'n' Roller Coaster Starring Aerosmith was opened in 1999 in Disney's Hollywood Studios that uses an LSM to launch the vehicles into the indoor, blacklight ride enclosure.

A launch loop, or Lofstrom loop, is a proposed system for launching objects into orbit using a moving cable-like system situated inside a sheath attached to the Earth at two ends and suspended above the atmosphere in the middle. The design concept was published by Keith Lofstrom and describes an active structure maglev cable transport system that would be around 2,000 km (1,240 mi) long and maintained at an altitude of up to 80 km (50 mi). A launch loop would be held up at this altitude by the momentum of a belt that circulates around the structure. This circulation, in effect, transfers the weight of the structure onto a pair of magnetic bearings, one at each end, which support it.

Launch loops are intended to achieve non-rocket spacelaunch of vehicles weighing 5 metric tons by electromagnetically accelerating them so that they are projected into Earth orbit or even beyond. This would be achieved by the flat part of the cable which forms an acceleration track above the atmosphere.

The system is designed to be suitable for launching humans for space tourism, space exploration and space colonization, and provides a relatively low 3g acceleration.

Launch loops were described by Keith Lofstrom in November 1981 Reader's Forum of the American Astronautical Society News Letter, and in the August 1982 L5 News.

In 1982, Paul Birch published a series of papers in Journal of the British Interplanetary Society which described orbital rings and described a form which he called Partial Orbital Ring System (PORS). The launch loop idea was worked on in more detail around 1983'961985 by Lofstrom. It is a fleshed-out version of PORS specifically arranged to form a mag-lev acceleration track suitable for launching humans into space; but whereas the orbital ring used superconducting magnetic levitation, launch loops use electromagnetic suspension (EMS).

Consider a large cannon on an island that shoots a shell into the high atmosphere. The shell will follow a roughly parabolic path for the initial flight, but drag will slow the shell and cause it to return to Earth in a much more vertical path. One could make the path purely ballistic by enclosing the predicted path in a tube and removing the air. Suspending such a tube would be a significant problem depending on the length of the path. However, one can use the shell to provide this lift force, at least temporarily. If the tube is not exactly along the flight path of the shell, but slightly below it, as the shell passes through it the shell will be forced downward, thereby producing an upward force on the tube. To stay aloft, the system would require the shells to be fired continually.

The launch loop is essentially a continuous version of this concept. Instead of a cannon firing a shell, a mass driver accelerates a cable into a similar trajectory. The cable is surrounded by an evacuated tube, which is held aloft by pushing down on the cable using electromagnets. When the cable falls back to Earth at the other end of the trajectory, it is captured by a second mass driver, bent through 180 degrees, and sent back up on the opposite trajectory. The result is a single loop that is continually travelling and keeping the tube aloft.

To use the system as a space launcher, a launch loop would be about 2,000 km long and 80 km high. The loop would be in the form of a tube, known as the sheath. Floating within the sheath is another continuous tube, known as the rotor which is a sort of belt or chain. The rotor is an iron tube approximately 5 cm (2 inches) in diameter, moving around the loop at 14 km/s (31,000 miles per hour). Keeping the system aloft requires a significant amount of lift, and the resulting path is much flatter than the natural ballistic path of the rotor.

Due to the possibility of the loop failing and falling to Earth, it is normally considered as running between two islands outside of heavy shipping routes.

When at rest, the loop is at ground level. The rotor is then accelerated up to speed. As the rotor speed increases, it curves to form an arc. The structure is held up by the force from the rotor, which attempts to follow a parabolic trajectory. The ground anchors force it to go parallel to the earth upon reaching the height of 80 kilometers. Once raised, the structure requires continuous power to overcome the energy dissipated. Additional energy would be needed to power any vehicles that are launched.

To launch, vehicles are raised up on an 'elevator' cable that hangs down from the West station loading dock at 80 km, and placed on the track. The payload applies a magnetic field that generates eddy currents in the fast-moving rotor. This both lifts the payload away from the cable, as well as pulls the payload along with 3g (30 m/s'b2) acceleration. The payload then rides the rotor until it reaches the required orbital velocity, and leaves the track.

If a stable or circular orbit is needed, once the payload reaches the highest part of its trajectory then an on-board rocket engine ("kick motor") or other means is needed to circularize the trajectory to the appropriate Earth orbit.

The eddy current technique is compact, lightweight and powerful, but inefficient. With each launch the rotor temperature increases by 80 kelvins due to power dissipation. If launches are spaced too close together, the rotor temperature can approach 770 'b0C (1043 K), at which point the iron rotor loses its ferromagnetic properties and rotor containment is lost.

Closed orbits with a perigee of 80 km quite quickly decay and re-enter, but in addition to such orbits, a launch loop by itself would also be capable of directly injecting payloads into escape orbits, gravity assist trajectories past the Moon, and other non closed orbits such as close to the Trojan points.

To access circular orbits using a launch loop a relatively small 'kick motor' would need to be launched with the payload which would fire at apogee and would circularise the orbit. For GEO insertion this would need to provide a delta-v of about 1.6 km/s, for LEO to circularise at 500 km would require a delta-v of just 120 m/s. Conventional rockets require delta-vs of roughly 14 and 10 km/s to reach GEO and LEO respectively.

Launch loops in Lofstrom's design are placed close to the equator and can only directly access equatorial orbits. However other orbital planes might be reached via high altitude plane changes, lunar perturbations or aerodynamic techniques.

Launch rate capacity of a launch loop is ultimately limited by the temperature and cooling rate of the rotor to 80 per hour, but that would require a 17 GW power station; a more modest 500 MW power station is sufficient for 35 launches per day.

For a launch loop to be economically viable it would require customers with sufficiently large payload launch requirements.

Lofstrom estimates that an initial loop costing roughly $10 billion with a one-year payback could launch 40,000 metric tons per year, and cut launch costs to $300/kg. For $30 billion, with a larger power generation capacity, the loop would be capable of launching 6 million metric tons per year, and given a five-year payback period, the costs for accessing space with a launch loop could be as low as $3/kg.

Compared to space elevators, no new high-tensile strength materials have to be developed, since the structure resists Earth's gravity by supporting its own weight with the kinetic energy of the moving loop, and not by tensile strength.

Lofstrom's launch loops are expected to launch at high rates (many launches per hour, independent of weather), and are not inherently polluting. Rockets create pollution such as nitrates in their exhausts due to high exhaust temperature, and can create greenhouse gases depending on propellant choices. Launch loops as a form of electric propulsion can be clean, and can be run on geothermal, nuclear, wind, solar or any other power source, even intermittent ones, as the system has huge built-in power storage capacity.

Unlike space elevators which would have to travel through the Van Allen belts over several days, launch loop passengers can be launched to low earth orbit, which is below the belts, or through them in a few hours. This would be a similar situation to that faced by the Apollo astronauts, who had radiation doses about 0.5% of what the space elevator would give.

Unlike space elevators which are subjected to the risks of space debris and meteorites along their whole length, launch loops are to be situated at an altitude where orbits are unstable due to air drag. Since debris does not persist, it only has one chance to impact the structure. Whereas the collapse period of space elevators is expected to be of the order of years, damage or collapse of loops in this way is expected to be rare. In addition, launch loops themselves are not a significant source of space debris, even in an accident. All debris generated has a perigee that intersects the atmosphere or is at escape velocity.

Launch loops are intended for human transportation, to give a safe 3g acceleration which the vast majority of people would be capable of tolerating well, and would be a much faster way of reaching space than space elevators.

Launch loops would be quiet in operation, and would not cause any sound pollution, unlike rockets.

Finally, their low payload costs are compatible with large-scale commercial space tourism and even space colonisation.

A running loop would have an extremely large amount of energy in its linear momentum. While the magnetic suspension system would be highly redundant, with failures of small sections having essentially no effect, if a major failure did occur the energy in the loop (1.5'd71015 joules or 1.5 petajoules) would approach the same total energy release as a nuclear bomb explosion (350 kilotons of TNT equivalent), although not emitting nuclear radiation.

While this is a large amount of energy, it is unlikely that this would destroy much of the structure due to its very large size, and because most of the energy would be deliberately dumped at preselected places when the failure is detected. Steps might need to be taken to lower the cable down from 80 km altitude with minimal damage, such as by the use of parachutes.

Therefore, for safety and astrodynamic reasons, launch loops are intended to be installed over an ocean near the equator, well away from habitation.

The published design of a launch loop requires electronic control of the magnetic levitation to minimize power dissipation and to stabilize the otherwise under-damped cable.

The two main points of instability are the turnaround sections and the cable.

The turnaround sections are potentially unstable, since movement of the rotor away from the magnets gives reduced magnetic attraction, whereas movements closer gives increased attraction. In either case, instability occurs. This problem is routinely solved with existing servo control systems that vary the strength of the magnets. Although servo reliability is a potential issue, at the high speed of the rotor, very many consecutive sections would need to fail for the rotor containment to be lost.

The cable sections also share this potential issue, although the forces are much lower. However, an additional instability is present in that the cable/sheath/rotor may undergo meandering modes (similar to a Lariat chain) that grow in amplitude without limit. Lofstrom believes that this instability also can be controlled in real time by servo mechanisms, although this has never been attempted.

In works by Alexander Bolonkin it is suggested that Lofstrom's project has many non-solved problems and that it is very far from a current technology. For example, the Lofstrom project has expansion joints between 1.5 meter iron plates. Their speeds (under gravitation, friction) can be different and Bolonkin claims that they could wedge in the tube; and the force and friction in the ground 28 km diameter turnaround sections are gigantic. In 2008, Bolonkin proposed a simple rotated close-loop cable to launch the space apparatus in a way suitable for current technology.

Another project, the space cable, is a smaller design by John Knapman that is intended for launch assist for conventional rockets and suborbital tourism. The space cable design uses discrete bolts rather than a continuous rotor, as with the launch loop architecture. John Knapman has also mathematically shown that the meander instability can be tamed.

The skyhook is another launch system concept. Skyhook could be either rotating or non-rotating. The non-rotating skyhook hangs from a low Earth orbit down to just above the Earth's atmosphere (skyhook cable is not attached to Earth). The rotating skyhook changes this design to decrease the speed of the lower end; the entire cable rotates around its center of gravity. The advantage of this is an even greater velocity reduction for the launch vehicle flying to the bottom end of the rotating skyhook which makes for an even larger payload and a lower launch cost. The two disadvantages of this are: the greatly reduced time available for the arriving launch vehicle to hook up at the lower end of the rotating skyhook (approximately 3 to 5 seconds), and the lack of choice regarding the destination orbit.

A space elevator is a proposed type of planet-to-space transportation system. The main component would be a cable (also called a tether) anchored to the surface and extending into space. The design would permit vehicles to travel up the cable from a planetary surface, such as the Earth's, directly into orbit, without the use of large rockets. An Earth-based space elevator could not feasibly be simply a tall tower supported from below, due to the immense weight - instead it would consist of a cable with one end attached to the surface near the equator and the other end attached to a counterweight in space beyond geostationary orbit (35,786 km altitude). The competing forces of gravity, which is stronger at the lower end, and the upward centrifugal force, which is stronger at the upper end, would result in the cable being held up, under tension, and stationary over a single position on Earth. With the tether deployed, climbers could repeatedly climb up and down the tether by mechanical means, releasing their cargo to and from orbit.

The concept of a tower reaching geosynchronous orbit was first published in 1895 by Konstantin Tsiolkovsky. His proposal was for a free-standing tower reaching from the surface of Earth to the height of geostationary orbit. Like all buildings, Tsiolkovsky's structure would be under compression, supporting its weight from below. Since 1959, most ideas for space elevators have focused on purely tensile structures, with the weight of the system held up from above by centrifugal forces. In the tensile concepts, a space tether reaches from a large mass (the counterweight) beyond geostationary orbit to the ground. This structure is held in tension between Earth and the counterweight like an upside-down plumb bob. The cable thickness is adjusted based on tension; it has its maximum at a geostationary orbit and the minimum on the ground.

Available materials are not strong enough to make an Earth space elevator practical. Some sources have speculated that future advances in carbon nanotubes (CNTs) could lead to a practical design. Other sources have concluded that CNTs will never be strong enough. Possible future alternatives include boron nitride nanotubes, diamond nanothreads and macro-scale single crystal graphene.

The concept is applicable to other planets and celestial bodies. For locations in the solar system with weaker gravity than Earth's (such as the Moon or Mars), the strength-to-density requirements for tether materials are not as problematic. Currently available materials (such as Kevlar) are strong and light enough that they could be practical as the tether material for elevators there.

The key concept of the space elevator appeared in 1895 when Russian scientist Konstantin Tsiolkovsky was inspired by the Eiffel Tower in Paris. He considered a similar tower that reached all the way into space and was built from the ground up to the altitude of 35,786 kilometers, the height of geostationary orbit. He noted that the top of such a tower would be circling Earth as in a geostationary orbit. Objects would acquire horizontal velocity due to the Earth's rotation as they rode up the tower, and an object released at the tower's top would have enough horizontal velocity to remain there in geostationary orbit. Tsiolkovsky's conceptual tower was a compression structure, while modern concepts call for a tensile structure (or "tether").

Building a compression structure from the ground up proved an unrealistic task as there was no material in existence with enough compressive strength to support its own weight under such conditions. In 1959, the Russian engineer Yuri N. Artsutanov suggested a more feasible proposal. Artsutanov suggested using a geostationary satellite as the base from which to deploy the structure downward. By using a counterweight, a cable would be lowered from geostationary orbit to the surface of Earth, while the counterweight was extended from the satellite away from Earth, keeping the cable constantly over the same spot on the surface of the Earth. Artsutanov's idea was introduced to the Russian-speaking public in an interview published in the Sunday supplement of Komsomolskaya Pravda in 1960, but was not available in English until much later. He also proposed tapering the cable thickness in order for the stress in the cable to remain constant. This gave a thinner cable at ground level that became thickest at the level of geostationary orbit.

Both the tower and cable ideas were proposed in David E. H. Jones' quasi-humorous Ariadne column in New Scientist, December 24, 1964.

In 1966, Isaacs, Vine, Bradner and Bachus, four American engineers, reinvented the concept, naming it a "Sky-Hook", and published their analysis in the journal Science. They decided to determine what type of material would be required to build a space elevator, assuming it would be a straight cable with no variations in its cross section area, and found that the strength required would be twice that of any then-existing material including graphite, quartz, and diamond.

In 1975, an American scientist, Jerome Pearson, reinvented the concept, publishing his analysis in the journal Acta Astronautica. He designed a cross-section-area altitude profile that tapered and would be better suited to building the elevator. The completed cable would be thickest at the geostationary orbit, where the tension was greatest, and would be narrowest at the tips to reduce the amount of weight per unit area of cross section that any point on the cable would have to bear. He suggested using a counterweight that would be slowly extended out to 144,000 kilometers (89,000 miles) (almost half the distance to the Moon) as the lower sections of the elevator were built. Without a large counterweight, the upper portion of the cable would have to be longer than the lower due to the way gravitational and centrifugal forces change with distance from Earth. His analysis included disturbances such as the gravitation of the Moon, wind and moving payloads up and down the cable. The weight of the material needed to build the elevator would have required thousands of Space Shuttle trips, although part of the material could be transported up the elevator when a minimum strength strand reached the ground or be manufactured in space from asteroidal or lunar ore.

After the development of carbon nanotubes in the 1990s, engineer David Smitherman of NASA/Marshall's Advanced Projects Office realized that the high strength of these materials might make the concept of a space elevator feasible, and put together a workshop at the Marshall Space Flight Center, inviting many scientists and engineers to discuss concepts and compile plans for an elevator to turn the concept into a reality.

In 2000, another American scientist, Bradley C. Edwards, suggested creating a 100,000 km (62,000 mi) long paper-thin ribbon using a carbon nanotube composite material. He chose the wide-thin ribbon-like cross-section shape rather than earlier circular cross-section concepts because that shape would stand a greater chance of surviving impacts by meteoroids. The ribbon cross-section shape also provided large surface area for climbers to climb with simple rollers. Supported by the NASA Institute for Advanced Concepts, Edwards' work was expanded to cover the deployment scenario, climber design, power delivery system, orbital debris avoidance, anchor system, surviving atomic oxygen, avoiding lightning and hurricanes by locating the anchor in the western equatorial Pacific, construction costs, construction schedule, and environmental hazards.

To speed space elevator development, proponents have organized several competitions, similar to the Ansari X Prize, for relevant technologies. Among them are Elevator:2010, which organized annual competitions for climbers, ribbons and power-beaming systems from 2005 to 2009, the Robogames Space Elevator Ribbon Climbing competition, as well as NASA's Centennial Challenges program, which, in March 2005, announced a partnership with the Spaceward Foundation (the operator of Elevator:2010), raising the total value of prizes to US$400,000. The first European Space Elevator Challenge (EuSEC) to establish a climber structure took place in August 2011.

In 2005, "the LiftPort Group of space elevator companies announced that it will be building a carbon nanotube manufacturing plant in Millville, New Jersey, to supply various glass, plastic and metal companies with these strong materials. Although LiftPort hopes to eventually use carbon nanotubes in the construction of a 100,000 km (62,000 mi) space elevator, this move will allow it to make money in the short term and conduct research and development into new production methods." Their announced goal was a space elevator launch in 2010. On February 13, 2006, the LiftPort Group announced that, earlier the same month, they had tested a mile of "space-elevator tether" made of carbon-fiber composite strings and fiberglass tape measuring 5 cm (2.0 in) wide and 1 mm (approx. 13 sheets of paper) thick, lifted with balloons. In April 2019, Liftport CEO Michael Laine admitted little progress has been made on the company's lofty space elevator ambitions, even after receiving more than $200,000 in seed funding. The carbon nanotube manufacturing facility that Liftport announced in 2005 was never built.

In 2006 the book "Leaving the Planet by Space Elevator" was published by Dr Brad Edwards and Philip Ragan, containing a comprehensive review of the history, construction challenges and implementation plans for future space elevators, including space elevators on the Moon and Mars.

In 2007, Elevator:2010 held the 2007 Space Elevator games, which featured US$500,000 awards for each of the two competitions, ($1,000,000 total) as well as an additional $4,000,000 to be awarded over the next five years for space elevator related technologies. No teams won the competition, but a team from MIT entered the first 2-gram (0.07 oz), 100-percent carbon nanotube entry into the competition. Japan held an international conference in November 2008 to draw up a timetable for building the elevator.

In 2008 the book Leaving the Planet by Space Elevator was published in Japanese and entered the Japanese best-seller list. This led to Shuichi Ono, chairman of the Japan Space Elevator Association, unveiling a space-elevator plan, putting forth what observers considered an extremely low cost estimate of a trillion yen ('a35 billion / $8 billion) to build one.

In 2012, the Obayashi Corporation announced that it could build a space elevator by 2050 using carbon nanotube technology. The design's passenger climber would be able to reach the GEO level after an 8 day trip. Further details have been published in 2016.

In 2013, the International Academy of Astronautics published a technological feasibility assessment which concluded that the critical capability improvement needed was the tether material, which was projected to achieve the necessary specific strength within 20 years. The four-year long study looked into many facets of space elevator development including missions, development schedules, financial investments, revenue flow, and benefits. It was reported that it would be possible to operationally survive smaller impacts and avoid larger impacts, with meteors and space debris, and that the estimated cost of lifting a kilogram of payload to GEO and beyond would be $500.

In 2014, Google X's Rapid Evaluation R&D team began the design of a Space Elevator, eventually finding that no one had yet manufactured a perfectly formed carbon nanotube strand longer than a meter. They thus decided to put the project in "deep freeze" and also keep tabs on any advances in the carbon nanotube field.

In 2018, researchers at Japan's Shizuoka University launched STARS-Me, two CubeSats connected by a tether, which a mini-elevator will travel on. The experiment was launched as a test bed for a larger structure.

In 2019, the International Academy of Astronautics published "Road to the Space Elevator Era", a study report summarizing the assessment of the space elevator as of summer 2018. The essence is that a broad group of space professionals gathered and assessed the status of the space elevator development, each contributing their expertise and coming to similar conclusions: (a) Earth Space Elevators seem feasible, reinforcing the IAA 2013 study conclusion (b) Space Elevator development initiation is nearer than most think. This last conclusion is based on a potential process for manufacturing macro-scale single crystal graphene with higher specific strength than carbon nanotubes.

In 1979, space elevators were introduced to a broader audience with the simultaneous publication of Arthur C. Clarke's novel, The Fountains of Paradise, in which engineers construct a space elevator on top of a mountain peak in the fictional island country of "Taprobane" (loosely based on Sri Lanka, albeit moved south to the Equator), and Charles Sheffield's first novel, The Web Between the Worlds, also featuring the building of a space elevator. Three years later, in Robert A. Heinlein's 1982 novel Friday, the principal character mentions a disaster at the '93Quito Sky Hook'94 and makes use of the "Nairobi Beanstalk" in the course of her travels. In Kim Stanley Robinson's 1993 novel Red Mars, colonists build a space elevator on Mars that allows both for more colonists to arrive and also for natural resources mined there to be able to leave for Earth. In David Gerrold's 2000 novel, Jumping Off The Planet, a family excursion up the Ecuador "beanstalk" is actually a child-custody kidnapping. Gerrold's book also examines some of the industrial applications of a mature elevator technology. The concept of a space elevator, called the Beanstalk, is also depicted in John Scalzi's 2005 novel, Old Man's War. In a biological version, Joan Slonczewski's 2011 novel The Highest Frontier depicts a college student ascending a space elevator constructed of self-healing cables of anthrax bacilli. The engineered bacteria can regrow the cables when severed by space debris. Analemma Tower is an inhabitable variant of a space elevator, proposed as the 'tallest building in the world'.

An Earth space elevator cable rotates along with the rotation of the Earth. Therefore, the cable, and objects attached to it, would experience upward centrifugal force in the direction opposing the downward gravitational force. The higher up the cable the object is located, the less the gravitational pull of the Earth, and the stronger the upward centrifugal force due to the rotation, so that more centrifugal force opposes less gravity. The centrifugal force and the gravity are balanced at geosynchronous equatorial orbit (GEO). Above GEO, the centrifugal force is stronger than gravity, causing objects attached to the cable there to pull upward on it.

The net force for objects attached to the cable is called the apparent gravitational field. The apparent gravitational field for attached objects is the (downward) gravity minus the (upward) centrifugal force. The apparent gravity experienced by an object on the cable is zero at GEO, downward below GEO, and upward above GEO.

The apparent gravitational field can be represented this way:

where

At some point up the cable, the two terms (downward gravity and upward centrifugal force) are equal and opposite. Objects fixed to the cable at that point put no weight on the cable. This altitude (r1) depends on the mass of the planet and its rotation rate. Setting actual gravity equal to centrifugal acceleration gives:

This is 35,786 km (22,236 mi) above Earth's surface, the altitude of geostationary orbit.

On the cable below geostationary orbit, downward gravity would be greater than the upward centrifugal force, so the apparent gravity would pull objects attached to the cable downward. Any object released from the cable below that level would initially accelerate downward along the cable. Then gradually it would deflect eastward from the cable. On the cable above the level of stationary orbit, upward centrifugal force would be greater than downward gravity, so the apparent gravity would pull objects attached to the cable upward. Any object released from the cable above the geosynchronous level would initially accelerate upward along the cable. Then gradually it would deflect westward from the cable.

Historically, the main technical problem has been considered the ability of the cable to hold up, with tension, the weight of itself below any given point. The greatest tension on a space elevator cable is at the point of geostationary orbit, 35,786 km (22,236 mi) above the Earth's equator. This means that the cable material, combined with its design, must be strong enough to hold up its own weight from the surface up to 35,786 km (22,236 mi). A cable which is thicker in cross section area at that height than at the surface could better hold up its own weight over a longer length. How the cross section area tapers from the maximum at 35,786 km (22,236 mi) to the minimum at the surface is therefore an important design factor for a space elevator cable.

To maximize the usable excess strength for a given amount of cable material, the cable's cross section area would need to be designed for the most part in such a way that the stress (i.e., the tension per unit of cross sectional area) is constant along the length of the cable. The constant-stress criterion is a starting point in the design of the cable cross section area as it changes with altitude. Other factors considered in more detailed designs include thickening at altitudes where more space junk is present, consideration of the point stresses imposed by climbers, and the use of varied materials. To account for these and other factors, modern detailed designs seek to achieve the largest safety margin possible, with as little variation over altitude and time as possible. In simple starting-point designs, that equates to constant-stress.

For a constant-stress cable with no safety margin, the cross-section-area as a function of distance from Earth's center is given by the following equation:

where

Safety margin can be accounted for by dividing T by the desired safety factor.

Using the above formula we can calculate the ratio between the cross-section at geostationary orbit and the cross-section at Earth's surface, known as taper ratio:

The taper ratio becomes very large unless the specific strength of the material used approaches 48 (MPa)/(kg/m3). Low specific strength materials require very large taper ratios which equates to large (or astronomical) total mass of the cable with associated large or impossible costs.

There are a variety of space elevator designs proposed for many planetary bodies. Almost every design includes a base station, a cable, climbers, and a counterweight. For an Earth Space Elevator the Earth's rotation creates upward centrifugal force on the counterweight. The counterweight is held down by the cable while the cable is held up and taut by the counterweight. The base station anchors the whole system to the surface of the Earth. Climbers climb up and down the cable with cargo.

Modern concepts for the base station/anchor are typically mobile stations, large oceangoing vessels or other mobile platforms. Mobile base stations would have the advantage over the earlier stationary concepts (with land-based anchors) by being able to maneuver to avoid high winds, storms, and space debris. Oceanic anchor points are also typically in international waters, simplifying and reducing the cost of negotiating territory use for the base station.

Stationary land-based platforms would have simpler and less costly logistical access to the base. They also would have the advantage of being able to be at high altitudes, such as on top of mountains. In an alternate concept, the base station could be a tower, forming a space elevator which comprises both a compression tower close to the surface, and a tether structure at higher altitudes. Combining a compression structure with a tension structure would reduce loads from the atmosphere at the Earth end of the tether, and reduce the distance into the Earth's gravity field the cable needs to extend, and thus reduce the critical strength-to-density requirements for the cable material, all other design factors being equal.

A space elevator cable would need to carry its own weight as well as the additional weight of climbers. The required strength of the cable would vary along its length. This is because at various points it would have to carry the weight of the cable below, or provide a downward force to retain the cable and counterweight above. Maximum tension on a space elevator cable would be at geosynchronous altitude so the cable would have to be thickest there and taper as it approaches Earth. Any potential cable design may be characterized by the taper factor '96 the ratio between the cable's radius at geosynchronous altitude and at the Earth's surface.

The cable would need to be made of a material with a high tensile strength/density ratio. For example, the Edwards space elevator design assumes a cable material with a tensile strength of at least 100 gigapascals. Since Edwards consistently assumed the density of his carbon nanotube cable to be 1300 kg/m3, that implies a specific strength of 77 megapascal/(kg/m3). This value takes into consideration the entire weight of the space elevator. An untapered space elevator cable would need a material capable of sustaining a length of 4,960 kilometers (3,080 mi) of its own weight at sea level to reach a geostationary altitude of 35,786 km (22,236 mi) without yielding. Therefore, a material with very high strength and lightness is needed.

For comparison, metals like titanium, steel or aluminium alloys have breaking lengths of only 20'9630 km (0.2'960.3 MPa/(kg/m3)). Modern fiber materials such as kevlar, fiberglass and carbon/graphite fiber have breaking lengths of 100'96400 km (1.0'964.0 MPa/(kg/m3)). Nanoengineered materials such as carbon nanotubes and, more recently discovered, graphene ribbons (perfect two-dimensional sheets of carbon) are expected to have breaking lengths of 5000'966000 km (50'9660 MPa/(kg/m3)), and also are able to conduct electrical power.

For a space elevator on Earth, with its comparatively high gravity, the cable material would need to be stronger and lighter than currently available materials. For this reason, there has been a focus on the development of new materials that meet the demanding specific strength requirement. For high specific strength, carbon has advantages because it is only the 6th element in the periodic table. Carbon has comparatively few of the protons and neutrons which contribute most of the dead weight of any material. Most of the interatomic bonding forces of any element are contributed by only the outer few electrons. For carbon, the strength and stability of those bonds is high compared to the mass of the atom. The challenge in using carbon nanotubes remains to extend to macroscopic sizes the production of such material that are still perfect on the microscopic scale (as microscopic defects are most responsible for material weakness). As of 2014, carbon nanotube technology allowed growing tubes up to a few tenths of meters.

In 2014, diamond nanothreads were first synthesized. Since they have strength properties similar to carbon nanotubes, diamond nanothreads were quickly seen as candidate cable material as well.

A space elevator cannot be an elevator in the typical sense (with moving cables) due to the need for the cable to be significantly wider at the center than at the tips. While various designs employing moving cables have been proposed, most cable designs call for the "elevator" to climb up a stationary cable.

Climbers cover a wide range of designs. On elevator designs whose cables are planar ribbons, most propose to use pairs of rollers to hold the cable with friction.

Climbers would need to be paced at optimal timings so as to minimize cable stress and oscillations and to maximize throughput. Lighter climbers could be sent up more often, with several going up at the same time. This would increase throughput somewhat, but would lower the mass of each individual payload.

The horizontal speed, i.e. due to orbital rotation, of each part of the cable increases with altitude, proportional to distance from the center of the Earth, reaching low orbital speed at a point approximately 66 percent of the height between the surface and geostationary orbit, or a height of about 23,400 km. A payload released at this point would go into a highly eccentric elliptical orbit, staying just barely clear from atmospheric reentry, with the periapsis at the same altitude as LEO and the apoapsis at the release height. With increasing release height the orbit would become less eccentric as both periapsis and apoapsis increase, becoming circular at geostationary level. When the payload has reached GEO, the horizontal speed is exactly the speed of a circular orbit at that level, so that if released, it would remain adjacent to that point on the cable. The payload can also continue climbing further up the cable beyond GEO, allowing it to obtain higher speed at jettison. If released from 100,000 km, the payload would have enough speed to reach the asteroid belt.

As a payload is lifted up a space elevator, it would gain not only altitude, but horizontal speed (angular momentum) as well. The angular momentum is taken from the Earth's rotation. As the climber ascends, it is initially moving slower than each successive part of cable it is moving on to. This is the Coriolis force: the climber "drags" (westward) on the cable, as it climbs, and slightly decreases the Earth's rotation speed. The opposite process would occur for descending payloads: the cable is tilted eastward, thus slightly increasing Earth's rotation speed.

The overall effect of the centrifugal force acting on the cable would cause it to constantly try to return to the energetically favorable vertical orientation, so after an object has been lifted on the cable, the counterweight would swing back toward the vertical, a bit like a pendulum. Space elevators and their loads would be designed so that the center of mass is always well-enough above the level of geostationary orbit to hold up the whole system. Lift and descent operations would need to be carefully planned so as to keep the pendulum-like motion of the counterweight around the tether point under control.

Climber speed would be limited by the Coriolis force, available power, and by the need to ensure the climber's accelerating force does not break the cable. Climbers would also need to maintain a minimum average speed in order to move material up and down economically and expeditiously. At the speed of a very fast car or train of 300 km/h (190 mph) it will take about 5 days to climb to geosynchronous orbit.

Both power and energy are significant issues for climbers '96 the climbers would need to gain a large amount of potential energy as quickly as possible to clear the cable for the next payload.

Various methods have been proposed to get that energy to the climber:

Transfer the energy to the climber through wireless energy transfer while it is climbing.
Transfer the energy to the climber through some material structure while it is climbing.
Store the energy in the climber before it starts '96 requires an extremely high specific energy such as nuclear energy.
Solar power '96 After the first 40 km it is possible to use solar energy to power the climber
Wireless energy transfer such as laser power beaming is currently considered the most likely method, using megawatt-powered free electron or solid state lasers in combination with adaptive mirrors approximately 10 m (33 ft) wide and a photovoltaic array on the climber tuned to the laser frequency for efficiency. For climber designs powered by power beaming, this efficiency is an important design goal. Unused energy would need to be re-radiated away with heat-dissipation systems, which add to weight.

Yoshio Aoki, a professor of precision machinery engineering at Nihon University and director of the Japan Space Elevator Association, suggested including a second cable and using the conductivity of carbon nanotubes to provide power.

Several solutions have been proposed to act as a counterweight:

a heavy, captured asteroid;
a space dock, space station or spaceport positioned past geostationary orbit
a further upward extension of the cable itself so that the net upward pull would be the same as an equivalent counterweight;
parked spent climbers that had been used to thicken the cable during construction, other junk, and material lifted up the cable for the purpose of increasing the counterweight.
Extending the cable has the advantage of some simplicity of the task and the fact that a payload that went to the end of the counterweight-cable would acquire considerable velocity relative to the Earth, allowing it to be launched into interplanetary space. Its disadvantage is the need to produce greater amounts of cable material as opposed to using just anything available that has mass.

An object attached to a space elevator at a radius of approximately 53,100 km would be at escape velocity when released. Transfer orbits to the L1 and L2 Lagrangian points could be attained by release at 50,630 and 51,240 km, respectively, and transfer to lunar orbit from 50,960 km.

At the end of Pearson's 144,000 km (89,000 mi) cable, the tangential velocity is 10.93 kilometers per second (6.79 mi/s). That is more than enough to escape Earth's gravitational field and send probes at least as far out as Jupiter. Once at Jupiter, a gravitational assist maneuver could permit solar escape velocity to be reached.

A space elevator could also be constructed on other planets, asteroids and moons.

A Martian tether could be much shorter than one on Earth. Mars' surface gravity is 38 percent of Earth's, while it rotates around its axis in about the same time as Earth. Because of this, Martian stationary orbit is much closer to the surface, and hence the elevator could be much shorter. Current materials are already sufficiently strong to construct such an elevator. Building a Martian elevator would be complicated by the Martian moon Phobos, which is in a low orbit and intersects the Equator regularly (twice every orbital period of 11 h 6 min). Phobos and Deimos may get in the way of a geostationary space elevator, however, they may contribute useful resources to the project. Phobos is projected to contain high amounts of carbon. If carbon nanotubes become feasible for a tether material, there will be an abundance of carbon in Mars local region. This could provide readily available resources for the future colonization on Mars.

Phobos also could be a good counterweight for a space elevator. It's massive enough that unbalanced forces created by a space elevator would not affect the orbit of the planet. But since Phobos is not in geostationary orbit, the tether would not be able to anchor to the ground. The end of the tether would have to be in the outer atmosphere and would pass over the same place twice a Martian day.

The Earth's Moon is a potential location for a Lunar space elevator, especially as the specific strength required for the tether is low enough to use currently available materials. The Moon does not rotate fast enough for an elevator to be supported by centrifugal force (the proximity of the Earth means there is no effective lunar-stationary orbit), but differential gravity forces means that an elevator could be constructed through Lagrangian points. A near-side elevator would extend through the Earth-Moon L1 point from an anchor point near the center of the visible part of Earth's Moon: the length of such an elevator must exceed the maximum L1 altitude of 59,548 km, and would be considerably longer to reduce the mass of the required apex counterweight. A far-side lunar elevator would pass through the L2 Lagrangian point and would need to be longer than on the near-side: again, the tether length depends on the chosen apex anchor mass, but it could also be made of existing engineering materials.

Rapidly spinning asteroids or moons could use cables to eject materials to convenient points, such as Earth orbits; or conversely, to eject materials to send a portion of the mass of the asteroid or moon to Earth orbit or a Lagrangian point. Freeman Dyson, a physicist and mathematician, has suggested using such smaller systems as power generators at points distant from the Sun where solar power is uneconomical.

A space elevator using presently available engineering materials could be constructed between mutually tidally locked worlds, such as Pluto and Charon or the components of binary asteroid 90 Antiope, with no terminus disconnect, according to Francis Graham of Kent State University. However, spooled variable lengths of cable must be used due to ellipticity of the orbits.

The construction of a space elevator would need reduction of some technical risk. Some advances in engineering, manufacturing and physical technology are required. Once a first space elevator is built, the second one and all others would have the use of the previous ones to assist in construction, making their costs considerably lower. Such follow-on space elevators would also benefit from the great reduction in technical risk achieved by the construction of the first space elevator.

Prior to the work of Edwards in 2000, most concepts for constructing a space elevator had the cable manufactured in space. That was thought to be necessary for such a large and long object and for such a large counterweight. Manufacturing the cable in space would be done in principle by using an asteroid or Near-Earth object for source material. These earlier concepts for construction require a large preexisting space-faring infrastructure to maneuver an asteroid into its needed orbit around Earth. They also required the development of technologies for manufacture in space of large quantities of exacting materials.

Since 2001, most work has focused on simpler methods of construction requiring much smaller space infrastructures. They conceive the launch of a long cable on a large spool, followed by deployment of it in space. The spool would be initially parked in a geostationary orbit above the planned anchor point. A long cable would be dropped "downward" (toward Earth) and would be balanced by a mass being dropped "upward" (away from Earth) for the whole system to remain on the geosynchronous orbit. Earlier designs imagined the balancing mass to be another cable (with counterweight) extending upward, with the main spool remaining at the original geosynchronous orbit level. Most current designs elevate the spool itself as the main cable is paid out, a simpler process. When the lower end of the cable is long enough to reach the surface of the Earth (at the equator), it would be anchored. Once anchored, the center of mass would be elevated more (by adding mass at the upper end or by paying out more cable). This would add more tension to the whole cable, which could then be used as an elevator cable.

One plan for construction uses conventional rockets to place a "minimum size" initial seed cable of only 19,800 kg. This first very small ribbon would be adequate to support the first 619 kg climber. The first 207 climbers would carry up and attach more cable to the original, increasing its cross section area and widening the initial ribbon to about 160 mm wide at its widest point. The result would be a 750-ton cable with a lift capacity of 20 tons per climber.

For early systems, transit times from the surface to the level of geosynchronous orbit would be about five days. On these early systems, the time spent moving through the Van Allen radiation belts would be enough that passengers would need to be protected from radiation by shielding, which would add mass to the climber and decrease payload.

A space elevator would present a navigational hazard, both to aircraft and spacecraft. Aircraft could be diverted by air-traffic control restrictions. All objects in stable orbits that have perigee below the maximum altitude of the cable that are not synchronous with the cable would impact the cable eventually, unless avoiding action is taken. One potential solution proposed by Edwards is to use a movable anchor (a sea anchor) to allow the tether to "dodge" any space debris large enough to track.

Impacts by space objects such as meteoroids, micrometeorites and orbiting man-made debris pose another design constraint on the cable. A cable would need to be designed to maneuver out of the way of debris, or absorb impacts of small debris without breaking.

With a space elevator, materials might be sent into orbit at a fraction of the current cost. As of 2000, conventional rocket designs cost about US$25,000 per kilogram (US$11,000 per pound) for transfer to geostationary orbit. Current space elevator proposals envision payload prices starting as low as $220 per kilogram ($100 per pound), similar to the $5'96$300/kg estimates of the Launch loop, but higher than the $310/ton to 500 km orbit quoted to Dr. Jerry Pournelle for an orbital airship system.

Philip Ragan, co-author of the book Leaving the Planet by Space Elevator, states that "The first country to deploy a space elevator will have a 95 percent cost advantage and could potentially control all space activities."

The International Space Elevator Consortium (ISEC) is a US Non-Profit 501(c)(3) Corporation formed to promote the development, construction, and operation of a space elevator as "a revolutionary and efficient way to space for all humanity". It was formed after the Space Elevator Conference in Redmond, Washington in July 2008 and became an affiliate organization with the National Space Society in August 2013. ISEC hosts an annual Space Elevator conference at the Seattle Museum of Flight.

ISEC coordinates with the two other major societies focusing on space elevators: the Japanese Space Elevator Association and EuroSpaceward. ISEC supports symposia and presentations at the International Academy of Astronautics and the International Astronautical Federation Congress each year.

The conventional current concept of a "Space Elevator" has evolved from a static compressive structure reaching to the level of GEO, to the modern baseline idea of a static tensile structure anchored to the ground and extending to well above the level of GEO. In the current usage by practitioners (and in this article), a "Space Elevator" means the Tsiolkovsky-Artsutanov-Pearson type as considered by the International Space Elevator Consortium. This conventional type is a static structure fixed to the ground and extending into space high enough that cargo can climb the structure up from the ground to a level where simple release will put the cargo into an orbit.

Some concepts related to this modern baseline are not usually termed a "Space Elevator", but are similar in some way and are sometimes termed "Space Elevator" by their proponents. For example, Hans Moravec published an article in 1977 called "A Non-Synchronous Orbital Skyhook" describing a concept using a rotating cable. The rotation speed would exactly match the orbital speed in such a way that the tip velocity at the lowest point was zero compared to the object to be "elevated". It would dynamically grapple and then "elevate" high flying objects to orbit or low orbiting objects to higher orbit.

The original concept envisioned by Tsiolkovsky was a compression structure, a concept similar to an aerial mast. While such structures might reach space (100 km, 62 mi), they are unlikely to reach geostationary orbit. The concept of a Tsiolkovsky tower combined with a classic space elevator cable (reaching above the level of GEO) has been suggested. Other ideas use very tall compressive towers to reduce the demands on launch vehicles. The vehicle is "elevated" up the tower, which may extend as high as above the atmosphere, and is launched from the top. Such a tall tower to access near-space altitudes of 20 km (12 mi) has been proposed by various researchers.

Other concepts for non-rocket spacelaunch related to a space elevator (or parts of a space elevator) include an orbital ring, a pneumatic space tower, a space fountain, a launch loop, a skyhook, a space tether, and a buoyant "SpaceShaft".

Space tethers are long cables which can be used for propulsion, momentum exchange, stabilization and attitude control, or maintaining the relative positions of the components of a large dispersed satellite/spacecraft sensor system. Depending on the mission objectives and altitude, spaceflight using this form of spacecraft propulsion is theorized to be significantly less expensive than spaceflight using rocket engines.

Tether satellites might be used for various purposes, including research into tether propulsion, tidal stabilization and orbital plasma dynamics. Five main techniques for employing space tethers are in development:

Electrodynamic tethers

Electrodynamic tethers are primarily used for propulsion. These are conducting tethers that carry a current that can generate either thrust or drag from a planetary magnetic field, in much the same way as an electric motor does.

Momentum exchange tethers

These can be either rotating tethers, or non-rotating tethers, that capture an arriving spacecraft and then release it at a later time into a different orbit with a different velocity. Momentum exchange tethers can be used for orbital maneuvering, or as part of a planetary-surface-to-orbit / orbit-to-escape-velocity space transportation system.

Tethered formation flying

This is typically a non-conductive tether that accurately maintains a set distance between multiple space vehicles flying in formation.

Electric sail

A form of solar wind sail with electrically charged tethers that will be pushed by the momentum of solar wind ions.

Universal Orbital Support System

A concept for suspending an object from a tether orbiting in space.

Many uses for space tethers have been proposed, including deployment as space elevators, as skyhooks, and for doing propellant-free orbital transfers.

Konstantin Tsiolkovsky (1857-1935) once proposed a tower so tall that it reached into space, so that it would be held there by the rotation of Earth. However, at the time, there was no realistic way to build it.

In 1960, another Russian, Yuri Artsutanov, wrote in greater detail about the idea of a tensile cable to be deployed from a geosynchronous satellite, downwards towards the ground, and upwards away, keeping the cable balanced. This is the space elevator idea, a type of synchronous tether that would rotate with the earth. However, given the materials technology of the time, this too was impractical on Earth.

In the 1970s, Jerome Pearson independently conceived the idea of a space elevator, sometimes referred to as a synchronous tether, and, in particular, analyzed a lunar elevator that can go through the L1 and L2 points, and this was found to be possible with materials then existing.

In 1977, Hans Moravec and later Robert L. Forward investigated the physics of non-synchronous skyhooks, also known as rotating skyhooks, and performed detailed simulations of tapered rotating tethers that could pick objects off, and place objects onto, the Moon, Mars and other planets, with little loss, or even a net gain of energy.

In 1979, NASA examined the feasibility of the idea and gave direction to the study of tethered systems, especially tethered satellites.

In 1990, E. Sarmont proposed a non-rotating Orbiting Skyhook for an Earth-to-orbit / orbit-to-escape-velocity Space Transportation System in a paper titled "An Orbiting Skyhook: Affordable Access to Space". In this concept a suborbital launch vehicle would fly to the bottom end of a Skyhook, while spacecraft bound for higher orbit, or returning from higher orbit, would use the upper end.

In 2000, NASA and Boeing considered a HASTOL concept, where a rotating tether would take payloads from a hypersonic aircraft (at half of orbital velocity) to orbit.

A tether satellite is a satellite connected to another by a space tether. A number of satellites have been launched to test tether technologies, with varying degrees of success.

There are many different (and overlapping) types of tether.

Momentum exchange tethers are one of many applications for space tethers. Momentum exchange tethers come in two types; rotating and non-rotating. A rotating tether will create a controlled force on the end-masses of the system due to centrifugal acceleration. While the tether system rotates, the objects on either end of the tether will experience continuous acceleration; the magnitude of the acceleration depends on the length of the tether and the rotation rate. Momentum exchange occurs when an end body is released during the rotation. The transfer of momentum to the released object will cause the rotating tether to lose energy, and thus lose velocity and altitude. However, using electrodynamic tether thrusting, or ion propulsion the system can then re-boost itself with little or no expenditure of consumable reaction mass.

A skyhook is a theoretical class of orbiting tether propulsion intended to lift payloads to high altitudes and speeds. Proposals for skyhooks include designs that employ tethers spinning at hypersonic speed for catching high speed payloads or high altitude aircraft and placing them in orbit.

Electrodynamic tethers are long conducting wires, such as one deployed from a tether satellite, which can operate on electromagnetic principles as generators, by converting their kinetic energy to electrical energy, or as motors, converting electrical energy to kinetic energy. Electric potential is generated across a conductive tether by its motion through the earth's magnetic field. The choice of the metal conductor to be used in an electrodynamic tether is determined by a variety of factors. Primary factors usually include high electrical conductivity and low density. Secondary factors, depending on the application, include cost, strength, and melting point.

An electrodynamic tether was profiled in the documentary film Orphans of Apollo as technology that was to be used to keep the Russian space station Mir in orbit.

This is the use of a (typically) non-conductive tether to connect multiple spacecraft. A proposed 2011 experiment to study the technique is the Tethered Experiment for Mars inter-Planetary Operations (TEMPO'b3).

A theoretical type of non-rotating tethered satellite system, it is a concept for providing space-based support to things suspended above an astronomical object. The orbital system is a coupled mass system wherein the upper supporting mass (A) is placed in an orbit around a given celestial body such that it can support a suspended mass (B) at a specific height above the surface of the celestial body, but lower than (A).

Instead of rotating end for end, tethers can also be kept straight by the slight difference in the strength of gravity over their length.

A non-rotating tether system has a stable orientation that is aligned along the local vertical (of the earth or other body). This can be understood by inspection of the figure on the right where two spacecraft at two different altitudes have been connected by a tether. Normally, each spacecraft would have a balance of gravitational (e.g. Fg1) and centrifugal (e.g. Fc1) forces, but when tied together by a tether, these values begin to change with respect to one another. This phenomenon occurs because, without the tether, the higher-altitude mass would travel slower than the lower mass. The system must move at a single speed, so the tether must therefore slow down the lower mass and speed up the upper one. The centrifugal force of the tethered upper body is increased, while that of the lower-altitude body is reduced. This results in the centrifugal force of the upper body and the gravitational force of the lower body being dominant. This difference in forces naturally aligns the system along the local vertical, as seen in the figure.

Objects in low Earth orbit are subjected to noticeable erosion from atomic oxygen due to the high orbital speed with which the molecules strike as well as their high reactivity. This could quickly erode a tether.

Simple single-strand tethers are susceptible to micrometeoroids and space junk. Several systems have since been proposed and tested to improve debris resistance:

The US Naval Research Laboratory has successfully flown a long term 6 km long, 2-3mm diameter tether with an outer layer of Spectra 1000 braid and a core of acrylic yarn. This satellite, the Tether Physics and Survivability Experiment (TiPS), was launched in June 1996 and remained in operation over 10 years, finally breaking in July 2006.
Robert P. Hoyt patented an engineered circular net, such that a cut strand's strains would be redistributed automatically around the severed strand. This is called a Hoytether. Hoytethers have theoretical lifetimes of decades.
Researchers with JAXA have also proposed net-based tethers for their future missions.
Large pieces of junk would still cut most tethers, including the improved versions listed here, but these are currently tracked on radar and have predictable orbits. A tether could be wiggled to dodge known pieces of junk, or thrusters used to change the orbit, avoiding a collision.

Radiation, including UV radiation tend to degrade tether materials, and reduce lifespan. Tethers that repeatedly traverse the Van Allen belts can have markedly lower life than those that stay in low earth orbit or are kept outside Earth's magnetosphere.

Tether properties and materials are dependent on the application. However, there are some common properties. To achieve maximum performance and low cost, tethers would need to be made of materials with the combination of high strength or electrical conductivity and low density. All space tethers are susceptible to space debris or micrometeroids. Therefore, system designers will need to decide whether or not a protective coating is needed, including relative to UV and atomic oxygen. Research is being conducted to assess the probability of a collision that would damage the tether.

For applications that exert high tensile forces on the tether, the materials need to be strong and light. Some current tether designs use crystalline plastics such as ultra high molecular weight polyethylene, aramid or carbon fiber. A possible future material would be carbon nanotubes, which have an estimated tensile strength between 140 and 177 GPa (20.3-25.6 million psi), and a proven tensile strength in the range 50-60 GPa for some individual nanotubes. (A number of other materials obtain 10 to 20 GPa in some samples on the nano scale, but translating such strengths to the macro scale has been challenging so far, with, as of 2011, CNT-based ropes being an order of magnitude less strong, not yet stronger than more conventional carbon fiber on that scale).

For some applications, the tensile force on the tether is projected to be less than 65 newtons (15 lbf). Material selection in this case depends on the purpose of the mission and design constraints. Electrodynamic tethers, such as the one used on TSS-1R, may use thin copper wires for high conductivity (see EDT).

There are design equations for certain applications that may be used to aid designers in identifying typical quantities that drive material selection.

Space elevator equations typically use a "characteristic length", Lc, which is also known as its "self-support length" and is the length of untapered cable it can support in a constant 1 g gravity field.

,
where uc0u963  is the stress limit (in pressure units) and u961  is the density of the material.

Hypersonic skyhook equations use the material's "specific velocity" which is equal to the maximum tangential velocity a spinning hoop can attain without breaking:

For rotating tethers (rotovators) the value used is the material's 'characteristic velocity' which is the maximum tip velocity a rotating untapered cable can attain without breaking,

The characteristic velocity equals the specific velocity multiplied by the square root of two.

These values are used in equations similar to the rocket equation and are analogous to specific impulse or exhaust velocity. The higher these values are, the more efficient and lighter the tether can be in relation to the payloads that they can carry. Eventually however, the mass of the tether propulsion system will be limited at the low end by other factors such as momentum storage.

Proposed materials include Kevlar, ultra high molecular weight polyethylene, carbon nanotubes and M5 fiber. M5 is a synthetic fiber that is lighter than Kevlar or Spectra. According to Pearson, Levin, Oldson, and Wykes in their article "The Lunar Space Elevator", an M5 ribbon 30 mm wide and 0.023 mm thick, would be able to support 2000 kg on the lunar surface. It would also be able to hold 100 cargo vehicles, each with a mass of 580 kg, evenly spaced along the length of the elevator. Other materials that could be used are T1000G carbon fiber, Spectra 2000, or Zylon.

For gravity stabilised tethers, to exceed the self-support length the tether material can be tapered so that the cross-sectional area varies with the total load at each point along the length of the cable. In practice this means that the central tether structure needs to be thicker than the tips. Correct tapering ensures that the tensile stress at every point in the cable is exactly the same. For very demanding applications, such as an Earth space elevator, the tapering can reduce the excessive ratios of cable weight to payload weight. In lieu of tapering a modular staged tether system maybe used to achieve the same goal. Multiple tethers would be used between stages. The number of tethers would determine the strength of any given cross-section.

For rotating tethers not significantly affected by gravity, the thickness also varies, and it can be shown that the area, A, is given as a function of r (the distance from the centre) as follows:

where R is the radius of tether, v is the velocity with respect to the centre, M is the tip mass, is the material density, and T is the design tensile strength.

Integrating the area to give the volume and multiplying by the density and dividing by the payload mass gives a payload mass / tether mass ratio of:

where erf is the normal probability error function.

Let ,

then:

This equation can be compared with the rocket equation, which is proportional to a simple exponent on a velocity, rather than a velocity squared. This difference effectively limits the delta-v that can be obtained from a single tether.

In addition the cable shape must be constructed to withstand micrometeorites and space junk. This can be achieved with the use of redundant cables, such as the Hoytether; redundancy can ensure that it is very unlikely that multiple redundant cables would be damaged near the same point on the cable, and hence a very large amount of total damage can occur over different parts of the cable before failure occurs.

Beanstalks and rotovators are currently limited by the strengths of available materials. Although ultra-high strength plastic fibers (Kevlar and Spectra) permit rotovators to pluck masses from the surface of the Moon and Mars, a rotovator from these materials cannot lift from the surface of the Earth. In theory, high flying, supersonic (or hypersonic) aircraft could deliver a payload to a rotovator that dipped into Earth's upper atmosphere briefly at predictable locations throughout the tropic (and temperate) zone of Earth. As of May 2013, all mechanical tethers (orbital and elevators) are on hold until stronger materials are available.

Cargo capture for rotovators is nontrivial, and failure to capture can cause problems. Several systems have been proposed, such as shooting nets at the cargo, but all add weight, complexity, and another failure mode. At least one lab scale demonstration of a working grapple system has been achieved, however.

Currently, the strongest materials in tension are plastics that require a coating for protection from UV radiation and (depending on the orbit) erosion by atomic oxygen. Disposal of waste heat is difficult in a vacuum, so overheating may cause tether failures or damage.

Electrodynamic tethers deployed along the local vertical ('hanging tethers') may suffer from dynamical instability. Pendular motion causes the tether vibration amplitude to build up under the action of electromagnetic interaction. As the mission time increases, this behavior can compromise the performance of the system. Over a few weeks, electrodynamic tethers in Earth orbit might build up vibrations in many modes, as their orbit interacts with irregularities in magnetic and gravitational fields.

One plan to control the vibrations is to actively vary the tether current to counteract the growth of the vibrations. Electrodynamic tethers can be stabilized by reducing their current when it would feed the oscillations, and increasing it when it opposes oscillations. Simulations have demonstrated that this can control tether vibration. This approach requires sensors to measure tether vibrations, which can either be an inertial navigation system on one end of the tether, or satellite navigation systems mounted on the tether, transmitting their positions to a receiver on the end.

Another proposed method is to use spinning electrodynamic tethers instead of hanging tethers. The gyroscopic effect provides passive stabilisation, avoiding the instability.

As mentioned earlier, conductive tethers have failed from unexpected current surges. Unexpected electrostatic discharges have cut tethers (e.g. see Tethered Satellite System Reflight (TSSuc0u8209 1R) on STSu8209 75), damaged electronics, and welded tether handling machinery. It may be that the Earth's magnetic field is not as homogeneous as some engineers have believed.

Computer models frequently show tethers can snap due to vibration.

Mechanical tether-handling equipment is often surprisingly heavy, with complex controls to damp vibrations. The one ton climber proposed by Brad Edwards for his Space Elevator may detect and suppress most vibrations by changing speed and direction. The climber can also repair or augment a tether by spinning more strands.

The vibration modes that may be a problem include skipping rope, transverse, longitudinal, and pendulum.

Tethers are nearly always tapered, and this can greatly amplify the movement at the thinnest tip in whip-like ways.

A tether is not a spherical object, and has significant extent. This means that as an extended object, it is not directly modelable as a point source, and this means that the center of mass and center of gravity are not usually colocated. Thus the inverse square law does not apply except at large distances, to the overall behaviour of a tether. Hence the orbits are not completely Keplerian, and in some cases they are actually chaotic.

With bolus designs, rotation of the cable interacting with the non-linear gravity fields found in elliptical orbits can cause exchange of orbital angular momentum and rotation angular momentum. This can make prediction and modelling extremely complex

Musk first mentioned that he was thinking about a concept for a "fifth mode of transport", calling it the Hyperloop, in July 2012 at a PandoDaily event in Santa Monica, California. This hypothetical high-speed mode of transportation would have the following characteristics: immunity to weather, collision free, twice the speed of a plane, low power consumption, and energy storage for 24-hour operations. The name Hyperloop was chosen because it would go in a loop. Musk envisions the more advanced versions will be able to go at hypersonic speed. In May 2013, Musk likened his Hyperloop to a "cross between a Concorde and a railgun and an air hockey table".

From late 2012 until August 2013, a group of engineers from both Tesla and SpaceX worked on the conceptual modeling of Musk's Hyperloop. An early system conceptual model was published in the Tesla and SpaceX blogs which describes one potential design, function, pathway, and cost of a hyperloop system. According to the alpha design, pods would accelerate to cruising speeds gradually using linear electric motors and glide above their track on air bearings through tubes above ground on columns or below ground in tunnels to avoid the dangers of grade crossings. An ideal hyperloop system will be more energy-efficient, quiet, and autonomous than existing modes of mass transit. Musk has also invited feedback to "see if the people can find ways to improve it". The Hyperloop Alpha was released as an open source design. The trademark "HYPERLOOP", applicable to "high-speed transportation of goods in tubes" was issued to SpaceX on 4 April 2017.

In June 2015, SpaceX announced that it would build a 1-mile-long (1.6 km) test track to be located next to SpaceX's Hawthorne facility. The track was completed and used to test pod designs supplied by third parties in the competition.

By November 2015, with several commercial companies and dozens of student teams pursuing the development of Hyperloop technologies, the Wall Street Journal asserted that "The Hyperloop Movement", as some of its unaffiliated members refer to themselves, is officially bigger than the man who started it."

The Massachusetts Institute of Technology (MIT) hyperloop team developed the first hyperloop pod prototype, which they unveiled at the MIT Museum on 13 May 2016. Their design uses electrodynamic suspension for levitating and eddy current braking.

On 29 January 2017, approximately one year after phase one of the hyperloop pod competition, the MIT Hyperloop pod demonstrated the first ever low-pressure hyperloop run in the world. Within this first competition the Delft University team from the Netherlands achieved the highest overall competition score, winning the prize for "best overall design". The award for the "fastest pod" was won by the Technical University of Munich (TUM), Germany, team WARR Hyperloop. The MIT team placed third overall in the competition, judged by SpaceX engineers.

The second hyperloop pod competition took place from 25 to 27 August 2017. The only judging criterion was top speed, provided it was followed by successful deceleration. The TUM WARR Hyperloop won the competition by reaching a top speed of 324 km/h (201 mph), breaking the previous record of 310 km/h (190 mph) for hyperloop prototypes set by Hyperloop One on their own test track.

A third hyperloop pod competition took place in July 2018. The defending champions, the TUM WARR hyperloop team, beat their own record with a top speed of 457 km/h (284 mph) during their run.

The fourth competition in August 2019 saw the TUM team, now known as TUM Hyperloop (by NEXT Prototypes e.V.), again winning the competition and beating their own record with a top speed of 463 km/h (288 mph).

The first passenger test of hyperloop technology was successfully conducted by Virgin Hyperloop with two employees of the company in November 2020, where the unit reached a maximum speed of 172 km/h (107 mph).

The vactrain concept resembles a high-speed rail system without substantial air resistance by employing magnetically levitating trains in evacuated (airless) or partly evacuated tubes. However, the difficulty of maintaining a vacuum over large distances has prevented this type of system from ever being built. The hyperloop is similar to a vactrain system but operates at approximately one millibar (100 Pa) of pressure.

The hyperloop concept operates by sending specially designed "capsules" or "pods" through a steel tube maintained at a partial vacuum. In Musk's original concept, each capsule would float on a 0.02'960.05 in (0.5'961.3 mm) layer of air provided under pressure to air-caster "skis", similar to how pucks are levitated above an air hockey table, while still allowing higher speeds than wheels can sustain. With rolling resistance eliminated and air resistance greatly reduced, the capsules can glide for the bulk of the journey. In the alpha design concept, an electrically driven inlet fan and axial compressor would be placed at the nose of the capsule to "actively transfer high-pressure air from the front to the rear of the vessel", resolving the problem of air pressure building in front of the vehicle, slowing it down. A fraction of the air was to be shunted to the skis for additional pressure, augmenting that gain passively from lift due to their shape.

In the alpha-level concept, passenger-only pods were to be 7 ft 4 in (2.23 m) in diameter and were projected to reach a top speed of 760 mph (1,220 km/h) to maintain aerodynamic efficiency. (Section 4.4) The design proposed passengers experience a maximum inertial acceleration of 0.5 g, about 2 or 3 times that of a commercial airliner on takeoff and landing.

A number of routes have been proposed for hyperloop systems that meet the approximate distance conditions for which a hyperloop is hypothesized to provide improved transport times (distances of under approximately 1,500 kilometres (930 miles)). Route proposals range from speculation described in company releases to business cases to signed agreements.

The route suggested in the 2013 alpha-level design document was from the Greater Los Angeles Area to the San Francisco Bay Area. That conceptual system would begin around Sylmar, just south of the Tejon Pass, follow Interstate 5 to the north, and arrive near Hayward on the east side of San Francisco Bay. Several proposed branches were also shown in the design document, including Sacramento, Anaheim, San Diego, and Las Vegas.

No work has been done on the route proposed in Musk's alpha-design; one cited reason is that it would terminate on the fringes of the two major metropolitan areas (Los Angeles and San Francisco), resulting in significant cost savings in construction, but requiring that passengers traveling to and from Downtown Los Angeles and San Francisco, and any other community beyond Sylmar and Hayward, to transfer to another transportation mode in order to reach their final destination. This would significantly lengthen the total travel time to those destinations.

A similar problem already affects present-day air travel, where on short routes (like LAX'96SFO) the flight time is only a rather small part of door to door travel time. Critics have argued that this would significantly reduce the proposed cost and/or time savings of hyperloop as compared to the California High-Speed Rail project that will serve downtown stations in both San Francisco and Los Angeles. Passengers traveling from financial center to financial center are estimated to save about two hours by taking the Hyperloop instead of driving the whole distance.

Others questioned the cost projections for the suggested California route. Some transportation engineers argued in 2013 that they found the alpha-level design cost estimates unrealistically low given the scale of construction and reliance on unproven technology. The technological and economic feasibility of the idea is unproven and a subject of significant debate.

In November 2017, Arrivo announced a concept for a maglev automobile transport system from Aurora, Colorado to Denver International Airport, the first leg of a system from downtown Denver. Its contract described potential completion of a first leg in 2021. In February 2018, Hyperloop Transportation Technologies announced a similar plan for a loop connecting Chicago and Cleveland and a loop connecting Washington and New York City.

In 2018 the Missouri Hyperloop Coalition was formed between Virgin Hyperloop One, the University of Missouri, and engineering firm Black & Veatch to study a proposed route connecting St. Louis, Columbia, and Kansas City.

On 19 December 2018, Elon Musk unveiled a 2-mile (3 km) tunnel below Los Angeles. In the presentation, a Tesla Model X drove in a tunnel on the predefined track (rather than in a low-pressure tube). According to Musk the costs for the system are US$10 million. Musk said: "The Loop is a stepping stone toward hyperloop. The Loop is for transport within a city. Hyperloop is for transport between cities, and that would go much faster than 150 mph."

The Northeast Ohio Areawide Coordinating Agency, or NOACA, partnered with Hyperloop Transportation Technologies to conduct a $1.3 million feasibility study for developing a hyperloop corridor route from Chicago to Cleveland and Pittsburgh for America's first multistate hyperloop system in the Great Lakes Megaregion. Hundreds of thousands of dollars already have been committed to the project. NOACA's Board of Directors has awarded a $550,029 contract to Transportation Economics & Management Systems, Inc. (TEMS) for the Great Lakes Hyperloop Feasibility Study to evaluate the feasibility of an ultra-high-speed hyperloop passenger and freight transport system initially linking Cleveland and Chicago.

Hyperloop Transportation Technologies were considering in 2016 with the Indian Government for a proposed route between Chennai and Bengaluru, with a conceptual travel time for 345 km (214 mi) of 30 minutes. HTT also signed an agreement with Andhra Pradesh government to build India's first hyperloop project connecting Amaravathi to Vijayawada in a 6-minute ride.

On 22 February 2018, Hyperloop One entered into a memorandum of understanding with the Government of Maharashtra to build a hyperloop transportation system between Mumbai and Pune that would cut the travel time from the current 180 minutes to 20 minutes.

Indore-based Dinclix GroundWorks' DGWHyperloop advocates a hyperloop corridor between Mumbai and Delhi, via Indore, Kota, and Jaipur.

The Ministry of Railways will collaborate with IIT Madras for the development of an "indigenous" Hyperloop system and will help set-up a Centre of Excellence for Hyperloop Technologies at IIT.

On 6 February 2020, the Ministry of Transport in the Kingdom of Saudi Arabia announced a contract agreement with Virgin Hyperloop One (VHO) to conduct a ground-breaking pre-feasibility study on the use of hyperloop technology for the transport of passengers and cargo. The study will serve as a blueprint for future hyperloop projects and build on the developers long-standing relationship with the kingdom, which has peaked when His Royal Highness Prince Mohammed bin Salman Abdulaziz Al Saud, Crown Prince of Saudi Arabia, viewed VHO's passenger pod during a visit to the United States.

On 29 December 2021, the Veneto Regional Council approved a memorandum of understanding with MIMS and CAV for the testing of hyper transfer technology. By mid 2023, the feasibility study by a company selected by CAV will have to be completed and the development of a first prototype completed in 2026. 4 million euro have been allocated for this phase.

Many of the active Hyperloop routes that have been considered are outside of the US. In 2016, Hyperloop One published the world's first detailed business case for a 300-mile (500 km) route between Helsinki and Stockholm, which would tunnel under the Baltic Sea to connect the two capitals in under 30 minutes. Hyperloop One undertook a feasibility study with DP World to move containers from its Port of Jebel Ali in Dubai. In late 2016, Hyperloop One announced a feasibility study with Dubai's Roads and Transport Authority for passenger and freight routes connecting Dubai with the greater United Arab Emirates. Hyperloop One was also considering passenger routes in Moscow during 2016, and a cargo hyperloop to connect Hunchun in north-eastern China to the Port of Zarubino, near Vladivostok and the North Korean border on Russia's Far East. In May 2016, Hyperloop One kicked off their Global Challenge with a call for comprehensive proposals of hyperloop networks around the world. In September 2017, Hyperloop One selected 10 routes from 35 of the strongest proposals: Toronto'96Montreal, Cheyenne'96Denver'96Pueblo, Miami'96Orlando, Dallas'96Laredo'96Houston, Chicago'96Columbus'96Pittsburgh, Mexico City'96Guadalajara, Edinburgh'96London, Glasgow'96Liverpool, Bengaluru'96Chennai, and Mumbai'96Chennai.

Others have put forward European routes, including a route beginning at Amsterdam or Schiphol to Frankfurt. In 2016, a Warsaw University of Technology team began evaluating potential routes from Cracow to Gdauc0u324 sk across Poland proposed by Hyper Poland.

TransPod explored the possibility of hyperloop routes which would connect Toronto and Montreal, Toronto to Windsor, and Calgary to Edmonton. Toronto and Montreal, the largest cities in Canada, are currently connected by Ontario Highway 401, the busiest highway in North America. In March 2019, Transport Canada commissioned the study of hyperloops, so it can be "better informed on the technical, operational, economic, safety, and regulatory aspects of the hyperloop and understand its construction requirements and commercial feasibility."

Hyperloop Transportation Technologies (HTT) reportedly signed an agreement with the government of Slovakia in March 2016 to perform impact studies, with potential links between Bratislava, Vienna, and Budapest, but there have been no further developments. In January 2017, HTT signed an agreement to explore the route Bratislava'97Brno'97Prague in Central Europe.

In 2017, SINTEF, the largest independent research organization in Scandinavia, announced they were considering building a test lab for hyperloop in Norway.

An agreement was signed in June 2017 to co-develop a hyperloop line between Seoul and Busan in South Korea.

According to Musk, hyperloop would be useful on Mars as no tubes would be needed because Mars' atmosphere is about 1% the density of the Earth's at sea level. For the hyperloop concept to work on Earth, low-pressure tubes are required to reduce air resistance. However, if they were to be built on Mars, the lower air resistance would allow a hyperloop to be created with no tube, only a track, and so would be just a magnetically levitating train.

In September 2013, Ansys Corporation ran computational fluid dynamics simulations to model the aerodynamics of the capsule and shear stress forces that the capsule would be subjected to. The simulation showed that the capsule design would need to be significantly reshaped to avoid creating supersonic airflow, and that the gap between the tube wall and capsule would need to be larger. Ansys employee Sandeep Sovani said the simulation showed that hyperloop has challenges but that he is convinced it is feasible.

In October 2013, the development team of the OpenMDAO software framework released an unfinished, conceptual open-source model of parts of the hyperloop's propulsion system. The team asserted that the model demonstrated the concept's feasibility, although the tube would need to be 13 feet (4 m) in diameter, significantly larger than originally projected. However, the team's model is not a true working model of the propulsion system, as it did not account for a wide range of technical factors required to physically construct a hyperloop based on Musk's concept, and in particular had no significant estimations of component weight.

In November 2013, MathWorks analyzed the proposal's suggested route and concluded that the route was mainly feasible. The analysis focused on the acceleration experienced by passengers and the necessary deviations from public roads in order to keep the accelerations reasonable; it did highlight that maintaining a trajectory along I-580 east of San Francisco at the planned speeds was not possible without significant deviation into heavily populated areas.

In January 2015, a paper based on the NASA OpenMDAO open-source model reiterated the need for a larger diameter tube and a reduced cruise speed closer to Mach 0.85. It recommended removing on-board heat exchangers based on thermal models of the interactions between the compressor cycle, tube, and ambient environment. The compression cycle would only contribute 5% of the heat added to the tube, with 95% of the heat attributed to radiation and convection into the tube. The weight and volume penalty of on-board heat exchangers would not be worth the minor benefit, and regardless the steady-state temperature in the tube would only reach 30'9640 'b0F (17'9622 'b0C) above ambient temperature.

According to Musk, various aspects of the hyperloop have technology applications to other Musk interests, including surface transportation on Mars and electric jet propulsion.

Researchers associated with MIT's department of Aeronautics and Astronautics published research in June 2017 that verified the challenge of aerodynamic design near the Kantrowitz limit that had been theorized in the original SpaceX Alpha-design concept released in 2013.

In 2017, Dr. Richard Geddes and others formed the Hyperloop Advanced Research Partnership to act as a clearinghouse of Hyperloop public domain reports and data.

In February 2020, Hardt Hyperloop, Hyper Poland, TransPod and Zeleros formed a consortium to drive standardisation efforts, as part of a joint technical committee (JTC20) set up by European standards bodies CEN and CENELEC to develop common standards aimed at ensuring the safety and interoperability of infrastructure, rolling stock, signalling and other systems.

TUM Hyperloop is a research program that emerged in 2019 from the team of hyperloop pod competition from the Technical University of Munich. The TUM Hyperloop team had won all four competitions in a row, achieving the world record of 463 km/h (288 mph), which is still valid today. The research program has the goals to investigate the technical feasibility by means of a demonstrator, as well as by simulation the economic and technical feasibility of the hyperloop system. The planned 24m demonstrator will consist of a tube and the full-size pod. The next steps after completion of the first project phase are the extension to 400m to investigate higher speeds. This is planned in the Munich area, in Taufkirchen, Ottobrunn or at the Oberpfaffenhofen airfield.

EuroTube is a non-profit research organization for the development of vacuum transport technology. EuroTube is currently developing a 3.1 km (1.9 mi) test tube in Collombey-Muraz, Switzerland. The organization was founded in 2017 at ETH Zurich as a Swiss association and became a Swiss foundation in 2019. The test tube is planned on a 2:1 scale with a diameter of 2.2 m and designed for 900 km/h (560 mph).

A number of student and non-student teams were participating in a hyperloop pod competition in 2015'9616, and at least 22 of them built hardware to compete on a sponsored hyperloop test track in mid-2016.

In June 2015, SpaceX announced that they would sponsor a hyperloop pod design competition, and would build a 1-mile-long (1.6 km) subscale test track near SpaceX's headquarters in Hawthorne, California, for the competitive event in 2016. SpaceX stated in their announcement, "Neither SpaceX nor Elon Musk is affiliated with any Hyperloop companies. While we are not developing a commercial Hyperloop ourselves, we are interested in helping to accelerate development of a functional Hyperloop prototype."

More than 700 teams had submitted preliminary applications by July, and detailed competition rules were released in August. Intent to Compete submissions were due in September 2015 with more detailed tube and technical specification released by SpaceX in October. A preliminary design briefing was held in November 2015, where more than 120 student engineering teams were selected to submit Final Design Packages due by 13 January 2016.

A Design Weekend was held at Texas A&M University 29'9630 January 2016, for all invited entrants. Engineers from the Massachusetts Institute of Technology were named the winners of the competition. While the University of Washington team won the Safety Subsystem Award, Delft University won the Pod Innovation Award as well as the second place, followed by the University of Wisconsin'96Madison, Virginia Tech, and the University of California, Irvine. In the Design Category, the winner team was Hyperloop UPV from Universitat Politecnica de Valencia, Spain. On 29 January 2017, Delft Hyperloop (Delft University of Technology) won the prize for the "best overall design" at the final stage of the SpaceX hyperloop competition, while WARR Hyperloop of the Technical University of Munich won the prize for "fastest pod". The Massachusetts Institute of Technology placed third.

The second hyperloop pod competition took place from 25 to 27 August 2017. The only judging criteria being top speed provided it is followed by successful deceleration. WARR Hyperloop from the Technical University of Munich won the competition by reaching a top speed of 324 km/h (201 mph).

A third hyperloop pod competition took place in July 2018. The defending champions, the WARR Hyperloop team from the Technical University of Munich, beat their own record with a top speed of 457 km/h (284 mph) during their run. The fourth competition in August 2019 saw the team from the Technical University of Munich, now known as TUM Hyperloop (by NEXT Prototypes e.V.), again winning the competition and beating their own record with a top speed of 463 km/h (288 mph).

Some critics of hyperloop focus on the experience'97possibly unpleasant and frightening'97of riding in a narrow, sealed, windowless capsule inside a sealed steel tunnel, that is subjected to significant acceleration forces; high noise levels due to air being compressed and ducted around the capsule at near-sonic speeds; and the vibration and jostling. Even if the tube is initially smooth, ground may shift with seismic activity. At high speeds, even minor deviations from a straight path may add considerable buffeting. This is in addition to practical and logistical questions regarding how to best deal with safety issues such as equipment malfunction, accidents, and emergency evacuations.

Other maglev trains are already in use, which avoid much of the added costs of hyperloop. The SCMaglev in Japan has demonstrated 603 km/h (375 mph) without a vacuum tube, by using an extremely aerodynamic train design. It also avoids the cost and time required to pressurize and depressurize the exit and entry points of a hyperloop tube.

There is also the criticism of design technicalities in the tube system. John Hansman, professor of aeronautics and astronautics at MIT, has stated problems, such as how a slight misalignment in the tube would be compensated for and the potential interplay between the air cushion and the low-pressure air. He has also questioned what would happen if the power were to go out when the pod was miles away from a city. UC Berkeley physics professor Richard Muller has also expressed concern regarding "[the Hyperloop's] novelty and the vulnerability of its tubes, [which] would be a tempting target for terrorists", and that the system could be disrupted by everyday dirt and grime.

The alpha proposal projected that cost savings compared with conventional rail would come from a combination of several factors. The small profile and elevated nature of the alpha route would enable hyperloop to be constructed primarily in the median of Interstate 5. However, whether this would be truly feasible is a matter of debate. The low profile would reduce tunnel boring requirements and the light weight of the capsules is projected to reduce construction costs over conventional passenger rail. It was asserted that there would be less right-of-way opposition and environmental impact as well due to its small, sealed, elevated profile versus that of a rail easement; however, other commentators contend that a smaller footprint does not guarantee less opposition. In criticizing this assumption, mass transportation writer Alon Levy said, "In reality, an all-elevated system (which is what Musk proposes with the Hyperloop) is a bug rather than a feature. Central Valley land is cheap; pylons are expensive, as can be readily seen by the costs of elevated highways and trains all over the world". Michael Anderson, a professor of agricultural and resource economics at UC Berkeley, predicted that costs would amount to around US$100 billion.

No total ticket price was suggested in the alpha design in Elon Musk's initial whitepaper. Projected low ticket prices by hyperloop developers have been questioned by Dan Sperling, director of the Institute of Transportation Studies at UC Davis, who stated that "there's no way the economics on that would ever work out." Some critics have argued that, since hyperloops are designed to carry fewer passengers than typical public train systems, it could make it difficult to price tickets to cover the costs of construction and running.

The early cost estimates of the hyperloop are a subject of debate. A number of economists and transportation experts have expressed the belief that the US$6 billion price tag dramatically understates the cost of designing, developing, constructing, and testing an all-new form of transportation. The Economist said that the estimates are unlikely to "be immune to the hypertrophication of cost that every other grand infrastructure project seems doomed to suffer."

Political impediments to the construction of such a project in California will be very large. There is a great deal of "political and reputation capital" invested in the existing mega-project of California High-Speed Rail. Replacing that with a different design would not be straightforward given California's political economy. Texas has been suggested as an alternate for its more amenable political and economic environment.

Building a successful hyperloop sub-scale demonstration project could reduce the political impediments and improve cost estimates. Musk has suggested that he may be personally involved in building a demonstration prototype of the hyperloop concept, including funding the development effort.

The solar panels Musk plans to install along the length of the hyperloop system have been criticized by engineering professor Roger Goodall of Loughborough University, as not being feasible enough to return enough energy to power the hyperloop system, arguing that the air pumps and propulsion would require much more power than the solar panels could generate.

The concept of transportation of passengers in pneumatic tubes is not new. The first patent to transport goods in tubes was created in 1799 by the British mechanical engineer and inventor George Medhurst. In 1812, Medhurst wrote a book detailing his idea of transporting passengers and goods through air-tight tubes using air propulsion.

In the early 1800s, there were other similar systems proposed or experimented with and were generally known as atmospheric railway projects, though this term is now also used for systems where the propulsion is provided by a separate pneumatic tube to the train tunnel itself.

One of the earliest constructions was the Dalkey Atmospheric Railway which operated near Dublin between 1844 and 1854.

The Crystal Palace pneumatic railway operated in London around 1864 and used large fans, some 22 ft (6.7 m) in diameter, that were powered by a steam engine. The tunnels are now lost but the line operated successfully for over a year.

Operated from 1870 to 1873, the Beach Pneumatic Transit was a one-block-long prototype of an underground tube transport public transit system in New York City. The system worked at near-atmospheric pressure, and the passenger car moved by means of higher pressure air applied to the back of the car while comparatively lower pressure air was maintained on the front of the car.

In the 1910s, vacuum trains were first described by American rocket pioneer Robert Goddard. While the hyperloop has significant innovations over early proposals for reduced pressure or vacuum-tube transportation apparatus, the work of Goddard "appears to have the greatest overlap with the Hyperloop".

In 1981 Princeton physicist Gerard K. O'Neill wrote about transcontinental trains using magnetic propulsion in his book 2081: A Hopeful View of the Human Future. Though being a work of fiction, this book was an attempt to predict future technologies in everyday life. In his prediction, he envisioned these trains which used magnetic levitation running in tunnels which had much of the air evacuated to increase speed and significantly reduce friction. He also demonstrated a scale prototype device that accelerated an object using magnetic propulsion to high speeds. It was called a mass driver and was a central theme in his non-fiction book on space colonization "The High Frontier".

Swissmetro was a proposal to run a maglev train in a low-pressure environment. Concessions were granted to Swissmetro in the early 2000s to connect the Swiss cities of St. Gallen, Zurich, Basel, and Geneva. Studies of commercial feasibility reached differing conclusions and the vactrain was never built.

The ET3 Global Alliance (ET3) was founded by Daryl Oster in 1997 with the goal of establishing a global transportation system using passenger capsules in frictionless maglev full-vacuum tubes. Oster and his team met with Elon Musk on 18 September 2013, to discuss the technology, resulting in Musk promising an investment in a 3-mile (5 km) prototype of ET3's proposed design.

From 2003 Franco Cotana led the development of Pipenet which had a small bore evacuated tube for moving freight at up to 2,000 km/h (1,200 mph) using linear synchronous motors and magnetic levitation. A prototype system - 100 m (110 yd) long and 1.25 m (1.37 yd) in diameter - was constructed in Italy in 2005. However development stopped after funding ceased.

China was reported to be building a vacuum based 600 mph (1,000 km/h) maglev train in August 2010 according to a laboratory at Southwest Jiaotong University. It was expected to cost CN'a510'9620 million (US$2.95 million at the August 2010 exchange rate) more per kilometer than regular high-speed rail. In 2018 a 45 m (49 yd) loop test track was completed to test some of the technology.

In 2018, the new concept of creating and using intermodal hyperloop capsules was presented. After detaching the drive elements, capsules could be used in a way similar to traditional containers for fast transport of goods or individuals. It has been proposed that specialized airplanes, dedicated high-speed trains, road tractors or watercrafts perform "last mile" transport for solving the problem of fast transportation to centers where hyperloop terminals are locally unavailable or infeasible to be constructed.

In May 2021, it was reported that a low-vacuum sealed tube test system capable of reaching speeds around 1,000 km/h (620 mph) had begun construction in Datong, Shanxi Province. An initial 2 km (1.2 mi) section is to be completed by July 2022 and a 15 km (9.3 mi) test line within two years. The line will be built by the North University of China and the Third Research Institute of China Aerospace Science and Industry Corporation.

A hyperloop is a proposed high-speed transportation system for both public and goods transport. The term was popularized by Elon Musk to describe a modern project based on the vactrain concept (first appearance 1799). Hyperloop designs employ three essential components: tubes, pods, and terminals. The tube is a large sealed, low-pressure system (usually a long tunnel). The pod is a coach pressurized at atmospheric pressure that runs substantially free of air resistance or friction inside this tube, using magnetic propulsion (in some cases augmented by a ducted fan). The terminal handles pod arrivals and departures. The Hyperloop, in the initial form proposed by Musk, differs from vactrains by relying on residual air pressure inside the tube to provide lift by aerofoils and propulsion by fans.

The hyperloop has its roots in a concept by George Medhurst in 1799 and subsequently developed under the names pneumatic railway, atmospheric railway or vactrain. Elon Musk renewed interest in hyperloop after mentioning it in a 2012 speaking event. Musk further promoted the concept by publishing a white paper in August 2013, which conceived of a hyperloop route running from the Los Angeles region to the San Francisco Bay Area, roughly following the Interstate 5 corridor. His initial concept incorporated reduced-pressure tubes in which pressurized capsules ride on air bearings driven by linear induction motors and axial compressors. Transportation analysts challenged the cost estimates included in the white paper, with some predicting that a realized hyperloop would be several billion dollars over budget.

The hyperloop concept has been promoted by Musk and SpaceX, and other companies or organizations have been encouraged to collaborate and develop the technology. Technical University of Munich Hyperloop set the hyperloop speed record of 463 km/h (288 mph) in July 2019 at the pod design competition hosted by SpaceX in Hawthorne, California. Virgin Hyperloop conducted the first human trial in November 2020 at its test site in Las Vegas, reaching a top speed of 172 km/h (107 mph).

After being conceived in 1799, the vactrain was invented in 1904 by Robert H. Goddard, a freshman at Worcester Polytechnic Institute.

An electromagnet is a type of magnet in which the magnetic field is produced by an electric current. Electromagnets usually consist of wire wound into a coil. A current through the wire creates a magnetic field which is concentrated in the hole, denoting the center of the coil. The magnetic field disappears when the current is turned off. The wire turns are often wound around a magnetic core made from a ferromagnetic or ferrimagnetic material such as iron; the magnetic core concentrates the magnetic flux and makes a more powerful magnet.

The main advantage of an electromagnet over a permanent magnet is that the magnetic field can be quickly changed by controlling the amount of electric current in the winding. However, unlike a permanent magnet that needs no power, an electromagnet requires a continuous supply of current to maintain the magnetic field.

Electromagnets are widely used as components of other electrical devices, such as motors, generators, electromechanical solenoids, relays, loudspeakers, hard disks, MRI machines, scientific instruments, and magnetic separation equipment. Electromagnets are also employed in industry for picking up and moving heavy iron objects such as scrap iron and steel.

Danish scientist Hans Christian 'd8rsted discovered in 1820 that electric currents create magnetic fields. British scientist William Sturgeon invented the electromagnet in 1824. His first electromagnet was a horseshoe-shaped piece of iron that was wrapped with about 18 turns of bare copper wire (insulated wire didn't exist yet). The iron was varnished to insulate it from the windings. When a current was passed through the coil, the iron became magnetized and attracted other pieces of iron; when the current was stopped, it lost magnetization. Sturgeon displayed its power by showing that although it only weighed seven ounces (roughly 200 grams), it could lift nine pounds (roughly 4 kilos) when the current of a single-cell power supply was applied. However, Sturgeon's magnets were weak because the uninsulated wire he used could only be wrapped in a single spaced out layer around the core, limiting the number of turns.

Beginning in 1830, US scientist Joseph Henry systematically improved and popularised the electromagnet. By using wire insulated by silk thread, and inspired by Schweigger's use of multiple turns of wire to make a galvanometer, he was able to wind multiple layers of wire on cores, creating powerful magnets with thousands of turns of wire, including one that could support 2,063 lb (936 kg). The first major use for electromagnets was in telegraph sounders.

The magnetic domain theory of how ferromagnetic cores work was first proposed in 1906 by French physicist Pierre-Ernest Weiss, and the detailed modern quantum mechanical theory of ferromagnetism was worked out in the 1920s by Werner Heisenberg, Lev Landau, Felix Bloch and others.

A portative electromagnet is one designed to just hold material in place; an example is a lifting magnet. A tractive electromagnet applies a force and moves something.

Electromagnets are very widely used in electric and electromechanical devices, including:

Motors and generators
Transformers
Relays
Electric bells and buzzers
Loudspeakers and headphones
Actuators such as valves
Magnetic recording and data storage equipment: tape recorders, VCRs, hard disks
MRI machines
Scientific equipment such as mass spectrometers
Particle accelerators
Magnetic locks
Magnetic separation equipment, used for separating magnetic from nonmagnetic material, for example separating ferrous metal from other material in scrap.
Industrial lifting magnets
magnetic levitation, used in a maglev train or trains
Induction heating for cooking, manufacturing, and hyperthermia therapy
A common tractive electromagnet is a uniformly-wound solenoid and plunger. The solenoid is a coil of wire, and the plunger is made of a material such as soft iron. Applying a current to the solenoid applies a force to the plunger and may make it move. The plunger stops moving when the forces upon it are balanced. For example, the forces are balanced when the plunger is centered in the solenoid.

The maximum uniform pull happens when one end of the plunger is at the middle of the solenoid. An approximation for the force F is

where C is a proportionality constant, A is the cross-sectional area of the plunger, n is the number of turns in the solenoid, I is the current through the solenoid wire, and l is the length of the solenoid. For units using inches, pounds force, and amperes with long, slender, solenoids, the value of C is around 0.009 to 0.010 psi (maximum pull pounds per square inch of plunger cross-sectional area). For example, a 12-inch long coil (l=12 in) with a long plunger of 1-square inch cross section (A=1 in2) and 11,200 ampere-turns (n I=11,200 Aturn) had a maximum pull of 8.75 pounds (corresponding to C=0.0094 psi).

The maximum pull is increased when a magnetic stop is inserted into the solenoid. The stop becomes a magnet that will attract the plunger; it adds little to the solenoid pull when the plunger is far away but dramatically increases the pull when they are close. An approximation for the pull P is

Here la is the distance between the end of the stop and the end of the plunger. The additional constant C1 for units of inches, pounds, and amperes with slender solenoids is about 2660. The second term within the bracket represents the same force as the stop-less solenoid above; the first term represents the attraction between the stop and the plunger.

Some improvements can be made on the basic design. The ends of the stop and plunger are often conical. For example, the plunger may have a pointed end that fits into a matching recess in the stop. The shape makes the solenoid's pull more uniform as a function of separation. Another improvement is to add a magnetic return path around the outside of the solenoid (an "iron-clad solenoid"). The magnetic return path, just as the stop, has little impact until the air gap is small.

An electric current flowing in a wire creates a magnetic field around the wire, due to Ampere's law (see drawing below). To concentrate the magnetic field, in an electromagnet the wire is wound into a coil with many turns of wire lying side by side. The magnetic field of all the turns of wire passes through the center of the coil, creating a strong magnetic field there. A coil forming the shape of a straight tube (a helix) is called a solenoid.

The direction of the magnetic field through a coil of wire can be found from a form of the right-hand rule. If the fingers of the right hand are curled around the coil in the direction of current flow (conventional current, flow of positive charge) through the windings, the thumb points in the direction of the field inside the coil. The side of the magnet that the field lines emerge from is defined to be the north pole.

Much stronger magnetic fields can be produced if a "magnetic core" of a soft ferromagnetic (or ferrimagnetic) material, such as iron, is placed inside the coil. A core can increase the magnetic field to thousands of times the strength of the field of the coil alone, due to the high magnetic permeability uc0u956  of the material. This is called a ferromagnetic-core or iron-core electromagnet. However, all electromagnets do not use cores, and the very strongest electromagnets, such as superconducting and the very high current electromagnets, cannot use them due to saturation.

For definitions of the variables below, see box at end of article.

The magnetic field of electromagnets in the general case is given by Ampere's Law:

which says that the integral of the magnetizing field around any closed loop is equal to the sum of the current flowing through the loop. Another equation used, that gives the magnetic field due to each small segment of current, is the Biot'96Savart law. Computing the magnetic field and force exerted by ferromagnetic materials is difficult for two reasons. First, because the strength of the field varies from point to point in a complicated way, particularly outside the core and in air gaps, where fringing fields and leakage flux must be considered. Second, because the magnetic field B and force are nonlinear functions of the current, depending on the nonlinear relation between B and H for the particular core material used. For precise calculations, computer programs that can produce a model of the magnetic field using the finite element method are employed.

The material of a magnetic core (often made of iron or steel) is composed of small regions called magnetic domains that act like tiny magnets (see ferromagnetism). Before the current in the electromagnet is turned on, the domains in the iron core point in random directions, so their tiny magnetic fields cancel each other out, and the iron has no large-scale magnetic field. When a current is passed through the wire wrapped around the iron, its magnetic field penetrates the iron, and causes the domains to turn, aligning parallel to the magnetic field, so their tiny magnetic fields add to the wire's field, creating a large magnetic field that extends into the space around the magnet. The effect of the core is to concentrate the field, and the magnetic field passes through the core more easily than it would pass through air.

The larger the current passed through the wire coil, the more the domains align, and the stronger the magnetic field is. Finally, all the domains are lined up, and further increases in current only cause slight increases in the magnetic field: this phenomenon is called saturation.

When the current in the coil is turned off, in the magnetically soft materials that are nearly always used as cores, most of the domains lose alignment and return to a random state and the field disappears. However, some of the alignment persists, because the domains have difficulty turning their direction of magnetization, leaving the core a weak permanent magnet. This phenomenon is called hysteresis and the remaining magnetic field is called remanent magnetism. The residual magnetization of the core can be removed by degaussing. In alternating current electromagnets, such as are used in motors, the core's magnetization is constantly reversed, and the remanence contributes to the motor's losses.

In many practical applications of electromagnets, such as motors, generators, transformers, lifting magnets, and loudspeakers, the iron core is in the form of a loop or magnetic circuit, possibly broken by a few narrow air gaps. This is because the magnetic field lines are in the form of closed loops. Iron presents much less "resistance" (reluctance) to the magnetic field than air, so a stronger field can be obtained if most of the magnetic field's path is within the core.

Since most of the magnetic field is confined within the outlines of the core loop, this allows a simplification of the mathematical analysis. See the drawing at right. A common simplifying assumption satisfied by many electromagnets, which will be used in this section, is that the magnetic field strength B is constant around the magnetic circuit (within the core and air gaps) and zero outside it. Most of the magnetic field will be concentrated in the core material (C). Within the core the magnetic field (B) will be approximately uniform across any cross section, so if in addition the core has roughly constant area throughout its length, the field in the core will be constant. This just leaves the air gaps (G), if any, between core sections. In the gaps the magnetic field lines are no longer confined by the core, so they 'bulge' out beyond the outlines of the core before curving back to enter the next piece of core material, reducing the field strength in the gap. The bulges (BF) are called fringing fields. However, as long as the length of the gap is smaller than the cross section dimensions of the core, the field in the gap will be approximately the same as in the core. In addition, some of the magnetic field lines (BL) will take 'short cuts' and not pass through the entire core circuit, and thus will not contribute to the force exerted by the magnet. This also includes field lines that encircle the wire windings but do not enter the core. This is called leakage flux. Therefore, the equations in this section are valid for electromagnets for which:

the magnetic circuit is a single loop of core material, possibly broken by a few air gaps
the core has roughly the same cross sectional area throughout its length.
any air gaps between sections of core material are not large compared with the cross sectional dimensions of the core.
there is negligible leakage flux
The main nonlinear feature of ferromagnetic materials is that the B field saturates at a certain value, which is around 1.6 to 2 teslas (T) for most high permeability core steels. The B field increases quickly with increasing current up to that value, but above that value the field levels off and becomes almost constant, regardless of how much current is sent through the windings. So the maximum strength of the magnetic field possible from an iron core electromagnet is limited to around 1.6 to 2 T.

The magnetic field created by an electromagnet is proportional to both the number of turns in the winding, N, and the current in the wire, I, hence this product, NI, in ampere-turns, is given the name magnetomotive force. For an electromagnet with a single magnetic circuit, of which length Lcore of the magnetic field path is in the core material and length Lgap is in air gaps, Ampere's Law reduces to:

where
is the magnetic permeability of the core material at the particular B field used.
is the permeability of free space (or air); note that in this definition is amperes.
This is a nonlinear equation, because the permeability of the core, uc0u956 , varies with the magnetic field B. For an exact solution, the value of u956  at the B value used must be obtained from the core material hysteresis curve. If B is unknown, the equation must be solved by numerical methods. However, if the magnetomotive force is well above saturation, so the core material is in saturation, the magnetic field will be approximately the saturation value Bsat for the material, and won't vary much with changes in NI. For a closed magnetic circuit (no air gap) most core materials saturate at a magnetomotive force of roughly 800 ampere-turns per meter of flux path.

For most core materials, . So in equation (1) above, the second term dominates. Therefore, in magnetic circuits with an air gap, the strength of the magnetic field B depends strongly on the length of the air gap, and the length of the flux path in the core doesn't matter much. Given an air gap of 1mm, a magnetomotive force of about 796 Ampere-turns is required to produce a magnetic field of 1T.

The force exerted by an electromagnet on a section of core material is:

where is the cross-sectional area of the core. The force equation can be derived from the energy stored in a magnetic field. Energy is force times distance. Rearranging terms yields the equation above.

The 1.6 T limit on the field mentioned above sets a limit on the maximum force per unit core area, or magnetic pressure, an iron-core electromagnet can exert; roughly:

In more intuitive units it's useful to remember that at 1 T the magnetic pressure is approximately 4 atmospheres, or kg/cm2.

Given a core geometry, the B field needed for a given force can be calculated from (2); if it comes out to much more than 1.6 T, a larger core must be used.

For a closed magnetic circuit (no air gap), such as would be found in an electromagnet lifting a piece of iron bridged across its poles, equation (1) becomes:

Substituting into (2), the force is:

It can be seen that to maximize the force, a core with a short flux path L and a wide cross-sectional area A is preferred (this also applies to magnets with an air gap). To achieve this, in applications like lifting magnets (see photo above) and loudspeakers a flat cylindrical design is often used. The winding is wrapped around a short wide cylindrical core that forms one pole, and a thick metal housing that wraps around the outside of the windings forms the other part of the magnetic circuit, bringing the magnetic field to the front to form the other pole.

The above methods are applicable to electromagnets with a magnetic circuit and do not apply when a large part of the magnetic field path is outside the core. An example would be a magnet with a straight cylindrical core like the one shown at the top of this article. For electromagnets (or permanent magnets) with well defined 'poles' where the field lines emerge from the core, the force between two electromagnets can be found using the a magnetic-charge model which assumes the magnetic field is produced by fictitious 'magnetic charges' on the surface of the poles, with pole strength m and units of Ampere-turn meter. Magnetic pole strength of electromagnets can be found from:

The force between two poles is:

Each electromagnet has two poles, so the total force on a given magnet due to another magnet is equal to the vector sum of the forces of the other magnet's poles acting on each pole of the given magnet. This model assumes point-like poles instead of the finite surfaces, and thus it only yields a good approximation when the distance between the magnets is much larger than their diameter.

There are several side effects which occur in electromagnets which must be provided for in their design. These generally become more significant in larger electromagnets.

The only power consumed in a DC electromagnet under steady state conditions is due to the resistance of the windings, and is dissipated as heat. Some large electromagnets require water cooling systems in the windings to carry off the waste heat.

Since the magnetic field is proportional to the product NI, the number of turns in the windings N and the current I can be chosen to minimize heat losses, as long as their product is constant. Since the power dissipation, P = I2R, increases with the square of the current but only increases approximately linearly with the number of windings, the power lost in the windings can be minimized by reducing I and increasing the number of turns N proportionally, or using thicker wire to reduce the resistance. For example, halving I and doubling N halves the power loss, as does doubling the area of the wire. In either case, increasing the amount of wire reduces the ohmic losses. For this reason, electromagnets often have a significant thickness of windings.

However, the limit to increasing N or lowering the resistance is that the windings take up more room between the magnet's core pieces. If the area available for the windings is filled up, more turns require going to a smaller diameter of wire, which has higher resistance, which cancels the advantage of using more turns. So in large magnets there is a minimum amount of heat loss that can't be reduced. This increases with the square of the magnetic flux B2.

An electromagnet has significant inductance, and resists changes in the current through its windings. Any sudden changes in the winding current cause large voltage spikes across the windings. This is because when the current through the magnet is increased, such as when it is turned on, energy from the circuit must be stored in the magnetic field. When it is turned off the energy in the field is returned to the circuit.

If an ordinary switch is used to control the winding current, this can cause sparks at the terminals of the switch. This does not occur when the magnet is switched on, because the limited supply voltage causes the current through the magnet and the field energy to increase slowly, but when it is switched off, the energy in the magnetic field is suddenly returned to the circuit, causing a large voltage spike and an arc across the switch contacts, which can damage them. With small electromagnets a capacitor is sometimes used across the contacts, which reduces arcing by temporarily storing the current. More often a diode is used to prevent voltage spikes by providing a path for the current to recirculate through the winding until the energy is dissipated as heat. The diode is connected across the winding, oriented so it is reverse-biased during steady state operation and does not conduct. When the supply voltage is removed, the voltage spike forward-biases the diode and the reactive current continues to flow through the winding, through the diode and back into the winding. A diode used in this way is called a freewheeling diode or flyback diode.

Large electromagnets are usually powered by variable current electronic power supplies, controlled by a microprocessor, which prevent voltage spikes by accomplishing current changes slowly, in gentle ramps. It may take several minutes to energize or deenergize a large magnet.

In powerful electromagnets, the magnetic field exerts a force on each turn of the windings, due to the Lorentz force acting on the moving charges within the wire. The Lorentz force is perpendicular to both the axis of the wire and the magnetic field. It can be visualized as a pressure between the magnetic field lines, pushing them apart. It has two effects on an electromagnet's windings:

The field lines within the axis of the coil exert a radial force on each turn of the windings, tending to push them outward in all directions. This causes a tensile stress in the wire.
The leakage field lines between each turn of the coil exert an attractive force between adjacent turns, tending to pull them together.
The Lorentz forces increase with B2. In large electromagnets the windings must be firmly clamped in place, to prevent motion on power-up and power-down from causing metal fatigue in the windings. In the Bitter design, below, used in very high-field research magnets, the windings are constructed as flat disks to resist the radial forces, and clamped in an axial direction to resist the axial ones.

In alternating current (AC) electromagnets, used in transformers, inductors, and AC motors and generators, the magnetic field is constantly changing. This causes energy losses in their magnetic cores that is dissipated as heat in the core. The losses stem from two processes:

Eddy currents: From Faraday's law of induction, the changing magnetic field induces circulating electric currents inside nearby conductors, called eddy currents. The energy in these currents is dissipated as heat in the electrical resistance of the conductor, so they are a cause of energy loss. Since the magnet's iron core is conductive, and most of the magnetic field is concentrated there, eddy currents in the core are the major problem. Eddy currents are closed loops of current that flow in planes perpendicular to the magnetic field. The energy dissipated is proportional to the area enclosed by the loop. To prevent them, the cores of AC electromagnets are made of stacks of thin steel sheets, or laminations, oriented parallel to the magnetic field, with an insulating coating on the surface. The insulation layers prevent eddy current from flowing between the sheets. Any remaining eddy currents must flow within the cross-section of each individual lamination, which reduces losses greatly. Another alternative is to use a ferrite core, which is a nonconductor.
Hysteresis losses: Reversing the direction of magnetization of the magnetic domains in the core material each cycle causes energy loss, because of the coercivity of the material. These losses are called hysteresis. The energy lost per cycle is proportional to the area of the hysteresis loop in the BH graph. To minimize this loss, magnetic cores used in transformers and other AC electromagnets are made of "soft" low coercivity materials, such as silicon steel or soft ferrite. The energy loss per cycle of the AC current is constant for each of these processes, so the power loss increases linearly with frequency.
When a magnetic field higher than the ferromagnetic limit of 1.6 T is needed, superconducting electromagnets can be used. Instead of using ferromagnetic materials, these use superconducting windings cooled with liquid helium, which conduct current without electrical resistance. These allow enormous currents to flow, which generate intense magnetic fields. Superconducting magnets are limited by the field strength at which the winding material ceases to be superconducting. Current designs are limited to 10'9620 T, with the current (2017) record of 32 T. The necessary refrigeration equipment and cryostat make them much more expensive than ordinary electromagnets. However, in high power applications this can be offset by lower operating costs, since after startup no power is required for the windings, since no energy is lost to ohmic heating. They are used in particle accelerators and MRI machines.

Both iron-core and superconducting electromagnets have limits to the field they can produce. Therefore, the most powerful man-made magnetic fields have been generated by air-core nonsuperconducting electromagnets of a design invented by Francis Bitter in 1933, called Bitter electromagnets. Instead of wire windings, a Bitter magnet consists of a solenoid made of a stack of conducting disks, arranged so that the current moves in a helical path through them, with a hole through the center where the maximum field is created. This design has the mechanical strength to withstand the extreme Lorentz forces of the field, which increase with B2. The disks are pierced with holes through which cooling water passes to carry away the heat caused by the high current. The strongest continuous field achieved solely with a resistive magnet is 37.5 T as of 31 March 2014, produced by a Bitter electromagnet at the Radboud University High Field Magnet Laboratory in Nijmegen, the Netherlands. The previous record was 35 T. The strongest continuous magnetic field overall, 45 T, was achieved in June 2000 with a hybrid device consisting of a Bitter magnet inside a superconducting magnet.

The factor limiting the strength of electromagnets is the inability to dissipate the enormous waste heat, so more powerful fields, up to 100 T, have been obtained from resistive magnets by sending brief pulses of high current through them; the inactive period after each pulse allows the heat produced during the pulse to be removed, before the next pulse.

The most powerful manmade magnetic fields have been created by using explosives to compress the magnetic field inside an electromagnet as it is pulsed; these are called explosively pumped flux compression generators. The implosion compresses the magnetic field to values of around 1000 T for a few microseconds. While this method may seem very destructive, it is possible to redirect the brunt of the blast radially outwards so that neither the experiment nor the magnetic structure are harmed. These devices are known as destructive pulsed electromagnets. They are used in physics and materials science research to study the properties of materials at high magnetic fields.

A modern torpedo is an underwater ranged weapon launched above or below the water surface, self-propelled towards a target, and with an explosive warhead designed to detonate either on contact with or in proximity to the target. Historically, such a device was called an automotive, automobile, locomotive, or fish torpedo; colloquially a fish. The term torpedo originally applied to a variety of devices, most of which would today be called mines. From about 1900, torpedo has been used strictly to designate a self-propelled underwater explosive device.

While the 19th-century battleship had evolved primarily with a view to engagements between armored warships with large-caliber guns, the invention and refinement of torpedoes from the 1860s onwards allowed small torpedo boats and other lighter surface vessels, submarines/submersibles, even improvised fishing boats or frogmen, and later light aircraft, to destroy large ships without the need of large guns, though sometimes at the risk of being hit by longer-range artillery fire.

One can divide modern torpedoes into lightweight and heavyweight classes; and into straight-running, autonomous homers, and wire-guided types. They can be launched from a variety of platforms.

The word torpedo comes from the name of a genus of electric rays in the order Torpediniformes, which in turn comes from the Latin torpere ("to be stiff or numb"). In naval usage, the American Robert Fulton introduced the name to refer to a towed gunpowder charge used by his French submarine Nautilus (first tested in 1800) to demonstrate that it could sink warships.

Torpedo-like weapons were first proposed many centuries before they were successfully developed. For example, in 1275, Arab engineer Hasan al-Rammah '96 who worked as a military scientist for the Mamluk Sultanate of Egypt '96 wrote that it might be possible to create a projectile resembling "an egg", which propelled itself through water, whilst carrying "fire".

In modern language, a "torpedo" is an underwater self-propelled explosive, but historically, the term also applied to primitive naval mines. These were used on an ad hoc basis during the early modern period up to the late 19th century. Early spar torpedoes were created by the Dutchman Cornelius Drebbel in the employ of King James I of England; he attached explosives to the end of a beam affixed to one of his submarines and they were used (to little effect) during the English expeditions to La Rochelle in 1626.

An early submarine, Turtle, attempted to lay a bomb with a timed fuse on the hull of HMS Eagle during the American Revolutionary War, but failed in the attempt.

In the early 1800s, the American inventor Robert Fulton, while in France, "conceived the idea of destroying ships by introducing floating mines under their bottoms in submarine boats". He coined the term "torpedo" about the explosive charges with which he outfitted his submarine Nautilus. However, both the French and the Dutch governments were uninterested in the submarine. Fulton then concentrated on developing the torpedo independent of a submarine deployment. On 15 October 1805, while in England, Fulton put on a public display of his "infernal machine", sinking the brig Dorothea with a submerged bomb filled with 180 lb (82 kg) of gunpowder and a clock set to explode in 18 minutes. However, the British government refused to purchase the invention, stating they did not wish to "introduce into naval warfare a system that would give great advantage to weaker maritime nations". Fulton carried out a similar demonstration for the US government on 20 July 1807, destroying a vessel in New York's harbor. Further development languished as Fulton focused on his "steam-boat matters". During the War of 1812, torpedoes were employed in attempts to destroy British vessels and protect American harbors. A submarine-deployed torpedo was used in an unsuccessful attempt to destroy HMS Ramillies while in New London's harbor. This prompted the British Captain Hardy to warn the Americans to cease efforts with the use of any "torpedo boat" in this "cruel and unheard-of warfare", or he would "order every house near the shore to be destroyed".

Torpedoes were used by the Russian Empire during the Crimean War in 1855 against British warships in the Gulf of Finland. They used an early form of chemical detonator.

During the American Civil War, the term torpedo was used for what is today called a contact mine, floating on or below the water surface using an air-filled demijohn or similar flotation device. These devices were very primitive and apt to prematurely explode. They would be detonated on contact with the ship or after a set time, although electrical detonators were also occasionally used. USS Cairo was the first warship to be sunk in 1862 by an electrically-detonated mine. Spar torpedoes were also used; an explosive device was mounted at the end of a spar up to 30 feet (9.1 m) long projecting forward underwater from the bow of the attacking vessel, which would then ram the opponent with the explosives. These were used by the Confederate submarine H. L. Hunley to sink USS Housatonic although the weapon was apt to cause as much harm to its user as to its target. Rear Admiral David Farragut's famous/apocryphal command during the Battle of Mobile Bay in 1864, "Damn the torpedoes, full speed ahead!" refers to a minefield laid at Mobile, Alabama.

On 26 May 1877, during the Romanian War of Independence, the Romanian spar torpedo boat R'e2ndunica attacked and sank the Ottoman river monitor Seyfi. This was the first instance in history when a torpedo craft sank its targets without also sinking.

A prototype of the self-propelled torpedo was created on a commission placed by Giovanni Luppis, an Austro-Hungarian naval officer from Rijeka (modern-day Croatia), at the time a port city of the Austro-Hungarian Monarchy and Robert Whitehead, an English engineer who was the manager of a town factory. In 1864, Luppis presented Whitehead with the plans of the Salvacoste ("Coastsaver"), a floating weapon driven by ropes from the land that had been dismissed by the naval authorities due to the impractical steering and propulsion mechanisms.

In 1866, Whitehead invented the first effective self-propelled torpedo, the eponymous Whitehead torpedo. French and German inventions followed closely, and the term torpedo came to describe self-propelled projectiles that traveled under or on water. By 1900, the term no longer included mines and booby-traps as the navies of the world added submarines, torpedo boats and torpedo boat destroyers to their fleets.

Whitehead was unable to improve the machine substantially, since the clockwork motor, attached ropes, and surface attack mode all contributed to a slow and cumbersome weapon. However, he kept considering the problem after the contract had finished, and eventually developed a tubular device, designed to run underwater on its own, and powered by compressed air. The result was a submarine weapon, the Minenschiff (mine ship), the first modern self-propelled torpedo, officially presented to the Austrian Imperial Naval commission on 21 December 1866.

The first trials were not successful as the weapon was unable to maintain a course at a steady depth. After much work, Whitehead introduced his "secret" in 1868 which overcame this. It was a mechanism consisting of a hydrostatic valve and pendulum that caused the torpedo's hydroplanes to be adjusted to maintain a preset depth.

After the Austrian government decided to invest in the invention, Whitehead started the first torpedo factory in Rijeka. In 1870, he improved the devices to travel up to approximately 1,000 yards (910 m) at a speed of up to 6 knots (11 km/h), and by 1881 the factory was exporting torpedoes to ten other countries. The torpedo was powered by compressed air and had an explosive charge of gun-cotton. Whitehead went on to develop more efficient devices, demonstrating torpedoes capable of 18 knots (33 km/h) in 1876, 24 knots (44 km/h) in 1886, and, finally, 30 knots (56 km/h) in 1890.

Royal Navy (RN) representatives visited Rijeka for a demonstration in late 1869, and in 1870 a batch of torpedoes was ordered. In 1871, the British Admiralty paid Whitehead 'a315,000 for certain of his developments and production started at the Royal Laboratories in Woolwich the following year. In 1893, RN torpedo production was transferred to the Royal Gun Factory. The British later established a Torpedo Experimental Establishment at HMS Vernon and a production facility at the Royal Naval Torpedo Factory, Greenock in 1910. These are now closed.

Whitehead opened a new factory near Portland Harbour, England in 1890, which continued making torpedoes until the end of World War II. Because orders from the RN were not as large as expected, torpedoes were mostly exported. A series of devices was produced at Rijeka, with diameters from 14 in (36 cm) upward. The largest Whitehead torpedo was 18 in (46 cm) in diameter and 19 ft (5.8 m) long, made of polished steel or phosphor bronze, with a 200-pound (91 kg) gun-cotton warhead. It was propelled by a three-cylinder Brotherhood radial engine, using compressed air at around 1,300 psi (9.0 MPa) and driving two contra-rotating propellers, and was designed to self-regulate its course and depth as far as possible. By 1881, nearly 1,500 torpedoes had been produced. Whitehead also opened a factory at St Tropez in 1890 that exported torpedoes to Brazil, The Netherlands, Turkey, and Greece.

Whitehead purchased rights to the gyroscope of Ludwig Obry in 1888 but it was not sufficiently accurate, so in 1890 he purchased a better design to improve control of his designs, which came to be called the "Devil's Device". The firm of L. Schwartzkopff in Germany also produced torpedoes and exported them to Russia, Japan, and Spain. In 1885, Britain ordered a batch of 50 as torpedo production at home and Rijeka could not meet demand.

By World War I, Whitehead's torpedo remained a worldwide success, and his company was able to maintain a monopoly on torpedo production. By that point, his torpedo had grown to a diameter of 18 inches with a maximum speed of 30.5 knots (56.5 km/h; 35.1 mph) with a warhead weighing 170 pounds (77 kg).

Whitehead faced competition from the American Lieutenant Commander John A. Howell, whose design, driven by a flywheel, was simpler and cheaper. It was produced from 1885 to 1895, and it ran straight, leaving no wake. A Torpedo Test Station was set up in Rhode Island in 1870. The Howell torpedo was the only United States Navy model until Whitehead torpedoes produced by Bliss and Williams entered service in 1894. Five varieties were produced, all 18-inch diameter. The United States Navy started using the Whitehead torpedo in 1892 after an American company, E.W. Bliss, secured manufacturing rights.

The Royal Navy introduced the Brotherhood wet heater engine in 1907 with the 18 in. Mk. VII & VII* which greatly increased the speed and/or range over compressed air engines and wet heater type engines became the standard in many major navies up to and during the Second World War.

Ships of the line were superseded by ironclads, large steam-powered ships with heavy gun armament and heavy armor, in the mid 19th century. Ultimately this line of development led to the dreadnought category of all-big-gun battleships, starting with HMS Dreadnought.

Although these ships were incredibly powerful, the new weight of armor slowed them down, and the huge guns needed to penetrate that armor fired at very slow rates. This allowed for the possibility of a small and fast ship that could attack the battleships, at a much lower cost. The introduction of the torpedo provided a weapon that could cripple, or sink, any battleship.

The first boat designed to fire the self-propelled Whitehead torpedo was HMS Lightning, completed in 1877. The French Navy followed suit in 1878 with Torpilleur No 1, launched in 1878 though she had been ordered in 1875. The first torpedo boats were built at the shipyards of Sir John Thornycroft and gained recognition for their effectiveness.

At the same time, inventors were working on building a guided torpedo. Prototypes were built by John Ericsson, John Louis Lay, and Victor von Scheliha, but the first practical guided missile was patented by Louis Brennan, an emigre to Australia, in 1877.

It was designed to run at a consistent depth of 12 feet (3.7 m), and was fitted with an indicator mast that just broke the surface of the water. At night the mast had a small light, only visible from the rear. Two steel drums were mounted one behind the other inside the torpedo, each carrying several thousand yards of high-tensile steel wire. The drums connected via a differential gear to twin contra-rotating propellers. If one drum was rotated faster than the other, then the rudder was activated. The other ends of the wires were connected to steam-powered winding engines, which were arranged so that speeds could be varied within fine limits, giving sensitive steering control for the torpedo.

The torpedo attained a speed of 20 knots (37 km/h; 23 mph) using a wire 1.0 millimetre (0.04 in) in diameter but later this was changed to 1.8 mm (0.07 in) to increase the speed to 27 knots (50 km/h; 31 mph). The torpedo was fitted with elevators controlled by a depth-keeping mechanism, and the fore and aft rudders operated by the differential between the drums.

Brennan traveled to Britain, where the Admiralty examined the torpedo and found it unsuitable for shipboard use. However, the War Office proved more amenable, and in early August 1881, a special Royal Engineer committee was instructed to inspect the torpedo at Chatham and report back directly to the Secretary of State for War, Hugh Childers. The report strongly recommended that an improved model be built at government expense. In 1883 an agreement was reached between the Brennan Torpedo Company and the government. The newly appointed Inspector-General of Fortifications in England, Sir Andrew Clarke, appreciated the value of the torpedo and in spring 1883 an experimental station was established at Garrison Point Fort, Sheerness on the River Medway and a workshop for Brennan was set up at the Chatham Barracks, the home of the Royal Engineers. Between 1883 and 1885 the Royal Engineers held trials and in 1886 the torpedo was recommended for adoption as a harbor defense torpedo. It was used throughout the British Empire for more than fifteen years.

The Royal Navy frigate HMS Shah was the first naval vessel to fire a torpedo in anger during the Battle of Pacocha against rebel Peruvian ironclad Hu'e1scar on 29 May 1877. The Peruvian ship successfully outran the device. On 16 January 1878, the Turkish steamer Intibah became the first vessel to be sunk by self-propelled torpedoes, launched from torpedo boats operating from the tender Velikiy Knyaz Konstantin under the command of Stepan Osipovich Makarov during the Russo-Turkish War of 1877'9678. In another early use of the torpedo, the Chilean ironclad Blanco Encalada was sunk on 23 April 1891 by a self-propelled torpedo from gunboat Almirante Lynch, during the Chilean Civil War of 1891, becoming the first ironclad warship sunk by this weapon. The Chinese turret ship Dingyuan was purportedly hit and disabled by a torpedo after numerous attacks by Japanese torpedo boats during the First Sino-Japanese War in 1894. At this time torpedo attacks were still very close range and very dangerous to the attackers.

Several western sources reported that the Qing dynasty Imperial Chinese military, under the direction of Li Hongzhang, acquired electric torpedoes, which they deployed in numerous waterways, along with fortresses and numerous other modern military weapons acquired by China. At the Tientsin Arsenal in 1876, the Chinese developed the capacity to manufacture these "electric torpedoes" on their own. Although a form of Chinese art, the Nianhua, depict such torpedoes being used against Russian ships during the Boxer Rebellion, whether they were actually used in battle against them is undocumented and unknown.

The Russo-Japanese War (1904'961905) was the first great war of the 20th century. During the war the Imperial Russian and Imperial Japanese navies launched nearly 300 torpedoes at each other, all of them of the "self-propelled automotive" type. The deployment of these new underwater weapons resulted in one battleship, two armored cruisers, and two destroyers being sunk in action, with the remainder of the roughly 80 warships being sunk by the more conventional methods of gunfire, mines, and scuttling.

On 27 May 1905, during the Battle of Tsushima, Admiral Rozhestvensky's flagship, the battleship Knyaz Suvorov, had been gunned to a wreck by Admiral Tuc0u333 gu333 's 12-inch gunned battleline. With the Russians sunk and scattering, Tu333 gu333  prepared for pursuit, and while doing so ordered his torpedo boat destroyers (TBDs) (mostly referred to as just destroyers in most written accounts) to finish off the Russian battleship. Knyaz Suvorov was set upon by 17 torpedo-firing warships, ten of which were destroyers and four torpedo boats. Twenty-one torpedoes were launched at the pre-dreadnought, and three struck home, one fired from the destroyer Murasame and two from torpedo boats No. 72 and No. 75. The flagship slipped under the waves shortly thereafter, taking over 900 men with her to the bottom. On December 9, 1912, the Greek submarine "Dolphin" launched a torpedo against the Ottoman cruiser "Medjidieh".

The end of the Russo-Japanese War fuelled new theories, and the idea of dropping lightweight torpedoes from aircraft was conceived in the early 1910s by Bradley A. Fiske, an officer in the United States Navy. Awarded a patent in 1912, Fiske worked out the mechanics of carrying and releasing the aerial torpedo from a bomber, and defined tactics that included a night-time approach so that the target ship would be less able to defend itself. Fiske determined that the notional torpedo bomber should descend rapidly in a sharp spiral to evade enemy guns, then when about 10 to 20 feet (3 to 6 m) above the water the aircraft would straighten its flight long enough to line up with the torpedo's intended path. The aircraft would release the torpedo at a distance of 1,500 to 2,000 yards (1,400 to 1,800 m) from the target. Fiske reported in 1915 that, using this method, enemy fleets could be attacked within their harbors if there was enough room for the torpedo track.

Meanwhile, the Royal Naval Air Service began actively experimenting with this possibility. The first successful aerial torpedo drop was performed by Gordon Bell in 1914 '96 dropping a Whitehead torpedo from a Short S.64 seaplane. The success of these experiments led to the construction of the first purpose-built operational torpedo aircraft, the Short Type 184, built-in 1915.

An order for ten aircraft was placed, and 936 aircraft were built by ten different British aircraft companies during the First World War. The two prototype aircraft were embarked upon HMS Ben-my-Chree, which sailed for the Aegean on 21 March 1915 to take part in the Gallipoli campaign. On 12 August 1915 one of these, piloted by Flight Commander Charles Edmonds, was the first aircraft in the world to attack an enemy ship with an air-launched torpedo.

On 17 August 1915 Flight Commander Edmonds torpedoed and sank an Ottoman transport ship a few miles north of the Dardanelles. His formation colleague, Flight Lieutenant G B Dacre, was forced to land on the water owing to engine trouble but, seeing an enemy tug close by, taxied up to it and released his torpedo, sinking the tug. Without the weight of the torpedo Dacre was able to take off and return to Ben-My-Chree.

Torpedoes were widely used in World War I, both against shipping and against submarines. Germany disrupted the supply lines to Britain largely by use of submarine torpedoes, though submarines also extensively used guns. Britain and its allies also used torpedoes throughout the war. U-boats themselves were often targeted, twenty being sunk by torpedo. Two Royal Italian Navy torpedo boats scored a success against an Austrian-Hungarian squadron, sinking the battleship SMS Szent Istv'e1n with two torpedoes.

The Royal Navy had been experimenting with ways to further increase the range of torpedoes during World War 1 using pure oxygen instead of compressed air, this work ultimately leading to the development of the oxygen-enriched air 24.5 in. Mk. I intended originally for the G3-class battlecruisers and N3 class battleships of 1921, both being cancelled due to the Washington Naval Treaty.

Initially, the Imperial Japanese Navy purchased Whitehead or Schwartzkopf torpedoes but by 1917, like the Royal Navy, they were conducting experiments with pure oxygen instead of compressed air. Because of explosions they abandoned the experiments but resumed them in 1926 and by 1933 had a working torpedo. They also used conventional wet-heater torpedoes.

In the inter-war years, financial stringency caused nearly all navies to skimp on testing their torpedoes. Only the British and Japanese had fully tested torpedoes (in particular the Type 93, nicknamed Long Lance postwar by the US official historian Samuel E. Morison) at the start of World War II. Unreliable torpedoes caused many problems for the American submarine force in the early years of the war, primarily in the Pacific Theater. One possible exception to the pre-war neglect of torpedo development was the 45-cm caliber, 1931-premiered Japanese Type 91 torpedo, the sole aerial torpedo (Koku Gyorai) developed and brought into service by the Japanese Empire before the war. The Type 91 had an advanced PID controller and jettisonable, wooden Kyoban aerial stabilizing surfaces which released upon entering the water, making it a formidable anti-ship weapon; Nazi Germany considered manufacturing it as the Luftorpedo LT 850 after August 1942.

The Royal Navy's 24.5-inch oxygen-enriched air torpedo saw service in the two Nelson class battleships although by World War II the use of enriched oxygen had been discontinued due to safety concerns. In the final phase of the action against German battleship Bismarck, Rodney fired a pair of 24.5-inch torpedoes from her port-side tube and claimed one hit. According to Ludovic Kennedy, "if true, [this is] the only instance in history of one battleship torpedoing another". The Royal Navy continued the development of oxygen-enriched air torpedoes with the 21 in. Mk. VII of the 1920s designed for the County-class cruisers although once again these were converted to run on normal air at the start of World War II. Around this time too the Royal Navy were perfecting the Brotherhood burner cycle engine which offered a performance as good as the oxygen-enriched air engine but without the issues arising from the oxygen equipment and which was first used in the extremely successful and long-lived 21 in. Mk. VIII torpedo of 1925. This torpedo served throughout WW II (with 3,732 being fired by September 1944) and is still in limited service in the 21st Century. The improved Mark VIII** was used in two particularly notable incidents; on 6 February 1945 the only intentional wartime sinking of one submarine by another while both were submerged took place when HMS Venturer sank the German submarine U-864 with four Mark VIII** torpedoes and on 2 May 1982 when the Royal Navy submarine HMS Conqueror sank the Argentine cruiser ARA General Belgrano with two Mark VIII** torpedoes during the Falklands War. This is the only sinking of a surface ship by a nuclear-powered submarine in wartime and the second (of three) sinkings of a surface ship by any submarine since the end of World War II). The other two sinkings were of the Indian frigate INS Khukri and the South Korean corvette ROKS Cheonan.

Many classes of surface ships, submarines, and aircraft were armed with torpedoes. Naval strategy at the time was to use torpedoes, launched from submarines or warships, against enemy warships in a fleet action on the high seas. There were concerns torpedoes would be ineffective against warships' heavy armor; an answer to this was to detonate torpedoes underneath a ship, badly damaging its keel and the other structural members in the hull, commonly called "breaking its back". This was demonstrated by magnetic influence mines in World War I. The torpedo would be set to run at a depth just beneath the ship, relying on a magnetic exploder to activate at the appropriate time.

Germany, Britain, and the U.S. independently devised ways to do this; German and American torpedoes, however, suffered problems with their depth-keeping mechanisms, coupled with faults in magnetic pistols shared by all designs. Inadequate testing had failed to reveal the effect of the Earth's magnetic field on ships and exploder mechanisms, which resulted in premature detonation. The Kriegsmarine and Royal Navy promptly identified and eliminated the problems. In the United States Navy (USN), there was an extended wrangle over the problems plaguing the Mark 14 torpedo (and its Mark 6 exploder). Cursory trials had allowed bad designs to enter service. Both the Navy Bureau of Ordnance and the United States Congress were too busy protecting their interests to correct the errors, and fully functioning torpedoes only became available to the USN twenty-one months into the Pacific War.

British submarines used torpedoes to interdict the Axis supply shipping to North Africa, while Fleet Air Arm Swordfish sank three Italian battleships at Taranto by a torpedo and (after a mistaken, but abortive, attack on Sheffield) scored one crucial hit in the hunt for the German battleship Bismarck. Large tonnages of merchant shipping were sunk by submarines with torpedoes in both the Battle of the Atlantic and the Pacific War.

Torpedo boats, such as MTBs, PT boats, or S-boats, enabled the relatively small but fast craft to carry enough firepower, in theory, to destroy a larger ship, though this rarely occurred in practice. The largest warship sunk by torpedoes from small craft in World War II was the British cruiser Manchester, sunk by Italian MAS boats on the night of 12/13 August 1942 during Operation Pedestal. Destroyers of all navies were also armed with torpedoes to attack larger ships. In the Battle off Samar, destroyer torpedoes from the escorts of the American task force "Taffy 3" showed effectiveness at defeating armor. Damage and confusion caused by torpedo attacks were instrumental in beating back a superior Japanese force of battleships and cruisers. In the Battle of the North Cape in December 1943, torpedo hits from British destroyers Savage and Saumarez slowed the German battleship Scharnhorst enough for the British battleship Duke of York to catch and sink her, and in May 1945 the British 26th Destroyer Flotilla (coincidentally led by Saumarez again) ambushed and sank Japanese heavy cruiser Haguro.

During World War II, Hedy Lamarr and composer George Antheil developed a radio guidance system for Allied torpedoes, it intended to use frequency-hopping technology to defeat the threat of jamming by the Axis powers. As radio guidance had been abandoned some years earlier, it was not pursued. Although the US Navy never adopt the technology, it did, in the 1960s, investigate various spread-spectrum techniques. Spread-spectrum techniques are incorporated into Bluetooth technology and are similar to methods used in legacy versions of Wi-Fi. This work led to their induction into the National Inventors Hall of Fame in 2014.

Because of improved submarine strength and speed, torpedoes had to be given improved warheads and better motors. During the Cold War torpedoes were an important asset with the advent of nuclear-powered submarines, which did not have to surface often, particularly those carrying strategic nuclear missiles.

Several navies have launched torpedo strikes since World War II, including:

During the Korean War the United States Navy successfully attacked a dam with air-launched torpedoes.
Israeli Navy fast attack craft crippled the electronic intelligence vessel USS Liberty with gunfire and torpedoes during the 1967 Six-Day War, resulting in the loss of 46 crew.
A Pakistan Navy Daphn'e9-class submarine sank the Indian frigate INS Khukri on 9 December 1971 during the Indo-Pakistani War of 1971, with the loss of over 18 officers and 176 sailors.
The British Royal Navy nuclear attack submarine HMS Conqueror sank the Argentine Navy light cruiser ARA General Belgrano with two Mark 8 torpedoes during the Falklands War with the loss of 323 lives.
The Croatian Navy disabled the Yugoslav patrol boat Puc0u268 -176 Mukos with a torpedo launched by Croatian naval commandos from an improvised device during the Battle of the Dalmatian channels on 14 November 1991, in the course of the Croatian War of Independence. Three members of the crew were killed. The stranded boat was later recovered by Croatian trawlers, salvaged and put in service with the Croatian Navy as OB-02 '8aolta.
On 26 March 2010 the South Korean Navy ship ROKS Cheonan was sunk with the loss of 46 personnel. A subsequent investigation concluded that the warship had been sunk by a North Korean torpedo fired by a midget submarine.
The Whitehead torpedo of 1866, the first successful self-propelled torpedo, used compressed air as its energy source. The air was stored at pressures of up to 2.55 MPa (370 psi) and fed to a piston engine that turned a single propeller at about 100 rpm. It could travel about 180 metres (200 yd) at an average speed of 6.5 knots (12.0 km/h). The speed and range of later models were improved by increasing the pressure of the stored air. In 1906 Whitehead built torpedoes that could cover nearly 1,000 metres (1,100 yd) at an average speed of 35 knots (65 km/h).

At higher pressures the adiabatic cooling, experienced by the air as it expanded in the engine caused icing problems. This drawback was remedied by heating the air with seawater before it was fed to the engine, which increased engine performance further because the air expanded even more after heating. This was the principle used by the Brotherhood engine.

Passing the air through an engine led to the idea of injecting a liquid fuel, like kerosene, into the air and igniting it. In this manner, the air is heated more and expands even further, and the burned propellant adds more gas to drive the engine. Construction of such heated torpedoes started circa 1904 by Whitehead's company.

A further improvement was the use of water to cool the combustion chamber of the fuel-burning torpedo. This not only solved heating problems so more fuel could be burned but also allowed additional power to be generated by feeding the resulting steam into the engine together with the combustion products. Torpedoes with such a propulsion system became known as wet heaters, while heated torpedoes without steam generation were retrospectively called dry heaters. A simpler system was introduced by the British Royal Gun factory in 1908. Most torpedoes used in World War I and World War II were wet-heaters.

The amount of fuel that can be burned by a torpedo engine (i.e. wet engine) is limited by the amount of oxygen it can carry. Since compressed air contains only about 21% oxygen, engineers in Japan developed the Type 93 (nicknamed "Long Lance" postwar) for destroyers and cruisers in the 1930s. It used pure compressed oxygen instead of compressed air and had performance unmatched by any contemporary torpedo in service, through the end of World War II. However, oxygen systems posed a danger to any ship that came under attack while still carrying such torpedoes; Japan lost several cruisers partly due to catastrophic secondary explosions of Type 93s. During the war, Germany experimented with hydrogen peroxide for the same purpose.

The British approached the problem of providing additional oxygen for the torpedo engine by the use of oxygen-enriched air, up to 57% instead of the 21% of normal atmospheric compressed air rather than pure oxygen. This significantly increased the range of the torpedo, the 24.5 inch Mk 1 having a range of 15,000 yards (14,000 m) at 35 knots (65 km/h) or 20,000 yards (18,000 m) at 30 knots (56 km/h) with a 750 pounds (340 kg) warhead. There was a general nervousness about the oxygen enrichment equipment, known for reasons of secrecy as 'No 1 Air Compressor Room' onboard ships, and development shifted to the highly efficient Brotherhood Burner Cycle engine that used un-enriched air.

After the First World War Brotherhood developed a 4 cylinder burner cycle engine which was roughly twice as powerful as the older wet heater engine. It was first used in the British Mk VIII torpedoes, which were still in service in 1982. It used a modified diesel cycle, using a small amount of paraffin to heat the incoming air, which was then compressed and further heated by the piston, and then more fuel was injected. In the introduction, it produced about 322 hp when introduced, but by the end of WW2 was at 465 hp, and there was a proposal to fuel it with nitric acid when it was projected to develop 750 hp.

The Brennan torpedo had two wires wound around internal drums. Shore-based steam winches pulled the wires, which spun the drums and drove the propellers. An operator controlled the relative speeds of the winches, providing guidance. Such systems were used for coastal defense of the British homeland and colonies from 1887 to 1903 and were purchased by, and under the control of, the Army as opposed to the Navy. Speed was about 25 knots (46 km/h) for over 2,400 m.

The Howell torpedo used by the US Navy in the late 19th century featured a heavy flywheel that had to be spun up before launch. It was able to travel about 400 yards (370 m) at 25 knots (46 km/h). The Howell had the advantage of not leaving a trail of bubbles behind it, unlike compressed air torpedoes. This gave the target vessel less chance to detect and evade the torpedo and avoided giving away the attacker's position. Additionally, it ran at a constant depth, unlike Whitehead models.

Electric propulsion systems avoided tell-tale bubbles. John Ericsson invented an electrically propelled torpedo in 1873; it was powered by a cable from an external power source, because batteries of the time had insufficient capacity. The Sims-Edison torpedo was similarly powered. The Nordfelt torpedo was also electrically powered and was steered by impulses down a trailing wire.

Germany introduced its first battery-powered torpedo shortly before World War II, the G7e. It was slower and had a shorter range than the conventional G7a, but was wakeless and much cheaper. Its lead-acid rechargeable battery was sensitive to shock, required frequent maintenance before use, and required preheating for best performance. The experimental G7es, an enhancement of the G7e, used primary cells.

The United States had an electric design, the Mark 18, largely copied from the German torpedo (although with improved batteries), as well as FIDO, an air-dropped acoustic homing torpedo for anti-submarine use.

Modern electric torpedoes such as the Mark 24 Tigerfish, the Black Shark or DM2 series commonly use silver oxide batteries that need no maintenance, so torpedoes can be stored for years without losing performance.

Several experimental rocket-propelled torpedoes were tried soon after Whitehead's invention but were not successful. Rocket propulsion has been implemented successfully by the Soviet Union, for example in the VA-111 Shkval'97and has been recently revived in Russian and German torpedoes, as it is especially suitable for supercavitating devices.

Modern torpedoes use a variety of propellants, including electric batteries (as with the French F21 torpedo or Italian Black Shark), monopropellants (e.g., Otto fuel II as with the US Mark 48 torpedo), and bipropellants (e.g., hydrogen peroxide plus kerosene as with the Swedish Torped 62, sulfur hexafluoride plus lithium as with the US Mark 50 torpedo, or Otto fuel II plus hydroxyl ammonium perchlorate as with the British Spearfish torpedo).

The first of Whitehead's torpedoes had a single propeller and needed a large vane to stop it spinning about its longitudinal axis. Not long afterward the idea of contra-rotating propellers was introduced, to avoid the need for the vane. The three-bladed propeller came in 1893 and the four-bladed one in 1897. To minimize noise, today's torpedoes often use pump-jets.

Some torpedoes'97like the Russian VA-111 Shkval, Iranian Hoot, and German Unterwasserlaufk'f6rper/ Barracuda'97use supercavitation to increase speed to over 200 knots (370 km/h). Torpedoes that don't use supercavitation, such as the American Mark 48 and British Spearfish, are limited to under 100 kn (120 mph; 190 km/h), though manufacturers and the military don't always release exact figures.

Torpedoes may be aimed at the target and fired unguided, similarly to a traditional artillery shell, or they may be guided onto the target. They may be guided automatically towards the target by some procedure, e.g., sound (homing), or by the operator, typically via commands sent over a signal-carrying cable (wire guidance).

The Victorian era Brennan torpedo could be steered onto its target by varying the relative speeds of its propulsion cables. However, the Brennan required a substantial infrastructure and was not suitable for shipboard use. Therefore, for the first part of its history, the torpedo was guided only in the sense that its course could be regulated to achieve an intended impact depth (because of the sine wave running path of the Whitehead, this was a hit or miss proposition, even when everything worked correctly) and, through gyroscopes, a straight course. With such torpedoes the method of attack in small torpedo boats, torpedo bombers and small submarines was to steer a predictable collision course abeam to the target and release the torpedo at the last minute, then veer away, all the time subject to defensive fire.

In larger ships and submarines, fire control calculators gave a wider engagement envelope. Originally, plotting tables (in large ships), combined with specialized slide rules (known in U.S. service as the "banjo" and "Is/Was"), reconciled the speed, distance, and course of a target with the firing ship's speed and course, together with the performance of its torpedoes, to provide a firing solution. By the Second World War, all sides had developed automatic electro-mechanical calculators, exemplified by the U.S. Navy's Torpedo Data Computer. Submarine commanders were still expected to be able to calculate a firing solution by hand as a backup against mechanical failure, and because many submarines existed at the start of the war were not equipped with a TDC; most could keep the "picture" in their heads and do much of the calculations (simple trigonometry) mentally, from extensive training.

Against high-value targets and multiple targets, submarines would launch a spread of torpedoes, to increase the probability of success. Similarly, squadrons of torpedo boats and torpedo bombers would attack together, creating a "fan" of torpedoes across the target's course. Faced with such an attack, the prudent thing for a target to do was to turn to parallel the course of the incoming torpedo and steam away from the torpedoes and the firer, allowing the relatively short-range torpedoes to use up their fuel. An alternative was to "comb the tracks", turning to parallel the incoming torpedo's course, but turning towards the torpedoes. The intention of such a tactic was still to minimize the size of the target offered to the torpedoes, but at the same time be able to aggressively engage the firer. This was the tactic advocated by critics of Jellicoe's actions at Jutland, his caution at turning away from the torpedoes being seen as the reason the Germans escaped.

The use of multiple torpedoes to engage single targets depletes torpedo supplies and greatly reduces a submarine's combat endurance. Endurance can be improved by ensuring a target can be effectively engaged by a single torpedo, which gave rise to the guided torpedo.

In World War II the Germans introduced programmable pattern-running torpedoes, which would run a predetermined pattern until they either ran out of fuel or hit something. The earlier version, FaT, ran out after launch in a straight line, and then weaved backward and forwards parallel to that initial course, whilst the more advanced LuT could transit to a different angle after launch, and then enter a more complex weaving pattern.

Though Luppis' original design had been rope-guided, torpedoes were not wire-guided until the 1960s.

During the First World War the U.S. Navy evaluated a radio controlled torpedo launched from a surface ship called the Hammond Torpedo. A later version tested in the 1930s was claimed to have an effective range of 6 miles (9.7 km).

Modern torpedoes use an umbilical wire, which nowadays allows the computer processing power of the submarine or ship to be used. Torpedoes such as the U.S. Mark 48 can operate in a variety of modes, increasing tactical flexibility.

Homing "fire and forget" torpedoes can use passive or active guidance or a combination of both. Passive acoustic torpedoes home in on emissions from a target. Active acoustic torpedoes home in on the reflection of a signal, or "ping", from the torpedo or its parent vehicle; this has the disadvantage of giving away the presence of the torpedo. In semi-active mode, a torpedo can be fired to the last known position or calculated position of a target, which is then acoustically illuminated ("pinged") once the torpedo is within attack range.

Later in the Second World War torpedoes were given acoustic (homing) guidance systems, with the American Mark 24 mine and Mark 27 torpedo and the German G7es torpedo. Pattern-following and wake homing torpedoes were also developed. Acoustic homing formed the basis for torpedo guidance after the Second World War.

The homing systems for torpedoes are generally acoustic, though there have been other target sensor types used. A ship's acoustic signature is not the only emission a torpedo can home in on to engage U.S. supercarriers, the Soviet Union developed the 53'9665 wake-homing torpedo. As standard acoustic lures can't distract a wake homing torpedo, the US Navy has installed the Surface Ship Torpedo Defense on aircraft carriers that use a Countermeasure Anti-Torpedo to home in on and destroy the attacking torpedo.

The warhead is generally some form of aluminized explosive, because the sustained explosive pulse produced by the powdered aluminum is particularly destructive against underwater targets. Torpex was popular until the 1950s, but has been superseded by PBX compositions. Nuclear torpedoes have also been developed, e.g. the Mark 45 torpedo. In lightweight antisubmarine torpedoes designed to penetrate submarine hulls, a shaped charge can be used. Detonation can be triggered by direct contact with the target or by a proximity fuze incorporating sonar and/or magnetic sensors.

When a torpedo with a contact fuze strikes the side of the target hull, the resulting explosion creates a bubble of expanding gas, the walls of which move faster than the speed of sound in water, thus creating a shock wave. The side of the bubble which is against the hull rips away the external plating creating a large breach. The bubble then collapses in on itself, forcing a high-speed stream of water into the breach which can destroy bulkheads and machinery in its path.

A torpedo fitted with a proximity fuze can be detonated directly under the keel of a target ship. The explosion creates a gas bubble which may damage the keel or underside plating of the target. However, the most destructive part of the explosion is the upthrust of the gas bubble, which will bodily lift the hull in the water. The structure of the hull is designed to resist downward rather than upward pressure, causing severe strain in this phase of the explosion. When the gas bubble collapses, the hull will tend to fall into the void in the water, creating a sagging effect. Finally, the weakened hull will be hit by the uprush of water caused by the collapsing gas bubble, causing structural failure. On vessels up to the size of a modern frigate, this can result in the ship breaking in two and sinking. This effect is likely to prove less catastrophic on a much larger hull, for instance, that of an aircraft carrier.

The damage that may be caused by a torpedo depends on the "shock factor value", a combination of the initial strength of the explosion and the distance between the target and the detonation. When taken about ship hull plating, the term "hull shock factor" (HSF) is used, while keel damage is termed "keel shock factor" (KSF). If the explosion is directly underneath the keel, then HSF is equal to KSF, but explosions that are not directly underneath the ship will have a lower value of KSF.

Usually only created by contact detonation, direct damage is a hole blown in the ship. Among the crew, fragmentation wounds are the most common form of injury. Flooding typically occurs in one or two main watertight compartments, which can sink smaller ships or disable larger ones.

The bubble jet effect occurs when a mine or torpedo detonates in the water a short distance away from the targeted ship. The explosion creates a bubble in the water, and due to the pressure difference, the bubble will collapse from the bottom. The bubble is buoyant, and so it rises towards the surface. If the bubble reaches the surface as it collapses, it can create a pillar of water that can go over a hundred meters into the air (a "columnar plume"). If conditions are right and the bubble collapses onto the ship's hull, the damage to the ship can be extremely serious; the collapsing bubble forms a high-energy jet that can break a meter-wide hole straight through the ship, flooding one or more compartments, and is capable of breaking smaller ships apart. The crew in the areas hit by the pillar are usually killed instantly. Other damage is usually limited.

The Baengnyeong incident, in which ROKS Cheonan broke in half and sank off the coast South Korea in 2010, was caused by the bubble jet effect, according to an international investigation.

If the torpedo detonates at a distance from the ship, and especially under the keel, the change in water pressure causes the ship to resonate. This is frequently the most deadly type of explosion if it is strong enough. The whole ship is dangerously shaken and everything onboard is tossed around. Engines rip from their beds, cables from their holders, etc. A badly shaken ship usually sinks quickly, with hundreds, or even thousands of small leaks all over the ship and no way to power the pumps. The crew fares no better, as the violent shaking tosses them around. This shaking is powerful enough to cause disabling injury to knees and other joints in the body, particularly if the affected person stands on surfaces connected directly to the hull (such as steel decks).

The resulting gas cavitation and shock-front-differential over the width of the human body is sufficient to stun or kill divers.

Control surfaces are essential for a torpedo to maintain its course and depth. A homing torpedo also needs to be able to outmaneuver a target. Good hydrodynamics are needed for it to attain high speed efficiently and also to give a long range since the torpedo has limited stored energy.

Torpedoes may be launched from submarines, surface ships, helicopters and fixed-wing aircraft, unmanned naval mines and naval fortresses. They are also used in conjunction with other weapons; for example, the Mark 46 torpedo used by the United States is the warhead section of the ASROC (Anti-Submarine ROCket) and the CAPTOR mine (CAPsulated TORpedo) is a submerged sensor platform which releases a torpedo when a hostile contact is detected.

Originally, Whitehead torpedoes were intended for launch underwater and the firm was upset when they found out the British were launching them above water, as they considered their torpedoes too delicate for this. However, the torpedoes survived. The launch tubes could be fitted in a ship's bow, which weakened it for ramming, or on the broadside; this introduced problems because of water flow twisting the torpedo, so guide rails and sleeves were used to prevent it. The torpedoes were originally ejected from the tubes by compressed air but later slow-burning gunpowder was used. Torpedo boats originally used a frame that dropped the torpedo into the sea. Royal Navy Coastal Motor Boats of World War I used a rear-facing trough and a cordite ram to push the torpedoes into the water tail-first; they then had to move rapidly out of the way to avoid being hit by their torpedo.

Developed in the run-up to the First World War, multiple-tube mounts (initially twin, later triple and in WW2 up to quintuple in some ships) for 21 to 24 in (53 to 61 cm) torpedoes in rotating turntable mounts appeared. Destroyers could be found with two or three of these mounts with between five and twelve tubes in total. The Japanese went one better, covering their tube mounts with splinter protection and adding reloading gear (both unlike any other navy in the world), making them true turrets and increasing the broadside without adding tubes and top hamper (as the quadruple and quintuple mounts did). Considering that their Type 93s were very effective weapons, the IJN equipped their cruisers with torpedoes. The Germans also equipped their capital ships with torpedoes.

Smaller vessels such as PT boats carried their torpedoes in fixed deck-mounted tubes using compressed air. These were either aligned to fire forward or at an offset angle from the centerline.

Later, lightweight mounts for 12.75 in (32.4 cm) homing torpedoes were developed for anti-submarine use consisting of triple launch tubes used on the decks of ships. These were the 1960 Mk 32 torpedo launcher in the US and part of STWS (Shipborne Torpedo Weapon System) in the UK. Later a below-decks launcher was used by the RN. This basic launch system continues to be used today with improved torpedoes and fire control systems.

Modern submarines use either swim-out systems or a pulse of water to discharge the torpedo from the tube, both of which have the advantage of being significantly quieter than previous systems, helping avoid detection of the firing from passive sonar. Earlier designs used a pulse of compressed air or a hydraulic ram.

Early submarines, when they carried torpedoes, were fitted with a variety of torpedo launching mechanisms in a range of locations; on the deck, in the bow or stern, amidships, with some launch mechanisms permitting the torpedo to be aimed over a wide arc. By World War II, designs favored multiple bow tubes and fewer or no stern tubes. Modern submarine bows are usually occupied by a large sonar array, necessitating midships tubes angled outward, while stern tubes have largely disappeared. The first French and Russian submarines carried their torpedoes externally in Drzewiecki drop collars. These were cheaper than tubes but less reliable. Both the United Kingdom and the United States experimented with external tubes in World War II. External tubes offered a cheap and easy way of increasing torpedo capacity without radical redesign, something neither had time or resources to do before nor early in, the war. British T-class submarines carried up to 13 torpedo tubes, up to 5 of them external. America's use was mainly limited to earlier Porpoise-, Salmon-, and Sargo-class boats. Until the appearance of the Tambor class, most American submarines only carried 4 bow and either 2 or 4 stern tubes, something many American submarine officers felt provided inadequate firepower. This problem was compounded by the notorious unreliability of the Mark 14 torpedo.

Late in World War II, the U.S. adopted a 16 in (41 cm) homing torpedo (known as "Cutie") for use against escorts. It was basically a modified Mark 24 Mine with wooden rails to allow firing from a 21 in (53 cm) torpedo tube.

Aerial torpedoes may be carried by fixed-wing aircraft, helicopters, or missiles. They are launched from the first two at prescribed speeds and altitudes, dropped from bomb-bays or underwing hardpoints.

Although lightweight torpedoes are fairly easily handled, the transport and handling of heavyweight torpedoes is difficult, especially in the tight spaces in a submarine. After the Second World War, some Type XXI submarines were obtained from Germany by the United States and Britain. One of the main novel developments seen was a mechanical handling system for torpedoes. Such systems were widely adopted as a result of this discovery.

Torpedoes are launched in several ways:

From a torpedo tube mounted either in a trainable deck mount (common in destroyers), or fixed above or below the waterline of a surface vessel (as in cruisers, battleships, and armed merchant cruisers) or submarine.
Early submarines and some torpedo boats (such as the U.S. World War II PT boats, which used the Mark 13 aircraft torpedo) used deck-mounted "drop collars", which simply relied on gravity.
From shackles aboard low-flying aircraft or helicopters.
As the final stage of a compound rocket or ramjet powered munition (sometimes called an assisted torpedo).
Many navies have two weights of torpedoes:

A light torpedo used primarily as a close attack weapon, particularly by aircraft.
A heavy torpedo used primarily as a standoff weapon, particularly by submerged submarines.
In the case of deck or tube launched torpedoes, the diameter of the torpedo is a key factor in determining the suitability of a particular torpedo to a tube or launcher, similar to the caliber of the gun. The size is not quite as critical as for a gun, but the diameter has become the most common way of classifying torpedoes.

Length, weight, and other factors also contribute to compatibility. In the case of aircraft launched torpedoes, the key factors are weight, provision of suitable attachment points, and launch speed. Assisted torpedoes are the most recent development in torpedo design, and are normally engineered as an integrated package. Versions for aircraft and assisted launching have sometimes been based on deck or tube launched versions, and there has been at least one case of a submarine torpedo tube being designed to fire an aircraft torpedo.

As in all munition design, there is a compromise between standardization, which simplifies manufacture, and logistics, and specialization, which may make the weapon significantly more effective. Small improvements in either logistics or effectiveness can translate into enormous operational advantages.

In physics, a projectile launched with specific initial conditions will have a range. It may be more predictable assuming a flat Earth with a uniform gravity field, and no air resistance.

The following applies for ranges which are small compared to the size of the Earth. For longer ranges see sub-orbital spaceflight. The maximum horizontal distance traveled by the projectile, neglecting air resistance, can be calculated as follows:

where

d is the total horizontal distance travelled by the projectile.
v is the velocity at which the projectile is launched
g is the gravitational acceleration'97usually taken to be 9.81 m/s2 (32 f/s2) near the Earth's surface
uc0u952  is the angle at which the projectile is launched
y0 is the initial height of the projectile
If y0 is taken to be zero, meaning that the object is being launched on flat ground, the range of the projectile will simplify to:

Ideal projectile motion states that there is no air resistance and no change in gravitational acceleration. This assumption simplifies the mathematics greatly, and is a close approximation of actual projectile motion in cases where the distances travelled are small. Ideal projectile motion is also a good introduction to the topic before adding the complications of air resistance.

A launch angle of 45 degrees displaces the projectile the farthest horizontally. This is due to the nature of right triangles. Additionally, from the equation for the range :

We can see that the range will be maximum when the value of is the highest (i.e. when it is equal to 1). Clearly, has to be 90 degrees. That is to say, is 45 degrees.

First we examine the case where (y0) is zero. The horizontal position of the projectile is

In the vertical direction

We are interested in the time when the projectile returns to the same height it originated. Let tg be any time when the height of the projectile is equal to its initial value.

By factoring:

or

but t = T = time of flight

The first solution corresponds to when the projectile is first launched. The second solution is the useful one for determining the range of the projectile. Plugging this value for (t) into the horizontal equation yields

Applying the trigonometric identity

If x and y are same,

allows us to simplify the solution to

Note that when (uc0u952 ) is 45'b0, the solution becomes

Now we will allow (y0) to be nonzero. Our equations of motion are now

and

Once again we solve for (t) in the case where the (y) position of the projectile is at zero (since this is how we defined our starting height to begin with)

Again by applying the quadratic formula we find two solutions for the time. After several steps of algebraic manipulation

The square root must be a positive number, and since the velocity and the sine of the launch angle can also be assumed to be positive, the solution with the greater time will occur when the positive of the plus or minus sign is used. Thus, the solution is

Solving for the range once again

To maximize the range at any height

Checking the limit as approaches 0

The angle uc0u968  at which the projectile lands is given by:

For maximum range, this results in the following equation:

Rewriting the original solution for uc0u952 , we get:

Multiplying with the equation for (tan uc0u968 )^2 gives:

Because of the trigonometric identity

,
this means that uc0u952  + u968  must be 90 degrees.

In addition to air resistance, which slows a projectile and reduces its range, many other factors also have to be accounted for when actual projectile motion is considered.

Generally speaking, a projectile with greater volume faces greater air resistance, reducing the range of the projectile. (And see Trajectory of a projectile.) Air resistance drag can be modified by the projectile shape: a tall and wide, but short projectile will face greater air resistance than a low and narrow, but long, projectile of the same volume. The surface of the projectile also must be considered: a smooth projectile will face less air resistance than a rough-surfaced one, and irregularities on the surface of a projectile may change its trajectory if they create more drag on one side of the projectile than on the other. However, certain irregularities such as dimples on a golf ball may actually increase its range by reducing the amount of turbulence caused behind the projectile as it travels. Mass also becomes important, as a more massive projectile will have more kinetic energy, and will thus be less affected by air resistance. The distribution of mass within the projectile can also be important, as an unevenly weighted projectile may spin undesirably, causing irregularities in its trajectory due to the magnus effect.

If a projectile is given rotation along its axes of travel, irregularities in the projectile's shape and weight distribution tend to be cancelled out. See rifling for a greater explanation.

For projectiles that are launched by firearms and artillery, the nature of the gun's barrel is also important. Longer barrels allow more of the propellant's energy to be given to the projectile, yielding greater range. Rifling, while it may not increase the average (arithmetic mean) range of many shots from the same gun, will increase the accuracy and precision of the gun.

Some cannons or howitzers have been created with a very large range.

During World War I the Germans created an exceptionally large cannon, the Paris Gun, which could fire a shell more than 80 miles (130 km). North Korea has developed a gun known in the West as Koksan, with a range of 60 km using rocket-assisted projectiles. (And see Trajectory of a projectile.)

Such cannons are distinguished from rockets, or ballistic missiles, which have their own rocket engines, which continue to accelerate the missile for a period after they have been launched.

Space debris (also known as space junk, space pollution, space waste, space trash, or space garbage) is defunct human-made objects in space'97principally in Earth orbit'97which no longer serve a useful function. These include derelict spacecraft'97nonfunctional spacecraft and abandoned launch vehicle stages'97mission-related debris, and particularly numerous in Earth orbit, fragmentation debris from the breakup of derelict rocket bodies and spacecraft. In addition to derelict human-made objects left in orbit, other examples of space debris include fragments from their disintegration, erosion and collisions or even paint flecks, solidified liquids expelled from spacecraft, and unburned particles from solid rocket motors. Space debris represents a risk to spacecraft.

Space debris is typically a negative externality'97it creates an external cost on others from the initial action to launch or use a spacecraft in near-Earth orbit'97a cost that is typically not taken into account nor fully accounted for in the cost by the launcher or payload owner.

Several spacecraft, both crewed and uncrewed, have been damaged or destroyed by space debris. The measurement, mitigation, and potential removal of debris are conducted by some participants in the space industry.

As of January 2021, the US Space Surveillance Network reported 21,901 artificial objects in orbit above the Earth, including 4,450 operational satellites. However, these are just the objects large enough to be tracked. As of January 2019, more than 128 million pieces of debris smaller than 1 cm (0.4 in), about 900,000 pieces of debris 1'9610 cm, and around 34,000 of pieces larger than 10 cm (3.9 in) were estimated to be in orbit around the Earth. When the smallest objects of artificial space debris (paint flecks, solid rocket exhaust particles, etc.) are grouped with micrometeoroids, they are together sometimes referred to by space agencies as MMOD (Micrometeoroid and Orbital Debris). Collisions with debris have become a hazard to spacecraft; the smallest objects cause damage akin to sandblasting, especially to solar panels and optics like telescopes or star trackers that cannot easily be protected by a ballistic shield.

Below 2,000 km (1,200 mi) Earth-altitude, pieces of debris are denser than meteoroids; most are dust from solid rocket motors, surface erosion debris like paint flakes, and frozen coolant from RORSAT (nuclear-powered satellites). For comparison, the International Space Station orbits in the 300'96400 kilometres (190'96250 mi) range, while the two most recent large debris events'97the 2007 Chinese antisat weapon test and the 2009 satellite collision'97occurred at 800 to 900 kilometres (500 to 560 mi) altitude. The ISS has Whipple shielding to resist damage from small MMOD; however, known debris with a collision chance over 1/10,000 are avoided by maneuvering the station.

Space debris began to accumulate in Earth orbit immediately with the first launch of an artificial satellite Sputnik 1 into orbit in October 1957. But even before that, beside natural ejecta from Earth, humans might have produced ejecta that became space debris, as in the August 1957 Pascal B test. After the launch of Sputnik, the North American Aerospace Defense Command (NORAD) began compiling a database (the Space Object Catalog) of all known rocket launches and objects reaching orbit: satellites, protective shields and upper-stages of launch vehicles. NASA later published modified versions of the database in two-line element set, and beginning in the early 1980s the CelesTrak bulletin board system re-published them.

The trackers (NORAD) who fed the database were aware of other objects in orbit, many of which were the result of in-orbit explosions. Some were deliberately caused during the 1960s anti-satellite weapon (ASAT) testing, and others were the result of rocket stages blowing up in orbit as leftover propellant expanded and ruptured their tanks. To improve tracking, NORAD employee John Gabbard kept a separate database. Studying the explosions, Gabbard developed a technique for predicting the orbital paths of their products, and Gabbard diagrams (or plots) are now widely used. These studies were used to improve the modeling of orbital evolution and decay.

When the NORAD database became publicly available during the 1970s, techniques developed for the asteroid-belt were applied to the study to the database of known artificial satellite Earth objects.

In addition to approaches to debris reduction where time and natural gravitational/atmospheric effects help to clear space debris, or a variety of technological approaches that have been proposed (with most not implemented) to reduce space debris, a number of scholars have observed that institutional factors'97political, legal, economic and cultural "rules of the game"'97are the greatest impediment to the cleanup of near-Earth space. By 2014, there was little commercial incentive to reduce space debris, since the cost of dealing with it is not assigned to the entity producing it, but rather falls on all users of the space environment, and rely on human society as a whole that benefits from space technologies and knowledge. A number of suggestions for improving institutions so as to increase the incentives to reduce space debris have been made. These include government mandates to create incentives, as well as companies coming to see economic benefit to reducing debris more aggressively than existing government standard practices. In 1979 NASA founded the Orbital Debris Program to research mitigation measures for space debris in Earth orbit.

During the 1980s, NASA and other U.S. groups attempted to limit the growth of debris. One trial solution was implemented by McDonnell Douglas for the Delta launch vehicle, by having the booster move away from its payload and vent any propellant remaining in its tanks. This eliminated one source for pressure buildup in the tanks which had previously caused them to explode and create additional orbital debris. Other countries were slower to adopt this measure and, due especially to a number of launches by the Soviet Union, the problem grew throughout the decade.

A new battery of studies followed as NASA, NORAD and others attempted to better understand the orbital environment, with each adjusting the number of pieces of debris in the critical-mass zone upward. Although in 1981 (when Schefter's article was published) the number of objects was estimated at 5,000, new detectors in the Ground-based Electro-Optical Deep Space Surveillance system found new objects. By the late 1990s, it was thought that most of the 28,000 launched objects had already decayed and about 8,500 remained in orbit. By 2005 this was adjusted upward to 13,000 objects, and a 2006 study increased the number to 19,000 as a result of an ASAT test and a satellite collision. In 2011, NASA said that 22,000 objects were being tracked.

A 2006 NASA model suggested that if no new launches took place the environment would retain the then-known population until about 2055, when it would increase on its own. Richard Crowther of Britain's Defence Evaluation and Research Agency said in 2002 that he believed the cascade would begin about 2015. The National Academy of Sciences, summarizing the professional view, noted widespread agreement that two bands of LEO space'97900 to 1,000 km (620 mi) and 1,500 km (930 mi)'97were already past critical density.

In the 2009 European Air and Space Conference, University of Southampton researcher Hugh Lewis predicted that the threat from space debris would rise 50 percent in the next decade and quadruple in the next 50 years. As of 2009, more than 13,000 close calls were tracked weekly.

A 2011 report by the U.S. National Research Council warned NASA that the amount of orbiting space debris was at a critical level. According to some computer models, the amount of space debris "has reached a tipping point, with enough currently in orbit to continually collide and create even more debris, raising the risk of spacecraft failures". The report called for international regulations limiting debris and research of disposal methods.

As of 2009, 19,000 debris over 5 cm (2 in) were tracked by United States Space Surveillance Network.
As of July 2013, estimates of more than 170 million debris smaller than 1 cm (0.4 in), about 670,000 debris 1'9610 cm, and approximately 29,000 larger pieces of debris are in orbit.
As of July 2016, nearly 18,000 artificial objects are orbiting above Earth, including 1,419 operational satellites.
As of October 2019, nearly 20,000 artificial objects in orbit above the Earth, including 2,218 operational satellites.
There are estimated to be over 128 million pieces of debris smaller than 1 cm (0.39 in) as of January 2019. There are approximately 900,000 pieces from 1 to 10 cm. The current count of large debris (defined as 10 cm across or larger) is 34,000. The technical measurement cutoff is c. 3 mm (0.12 in). As of 2020 there is 8000 metric tons of debris in orbit with no signs of slowing down.

In the orbits nearest to Earth'97less than 2,000 km (1,200 mi) orbital altitude, referred to as low-Earth orbit (LEO)'97 there have traditionally been few "universal orbits" that keep a number of spacecraft in particular rings (in contrast to GEO, a single orbit that is widely used by over 500 satellites). This is beginning to change in 2019, and several companies have begun to deploy the early phases of satellite internet constellations, which will have many universal orbits in LEO with 30 to 50 satellites per orbital plane and altitude. Traditionally, the most populated LEO orbits have been a number of sun-synchronous satellites that keep a constant angle between the Sun and the orbital plane, making Earth observation easier with consistent sun angle and lighting. Sun-synchronous orbits are polar, meaning they cross over the polar regions. LEO satellites orbit in many planes, typically up to 15 times a day, causing frequent approaches between objects. The density of satellites'97both active and derelict'97is much higher in LEO.

Orbits are affected by gravitational perturbations (which in LEO include unevenness of the Earth's gravitational field due to variations in the density of the planet), and collisions can occur from any direction. The average impact speed of collisions in Low Earth Orbit is 10 km/s with maximums reaching above 14 km/s due to orbital eccentricity. The 2009 satellite collision occurred at a closing speed of 11.7 km/s (26,000 mph), creating over 2000 large debris fragments. These debris cross many other orbits and increase debris collision risk.

It is theorized that a sufficiently large collision of spacecraft could potentially lead to a cascade effect, or even make some particular low Earth orbits effectively unusable for long term use by orbiting satellites, a phenomenon known as the Kessler syndrome. The theoretical effect is projected to be a theoretical runaway chain reaction of collisions that could occur, exponentially increasing the number and density of space debris in low-Earth orbit, and has been hypothesized to ensue beyond some critical density.

Crewed space missions are mostly at 400 km (250 mi) altitude and below, where air drag helps clear zones of fragments. The upper atmosphere is not a fixed density at any particular orbital altitude; it varies as a result of atmospheric tides and expands or contracts over longer time periods as a result of space weather. These longer-term effects can increase drag at lower altitudes; the 1990s expansion was a factor in reduced debris density. Another factor was fewer launches by Russia; the Soviet Union made most of their launches in the 1970s and 1980s.

At higher altitudes, where air drag is less significant, orbital decay takes longer. Slight atmospheric drag, lunar perturbations, Earth's gravity perturbations, solar wind and solar radiation pressure can gradually bring debris down to lower altitudes (where it decays), but at very high altitudes this may take millennia. Although high-altitude orbits are less commonly used than LEO and the onset of the problem is slower, the numbers progress toward the critical threshold more quickly.

Many communications satellites are in geostationary orbits (GEO), clustering over specific targets and sharing the same orbital path. Although velocities are low between GEO objects, when a satellite becomes derelict (such as Telstar 401) it assumes a geosynchronous orbit; its orbital inclination increases about .8'b0 and its speed increases about 160 km/h (99 mph) per year. Impact velocity peaks at about 1.5 km/s (0.93 mi/s). Orbital perturbations cause longitude drift of the inoperable spacecraft and precession of the orbital plane. Close approaches (within 50 meters) are estimated at one per year. The collision debris pose less short-term risk than from an LEO collision, but the satellite would likely become inoperable. Large objects, such as solar-power satellites, are especially vulnerable to collisions.

Although the ITU now requires proof a satellite can be moved out of its orbital slot at the end of its lifespan, studies suggest this is insufficient. Since GEO orbit is too distant to accurately measure objects under 1 m (3 ft 3 in), the nature of the problem is not well known. Satellites could be moved to empty spots in GEO, requiring less maneuvering and making it easier to predict future motion. Satellites or boosters in other orbits, especially stranded in geostationary transfer orbit, are an additional concern due to their typically high crossing velocity.

Despite efforts to reduce risk, spacecraft collisions have occurred. The European Space Agency telecom satellite Olympus-1 was struck by a meteoroid on 11 August 1993 and eventually moved to a graveyard orbit. On 29 March 2006, the Russian Express-AM11 communications satellite was struck by an unknown object and rendered inoperable; its engineers had enough contact time with the satellite to send it into a graveyard orbit.

In 1958, the United States launched Vanguard I into a medium Earth orbit (MEO). As of October 2009, it, and the upper stage of its launch rocket, were the oldest surviving artificial space objects still in orbit. In a catalog of known launches until July 2009, the Union of Concerned Scientists listed 902 operational satellites from a known population of 19,000 large objects and about 30,000 objects launched.

An example of additional derelict satellite debris is the remains of the 1970s/80s Soviet RORSAT naval surveillance satellite program. The satellites' BES-5 nuclear reactors were cooled with a coolant loop of sodium-potassium alloy, creating a potential problem when the satellite reached end of life. While many satellites were nominally boosted into medium-altitude graveyard orbits, not all were. Even satellites that had been properly moved to a higher orbit had an eight-percent probability of puncture and coolant release over a 50-year period. The coolant freezes into droplets of solid sodium-potassium alloy, forming additional debris.

In February 2015, the USAF Defense Meteorological Satellite Program Flight 13 (DMSP-F13) exploded on orbit, creating at least 149 debris objects, which were expected to remain in orbit for decades.

Orbiting satellites have been deliberately destroyed. United States and USSR/Russia have conducted over 30 and 27 ASAT tests, respectively, followed by 10 from China and one from India. The most recent ASATs were Chinese interception of FY-1C, trials of Russian PL-19 Nudol, American interception of USA-193 and Indian interception of unstated live satellite.

Space debris includes a glove lost by astronaut Ed White on the first American space-walk (EVA), a camera lost by Michael Collins near Gemini 10, a thermal blanket lost during STS-88, garbage bags jettisoned by Soviet cosmonauts during Mir's 15-year life, a wrench, and a toothbrush. Sunita Williams of STS-116 lost a camera during an EVA. During an STS-120 EVA to reinforce a torn solar panel, a pair of pliers was lost, and in an STS-126 EVA, Heidemarie Stefanyshyn-Piper lost a briefcase-sized tool bag.

In characterizing the problem of space debris, it was learned that much debris was due to rocket upper stages (e.g. the Inertial Upper Stage) which end up in orbit, and break up due to decomposition of unvented unburned fuel. However, a major known impact event involved an (intact) Ariane booster. Although NASA and the United States Air Force now require upper-stage passivation, other launchers do not. Lower stages, like the Space Shuttle's solid rocket boosters or Apollo program's Saturn IB launch vehicles, do not reach orbit.

On 11 March 2000 a Chinese Long March 4 CBERS-1 upper stage exploded in orbit, creating a debris cloud. A Russian Briz-M booster stage exploded in orbit over South Australia on 19 February 2007. Launched on 28 February 2006 carrying an Arabsat-4A communications satellite, it malfunctioned before it could use up its propellant. Although the explosion was captured on film by astronomers, due to the orbit path the debris cloud has been difficult to measure with radar. By 21 February 2007, over 1,000 fragments were identified. A 14 February 2007 breakup was recorded by Celestrak. Eight breakups occurred in 2006, the most since 1993. Another Briz-M broke up on 16 October 2012 after a failed 6 August Proton-M launch. The amount and size of the debris was unknown. A Long March 7 rocket booster created a fireball visible from portions of Utah, Nevada, Colorado, Idaho and California on the evening of 27 July 2016; its disintegration was widely reported on social media. In 2018'962019, three different Atlas V Centaur second stages have broken up.

In December 2020, scientists confirmed that a previously detected near-Earth object, 2020 SO, was rocket booster space junk launched in 1966 orbiting Earth and the Sun.

A past debris source was the testing of anti-satellite weapons (ASATs) by the U.S. and Soviet Union during the 1960s and 1970s. North American Aerospace Defense Command (NORAD) files only contained data for Soviet tests, and debris from U.S. tests were only identified later. By the time the debris problem was understood, widespread ASAT testing had ended; the U.S. Program 437 was shut down in 1975.

The U.S. restarted their ASAT programs in the 1980s with the Vought ASM-135 ASAT. A 1985 test destroyed a 1-tonne (2,200 lb) satellite orbiting at 525 km (326 mi), creating thousands of debris larger than 1 cm (0.39 in). Due to the altitude, atmospheric drag decayed the orbit of most debris within a decade. A de facto moratorium followed the test.

China's government was condemned for the military implications and the amount of debris from the 2007 anti-satellite missile test, the largest single space debris incident in history (creating over 2,300 pieces golf-ball size or larger, over 35,000 1 cm (0.4 in) or larger, and one million pieces 1 mm (0.04 in) or larger). The target satellite orbited between 850 km (530 mi) and 882 km (548 mi), the portion of near-Earth space most densely populated with satellites. Since atmospheric drag is low at that altitude, the debris is slow to return to Earth, and in June 2007 NASA's Terra environmental spacecraft maneuvered to avoid impact from the debris. Dr. Brian Weeden, U.S. Air Force officer and Secure World Foundation staff member, noted that the 2007 Chinese satellite explosion created an orbital debris of more than 3,000 separate objects that then required tracking. On 20 February 2008, the U.S. launched an SM-3 missile from the USS Lake Erie to destroy a defective U.S. spy satellite thought to be carrying 450 kg (1,000 lb) of toxic hydrazine propellant. The event occurred at about 250 km (155 mi), and the resulting debris has a perigee of 250 km (155 mi) or lower. The missile was aimed to minimize the amount of debris, which (according to Pentagon Strategic Command chief Kevin Chilton) had decayed by early 2009.

On 27 March 2019, Indian Prime Minister Narendra Modi announced that India shot down one of its own LEO satellites with a ground-based missile. He stated that the operation, part of Mission Shakti, would defend the country's interests in space. Afterwards, US Air Force Space Command announced they were tracking 270 new pieces of debris but expected the number to grow as data collection continues.

On 15 November 2021 the Russian Defense Ministry destroyed Kosmos 1408 orbiting at around 450 km, creating "more than 1,500 pieces of trackable debris and hundreds of thousands of pieces of un-trackable debris" according to the US State Department.

The vulnerability of satellites to debris and the possibility of attacking LEO satellites to create debris clouds has triggered speculation that it is possible for countries unable to make a precision attack. An attack on a satellite of 10 t (22,000 lb) or more would heavily damage the LEO environment.

Space junk can be a hazard to active satellites and spacecraft. It has been theorized that Earth orbit could even become impassable if the risk of collision grows too high.

However, since the risk to spacecraft increases with the time of exposure to high debris densities, it is more accurate to say that LEO would be rendered unusable by orbiting craft. The threat to craft passing through LEO to reach higher orbit would be much lower owing to the very short time span of the crossing.

Although spacecraft are typically protected by Whipple shields, solar panels, which are exposed to the Sun, wear from low-mass impacts. Even small impacts can produce a cloud of plasma which is an electrical risk to the panels.

Satellites are believed to have been destroyed by micrometeorites and (small) orbital debris (MMOD). The earliest suspected loss was of Kosmos 1275, which disappeared on 24 July 1981 (a month after launch). Kosmos contained no volatile propellant, therefore, there appeared to be nothing internal to the satellite which could have caused the destructive explosion which took place. However, the case has not been proven and another hypothesis forwarded is that the battery exploded. Tracking showed it broke up, into 300 new objects.

Many impacts have been confirmed since. For example, on 24 July 1996, the French microsatellite Cerise was hit by fragments of an Ariane-1 H-10 upper-stage booster which exploded in November 1986. On 29 March 2006, the Russian Ekspress AM11 communications satellite was struck by an unknown object and rendered inoperable. On 13 October 2009, Terra suffered a single battery cell failure anomaly and a battery heater control anomaly which were subsequently considered likely the result of an MMOD strike. On 12 March 2010, Aura lost power from one-half of one of its 11 solar panels and this was also attributed to an MMOD strike. On 22 May 2013, GOES 13 was hit by an MMOD which caused it to lose track of the stars that it used to maintain an operational attitude. It took nearly a month for the spacecraft to return to operation.

The first major satellite collision occurred on 10 February 2009. The 950 kg (2,090 lb) derelict satellite Kosmos 2251 and the operational 560 kg (1,230 lb) Iridium 33 collided, 500 mi (800 km) over northern Siberia. The relative speed of impact was about 11.7 km/s (7.3 mi/s), or about 42,120 km/h (26,170 mph). Both satellites were destroyed, creating thousands of pieces of new smaller debris, with legal and political liability issues unresolved even years later. On 22 January 2013, BLITS (a Russian laser-ranging satellite) was struck by debris suspected to be from the 2007 Chinese anti-satellite missile test, changing both its orbit and rotation rate.

Satellites sometimes perform Collision Avoidance Maneuvers and satellite operators may monitor space debris as part of maneuver planning. For example, in January 2017, the European Space Agency made the decision to alter orbit of one of its three Swarm mission spacecraft, based on data from the US Joint Space Operations Center, to lower the risk of collision from Cosmos-375, a derelict Russian satellite.

Crewed flights are naturally particularly sensitive to the hazards that could be presented by space debris conjunctions in the orbital path of the spacecraft. Examples of occasional avoidance maneuvers, or longer-term space debris wear, have occurred in Space Shuttle missions, the MIR space station, and the International Space Station.

From the early Space Shuttle missions, NASA used NORAD space monitoring capabilities to assess the Shuttle's orbital path for debris. In the 1980s, this used a large proportion of NORAD capacity. The first collision-avoidance maneuver occurred during STS-48 in September 1991, a seven-second thruster burn to avoid debris from the derelict satellite Kosmos 955. Similar maneuvers were initiated on missions 53, 72 and 82.

One of the earliest events to publicize the debris problem occurred on Space Shuttle Challenger's second flight, STS-7. A fleck of paint struck its front window, creating a pit over 1 mm (0.04 in) wide. On STS-59 in 1994, Endeavour's front window was pitted about half its depth. Minor debris impacts increased from 1998.

Window chipping and minor damage to thermal protection system tiles (TPS) were already common by the 1990s. The Shuttle was later flown tail-first to take a greater proportion of the debris load on the engines and rear cargo bay, which are not used in orbit or during descent, and thus are less critical for post-launch operation. When flying attached to the ISS, the two connected spacecraft were flipped around so the better-armored station shielded the orbiter.

A NASA 2005 study concluded that debris accounted for approximately half of the overall risk to the Shuttle. Executive-level decision to proceed was required if catastrophic impact was likelier than 1 in 200. On a normal (low-orbit) mission to the ISS the risk was approximately 1 in 300, but the Hubble telescope repair mission was flown at the higher orbital altitude of 560 km (350 mi) where the risk was initially calculated at a 1-in-185 (due in part to the 2009 satellite collision). A re-analysis with better debris numbers reduced the estimated risk to 1 in 221, and the mission went ahead.

Debris incidents continued on later Shuttle missions. During STS-115 in 2006 a fragment of circuit board bored a small hole through the radiator panels in Atlantis's cargo bay. On STS-118 in 2007 debris blew a bullet-like hole through Endeavour's radiator panel.

Impact wear was notable on Mir, the Soviet space station, since it remained in space for long periods with its original solar module panels.

The ISS also uses Whipple shielding to protect its interior from minor debris. However, exterior portions (notably its solar panels) cannot be protected easily. In 1989, the ISS panels were predicted to degrade approximately 0.23% in four years due to the "sandblasting" effect of impacts with small orbital debris. An avoidance maneuver is typically performed for the ISS if "there is a greater than one-in-10,000 chance of a debris strike". As of January 2014, there have been sixteen maneuvers in the fifteen years the ISS had been in orbit. By 2019, over 1,400 meteoroid and orbital debris (MMOD) impacts had been recorded on the ISS.

As another method to reduce the risk to humans on board, ISS operational management asked the crew to shelter in the Soyuz on three occasions due to late debris-proximity warnings. In addition to the sixteen thruster firings and three Soyuz-capsule shelter orders, one attempted maneuver was not completed due to not having the several days' warning necessary to upload the maneuver timeline to the station's computer. A March 2009 event involved debris believed to be a 10 cm (3.9 in) piece of the Kosmos 1275 satellite. In 2013, the ISS operations management did not make a maneuver to avoid any debris, after making a record four debris maneuvers the previous year.

The Kessler syndrome, proposed by NASA scientist Donald J. Kessler in 1978, is a theoretical scenario in which the density of objects in low Earth orbit (LEO) is high enough that collisions between objects could cause a cascade effect where each collision generates space debris that increases the likelihood of further collisions. He further theorized that one implication if this were to occur is that the distribution of debris in orbit could render space activities and the use of satellites in specific orbital ranges economically impractical for many generations.

The growth in the number of objects as a result of the late-1990s studies sparked debate in the space community on the nature of the problem and the earlier dire warnings. According to Kessler's 1991 derivation and 2001 updates, the LEO environment in the 1,000 km (620 mi) altitude range should be cascading. However, only one major satellite collision incident has occurred: the 2009 satellite collision between Iridium 33 and Cosmos 2251. The lack of obvious short-term cascading has led to speculation that the original estimates overstated the problem. According to Kessler in 2010 however, a cascade may not be obvious until it is well advanced, which might take years.

Although most debris burns up in the atmosphere, larger debris objects can reach the ground intact. According to NASA, an average of one cataloged piece of debris has fallen back to Earth each day for the past 50 years. Despite their size, there has been no significant property damage from the debris. Burning up in the atmosphere may also contribute to atmospheric pollution.

Notable examples of space junk falling to Earth and impacting human life include:

1969: five sailors on a Japanese ship were injured when space debris from what was believed to be a Soviet spacecraft struck the deck of their boat.
1978: the Soviet reconnaissance satellite Kosmos 954 reentered the atmosphere over northwest Canada and scattered radioactive debris over northern Canada, some landing in the Great Slave Lake.
1979: portions of Skylab came down over Australia, and several pieces landed in the area around the Shire of Esperance, which fined NASA $400 for littering.
1987: a 7-foot strip of metal from the Soviet Kosmos 1890 rocket landed between two homes in Lakeport, California, causing no damage.
1991: Salyut 7 underwent an uncontrolled reentry on 7 February over the city of Capit'e1n Berm'fadez in Argentina.
1997: an Oklahoma woman, Lottie Williams, was hit, without injury in the shoulder by a 10 cm 'd7 13 cm (3.9 in 'd7 5.1 in) piece of blackened, woven metallic material confirmed as part of the propellant tank of a Delta II rocket which launched a U.S. Air Force satellite the year before.
2001: a Star 48 Payload Assist Module (PAM-D) rocket upper stage re-entered the atmosphere after a "catastrophic orbital decay", crashing in the Saudi Arabian desert. It was identified as the upper-stage rocket for NAVSTAR 32, a GPS satellite launched in 1993.
2002: 6-year-old boy Wu Jie became the first person to be injured by direct impact from space debris. He suffered a fractured toe and a swelling on his forehead after a block of aluminum, 80 centimeters by 50 centimeters and weighing 10 kilograms, from the outer shell of the Resource Second satellite struck him as he sat beneath a persimmon tree in the Shaanxi province of China.
2003: Columbia disaster, large parts of the spacecraft reached the ground and entire equipment systems remained intact. More than 83,000 pieces, along with the remains of the six astronauts, were recovered in an area from three to ten miles around Hemphill in Sabine County, Texas. More pieces were found in a line from west Texas to east Louisiana, with the westernmost piece found in Littlefield, TX and the easternmost found southwest of Mora, Louisiana. Debris was found in Texas, Arkansas and Louisiana. In a rare case of property damage, a foot-long metal bracket smashed through the roof of a dentist office. NASA warned the public to avoid contact with the debris because of the possible presence of hazardous chemicals. 15 years after the failure, people were still sending in pieces with the most recent, as of February 2018, found in the spring of 2017.
2007: airborne debris from a Russian spy satellite was seen by the pilot of a LAN Airlines Airbus A340 carrying 270 passengers whilst flying over the Pacific Ocean between Santiago and Auckland. The debris was reported within 9.3 kilometres (5 nmi) of the aircraft.
2016: on 2 November, upper stage of Vega flight VV01 launched on 13 February 2012 reentered over Indian state of Tamil Nadu. A composite overwrapped pressure vessel survived reentry and was recovered.
2020: The empty core stage of a Long March-5B rocket made an uncontrolled re-entry - the largest object to do so since the Soviet Union's 39-ton Salyut 7 space station in 1991 '96 over Africa and the Atlantic Ocean and a 12-meter-long pipe originating from the rocket crashed into the village of Mahounou in C'f4te d'Ivoire.
2021: a Falcon 9 second stage made an uncontrolled re-entry over Washington state on March 25, producing a widely seen "light show". A composite-overwrapped pressure vessel survived the re-entry and landed on a farm field.
2022: on 2 April, pieces of reentered space debris impacted multiple locations in Indian state of Maharashtra, the event of reentry was witnessed by many. Recovered debris consisted of metallic ring almost 3 meter in diameter along at least six composite overwrapped pressure vessels with some bearing '3CCA301001 B' marking. The debris is likely from third stage of Long March 3B rocket with Y77 serial, launched in February 2021. A month later on 12 May another incidence of space debris reentry and impact was reported over Indian state of Gujarat, surviving debris consisted of metal fragments and at least three composite overwrapped pressure vessels. Allegedly the falling debris killed a livestock animal and injured another as one metal fragment struck a sheep pen. The debris is likely from third stage of Long March 3B rocket with Y86 serial, launched in September 2021. Indian space agency ISRO is investigating both incidences.
Radar and optical detectors such as lidar are the main tools for tracking space debris. Although objects under 10 cm (4 in) have reduced orbital stability, debris as small as 1 cm can be tracked, however determining orbits to allow re-acquisition is difficult. Most debris remain unobserved. The NASA Orbital Debris Observatory tracked space debris with a 3 m (10 ft) liquid mirror transit telescope. FM Radio waves can detect debris, after reflecting off them onto a receiver. Optical tracking may be a useful early-warning system on spacecraft.

The U.S. Strategic Command keeps a catalog of known orbital objects, using ground-based radar and telescopes, and a space-based telescope (originally to distinguish from hostile missiles). The 2009 edition listed about 19,000 objects. Other data come from the ESA Space Debris Telescope, TIRA, the Goldstone, Haystack, and EISCAT radars and the Cobra Dane phased array radar, to be used in debris-environment models like the ESA Meteoroid and Space Debris Terrestrial Environment Reference (MASTER).

Returned space hardware is a valuable source of information on the directional distribution and composition of the (sub-millimetre) debris flux. The LDEF satellite deployed by mission STS-41-C Challenger and retrieved by STS-32 Columbia spent 68 months in orbit to gather debris data. The EURECA satellite, deployed by STS-46 Atlantis in 1992 and retrieved by STS-57 Endeavour in 1993, was also used for debris study.

The solar arrays of Hubble were returned by missions STS-61 Endeavour and STS-109 Columbia, and the impact craters studied by the ESA to validate its models. Materials returned from Mir were also studied, notably the Mir Environmental Effects Payload (which also tested materials intended for the ISS).

A debris cloud resulting from a single event is studied with scatter plots known as Gabbard diagrams, where the perigee and apogee of fragments are plotted with respect to their orbital period. Gabbard diagrams of the early debris cloud prior to the effects of perturbations, if the data were available, are reconstructed. They often include data on newly observed, as yet uncatalogued fragments. Gabbard diagrams can provide important insights into the features of the fragmentation, the direction and point of impact.

An average of about one tracked object per day has been dropping out of orbit for the past 50 years, averaging almost three objects per day at solar maximum (due to the heating and expansion of the Earth's atmosphere), but one about every three days at solar minimum, usually five and a half years later. In addition to natural atmospheric effects, corporations, academics and government agencies have proposed plans and technology to deal with space debris, but as of November 2014, most of these are theoretical, and there is no extant business plan for debris reduction.

A number of scholars have also observed that institutional factors'97political, legal, economic, and cultural "rules of the game"'97are the greatest impediment to the cleanup of near-Earth space. There is little commercial incentive to act, since costs are not assigned to polluters, though a number of technological solutions have been suggested. However, effects to date are limited. In the US, governmental bodies have been accused of backsliding on previous commitments to limit debris growth, "let alone tackling the more complex issues of removing orbital debris." The different methods for removal of space debris have been evaluated by the Space Generation Advisory Council, including French astrophysicist Fatoumata K'e9b'e9.

As of the 2010s, several technical approaches to the mitigation of the growth of space debris are typically undertaken, yet no comprehensive legal regime or cost assignment structure is in place to reduce space debris in the way that terrestrial pollution has reduced since the mid-20th century.

To avoid excessive creation of artificial space debris, many'97but not all'97satellites launched to above-low-Earth-orbit are launched initially into elliptical orbits with perigees inside Earth's atmosphere so the orbit will quickly decay and the satellites then will destroy themselves upon reentry into the atmosphere. Other methods are used for spacecraft in higher orbits. These include passivation of the spacecraft at the end of its useful life; as well as the use of upper stages that can reignite to decelerate the stage to intentionally deorbit it, often on the first or second orbit following payload release; satellites that can, if they remain healthy for years, deorbit themselves from the lower orbits around Earth. Other satellites (such as many CubeSats) in low orbits below approximately 400 km (250 mi) orbital altitude depend on the energy-absorbing effects of the upper atmosphere to reliably deorbit a spacecraft within weeks or months.

Increasingly, spent upper stages in higher orbits'97orbits for which low-delta-v deorbit is not possible, or not planned for'97and architectures that support satellite passivation, at end of life are passivated at end of life. This removes any internal energy contained in the vehicle at the end of its mission or useful life. While this does not remove the debris of the now derelict rocket stage or satellite itself, it does substantially reduce the likelihood of the spacecraft destructing and creating many smaller pieces of space debris, a phenomenon that was common in many of the early generations of US and Soviet spacecraft.

Upper stage passivation (e.g. of Delta boosters) by releasing residual propellants reduces debris from orbital explosions; however even as late as 2011, not all upper stages implement this practice. SpaceX used the term "propulsive passivation" for the final maneuver of their six-hour demonstration mission (STP-2) of the Falcon 9 second stage for the US Air Force in 2019, but did not define what all that term encompassed.

With a "one-up, one-down" launch-license policy for Earth orbits, launchers would rendezvous with, capture and de-orbit a derelict satellite from approximately the same orbital plane. Another possibility is the robotic refueling of satellites. Experiments have been flown by NASA, and SpaceX is developing large-scale on-orbit propellant transfer technology.

Another approach to debris mitigation is to explicitly design the mission architecture to always leave the rocket second-stage in an elliptical geocentric orbit with a low-perigee, thus ensuring rapid orbital decay and avoiding long-term orbital debris from spent rocket bodies. Such missions will often complete the payload placement in a final orbit by the use of low-thrust electric propulsion or with the use of a small kick stage to circularize the orbit. The kick stage itself may be designed with the excess-propellant capability to be able to self-deorbit.

Although the ITU requires geostationary satellites to move to a graveyard orbit at the end of their lives, the selected orbital areas do not sufficiently protect GEO lanes from debris. Rocket stages (or satellites) with enough propellant may make a direct, controlled de-orbit, or if this would require too much propellant, a satellite may be brought to an orbit where atmospheric drag would cause it to eventually de-orbit. This was done with the French Spot-1 satellite, reducing its atmospheric re-entry time from a projected 200 years to about 15 by lowering its altitude from 830 km (516 mi) to about 550 km (342 mi).

The Iridium constellation '96 95 communication satellites launched during the five-year period between 1997 and 2002 '96 provides a set of data points on the limits of self-removal. The satellite operator '96 Iridium Communications '96 remained operational over the two-decade life of the satellites (albeit with a company name change through a corporate bankruptcy during the period) and, by December 2019, had "completed disposal of the last of its 65 working legacy satellites." However, this process left 30 satellites with a combined mass of (20,400 kg (45,000 lb), or nearly a third of the mass of this constellation) in LEO orbits at approximately 700 km (430 mi) altitude, where self-decay is quite slow. Of these satellites, 29 simply failed during their time in orbit and were thus unable to self-deorbit, while one '96 Iridium 33 '96 was involved in the 2009 satellite collision with the derelict Russian military satellite Kosmos-2251. No contingency plan was laid for the removal of satellites that were unable to remove themselves. In 2019, the Iridium CEO, Matt Desch, said that Iridium would be willing to pay an active-debris-removal company to deorbit its remaining first-generation satellites if it were possible for an unrealistically low cost, say "US$10,000 per deorbit, but [he] acknowledged that price would likely be far below what a debris-removal company could realistically offer. 'You know at what point [it'92s] a no-brainer, but [I] expect the cost is really in the millions or tens of millions, at which price I know it doesn'92t make sense.'"

Passive methods of increasing the orbital decay rate of spacecraft debris have been proposed. Instead of rockets, an electrodynamic tether could be attached to a spacecraft at launch; at the end of its lifetime, the tether would be rolled out to slow the spacecraft. Other proposals include a booster stage with a sail-like attachment and a large, thin, inflatable balloon envelope.

A variety of approaches have been proposed, studied, or had ground subsystems built to use other spacecraft to remove existing space debris. A consensus of speakers at a meeting in Brussels in October 2012, organized by the Secure World Foundation (a U.S. think tank) and the French International Relations Institute, reported that removal of the largest debris would be required to prevent the risk to spacecraft becoming unacceptable in the foreseeable future (without any addition to the inventory of dead spacecraft in LEO). To date in 2019, removal costs and legal questions about ownership and the authority to remove defunct satellites have stymied national or international action. Current space law retains ownership of all satellites with their original operators, even debris or spacecraft which are defunct or threaten active missions.

Multiple companies made plans in the late 2010s to conduct external removal on their satellites in mid-LEO orbits. For example, OneWeb planned to utilize onboard self-removal as "plan A" for satellite deorbiting at the end of life, but if a satellite were unable to remove itself within one year of end of life, OneWeb would implement "plan B" and dispatch a reusable (multi-transport mission) space tug to attach to the satellite at an already built-in capture target via a grappling fixture, to be towed to a lower orbit and released for re-entry.

A well-studied solution uses a remotely controlled vehicle to rendezvous with, capture, and return debris to a central station. One such system is Space Infrastructure Servicing, a commercially developed refueling depot and service spacecraft for communications satellites in geosynchronous orbit originally scheduled for a 2015 launch. The SIS would be able to "push dead satellites into graveyard orbits." The Advanced Common Evolved Stage family of upper stages is being designed with a high leftover-propellant margin (for derelict capture and de-orbit) and in-space refueling capability for the high delta-v required to de-orbit heavy objects from geosynchronous orbit. A tug-like satellite to drag debris to a safe altitude for it to burn up in the atmosphere has been researched. When debris is identified the satellite creates a difference in potential between the debris and itself, then using its thrusters to move itself and the debris to a safer orbit.

A variation of this approach is for the remotely controlled vehicle to rendezvous with debris, capture it temporarily to attach a smaller de-orbit satellite and drag the debris with a tether to the desired location. The "mothership" would then tow the debris-smallsat combination for atmospheric entry or move it to a graveyard orbit. One such system is the proposed Busek ORbital DEbris Remover (ORDER), which would carry over 40 SUL (satellite on umbilical line) de-orbit satellites and propellant sufficient for their removal.

On 7 January 2010 Star, Inc. reported that it received a contract from the Space and Naval Warfare Systems Command for a feasibility study of the ElectroDynamic Debris Eliminator (EDDE) propellantless spacecraft for space-debris removal. In February 2012 the Swiss Space Center at 'c9cole Polytechnique F'e9d'e9rale de Lausanne announced the Clean Space One project, a nanosatellite demonstration project for matching orbit with a defunct Swiss nanosatellite, capturing it and de-orbiting together. The mission has seen several evolutions to reach a pac-man inspired capture model. In 2013, Space Sweeper with Sling-Sat (4S), a grappling satellite which captures and ejects debris was studied. In 2022, a Chinese satellite, SJ-21, grabbed an unused satellite and "threw" it into an orbit with a lower risk for it to collide.

In December 2019, the European Space Agency awarded the first contract to clean up space debris. The '80120 million mission dubbed ClearSpace-1 (a spinoff from the EPFL project) is slated to launch in 2025. It aims to remove a 100 kg VEga Secondary Payload Adapter (Vespa) left by Vega flight VV02 in an 800 km (500 mi) orbit in 2013. A "chaser" will grab the junk with four robotic arms and drag it down to Earth's atmosphere where both will burn up.

The laser broom uses a ground-based laser to ablate the front of the debris, producing a rocket-like thrust that slows the object. With continued application, the debris would fall enough to be influenced by atmospheric drag. During the late 1990s, the U.S. Air Force's Project Orion was a laser-broom design. Although a test-bed device was scheduled to launch on a Space Shuttle in 2003, international agreements banning powerful laser testing in orbit limited its use to measurements. The 2003 Space Shuttle Columbia disaster postponed the project and according to Nicholas Johnson, chief scientist and program manager for NASA's Orbital Debris Program Office, "There are lots of little gotchas in the Orion final report. There's a reason why it's been sitting on the shelf for more than a decade."

The momentum of the laser-beam photons could directly impart a thrust on the debris sufficient to move small debris into new orbits out of the way of working satellites. NASA research in 2011 indicates that firing a laser beam at a piece of space junk could impart an impulse of 1 mm (0.039 in) per second, and keeping the laser on the debris for a few hours per day could alter its course by 200 m (660 ft) per day. One drawback is the potential for material degradation; the energy may break up the debris, adding to the problem. A similar proposal places the laser on a satellite in Sun-synchronous orbit, using a pulsed beam to push satellites into lower orbits to accelerate their reentry. A proposal to replace the laser with an Ion Beam Shepherd has been made, and other proposals use a foamy ball of aerogel or a spray of water, inflatable balloons, electrodynamic tethers, electroadhesion, and dedicated anti-satellite weapons.

On 28 February 2014, Japan's Japan Aerospace Exploration Agency (JAXA) launched a test "space net" satellite. The launch was an operational test only. In December 2016 the country sent a space junk collector via Kounotori 6 to the ISS by which JAXA scientists experiment to pull junk out of orbit using a tether. The system failed to extend a 700-meter tether from a space station resupply vehicle that was returning to Earth. On 6 February the mission was declared a failure and leading researcher Koichi Inoue told reporters that they "believe the tether did not get released".

Since 2012, the European Space Agency has been working on the design of a mission to remove large space debris from orbit. The mission, e.Deorbit, is scheduled for launch during 2023 with an objective to remove debris heavier than 4,000 kilograms (8,800 lb) from LEO. Several capture techniques are being studied, including a net, a harpoon and a combination robot arm and clamping mechanism.

The RemoveDEBRIS mission plan is to test the efficacy of several ADR technologies on mock targets in low Earth orbit. In order to complete its planned experiments the platform is equipped with a net, a harpoon, a laser ranging instrument, a dragsail, and two CubeSats (miniature research satellites). The mission was launched on 2 April 2018.

There is no international treaty minimizing space debris. However, the United Nations Committee on the Peaceful Uses of Outer Space (COPUOS) published voluntary guidelines in 2007, using a variety of earlier national regulatory attempts at developing standards for debris mitigation. As of 2008, the committee was discussing international "rules of the road" to prevent collisions between satellites. By 2013, a number of national legal regimes existed, typically instantiated in the launch licenses that are required for a launch in all spacefaring nations.

The U.S. issued a set of standard practices for civilian (NASA) and military (DoD and USAF) orbital-debris mitigation in 2001. The standard envisioned disposal for final mission orbits in one of three ways: 1) atmospheric reentry where even with "conservative projections for solar activity, atmospheric drag will limit the lifetime to no longer than 25 years after completion of mission;" 2) maneuver to a "storage orbit:" move the spacecraft to one of four very broad parking orbit ranges (2,000'9619,700 km (1,200'9612,200 mi), 20,700'9635,300 km (12,900'9621,900 mi), above 36,100 km (22,400 mi), or out of Earth orbit completely and into any heliocentric orbit; 3) "Direct retrieval: Retrieve the structure and remove it from orbit as soon as practicable after completion of mission." The standard articulated in option 1, which is the standard applicable to most satellites and derelict upper stages launched, has come to be known as the "25-year rule." The US updated the ODMSP in December 2019, but made no change to the 25-year rule even though "[m]any in the space community believe that the timeframe should be less than 25 years." There is no consensus however on what any new timeframe might be.

In 2002, the European Space Agency (ESA) worked with an international group to promulgate a similar set of standards, also with a "25-year rule" applying to most Earth-orbit satellites and upper stages. Space agencies in Europe began to develop technical guidelines in the mid-1990s, and ASI, UKSA, CNES, DLR and ESA signed a "European Code of Conduct" in 2006, which was a predecessor standard to the ISO international standard work that would begin the following year. In 2008, ESA further developed "its own "Requirements on Space Debris Mitigation for Agency Projects" which "came into force on 1 April 2008."

Germany and France have posted bonds to safeguard the property from debris damage. The "direct retrieval" option (option no. 3 in the US "standard practices" above) has rarely been done by any spacefaring nation (exception, USAF X-37) or commercial actor since the earliest days of spaceflight due to the cost and complexity of achieving direct retrieval, but the ESA has scheduled a 2025 demonstration mission (Clearspace-1) to do this with a single small 100 kg (220 lb) derelict upper stage at a projected cost of '80120 million not including the launch costs.

By 2006, the Indian Space Research Organization (ISRO) had developed a number of technical means of debris mitigation (upper stage passivation, propellant reserves for movement to graveyard orbits, etc.) for ISRO launch vehicles and satellites, and was actively contributing to inter-agency debris coordination and the efforts of the UN COPUOS committee.

In 2007, the ISO began preparing an international standard for space-debris mitigation. By 2010, ISO had published "a comprehensive set of space system engineering standards aimed at mitigating space debris. [with primary requirements] defined in the top-level standard, ISO 24113." By 2017, the standards were nearly complete. However, these standards are not binding on any party by ISO or any international jurisdiction. They are simply available for use in any of a variety of voluntary ways. They "can be adopted voluntarily by a spacecraft manufacturer or operator, or brought into effect through a commercial contract between a customer and supplier, or used as the basis for establishing a set of national regulations on space debris mitigation."

The voluntary ISO standard also adopted the "25-year rule" for the "LEO protected region" below 2,000 km (1,200 mi) altitude that has been previously (and still is, as of 2019) used by the US, ESA, and UN mitigation standards, and identifies it as "an upper limit for the amount of time that a space system shall remain in orbit after its mission is completed. Ideally, the time to deorbit should be as short as possible (i.e., much shorter than 25 years)".

Holger Krag of the European Space Agency states that as of 2017 there is no binding international regulatory framework with no progress occurring at the respective UN body in Vienna.

With the rapid development of the computer and digitalization industries, more countries and companies have engaged in space activities since the turn of the 20th century. The tragedy of the commons is an economic theory referring to a situation where maximizing self-interest through using a shared resource can finally lead to the resource degradation shared by all. Based on the theory, individuals'92 rational action in space will finally lead to an irrational collective result: orbits are crowded with debris. As a common-pool resource, the Earth's orbits, especially LEO and GEO that accommodate most satellites, are nonexcludable and rivalry. To address the tragedy and ensure space sustainability, many technical approaches have been developed. And in terms of governance mechanisms, the top-down centralized one is less suitable to tackle the complex debris problem due to the increasing number of space actors. Instead, much evidence has proved that polycentric form of governance developed by Elinor Ostrom can work in space.

In the process of promoting the polycentric network, there are some existing barriers needed to be dealt with.

As orbital debris is a global problem affecting both spacefaring and non-spacefaring nations, it is necessary to be handled in a worldwide context. Because of the complexity and dynamics of object movements like spacecrafts, debris, meteorites, etc., many countries and regions including the United States, Europe, Russia and China have developed their space situational awareness (SSA) to avoid potential threats in space or plan actions in advance. To a certain extent, SSA plays a role in tracking space debris. In order to build a powerful SSA system, there are two prerequisites: international cooperation and exchange of information and data. However, limitations still exist in spite of the substantially improving data quality over the past decades. Some space powers are not willing to share the information that they have collected, and those, such as the U.S., that have shared the data keep parts of it secret. Instead of joining in a coordinated way, a great deal of SSA programs and national databases run parallel to each other with some overlaps, hindering the formation of a collaborative monitoring system.

Some private actors are also trying to establish SSA systems. For example, the Space Data Association (SDA) formed in 2009 is a non-governmental entity. It currently consists of 21 global satellite operators and 4 executive members: Eutelsat, Inmarsat, Intelsat and SES. SDA is a non-profit platform, aiming to avoid radio interference and space collisions through pooling data from operators independently. Researchers suggest that it is essential to establish an international center for exchanging information on space debris because SSA networks do not completely equal debris tracking systems '97 the former ones focus more on active and threatening objects in space. And in terms of debris populations and defunct satellites, not very much operators have provided data.

In a polycentric governance network, a resource that cannot be holistically monitored is less possible to be well managed. Both insufficient transnational cooperation and information sharing bring resistance to addressing the debris problem. There is still a long way to go before building a global network that covers complete data and has strong interconnection and interoperability.

With the commercialization of satellites and space, the private sector is getting more interested in space activities. For example, SpaceX is planning to create a network of around 12,000 small satellites that can transmit high-speed internet to any place in the world. The proportion of commercial spacecrafts has increased from 4.6% in the 1980s to 55.6% in the 2010s. Despite the high participation rate of commercial entities, UN COPUOS once deliberately excluded them from having a voice in discussions unless being formally invited by a member state. Ostrom said that the involvement of all relevant stakeholders in the rule-design and implementation process is one of the critical elements of successful governance. The exclusion of private actors largely reduces the effectiveness of the committee's role in making collective-choice arrangements that reflect the interests of all space users.

The limited engagement of private actors slows down the process of addressing space debris to some degree. Ties existing between dissimilar stakeholders in the governance network offer access to diverse resources. Different competence among stakeholders can help allocate the tasks more reasonably. In that case, the expertise and experience of private operators are critical to help the world achieve space sustainability. The complementary strengths of different stakeholders enable the governance network to be more adaptable to changes and reach common goals more effectively. In recent years, many private actors have seen commercial opportunities of eliminating space debris. It is estimated that by 2022 the global market for debris monitoring and removal will generate a revenue of around $2.9 billion. For example, Astroscale has contracted with European and Japanese space agencies to develop the capacity of removing orbital debris. Despite that, they are still in small quantity compared to the number of those who have placed satellites in space. Privateer Space, a Hawaiian-based startup company started by American engineer Alex Fielding, space environmentalist Dr. Moriba Jah, and Apple co-founder Steve Wozniak, announced plans in September 2021 to launch hundreds of satellites into orbit in order to study space debris. However, the company stated it is in "stealth mode" and no such satellites have been launched.

Fortunately, the current space exploration is not completely driven by competition, and there still exists a chance for dialogues and cooperation among all stakeholders in both developed and developing countries, to reach an agreement on tackling space debris and assure an equitable and orderly exploration. Besides private actors, network governance does not necessarily exclude the states from playing a role. Instead, the different functions of states might promote the governance process. To improve the polycentric governance network of space debris, researchers suggest: encourage data-sharing among different national and organizational databases at the political level; develop shared standards for data collection systems to improve interoperability; enhance the participation of private actors through involving them in national and international discussions.

The continued practice of disposing of space debris on Earth in areas such as the spacecraft cemetery has raised environmental concerns. Klinger states that '91the environmental geopolitics of Earth and outer space are inextricably linked by the spatial politics of privilege and sacrifice '96 among people, places, and institutions.' Dunnett et al Since 1971, 273 spacecraft and satellites have been directed to Point Nemo; this number includes the Mir Space Station (142 tonnes) and will include the International Space Station in 2024 (240 tonnes). In 2018, it was found that the water had 26 microplastic particles per cubic metre, meaning it is highly polluted. The prevalence of orbital debris has been likened to the terrestrial environmental phenomenon of "sacrifice zones," which are designated geographic regions with high levels of environmental degradation.

Since the 1960s, over three hundred rocket launch sites have been built globally. Between these three hundred launch sites, 17 hosted 90 launches in 2017 alone. Rocket launches affect local and global environments through the construction of necessary infrastructure, exposure of local environments to toxic residue and the dispersal of pollutants. Rockets are the only source of direct anthropogenic emissions into the stratosphere and emit ozone depleting substances such as nitrous oxide, hydrogen chloride and aluminium oxide; these substances can destroy 105 ozone molecules before depleting. Each launch showers an area concentrated within a kilometre with toxins, heavy metals, and acids. This results in localised regional acid rain, plant death, fish kills, and failed seed germination. Furthermore, studies on trace elements concentration in alligators, near NASA launch activities in Florida (USA), showed that over 50% of alligators had '91greater than toxic levels'92 of trace elements in their liver. Similarly, research in Kazakhstan, Russia and China has found that unsymmetrical dimethylhydrazine (UDMH) has carcinogenic, mutagenic, convulsant, teratogenic, embryotoxic and DNA damaging effects on rodents living near the Baikonur cosmodrome, Kazakhstan. It is unknown, however, at what trace concentrations these toxic effects manifest in humans or how it may bioaccumulate up the food chain. A lack of adequate resourcing to maintain safe, non-toxic environments makes these areas sacrifice zones and spaces of waste. The relative remoteness of these spaces makes them attractive launch sites, yet this '91periphery'92 remain central to both their human and non-human inhabitants, who become '91sacrificial'92.

The issue of space debris has been raised as a mitigation challenge for missions around the Moon with the danger of increasing space debris around it.

Until the End of the World (1991) is a French sci-fi drama set under the backdrop of an out-of-control Indian nuclear satellite, predicted to re-enter the atmosphere, threatening vast populated areas of the Earth.

In the Planetes, a Japanese hard science fiction manga (1999'962004) and anime (2003'962004), the story revolves around the crew of a space debris collection craft in the year 2075.

Gravity, a 2013 survival film directed by Alfonso Cuaron, is about a disaster on a space mission caused by Kessler syndrome.

In season 1 of Love, Death & Robots (2019), episode 11, "Helping Hand", revolves around an astronaut being struck by a screw from space debris which knocks her off a satellite in orbit.

Thrust is a reaction force described quantitatively by Newton's third law. When a system expels or accelerates mass in one direction, the accelerated mass will cause a force of equal magnitude but opposite direction to be applied to that system. The force applied on a surface in a direction perpendicular or normal to the surface is also called thrust. Force, and thus thrust, is measured using the International System of Units (SI) in newtons (symbol: N), and represents the amount needed to accelerate 1 kilogram of mass at the rate of 1 meter per second per second. In mechanical engineering, force orthogonal to the main load (such as in parallel helical gears) is referred to as static thrust.

A fixed-wing aircraft propulsion system generates forward thrust when air is pushed in the direction opposite to flight. This can be done by different means such as the spinning blades of a propeller, the propelling jet of a jet engine, or by ejecting hot gases from a rocket engine. Reverse thrust can be generated to aid braking after landing by reversing the pitch of variable-pitch propeller blades, or using a thrust reverser on a jet engine. Rotary wing aircraft use rotors and thrust vectoring V/STOL aircraft use propellers or engine thrust to support the weight of the aircraft and to provide forward propulsion.

A motorboat propeller generates thrust when it rotates and forces water backwards.

A rocket is propelled forward by a thrust equal in magnitude, but opposite in direction, to the time-rate of momentum change of the exhaust gas accelerated from the combustion chamber through the rocket engine nozzle. This is the exhaust velocity with respect to the rocket, times the time-rate at which the mass is expelled, or in mathematical terms:

Where T is the thrust generated (force), is the rate of change of mass with respect to time (mass flow rate of exhaust), and v is the velocity of the exhaust gases measured relative to the rocket.

For vertical launch of a rocket the initial thrust at liftoff must be more than the weight.

Each of the three Space Shuttle Main Engines could produce a thrust of 1.8 meganewton, and each of the Space Shuttle's two Solid Rocket Boosters 14.7 MN (3,300,000 lbf), together 29.4 MN.

By contrast, the simplified Aid For EVA Rescue (SAFER) has 24 thrusters of 3.56 N (0.80 lbf) each.

In the air-breathing category, the AMT-USA AT-180 jet engine developed for radio-controlled aircraft produce 90 N (20 lbf) of thrust. The GE90-115B engine fitted on the Boeing 777-300ER, recognized by the Guinness Book of World Records as the "World's Most Powerful Commercial Jet Engine," has a thrust of 569 kN (127,900 lbf) until it was surpassed by the GE9X, fitted on the upcoming Boeing 777X, at 609 kN (134,300 lbf).

The power needed to generate thrust and the force of the thrust can be related in a non-linear way. In general, . The proportionality constant varies, and can be solved for a uniform flow, where is the incoming air velocity, is the velocity at the actuator disc, and is the final exit velocity:

Solving for the velocity at the disc, , we then have:

When incoming air is accelerated from a standstill '96 for example when hovering '96 then , and we can find:

From here we can see the relationship, finding:

The inverse of the proportionality constant, the "efficiency" of an otherwise-perfect thruster, is proportional to the area of the cross section of the propelled volume of fluid () and the density of the fluid (). This helps to explain why moving through water is easier and why aircraft have much larger propellers than watercraft.

A very common question is how to compare the thrust rating of a jet engine with the power rating of a piston engine. Such comparison is difficult, as these quantities are not equivalent. A piston engine does not move the aircraft by itself (the propeller does that), so piston engines are usually rated by how much power they deliver to the propeller. Except for changes in temperature and air pressure, this quantity depends basically on the throttle setting.

A jet engine has no propeller, so the propulsive power of a jet engine is determined from its thrust as follows. Power is the force (F) it takes to move something over some distance (d) divided by the time (t) it takes to move that distance:

In case of a rocket or a jet aircraft, the force is exactly the thrust (T) produced by the engine. If the rocket or aircraft is moving at about a constant speed, then distance divided by time is just speed, so power is thrust times speed:

This formula looks very surprising, but it is correct: the propulsive power (or power available ) of a jet engine increases with its speed. If the speed is zero, then the propulsive power is zero. If a jet aircraft is at full throttle but attached to a static test stand, then the jet engine produces no propulsive power, however thrust is still produced. The combination piston engine'96propeller also has a propulsive power with exactly the same formula, and it will also be zero at zero speed '96 but that is for the engine'96propeller set. The engine alone will continue to produce its rated power at a constant rate, whether the aircraft is moving or not.

Now, imagine the strong chain is broken, and the jet and the piston aircraft start to move. At low speeds:

The piston engine will have constant 100% power, and the propeller's thrust will vary with speed
The jet engine will have constant 100% thrust, and the engine's power will vary with speed
If a powered aircraft is generating thrust T and experiencing drag D, the difference between the two, T uc0u8722  D, is termed the excess thrust. The instantaneous performance of the aircraft is mostly dependent on the excess thrust.

Excess thrust is a vector and is determined as the vector difference between the thrust vector and the drag vector.

The thrust axis for an airplane is the line of action of the total thrust at any instant. It depends on the location, number, and characteristics of the jet engines or propellers. It usually differs from the drag axis. If so, the distance between the thrust axis and the drag axis will cause a moment that must be resisted by a change in the aerodynamic force on the horizontal stabiliser. Notably, the Boeing 737 MAX, with larger, lower-slung engines than previous 737 models, had a greater distance between the thrust axis and the drag axis, causing the nose to rise up in some flight regimes, necessitating a pitch-control system, MCAS. Early versions of MCAS malfunctioned in flight with catastrophic consequences, leading to the deaths of over 300 people in 2018 and 2019.

A rocket engine uses stored rocket propellants as the reaction mass for forming a high-speed propulsive jet of fluid, usually high-temperature gas. Rocket engines are reaction engines, producing thrust by ejecting mass rearward, in accordance with Newton's third law. Most rocket engines use the combustion of reactive chemicals to supply the necessary energy, but non-combusting forms such as cold gas thrusters and nuclear thermal rockets also exist. Vehicles propelled by rocket engines are commonly called rockets. Rocket vehicles carry their own oxidiser, unlike most combustion engines, so rocket engines can be used in a vacuum to propel spacecraft and ballistic missiles.

Compared to other types of jet engine, rocket engines are the lightest and have the highest thrust, but are the least propellant-efficient (they have the lowest specific impulse). The ideal exhaust is hydrogen, the lightest of all elements, but chemical rockets produce a mix of heavier species, reducing the exhaust velocity.

Rocket engines become more efficient at high speeds, due to the Oberth effect.

Here, "rocket" is used as an abbreviation for "rocket engine".

Thermal rockets use an inert propellant, heated by electricity (electrothermal propulsion) or a nuclear reactor (nuclear thermal rocket).

Chemical rockets are powered by exothermic reduction-oxidation chemical reactions of the propellant:

Solid-fuel rockets (or solid-propellant rockets or motors) are chemical rockets which use propellant in a solid state.
Liquid-propellant rockets use one or more propellants in a liquid state fed from tanks.
Hybrid rockets use a solid propellant in the combustion chamber, to which a second liquid or gas oxidiser or propellant is added to permit combustion.
Monopropellant rockets use a single propellant decomposed by a catalyst. The most common monopropellants are hydrazine and hydrogen peroxide.
Rocket engines produce thrust by the expulsion of an exhaust fluid that has been accelerated to high speed through a propelling nozzle. The fluid is usually a gas created by high pressure (150-to-4,350-pound-per-square-inch (10 to 300 bar)) combustion of solid or liquid propellants, consisting of fuel and oxidiser components, within a combustion chamber. As the gases expand through the nozzle, they are accelerated to very high (supersonic) speed, and the reaction to this pushes the engine in the opposite direction. Combustion is most frequently used for practical rockets, as the laws of thermodynamics (specifically Carnot's theorem) dictate that high temperatures and pressures are desirable for the best thermal efficiency. Nuclear thermal rockets are capable of higher efficiencies, but currently have environmental problems which preclude their routine use in the Earth's atmosphere and cislunar space.

For model rocketry, an available alternative to combustion is the water rocket pressurized by compressed air, carbon dioxide, nitrogen, or any other readily available, inert gas.

Rocket propellant is mass that is stored, usually in some form of tank, or within the combustion chamber itself, prior to being ejected from a rocket engine in the form of a fluid jet to produce thrust.

Chemical rocket propellants are the most commonly used. These undergo exothermic chemical reactions producing a hot gas jet for propulsion. Alternatively, a chemically inert reaction mass can be heated by a high-energy power source through a heat exchanger in lieu of a combustion chamber.

Solid rocket propellants are prepared in a mixture of fuel and oxidising components called grain, and the propellant storage casing effectively becomes the combustion chamber.

Liquid-fuelled rockets force separate fuel and oxidiser components into the combustion chamber, where they mix and burn. Hybrid rocket engines use a combination of solid and liquid or gaseous propellants. Both liquid and hybrid rockets use injectors to introduce the propellant into the chamber. These are often an array of simple jets '96 holes through which the propellant escapes under pressure; but sometimes may be more complex spray nozzles. When two or more propellants are injected, the jets usually deliberately cause the propellants to collide as this breaks up the flow into smaller droplets that burn more easily.

For chemical rockets the combustion chamber is typically cylindrical, and flame holders, used to hold a part of the combustion in a slower-flowing portion of the combustion chamber, are not needed. The dimensions of the cylinder are such that the propellant is able to combust thoroughly; different rocket propellants require different combustion chamber sizes for this to occur.

This leads to a number called , the characteristic length:

where:

is the volume of the chamber
is the area of the throat of the nozzle.
L* is typically in the range of 64'96152 centimetres (25'9660 in).

The temperatures and pressures typically reached in a rocket combustion chamber in order to achieve practical thermal efficiency are extreme compared to a non-afterburning airbreathing jet engine. No atmospheric nitrogen is present to dilute and cool the combustion, so the propellant mixture can reach true stoichiometric ratios. This, in combination with the high pressures, means that the rate of heat conduction through the walls is very high.

In order for fuel and oxidiser to flow into the chamber, the pressure of the propellants entering the combustion chamber must exceed the pressure inside the combustion chamber itself. This may be accomplished by a variety of design approaches including turbopumps or, in simpler engines, via sufficient tank pressure to advance fluid flow. Tank pressure may be maintained by several means, including a high-pressure helium pressurization system common to many large rocket engines or, in some newer rocket systems, by a bleed-off of high-pressure gas from the engine cycle to autogenously pressurize the propellant tanks For example, the self-pressurization gas system of the SpaceX Starship is a critical part of SpaceX strategy to reduce launch vehicle fluids from five in their legacy Falcon 9 vehicle family to just two in Starship, eliminating not only the helium tank pressurant but all hypergolic propellants as well as nitrogen for cold-gas reaction-control thrusters.

The hot gas produced in the combustion chamber is permitted to escape through an opening (the "throat"), and then through a diverging expansion section. When sufficient pressure is provided to the nozzle (about 2.5'963 times ambient pressure), the nozzle chokes and a supersonic jet is formed, dramatically accelerating the gas, converting most of the thermal energy into kinetic energy. Exhaust speeds vary, depending on the expansion ratio the nozzle is designed for, but exhaust speeds as high as ten times the speed of sound in air at sea level are not uncommon. About half of the rocket engine's thrust comes from the unbalanced pressures inside the combustion chamber, and the rest comes from the pressures acting against the inside of the nozzle (see diagram). As the gas expands (adiabatically) the pressure against the nozzle's walls forces the rocket engine in one direction while accelerating the gas in the other.

The most commonly used nozzle is the de Laval nozzle, a fixed geometry nozzle with a high expansion-ratio. The large bell- or cone-shaped nozzle extension beyond the throat gives the rocket engine its characteristic shape.

The exit static pressure of the exhaust jet depends on the chamber pressure and the ratio of exit to throat area of the nozzle. As exit pressure varies from the ambient (atmospheric) pressure, a choked nozzle is said to be

under-expanded (exit pressure greater than ambient),
perfectly expanded (exit pressure equals ambient),
over-expanded (exit pressure less than ambient; shock diamonds form outside the nozzle), or
grossly over-expanded (a shock wave forms inside the nozzle extension).
In practice, perfect expansion is only achievable with a variable-exit area nozzle (since ambient pressure decreases as altitude increases), and is not possible above a certain altitude as ambient pressure approaches zero. If the nozzle is not perfectly expanded, then loss of efficiency occurs. Grossly over-expanded nozzles lose less efficiency, but can cause mechanical problems with the nozzle. Fixed-area nozzles become progressively more under-expanded as they gain altitude. Almost all de Laval nozzles will be momentarily grossly over-expanded during startup in an atmosphere.

Nozzle efficiency is affected by operation in the atmosphere because atmospheric pressure changes with altitude; but due to the supersonic speeds of the gas exiting from a rocket engine, the pressure of the jet may be either below or above ambient, and equilibrium between the two is not reached at all altitudes (see diagram).

For optimal performance, the pressure of the gas at the end of the nozzle should just equal the ambient pressure: if the exhaust's pressure is lower than the ambient pressure, then the vehicle will be slowed by the difference in pressure between the top of the engine and the exit; on the other hand, if the exhaust's pressure is higher, then exhaust pressure that could have been converted into thrust is not converted, and energy is wasted.

To maintain this ideal of equality between the exhaust's exit pressure and the ambient pressure, the diameter of the nozzle would need to increase with altitude, giving the pressure a longer nozzle to act on (and reducing the exit pressure and temperature). This increase is difficult to arrange in a lightweight fashion, although is routinely done with other forms of jet engines. In rocketry a lightweight compromise nozzle is generally used and some reduction in atmospheric performance occurs when used at other than the 'design altitude' or when throttled. To improve on this, various exotic nozzle designs such as the plug nozzle, stepped nozzles, the expanding nozzle and the aerospike have been proposed, each providing some way to adapt to changing ambient air pressure and each allowing the gas to expand further against the nozzle, giving extra thrust at higher altitudes.

When exhausting into a sufficiently low ambient pressure (vacuum) several issues arise. One is the sheer weight of the nozzle'97beyond a certain point, for a particular vehicle, the extra weight of the nozzle outweighs any performance gained. Secondly, as the exhaust gases adiabatically expand within the nozzle they cool, and eventually some of the chemicals can freeze, producing 'snow' within the jet. This causes instabilities in the jet and must be avoided.

On a de Laval nozzle, exhaust gas flow detachment will occur in a grossly over-expanded nozzle. As the detachment point will not be uniform around the axis of the engine, a side force may be imparted to the engine. This side force may change over time and result in control problems with the launch vehicle.

Advanced altitude-compensating designs, such as the aerospike or plug nozzle, attempt to minimize performance losses by adjusting to varying expansion ratio caused by changing altitude.

For a rocket engine to be propellant efficient, it is important that the maximum pressures possible be created on the walls of the chamber and nozzle by a specific amount of propellant; as this is the source of the thrust. This can be achieved by all of:

heating the propellant to as high a temperature as possible (using a high energy fuel, containing hydrogen and carbon and sometimes metals such as aluminium, or even using nuclear energy)
using a low specific density gas (as hydrogen rich as possible)
using propellants which are, or decompose to, simple molecules with few degrees of freedom to maximise translational velocity
Since all of these things minimise the mass of the propellant used, and since pressure is proportional to the mass of propellant present to be accelerated as it pushes on the engine, and since from Newton's third law the pressure that acts on the engine also reciprocally acts on the propellant, it turns out that for any given engine, the speed that the propellant leaves the chamber is unaffected by the chamber pressure (although the thrust is proportional). However, speed is significantly affected by all three of the above factors and the exhaust speed is an excellent measure of the engine propellant efficiency. This is termed exhaust velocity, and after allowance is made for factors that can reduce it, the effective exhaust velocity is one of the most important parameters of a rocket engine (although weight, cost, ease of manufacture etc. are usually also very important).

For aerodynamic reasons the flow goes sonic ("chokes") at the narrowest part of the nozzle, the 'throat'. Since the speed of sound in gases increases with the square root of temperature, the use of hot exhaust gas greatly improves performance. By comparison, at room temperature the speed of sound in air is about 340 m/s while the speed of sound in the hot gas of a rocket engine can be over 1700 m/s; much of this performance is due to the higher temperature, but additionally rocket propellants are chosen to be of low molecular mass, and this also gives a higher velocity compared to air.

Expansion in the rocket nozzle then further multiplies the speed, typically between 1.5 and 2 times, giving a highly collimated hypersonic exhaust jet. The speed increase of a rocket nozzle is mostly determined by its area expansion ratio'97the ratio of the area of the exit to the area of the throat, but detailed properties of the gas are also important. Larger ratio nozzles are more massive but are able to extract more heat from the combustion gases, increasing the exhaust velocity.

Vehicles typically require the overall thrust to change direction over the length of the burn. A number of different ways to achieve this have been flown:

The entire engine is mounted on a hinge or gimbal and any propellant feeds reach the engine via low pressure flexible pipes or rotary couplings.
Just the combustion chamber and nozzle is gimballed, the pumps are fixed, and high pressure feeds attach to the engine.
Multiple engines (often canted at slight angles) are deployed but throttled to give the overall vector that is required, giving only a very small penalty.
High-temperature vanes protrude into the exhaust and can be tilted to deflect the jet.
Rocket technology can combine very high thrust (meganewtons), very high exhaust speeds (around 10 times the speed of sound in air at sea level) and very high thrust/weight ratios (>100) simultaneously as well as being able to operate outside the atmosphere, and while permitting the use of low pressure and hence lightweight tanks and structure.

Rockets can be further optimised to even more extreme performance along one or more of these axes at the expense of the others.

The most important metric for the efficiency of a rocket engine is impulse per unit of propellant, this is called specific impulse (usually written ). This is either measured as a speed (the effective exhaust velocity in metres/second or ft/s) or as a time (seconds). For example, if an engine producing 100 pounds of thrust runs for 320 seconds and burns 100 pounds of propellant, then the specific impulse is 320 seconds. The higher the specific impulse, the less propellant is required to provide the desired impulse.

The specific impulse that can be achieved is primarily a function of the propellant mix (and ultimately would limit the specific impulse), but practical limits on chamber pressures and the nozzle expansion ratios reduce the performance that can be achieved.

Below is an approximate equation for calculating the net thrust of a rocket engine:

where:	 
=  exhaust gas mass flow
=  effective exhaust velocity (sometimes otherwise denoted as c in publications)
=  effective jet velocity when Pamb = Pe
=  flow area at nozzle exit plane (or the plane where the jet leaves the nozzle if separated flow)
=  static pressure at nozzle exit plane
=  ambient (or atmospheric) pressure
Since, unlike a jet engine, a conventional rocket motor lacks an air intake, there is no 'ram drag' to deduct from the gross thrust. Consequently, the net thrust of a rocket motor is equal to the gross thrust (apart from static back pressure).

The term represents the momentum thrust, which remains constant at a given throttle setting, whereas the term represents the pressure thrust term. At full throttle, the net thrust of a rocket motor improves slightly with increasing altitude, because as atmospheric pressure decreases with altitude, the pressure thrust term increases. At the surface of the Earth the pressure thrust may be reduced by up to 30%, depending on the engine design. This reduction drops roughly exponentially to zero with increasing altitude.

Maximum efficiency for a rocket engine is achieved by maximising the momentum contribution of the equation without incurring penalties from over expanding the exhaust. This occurs when . Since ambient pressure changes with altitude, most rocket engines spend very little time operating at peak efficiency.

Since specific impulse is force divided by the rate of mass flow, this equation means that the specific impulse varies with altitude.

Due to the specific impulse varying with pressure, a quantity that is easy to compare and calculate with is useful. Because rockets choke at the throat, and because the supersonic exhaust prevents external pressure influences travelling upstream, it turns out that the pressure at the exit is ideally exactly proportional to the propellant flow , provided the mixture ratios and combustion efficiencies are maintained. It is thus quite usual to rearrange the above equation slightly:

and so define the vacuum Isp to be:

where:

And hence:

Rockets can be throttled by controlling the propellant combustion rate (usually measured in kg/s or lb/s). In liquid and hybrid rockets, the propellant flow entering the chamber is controlled using valves, in solid rockets it is controlled by changing the area of propellant that is burning and this can be designed into the propellant grain (and hence cannot be controlled in real-time).

Rockets can usually be throttled down to an exit pressure of about one-third of ambient pressure (often limited by flow separation in nozzles) and up to a maximum limit determined only by the mechanical strength of the engine.

In practice, the degree to which rockets can be throttled varies greatly, but most rockets can be throttled by a factor of 2 without great difficulty; the typical limitation is combustion stability, as for example, injectors need a minimum pressure to avoid triggering damaging oscillations (chugging or combustion instabilities); but injectors can be optimised and tested for wider ranges. For example, some more recent liquid-propellant engine designs that have been optimised for greater throttling capability (BE-3, Raptor) can be throttled to as low as 18'9620 percent of rated thrust. Solid rockets can be throttled by using shaped grains that will vary their surface area over the course of the burn.

Rocket engine nozzles are surprisingly efficient heat engines for generating a high speed jet, as a consequence of the high combustion temperature and high compression ratio. Rocket nozzles give an excellent approximation to adiabatic expansion which is a reversible process, and hence they give efficiencies which are very close to that of the Carnot cycle. Given the temperatures reached, over 60% efficiency can be achieved with chemical rockets.

For a vehicle employing a rocket engine the energetic efficiency is very good if the vehicle speed approaches or somewhat exceeds the exhaust velocity (relative to launch); but at low speeds the energy efficiency goes to 0% at zero speed (as with all jet propulsion). See Rocket energy efficiency for more details.

Rockets, of all the jet engines, indeed of essentially all engines, have the highest thrust to weight ratio. This is especially true for liquid rocket engines.

This high performance is due to the small volume of pressure vessels that make up the engine'97the pumps, pipes and combustion chambers involved. The lack of inlet duct and the use of dense liquid propellant allows the pressurisation system to be small and lightweight, whereas duct engines have to deal with air which has around three orders of magnitude lower density.

Of the liquid propellants used, density is lowest for liquid hydrogen. Although this propellant has the highest specific impulse, its very low density (about one fourteenth that of water) requires larger and heavier turbopumps and pipework, which decreases the engine's thrust-to-weight ratio (for example the RS-25) compared to those that do not (NK-33).

For efficiency reasons, higher temperatures are desirable, but materials lose their strength if the temperature becomes too high. Rockets run with combustion temperatures that can reach 3,500 K (3,200 'b0C; 5,800 'b0F).

Most other jet engines have gas turbines in the hot exhaust. Due to their larger surface area, they are harder to cool and hence there is a need to run the combustion processes at much lower temperatures, losing efficiency. In addition, duct engines use air as an oxidant, which contains 78% largely unreactive nitrogen, which dilutes the reaction and lowers the temperatures. Rockets have none of these inherent combustion temperature limiters.

The temperatures reached by combustion in rocket engines often substantially exceed the melting points of the nozzle and combustion chamber materials (about 1,200 K for copper). Most construction materials will also combust if exposed to high temperature oxidiser, which leads to a number of design challenges. The nozzle and combustion chamber walls must not be allowed to combust, melt, or vaporize (sometimes facetiously termed an "engine-rich exhaust").

Rockets that use the common construction materials such as aluminium, steel, nickel or copper alloys must employ cooling systems to limit the temperatures that engine structures experience. Regenerative cooling, where the propellant is passed through tubes around the combustion chamber or nozzle, and other techniques, such as curtain cooling or film cooling, are employed to give longer nozzle and chamber life. These techniques ensure that a gaseous thermal boundary layer touching the material is kept below the temperature which would cause the material to catastrophically fail.

Two material exceptions that can directly sustain rocket combustion temperatures are graphite and tungsten, although both are subject to oxidation if not protected. Materials technology, combined with the engine design, is a limiting factor in chemical rockets.

In rockets, the heat fluxes that can pass through the wall are among the highest in engineering; fluxes are generally in the range of 100'96200 MW/m2. The strongest heat fluxes are found at the throat, which often sees twice that found in the associated chamber and nozzle. This is due to the combination of high speeds (which gives a very thin boundary layer), and although lower than the chamber, the high temperatures seen there. (See 'a7 Nozzle above for temperatures in nozzle).

In rockets the coolant methods include

Ablative: the inside walls are lined with a material that traps heat and vaporizes.
Radiative cooling: the nozzle glows and radiates the heat away.
Dump cooling: a cryogenic propellant, usually hydrogen, is passed around the nozzle and dumped.
Regenerative cooling: liquid rockets route the fuel, or occasionally oxidiser around the nozzle before being injected into the combustion chamber or preburner.
Curtain cooling: Propellant injection is arranged so that there is extra fuel around the inside walls, which cools it.
Film cooling: surfaces are wetted with liquid propellant, which cools as it evaporates.
In all cases the cooling effect that prevents the wall from being destroyed is caused by a thin layer of insulating fluid (a boundary layer) that is in contact with the walls that is far cooler than the combustion temperature. Provided this boundary layer is intact the wall will not be damaged.

Disruption of the boundary layer may occur during cooling failures or combustion instabilities, and wall failure typically occurs soon after.

With regenerative cooling a second boundary layer is found in the coolant channels around the chamber. This boundary layer thickness needs to be as small as possible, since the boundary layer acts as an insulator between the wall and the coolant. This may be achieved by making the coolant velocity in the channels as high as possible.

In practice, regenerative cooling is nearly always used in conjunction with curtain cooling and/or film cooling.

Liquid-fuelled engines are often run fuel-rich, which lowers combustion temperatures. This reduces heat loads on the engine and allows lower cost materials and a simplified cooling system. This can also increase performance by lowering the average molecular weight of the exhaust and increasing the efficiency with which combustion heat is converted to kinetic exhaust energy.

Rocket combustion chambers are normally operated at fairly high pressure, typically 10'96200 bar (1'9620 MPa, 150'963,000 psi). When operated within significant atmospheric pressure, higher combustion chamber pressures give better performance by permitting a larger and more efficient nozzle to be fitted without it being grossly overexpanded.

However, these high pressures cause the outermost part of the chamber to be under very large hoop stresses '96 rocket engines are pressure vessels.

Worse, due to the high temperatures created in rocket engines the materials used tend to have a significantly lowered working tensile strength.

In addition, significant temperature gradients are set up in the walls of the chamber and nozzle, these cause differential expansion of the inner liner that create internal stresses.

The extreme vibration and acoustic environment inside a rocket motor commonly result in peak stresses well above mean values, especially in the presence of organ pipe-like resonances and gas turbulence.

The combustion may display undesired instabilities, of sudden or periodic nature. The pressure in the injection chamber may increase until the propellant flow through the injector plate decreases; a moment later the pressure drops and the flow increases, injecting more propellant in the combustion chamber which burns a moment later, and again increases the chamber pressure, repeating the cycle. This may lead to high-amplitude pressure oscillations, often in ultrasonic range, which may damage the motor. Oscillations of 'b1200 psi at 25 kHz were the cause of failures of early versions of the Titan II missile second stage engines. The other failure mode is a deflagration to detonation transition; the supersonic pressure wave formed in the combustion chamber may destroy the engine.

Combustion instability was also a problem during Atlas development. The Rocketdyne engines used in the Atlas family were found to suffer from this effect in several static firing tests, and three missile launches exploded on the pad due to rough combustion in the booster engines. In most cases, it occurred while attempting to start the engines with a "dry start" method whereby the igniter mechanism would be activated prior to propellant injection. During the process of man-rating Atlas for Project Mercury, solving combustion instability was a high priority, and the final two Mercury flights sported an upgraded propulsion system with baffled injectors and a hypergolic igniter.

The problem affecting Atlas vehicles was mainly the so-called "racetrack" phenomenon, where burning propellant would swirl around in a circle at faster and faster speeds, eventually producing vibration strong enough to rupture the engine, leading to complete destruction of the rocket. It was eventually solved by adding several baffles around the injector face to break up swirling propellant.

More significantly, combustion instability was a problem with the Saturn F-1 engines. Some of the early units tested exploded during static firing, which led to the addition of injector baffles.

In the Soviet space program, combustion instability also proved a problem on some rocket engines, including the RD-107 engine used in the R-7 family and the RD-216 used in the R-14 family, and several failures of these vehicles occurred before the problem was solved. Soviet engineering and manufacturing processes never satisfactorily resolved combustion instability in larger RP-1/LOX engines, so the RD-171 engine used to power the Zenit family still used four smaller thrust chambers fed by a common engine mechanism.

The combustion instabilities can be provoked by remains of cleaning solvents in the engine (e.g. the first attempted launch of a Titan II in 1962), reflected shock wave, initial instability after ignition, explosion near the nozzle that reflects into the combustion chamber, and many more factors. In stable engine designs the oscillations are quickly suppressed; in unstable designs they persist for prolonged periods. Oscillation suppressors are commonly used.

Periodic variations of thrust, caused by combustion instability or longitudinal vibrations of structures between the tanks and the engines which modulate the propellant flow, are known as "pogo oscillations" or "pogo", named after the pogo stick.

Three different types of combustion instabilities occur:

This is a low frequency oscillation at a few Hertz in chamber pressure usually caused by pressure variations in feed lines due to variations in acceleration of the vehicle. This can cause cyclic variation in thrust, and the effects can vary from merely annoying to actually damaging the payload or vehicle. Chugging can be minimised by using gas-filled damping tubes on feed lines of high density propellants.

This can be caused due to insufficient pressure drop across the injectors. It generally is mostly annoying, rather than being damaging. However, in extreme cases combustion can end up being forced backwards through the injectors '96 this can cause explosions with monopropellants.

This is the most immediately damaging, and the hardest to control. It is due to acoustics within the combustion chamber that often couples to the chemical combustion processes that are the primary drivers of the energy release, and can lead to unstable resonant "screeching" that commonly leads to catastrophic failure due to thinning of the insulating thermal boundary layer. Acoustic oscillations can be excited by thermal processes, such as the flow of hot air through a pipe or combustion in a chamber. Specifically, standing acoustic waves inside a chamber can be intensified if combustion occurs more intensely in regions where the pressure of the acoustic wave is maximal. Such effects are very difficult to predict analytically during the design process, and have usually been addressed by expensive, time-consuming and extensive testing, combined with trial and error remedial correction measures.

Screeching is often dealt with by detailed changes to injectors, or changes in the propellant chemistry, or vaporising the propellant before injection, or use of Helmholtz dampers within the combustion chambers to change the resonant modes of the chamber.

Testing for the possibility of screeching is sometimes done by exploding small explosive charges outside the combustion chamber with a tube set tangentially to the combustion chamber near the injectors to determine the engine's impulse response and then evaluating the time response of the chamber pressure- a fast recovery indicates a stable system.

For all but the very smallest sizes, rocket exhaust compared to other engines is generally very noisy. As the hypersonic exhaust mixes with the ambient air, shock waves are formed. The Space Shuttle generated over 200 dB(A) of noise around its base. To reduce this, and the risk of payload damage or injury to the crew atop the stack, the mobile launcher platform was fitted with a Sound Suppression System that sprayed 1.1 million litres (290,000 US gal) of water around the base of the rocket in 41 seconds at launch time. Using this system kept sound levels within the payload bay to 142 dB.

The sound intensity from the shock waves generated depends on the size of the rocket and on the exhaust velocity. Such shock waves seem to account for the characteristic crackling and popping sounds produced by large rocket engines when heard live. These noise peaks typically overload microphones and audio electronics, and so are generally weakened or entirely absent in recorded or broadcast audio reproductions. For large rockets at close range, the acoustic effects could actually kill.

More worryingly for space agencies, such sound levels can also damage the launch structure, or worse, be reflected back at the comparatively delicate rocket above. This is why so much water is typically used at launches. The water spray changes the acoustic qualities of the air and reduces or deflects the sound energy away from the rocket.

Generally speaking, noise is most intense when a rocket is close to the ground, since the noise from the engines radiates up away from the jet, as well as reflecting off the ground. Also, when the vehicle is moving slowly, little of the chemical energy input to the engine can go into increasing the kinetic energy of the rocket (since useful power P transmitted to the vehicle is for thrust F and speed V). Then the largest portion of the energy is dissipated in the exhaust's interaction with the ambient air, producing noise. This noise can be reduced somewhat by flame trenches with roofs, by water injection around the jet and by deflecting the jet at an angle.

Rocket engines are usually statically tested at a test facility before being put into production. For high altitude engines, either a shorter nozzle must be used, or the rocket must be tested in a large vacuum chamber.

Rocket vehicles have a reputation for unreliability and danger; especially catastrophic failures. Contrary to this reputation, carefully designed rockets can be made arbitrarily reliable. In military use, rockets are not unreliable. However, one of the main non-military uses of rockets is for orbital launch. In this application, the premium has typically been placed on minimum weight, and it is difficult to achieve high reliability and low weight simultaneously. In addition, if the number of flights launched is low, there is a very high chance of a design, operations or manufacturing error causing destruction of the vehicle.

The Rocketdyne H-1 engine, used in a cluster of eight in the first stage of the Saturn I and Saturn IB launch vehicles, had no catastrophic failures in 152 engine-flights. The Pratt and Whitney RL10 engine, used in a cluster of six in the Saturn I second stage, had no catastrophic failures in 36 engine-flights. The Rocketdyne F-1 engine, used in a cluster of five in the first stage of the Saturn V, had no failures in 65 engine-flights. The Rocketdyne J-2 engine, used in a cluster of five in the Saturn V second stage, and singly in the Saturn IB second stage and Saturn V third stage, had no catastrophic failures in 86 engine-flights.

The Space Shuttle Solid Rocket Booster, used in pairs, caused one notable catastrophic failure in 270 engine-flights.

The RS-25, used in a cluster of three, flew in 46 refurbished engine units. These made a total of 405 engine-flights with no catastrophic in-flight failures. A single in-flight RS-25 engine failure occurred during Space Shuttle Challenger's STS-51-F mission. This failure had no effect on mission objectives or duration.

Rocket propellants require a high energy per unit mass (specific energy), which must be balanced against the tendency of highly energetic propellants to spontaneously explode. Assuming that the chemical potential energy of the propellants can be safely stored, the combustion process results in a great deal of heat being released. A significant fraction of this heat is transferred to kinetic energy in the engine nozzle, propelling the rocket forward in combination with the mass of combustion products released.

Ideally all the reaction energy appears as kinetic energy of the exhaust gases, as exhaust velocity is the single most important performance parameter of an engine. However, real exhaust species are molecules, which typically have translation, vibrational, and rotational modes with which to dissipate energy. Of these, only translation can do useful work to the vehicle, and while energy does transfer between modes this process occurs on a timescale far in excess of the time required for the exhaust to leave the nozzle.

The more chemical bonds an exhaust molecule has, the more rotational and vibrational modes it will have. Consequently, it is generally desirable for the exhaust species to be as simple as possible, with a diatomic molecule composed of light, abundant atoms such as H2 being ideal in practical terms. However, in the case of a chemical rocket, hydrogen is a reactant and reducing agent, not a product. An oxidizing agent, most typically oxygen or an oxygen-rich species, must be introduced into the combustion process, adding mass and chemical bonds to the exhaust species.

An additional advantage of light molecules is that they may be accelerated to high velocity at temperatures that can be contained by currently available materials - the high gas temperatures in rocket engines pose serious problems for the engineering of survivable motors.

Liquid hydrogen (LH2) and oxygen (LOX, or LO2), are the most effective propellants in terms of exhaust velocity that have been widely used to date, though a few exotic combinations involving boron or liquid ozone are potentially somewhat better in theory if various practical problems could be solved.

It is important to note that, when computing the specific reaction energy of a given propellant combination, the entire mass of the propellants (both fuel and oxidiser) must be included. The exception is in the case of air-breathing engines, which use atmospheric oxygen and consequently have to carry less mass for a given energy output. Fuels for car or turbojet engines have a much better effective energy output per unit mass of propellant that must be carried, but are similar per unit mass of fuel.

Computer programs that predict the performance of propellants in rocket engines are available.

With liquid and hybrid rockets, immediate ignition of the propellants as they first enter the combustion chamber is essential.

With liquid propellants (but not gaseous), failure to ignite within milliseconds usually causes too much liquid propellant to be inside the chamber, and if/when ignition occurs the amount of hot gas created can exceed the maximum design pressure of the chamber, causing a catastrophic failure of the pressure vessel. This is sometimes called a hard start or a rapid unscheduled disassembly (RUD).

Ignition can be achieved by a number of different methods; a pyrotechnic charge can be used, a plasma torch can be used, or electric spark ignition may be employed. Some fuel/oxidiser combinations ignite on contact (hypergolic), and non-hypergolic fuels can be "chemically ignited" by priming the fuel lines with hypergolic propellants (popular in Russian engines).

Gaseous propellants generally will not cause hard starts, with rockets the total injector area is less than the throat thus the chamber pressure tends to ambient prior to ignition and high pressures cannot form even if the entire chamber is full of flammable gas at ignition.

Solid propellants are usually ignited with one-shot pyrotechnic devices and combustion usually proceeds through total consumption of the propellants.

Once ignited, rocket chambers are self-sustaining and igniters are not needed and combustion usually proceeds through total consumption of the propellants. Indeed, chambers often spontaneously reignite if they are restarted after being shut down for a few seconds. Unless designed for re-ignition, when cooled, many rockets cannot be restarted without at least minor maintenance, such as replacement of the pyrotechnic igniter or even refueling of the propellants.

Rocket jets vary depending on the rocket engine, design altitude, altitude, thrust and other factors.

Carbon-rich exhausts from kerosene-based fuels such as RP-1 are often orange in colour due to the black-body radiation of the unburnt particles, in addition to the blue Swan bands. Peroxide oxidiser-based rockets and hydrogen rocket jets contain largely steam and are nearly invisible to the naked eye but shine brightly in the ultraviolet and infrared ranges. Jets from solid-propellant rockets can be highly visible, as the propellant frequently contains metals such as elemental aluminium which burns with an orange-white flame and adds energy to the combustion process. Rocket engines which burn liquid hydrogen and oxygen will exhibit a nearly transparent exhaust, due to it being mostly superheated steam (water vapour), plus some unburned hydrogen.

The nozzle is usually over-expanded at sea level, and the exhaust can exhibit visible shock diamonds through a schlieren effect caused by the incandescence of the exhaust gas.

The shape of the jet varies for a fixed-area nozzle as the expansion ratio varies with altitude: at high altitude all rockets are grossly under-expanded, and a quite small percentage of exhaust gases actually end up expanding forwards.

The solar thermal rocket would make use of solar power to directly heat reaction mass, and therefore does not require an electrical generator as most other forms of solar-powered propulsion do. A solar thermal rocket only has to carry the means of capturing solar energy, such as concentrators and mirrors. The heated propellant is fed through a conventional rocket nozzle to produce thrust. The engine thrust is directly related to the surface area of the solar collector and to the local intensity of the solar radiation and inversely proportional to the Isp.

Nuclear propulsion includes a wide variety of propulsion methods that use some form of nuclear reaction as their primary power source. Various types of nuclear propulsion have been proposed, and some of them tested, for spacecraft applications:

According to the writings of the Roman Aulus Gellius, the earliest known example of jet propulsion was in c. 400 BC, when a Greek Pythagorean named Archytas, propelled a wooden bird along wires using steam. However, it was not powerful enough to take off under its own thrust.

The aeolipile described in the first century BC, often known as Hero's engine, consisted of a pair of steam rocket nozzles mounted on a bearing. It was created almost two millennia before the Industrial Revolution but the principles behind it were not well understood, and it was not developed into a practical power source.

The availability of black powder to propel projectiles was a precursor to the development of the first solid rocket. Ninth Century Chinese Taoist alchemists discovered black powder in a search for the elixir of life; this accidental discovery led to fire arrows which were the first rocket engines to leave the ground.

It is stated that "the reactive forces of incendiaries were probably not applied to the propulsion of projectiles prior to the 13th century". A turning point in rocket technology emerged with a short manuscript entitled Liber Ignium ad Comburendos Hostes (abbreviated as The Book of Fires). The manuscript is composed of recipes for creating incendiary weapons from the mid-eighth to the end of the thirteenth centuries'97two of which are rockets. The first recipe calls for one part of colophonium and sulfur added to six parts of saltpeter (potassium nitrate) dissolved in laurel oil, then inserted into hollow wood and lit to "fly away suddenly to whatever place you wish and burn up everything". The second recipe combines one pound of sulfur, two pounds of charcoal, and six pounds of saltpeter'97all finely powdered on a marble slab. This powder mixture is packed firmly into a long and narrow case. The introduction of saltpeter into pyrotechnic mixtures connected the shift from hurled Greek fire into self-propelled rocketry. .

Articles and books on the subject of rocketry appeared increasingly from the fifteenth through seventeenth centuries. In the sixteenth century, German military engineer Conrad Haas (1509'961576) wrote a manuscript which introduced the construction of multi-staged rockets.

Rocket engines were also put in use by Tippu Sultan, the king of Mysore. These usually consisted of a tube of soft hammered iron about 8 in (20 cm) long and 1+1uc0u8260 2'963 in (3.8'967.6 cm) diameter, closed at one end, packed with black powder propellant and strapped to a shaft of bamboo about 4 ft (120 cm) long. A rocket carrying about one pound of powder could travel almost 1,000 yards (910 m). These 'rockets', fitted with swords, would travel several meters in the air before coming down with sword edges facing the enemy. These were used very effectively against the British empire.

Slow development of this technology continued up to the later 19th century, when Russian Konstantin Tsiolkovsky first wrote about liquid-fueled rocket engines. He was the first to develop the Tsiolkovsky rocket equation, though it was not published widely for some years.

The modern solid- and liquid-fueled engines became realities early in the 20th century, thanks to the American physicist Robert Goddard. Goddard was the first to use a De Laval nozzle on a solid-propellant (gunpowder) rocket engine, doubling the thrust and increasing the efficiency by a factor of about twenty-five. This was the birth of the modern rocket engine. He calculated from his independently derived rocket equation that a reasonably sized rocket, using solid fuel, could place a one-pound payload on the Moon.

Fritz von Opel was instrumental in popularizing rockets as means of propulsion. In the 1920s, he initiated together with Max Valier, co-founder of the "Verein f'fcr Raumschiffahrt", the world's first rocket program, Opel-RAK, leading to speed records for automobiles, rail vehicles and the first manned rocket-powered flight in September of 1929. Months earlier in 1928, one of his rocket-powered prototypes, the Opel RAK2, reached piloted by von Opel himself at the AVUS speedway in Berlin a record speed of 238 km/h, watched by 3000 spectators and world media. A world record for rail vehicles was reached with RAK3 and a top speed of 256 km/h. After these successes, von Opel piloted the world's first public rocket-powered flight using Opel RAK.1, a rocket plane designed by Julius Hatry. World media reported on these efforts, including UNIVERSAL Newsreel of the US, causing as "Raketen-Rummel" or "Rocket Rumble" immense global public excitement, and in particular in Germany, where inter alia Wernher von Braun was highly influenced. The Great Depression led to an end of the Opel-RAK program, but Max Valier continued the efforts. After switching from solid-fuel to liquid-fuel rockets, he died while testing and is considered the first fatality of the dawning space age.

Goddard began to use liquid propellants in 1921, and in 1926 became the first to launch a liquid-propellant rocket. Goddard pioneered the use of the De Laval nozzle, lightweight propellant tanks, small light turbopumps, thrust vectoring, the smoothly-throttled liquid fuel engine, regenerative cooling, and curtain cooling.

During the late 1930s, German scientists, such as Wernher von Braun and Hellmuth Walter, investigated installing liquid-fueled rockets in military aircraft (Heinkel He 112, He 111, He 176 and Messerschmitt Me 163).

The turbopump was employed by German scientists in World War II. Until then cooling the nozzle had been problematic, and the A4 ballistic missile used dilute alcohol for the fuel, which reduced the combustion temperature sufficiently.

Staged combustion (uc0u1047 u1072 u1084 u1082 u1085 u1091 u1090 u1072 u1103  u1089 u1093 u1077 u1084 u1072 ) was first proposed by Alexey Isaev in 1949. The first staged combustion engine was the S1.5400 used in the Soviet planetary rocket, designed by Melnikov, a former assistant to Isaev. About the same time (1959), Nikolai Kuznetsov began work on the closed cycle engine NK-9 for Korolev's orbital ICBM, GR-1. Kuznetsov later evolved that design into the NK-15 and NK-33 engines for the unsuccessful Lunar N1 rocket.

In the West, the first laboratory staged-combustion test engine was built in Germany in 1963, by Ludwig Boelkow.

Hydrogen peroxide / kerosene fueled engines such as the British Gamma of the 1950s used a closed-cycle process by catalytically decomposing the peroxide to drive turbines before combustion with the kerosene in the combustion chamber proper. This gave the efficiency advantages of staged combustion, without the major engineering problems.

Liquid hydrogen engines were first successfully developed in America: the RL-10 engine first flew in 1962. Its successor, the Rocketdyne J-2, was used in the Apollo program's Saturn V rocket to send humans to the Moon. The high specific impulse and low density of liquid hydrogen lowered the upper stage mass and the overall size and cost of the vehicle.

The record for most engines on one rocket flight is 44, set by NASA in 2016 on a Black Brant.

A vacuum is a space devoid of matter. The word is derived from the Latin adjective vacuus for "vacant" or "void". An approximation to such vacuum is a region with a gaseous pressure much less than atmospheric pressure. Physicists often discuss ideal test results that would occur in a perfect vacuum, which they sometimes simply call "vacuum" or free space, and use the term partial vacuum to refer to an actual imperfect vacuum as one might have in a laboratory or in space. In engineering and applied physics on the other hand, vacuum refers to any space in which the pressure is considerably lower than atmospheric pressure. The Latin term in vacuo is used to describe an object that is surrounded by a vacuum.

The quality of a partial vacuum refers to how closely it approaches a perfect vacuum. Other things equal, lower gas pressure means higher-quality vacuum. For example, a typical vacuum cleaner produces enough suction to reduce air pressure by around 20%. But higher-quality vacuums are possible. Ultra-high vacuum chambers, common in chemistry, physics, and engineering, operate below one trillionth (10uc0u8722 12) of atmospheric pressure (100 nPa), and can reach around 100 particles/cm3. Outer space is an even higher-quality vacuum, with the equivalent of just a few hydrogen atoms per cubic meter on average in intergalactic space.

Vacuum has been a frequent topic of philosophical debate since ancient Greek times, but was not studied empirically until the 17th century. Evangelista Torricelli produced the first laboratory vacuum in 1643, and other experimental techniques were developed as a result of his theories of atmospheric pressure. A Torricellian vacuum is created by filling a tall glass container closed at one end with mercury, and then inverting it in a bowl to contain the mercury (see below).

Vacuum became a valuable industrial tool in the 20th century with the introduction of incandescent light bulbs and vacuum tubes, and a wide array of vacuum technologies has since become available. The development of human spaceflight has raised interest in the impact of vacuum on human health, and on life forms in general.

The word vacuum comes from Latin 'an empty space, void', noun use of neuter of vacuus, meaning "empty", related to vacare, meaning "to be empty".

Vacuum is one of the few words in the English language that contains two consecutive letters u.

Historically, there has been much dispute over whether such a thing as a vacuum can exist. Ancient Greek philosophers debated the existence of a vacuum, or void, in the context of atomism, which posited void and atom as the fundamental explanatory elements of physics. Following Plato, even the abstract concept of a featureless void faced considerable skepticism: it could not be apprehended by the senses, it could not, itself, provide additional explanatory power beyond the physical volume with which it was commensurate and, by definition, it was quite literally nothing at all, which cannot rightly be said to exist. Aristotle believed that no void could occur naturally, because the denser surrounding material continuum would immediately fill any incipient rarity that might give rise to a void.

In his Physics, book IV, Aristotle offered numerous arguments against the void: for example, that motion through a medium which offered no impediment could continue ad infinitum, there being no reason that something would come to rest anywhere in particular. Lucretius argued for the existence of vacuum in the first century BC and Hero of Alexandria tried unsuccessfully to create an artificial vacuum in the first century AD.

In the medieval Muslim world, the physicist and Islamic scholar Al-Farabi wrote a treatise rejecting the existence of the vacuum in the 10th century. He concluded that air's volume can expand to fill available space, and therefore the concept of a perfect vacuum was incoherent. According to Nader El-Bizri, the physicist Ibn al-Haytham and the Mu'tazili theologians disagreed with Aristotle and Al-Farabi, and they supported the existence of a void. Using geometry, Ibn al-Haytham mathematically demonstrated that place (al-makan) is the imagined three-dimensional void between the inner surfaces of a containing body. According to Ahmad Dallal, Abuc0u363  Rayhu257 n al-Bu299 ru363 nu299  also states that "there is no observable evidence that rules out the possibility of vacuum". The suction pump was described by Arab engineer Al-Jazari in the 13th century, and later appeared in Europe from the 15th century.

European scholars such as Roger Bacon, Blasius of Parma and Walter Burley in the 13th and 14th century focused considerable attention on issues concerning the concept of a vacuum. Eventually following Stoic physics in this instance, scholars from the 14th century onward increasingly departed from the Aristotelian perspective in favor of a supernatural void beyond the confines of the cosmos itself, a conclusion widely acknowledged by the 17th century, which helped to segregate natural and theological concerns.

Almost two thousand years after Plato, Ren'e9 Descartes also proposed a geometrically based alternative theory of atomism, without the problematic nothing'96everything dichotomy of void and atom. Although Descartes agreed with the contemporary position, that a vacuum does not occur in nature, the success of his namesake coordinate system and more implicitly, the spatial'96corporeal component of his metaphysics would come to define the philosophically modern notion of empty space as a quantified extension of volume. By the ancient definition however, directional information and magnitude were conceptually distinct.

Medieval thought experiments into the idea of a vacuum considered whether a vacuum was present, if only for an instant, between two flat plates when they were rapidly separated. There was much discussion of whether the air moved in quickly enough as the plates were separated, or, as Walter Burley postulated, whether a 'celestial agent' prevented the vacuum arising. The commonly held view that nature abhorred a vacuum was called horror vacui. There was even speculation that even God could not create a vacuum if he wanted and the 1277 Paris condemnations of Bishop Etienne Tempier, which required there to be no restrictions on the powers of God, led to the conclusion that God could create a vacuum if he so wished. Jean Buridan reported in the 14th century that teams of ten horses could not pull open bellows when the port was sealed.

The 17th century saw the first attempts to quantify measurements of partial vacuum. Evangelista Torricelli's mercury barometer of 1643 and Blaise Pascal's experiments both demonstrated a partial vacuum.

In 1654, Otto von Guericke invented the first vacuum pump and conducted his famous Magdeburg hemispheres experiment, showing that, owing to atmospheric pressure outside the hemispheres, teams of horses could not separate two hemispheres from which the air had been partially evacuated. Robert Boyle improved Guericke's design and with the help of Robert Hooke further developed vacuum pump technology. Thereafter, research into the partial vacuum lapsed until 1850 when August Toepler invented the Toepler Pump and in 1855 when Heinrich Geissler invented the mercury displacement pump, achieving a partial vacuum of about 10 Pa (0.1 Torr). A number of electrical properties become observable at this vacuum level, which renewed interest in further research.

While outer space provides the most rarefied example of a naturally occurring partial vacuum, the heavens were originally thought to be seamlessly filled by a rigid indestructible material called aether. Borrowing somewhat from the pneuma of Stoic physics, aether came to be regarded as the rarefied air from which it took its name, (see Aether (mythology)). Early theories of light posited a ubiquitous terrestrial and celestial medium through which light propagated. Additionally, the concept informed Isaac Newton's explanations of both refraction and of radiant heat. 19th century experiments into this luminiferous aether attempted to detect a minute drag on the Earth's orbit. While the Earth does, in fact, move through a relatively dense medium in comparison to that of interstellar space, the drag is so minuscule that it could not be detected. In 1912, astronomer Henry Pickering commented: "While the interstellar absorbing medium may be simply the ether, [it] is characteristic of a gas, and free gaseous molecules are certainly there".

Later, in 1930, Paul Dirac proposed a model of the vacuum as an infinite sea of particles possessing negative energy, called the Dirac sea. This theory helped refine the predictions of his earlier formulated Dirac equation, and successfully predicted the existence of the positron, confirmed two years later. Werner Heisenberg's uncertainty principle, formulated in 1927, predicted a fundamental limit within which instantaneous position and momentum, or energy and time can be measured. This has far reaching consequences on the "emptiness" of space between particles. In the late 20th century, so-called virtual particles that arise spontaneously from empty space were confirmed.

The strictest criterion to define a vacuum is a region of space and time where all the components of the stress'96energy tensor are zero. This means that this region is devoid of energy and momentum, and by consequence, it must be empty of particles and other physical fields (such as electromagnetism) that contain energy and momentum.

In general relativity, a vanishing stress'96energy tensor implies, through Einstein field equations, the vanishing of all the components of the Ricci tensor. Vacuum does not mean that the curvature of space-time is necessarily flat: the gravitational field can still produce curvature in a vacuum in the form of tidal forces and gravitational waves (technically, these phenomena are the components of the Weyl tensor). The black hole (with zero electric charge) is an elegant example of a region completely "filled" with vacuum, but still showing a strong curvature.

In classical electromagnetism, the vacuum of free space, or sometimes just free space or perfect vacuum, is a standard reference medium for electromagnetic effects. Some authors refer to this reference medium as classical vacuum, a terminology intended to separate this concept from QED vacuum or QCD vacuum, where vacuum fluctuations can produce transient virtual particle densities and a relative permittivity and relative permeability that are not identically unity.

In the theory of classical electromagnetism, free space has the following properties:

Electromagnetic radiation travels, when unobstructed, at the speed of light, the defined value 299,792,458 m/s in SI units.
The superposition principle is always exactly true. For example, the electric potential generated by two charges is the simple addition of the potentials generated by each charge in isolation. The value of the electric field at any point around these two charges is found by calculating the vector sum of the two electric fields from each of the charges acting alone.
The permittivity and permeability are exactly the electric constant uc0u949 0 and magnetic constant u956 0, respectively (in SI units), or exactly 1 (in Gaussian units).
The characteristic impedance (uc0u951 ) equals the impedance of free space Z0 u8776  376.73 u937 .
The vacuum of classical electromagnetism can be viewed as an idealized electromagnetic medium with the constitutive relations in SI units:

relating the electric displacement field D to the electric field E and the magnetic field or H-field H to the magnetic induction or B-field B. Here r is a spatial location and t is time.

In quantum mechanics and quantum field theory, the vacuum is defined as the state (that is, the solution to the equations of the theory) with the lowest possible energy (the ground state of the Hilbert space). In quantum electrodynamics this vacuum is referred to as 'QED vacuum' to distinguish it from the vacuum of quantum chromodynamics, denoted as QCD vacuum. QED vacuum is a state with no matter particles (hence the name), and no photons. As described above, this state is impossible to achieve experimentally. (Even if every matter particle could somehow be removed from a volume, it would be impossible to eliminate all the blackbody photons.) Nonetheless, it provides a good model for realizable vacuum, and agrees with a number of experimental observations as described next.

QED vacuum has interesting and complex properties. In QED vacuum, the electric and magnetic fields have zero average values, but their variances are not zero. As a result, QED vacuum contains vacuum fluctuations (virtual particles that hop into and out of existence), and a finite energy called vacuum energy. Vacuum fluctuations are an essential and ubiquitous part of quantum field theory. Some experimentally verified effects of vacuum fluctuations include spontaneous emission and the Lamb shift. Coulomb's law and the electric potential in vacuum near an electric charge are modified.

Theoretically, in QCD multiple vacuum states can coexist. The starting and ending of cosmological inflation is thought to have arisen from transitions between different vacuum states. For theories obtained by quantization of a classical theory, each stationary point of the energy in the configuration space gives rise to a single vacuum. String theory is believed to have a huge number of vacua '96 the so-called string theory landscape.

Outer space has very low density and pressure, and is the closest physical approximation of a perfect vacuum. But no vacuum is truly perfect, not even in interstellar space, where there are still a few hydrogen atoms per cubic meter.

Stars, planets, and moons keep their atmospheres by gravitational attraction, and as such, atmospheres have no clearly delineated boundary: the density of atmospheric gas simply decreases with distance from the object. The Earth's atmospheric pressure drops to about 32 millipascals (4.6'd710uc0u8722 6 psi) at 100 kilometres (62 mi) of altitude, the K'e1rm'e1n line, which is a common definition of the boundary with outer space. Beyond this line, isotropic gas pressure rapidly becomes insignificant when compared to radiation pressure from the Sun and the dynamic pressure of the solar winds, so the definition of pressure becomes difficult to interpret. The thermosphere in this range has large gradients of pressure, temperature and composition, and varies greatly due to space weather. Astrophysicists prefer to use number density to describe these environments, in units of particles per cubic centimetre.

But although it meets the definition of outer space, the atmospheric density within the first few hundred kilometers above the K'e1rm'e1n line is still sufficient to produce significant drag on satellites. Most artificial satellites operate in this region called low Earth orbit and must fire their engines every couple of weeks or a few times a year (depending on solar activity). The drag here is low enough that it could theoretically be overcome by radiation pressure on solar sails, a proposed propulsion system for interplanetary travel. Planets are too massive for their trajectories to be significantly affected by these forces, although their atmospheres are eroded by the solar winds.

All of the observable universe is filled with large numbers of photons, the so-called cosmic background radiation, and quite likely a correspondingly large number of neutrinos. The current temperature of this radiation is about 3 K (uc0u8722 270.15 'b0C; u8722 454.27 'b0F).

The quality of a vacuum is indicated by the amount of matter remaining in the system, so that a high quality vacuum is one with very little matter left in it. Vacuum is primarily measured by its absolute pressure, but a complete characterization requires further parameters, such as temperature and chemical composition. One of the most important parameters is the mean free path (MFP) of residual gases, which indicates the average distance that molecules will travel between collisions with each other. As the gas density decreases, the MFP increases, and when the MFP is longer than the chamber, pump, spacecraft, or other objects present, the continuum assumptions of fluid mechanics do not apply. This vacuum state is called high vacuum, and the study of fluid flows in this regime is called particle gas dynamics. The MFP of air at atmospheric pressure is very short, 70 nm, but at 100 mPa (~1'd710uc0u8722 3 Torr) the MFP of room temperature air is roughly 100 mm, which is on the order of everyday objects such as vacuum tubes. The Crookes radiometer turns when the MFP is larger than the size of the vanes.

Vacuum quality is subdivided into ranges according to the technology required to achieve it or measure it. These ranges were defined in ISO 3529-1:2019 as shown in the following table (100 Pa corresponds to 0.75 Torr; Torr is non-SI unit):

Atmospheric pressure is variable but standardized at 101.325 kPa (760 Torr).
Deep space is generally much more empty than any artificial vacuum. It may or may not meet the definition of high vacuum above, depending on what region of space and astronomical bodies are being considered. For example, the MFP of interplanetary space is smaller than the size of the Solar System, but larger than small planets and moons. As a result, solar winds exhibit continuum flow on the scale of the Solar System, but must be considered a bombardment of particles with respect to the Earth and Moon.
Perfect vacuum is an ideal state of no particles at all. It cannot be achieved in a laboratory, although there may be small volumes which, for a brief moment, happen to have no particles of matter in them. Even if all particles of matter were removed, there would still be photons and gravitons, as well as dark energy, virtual particles, and other aspects of the quantum vacuum.
Vacuum is measured in units of pressure, typically as a subtraction relative to ambient atmospheric pressure on Earth. But the amount of relative measurable vacuum varies with local conditions. On the surface of Venus, where ground-level atmospheric pressure is much higher than on Earth, much higher relative vacuum readings would be possible. On the surface of the moon with almost no atmosphere, it would be extremely difficult to create a measurable vacuum relative to the local environment.

Similarly, much higher than normal relative vacuum readings are possible deep in the Earth's ocean. A submarine maintaining an internal pressure of 1 atmosphere submerged to a depth of 10 atmospheres (98 metres; a 9.8-metre column of seawater has the equivalent weight of 1 atm) is effectively a vacuum chamber keeping out the crushing exterior water pressures, though the 1 atm inside the submarine would not normally be considered a vacuum.

Therefore, to properly understand the following discussions of vacuum measurement, it is important that the reader assumes the relative measurements are being done on Earth at sea level, at exactly 1 atmosphere of ambient atmospheric pressure.

The SI unit of pressure is the pascal (symbol Pa), but vacuum is often measured in torrs, named for an Italian physicist Torricelli (1608'961647). A torr is equal to the displacement of a millimeter of mercury (mmHg) in a manometer with 1 torr equaling 133.3223684 pascals above absolute zero pressure. Vacuum is often also measured on the barometric scale or as a percentage of atmospheric pressure in bars or atmospheres. Low vacuum is often measured in millimeters of mercury (mmHg) or pascals (Pa) below standard atmospheric pressure. "Below atmospheric" means that the absolute pressure is equal to the current atmospheric pressure.

In other words, most low vacuum gauges that read, for example 50.79 Torr. Many inexpensive low vacuum gauges have a margin of error and may report a vacuum of 0 Torr but in practice this generally requires a two-stage rotary vane or other medium type of vacuum pump to go much beyond (lower than) 1 torr.

Many devices are used to measure the pressure in a vacuum, depending on what range of vacuum is needed.

Hydrostatic gauges (such as the mercury column manometer) consist of a vertical column of liquid in a tube whose ends are exposed to different pressures. The column will rise or fall until its weight is in equilibrium with the pressure differential between the two ends of the tube. The simplest design is a closed-end U-shaped tube, one side of which is connected to the region of interest. Any fluid can be used, but mercury is preferred for its high density and low vapour pressure. Simple hydrostatic gauges can measure pressures ranging from 1 torr (100 Pa) to above atmospheric. An important variation is the McLeod gauge which isolates a known volume of vacuum and compresses it to multiply the height variation of the liquid column. The McLeod gauge can measure vacuums as high as 10uc0u8722 6 torr (0.1 mPa), which is the lowest direct measurement of pressure that is possible with current technology. Other vacuum gauges can measure lower pressures, but only indirectly by measurement of other pressure-controlled properties. These indirect measurements must be calibrated via a direct measurement, most commonly a McLeod gauge.

The kenotometer is a particular type of hydrostatic gauge, typically used in power plants using steam turbines. The kenotometer measures the vacuum in the steam space of the condenser, that is, the exhaust of the last stage of the turbine.

Mechanical or elastic gauges depend on a Bourdon tube, diaphragm, or capsule, usually made of metal, which will change shape in response to the pressure of the region in question. A variation on this idea is the capacitance manometer, in which the diaphragm makes up a part of a capacitor. A change in pressure leads to the flexure of the diaphragm, which results in a change in capacitance. These gauges are effective from 103 torr to 10uc0u8722 4 torr, and beyond.

Thermal conductivity gauges rely on the fact that the ability of a gas to conduct heat decreases with pressure. In this type of gauge, a wire filament is heated by running current through it. A thermocouple or Resistance Temperature Detector (RTD) can then be used to measure the temperature of the filament. This temperature is dependent on the rate at which the filament loses heat to the surrounding gas, and therefore on the thermal conductivity. A common variant is the Pirani gauge which uses a single platinum filament as both the heated element and RTD. These gauges are accurate from 10 torr to 10uc0u8722 3 torr, but they are sensitive to the chemical composition of the gases being measured.

Ionization gauges are used in ultrahigh vacuum. They come in two types: hot cathode and cold cathode. In the hot cathode version an electrically heated filament produces an electron beam. The electrons travel through the gauge and ionize gas molecules around them. The resulting ions are collected at a negative electrode. The current depends on the number of ions, which depends on the pressure in the gauge. Hot cathode gauges are accurate from 10uc0u8722 3 torr to 10u8722 10 torr. The principle behind cold cathode version is the same, except that electrons are produced in a discharge created by a high voltage electrical discharge. Cold cathode gauges are accurate from 10u8722 2 torr to 10u8722 9 torr. Ionization gauge calibration is very sensitive to construction geometry, chemical composition of gases being measured, corrosion and surface deposits. Their calibration can be invalidated by activation at atmospheric pressure or low vacuum. The composition of gases at high vacuums will usually be unpredictable, so a mass spectrometer must be used in conjunction with the ionization gauge for accurate measurement.

Vacuum is useful in a variety of processes and devices. Its first widespread use was in the incandescent light bulb to protect the filament from chemical degradation. The chemical inertness produced by a vacuum is also useful for electron beam welding, cold welding, vacuum packing and vacuum frying. Ultra-high vacuum is used in the study of atomically clean substrates, as only a very good vacuum preserves atomic-scale clean surfaces for a reasonably long time (on the order of minutes to days). High to ultra-high vacuum removes the obstruction of air, allowing particle beams to deposit or remove materials without contamination. This is the principle behind chemical vapor deposition, physical vapor deposition, and dry etching which are essential to the fabrication of semiconductors and optical coatings, and to surface science. The reduction of convection provides the thermal insulation of thermos bottles. Deep vacuum lowers the boiling point of liquids and promotes low temperature outgassing which is used in freeze drying, adhesive preparation, distillation, metallurgy, and process purging. The electrical properties of vacuum make electron microscopes and vacuum tubes possible, including cathode ray tubes. Vacuum interrupters are used in electrical switchgear. Vacuum arc processes are industrially important for production of certain grades of steel or high purity materials. The elimination of air friction is useful for flywheel energy storage and ultracentrifuges.

Vacuums are commonly used to produce suction, which has an even wider variety of applications. The Newcomen steam engine used vacuum instead of pressure to drive a piston. In the 19th century, vacuum was used for traction on Isambard Kingdom Brunel's experimental atmospheric railway. Vacuum brakes were once widely used on trains in the UK but, except on heritage railways, they have been replaced by air brakes.

Manifold vacuum can be used to drive accessories on automobiles. The best known application is the vacuum servo, used to provide power assistance for the brakes. Obsolete applications include vacuum-driven windscreen wipers and Autovac fuel pumps. Some aircraft instruments (Attitude Indicator (AI) and the Heading Indicator (HI)) are typically vacuum-powered, as protection against loss of all (electrically powered) instruments, since early aircraft often did not have electrical systems, and since there are two readily available sources of vacuum on a moving aircraft, the engine and an external venturi. Vacuum induction melting uses electromagnetic induction within a vacuum.

Maintaining a vacuum in the condenser is an important aspect of the efficient operation of steam turbines. A steam jet ejector or liquid ring vacuum pump is used for this purpose. The typical vacuum maintained in the condenser steam space at the exhaust of the turbine (also called condenser backpressure) is in the range 5 to 15 kPa (absolute), depending on the type of condenser and the ambient conditions.

Evaporation and sublimation into a vacuum is called outgassing. All materials, solid or liquid, have a small vapour pressure, and their outgassing becomes important when the vacuum pressure falls below this vapour pressure. Outgassing has the same effect as a leak and will limit the achievable vacuum. Outgassing products may condense on nearby colder surfaces, which can be troublesome if they obscure optical instruments or react with other materials. This is of great concern to space missions, where an obscured telescope or solar cell can ruin an expensive mission.

The most prevalent outgassing product in vacuum systems is water absorbed by chamber materials. It can be reduced by desiccating or baking the chamber, and removing absorbent materials. Outgassed water can condense in the oil of rotary vane pumps and reduce their net speed drastically if gas ballasting is not used. High vacuum systems must be clean and free of organic matter to minimize outgassing.

Ultra-high vacuum systems are usually baked, preferably under vacuum, to temporarily raise the vapour pressure of all outgassing materials and boil them off. Once the bulk of the outgassing materials are boiled off and evacuated, the system may be cooled to lower vapour pressures and minimize residual outgassing during actual operation. Some systems are cooled well below room temperature by liquid nitrogen to shut down residual outgassing and simultaneously cryopump the system.

Fluids cannot generally be pulled, so a vacuum cannot be created by suction. Suction can spread and dilute a vacuum by letting a higher pressure push fluids into it, but the vacuum has to be created first before suction can occur. The easiest way to create an artificial vacuum is to expand the volume of a container. For example, the diaphragm muscle expands the chest cavity, which causes the volume of the lungs to increase. This expansion reduces the pressure and creates a partial vacuum, which is soon filled by air pushed in by atmospheric pressure.

To continue evacuating a chamber indefinitely without requiring infinite growth, a compartment of the vacuum can be repeatedly closed off, exhausted, and expanded again. This is the principle behind positive displacement pumps, like the manual water pump for example. Inside the pump, a mechanism expands a small sealed cavity to create a vacuum. Because of the pressure differential, some fluid from the chamber (or the well, in our example) is pushed into the pump's small cavity. The pump's cavity is then sealed from the chamber, opened to the atmosphere, and squeezed back to a minute size.

The above explanation is merely a simple introduction to vacuum pumping, and is not representative of the entire range of pumps in use. Many variations of the positive displacement pump have been developed, and many other pump designs rely on fundamentally different principles. Momentum transfer pumps, which bear some similarities to dynamic pumps used at higher pressures, can achieve much higher quality vacuums than positive displacement pumps. Entrapment pumps can capture gases in a solid or absorbed state, often with no moving parts, no seals and no vibration. None of these pumps are universal; each type has important performance limitations. They all share a difficulty in pumping low molecular weight gases, especially hydrogen, helium, and neon.

The lowest pressure that can be attained in a system is also dependent on many things other than the nature of the pumps. Multiple pumps may be connected in series, called stages, to achieve higher vacuums. The choice of seals, chamber geometry, materials, and pump-down procedures will all have an impact. Collectively, these are called vacuum technique. And sometimes, the final pressure is not the only relevant characteristic. Pumping systems differ in oil contamination, vibration, preferential pumping of certain gases, pump-down speeds, intermittent duty cycle, reliability, or tolerance to high leakage rates.

In ultra high vacuum systems, some very "odd" leakage paths and outgassing sources must be considered. The water absorption of aluminium and palladium becomes an unacceptable source of outgassing, and even the adsorptivity of hard metals such as stainless steel or titanium must be considered. Some oils and greases will boil off in extreme vacuums. The permeability of the metallic chamber walls may have to be considered, and the grain direction of the metallic flanges should be parallel to the flange face.

The lowest pressures currently achievable in laboratory are about 1'd710uc0u8722 13 torrs (13 pPa). However, pressures as low as 5'd710u8722 17 torrs (6.7 fPa) have been indirectly measured in a 4 K (u8722 269.15 'b0C; u8722 452.47 'b0F) cryogenic vacuum system. This corresponds to u8776 100 particles/cm3.

Humans and animals exposed to vacuum will lose consciousness after a few seconds and die of hypoxia within minutes, but the symptoms are not nearly as graphic as commonly depicted in media and popular culture. The reduction in pressure lowers the temperature at which blood and other body fluids boil, but the elastic pressure of blood vessels ensures that this boiling point remains above the internal body temperature of 37 'b0C. Although the blood will not boil, the formation of gas bubbles in bodily fluids at reduced pressures, known as ebullism, is still a concern. The gas may bloat the body to twice its normal size and slow circulation, but tissues are elastic and porous enough to prevent rupture. Swelling and ebullism can be restrained by containment in a flight suit. Shuttle astronauts wore a fitted elastic garment called the Crew Altitude Protection Suit (CAPS) which prevents ebullism at pressures as low as 2 kPa (15 Torr). Rapid boiling will cool the skin and create frost, particularly in the mouth, but this is not a significant hazard.

Animal experiments show that rapid and complete recovery is normal for exposures shorter than 90 seconds, while longer full-body exposures are fatal and resuscitation has never been successful. A study by NASA on eight chimpanzees found all of them survived two and a half minute exposures to vacuum. There is only a limited amount of data available from human accidents, but it is consistent with animal data. Limbs may be exposed for much longer if breathing is not impaired. Robert Boyle was the first to show in 1660 that vacuum is lethal to small animals.

An experiment indicates that plants are able to survive in a low pressure environment (1.5 kPa) for about 30 minutes.

Cold or oxygen-rich atmospheres can sustain life at pressures much lower than atmospheric, as long as the density of oxygen is similar to that of standard sea-level atmosphere. The colder air temperatures found at altitudes of up to 3 km generally compensate for the lower pressures there. Above this altitude, oxygen enrichment is necessary to prevent altitude sickness in humans that did not undergo prior acclimatization, and spacesuits are necessary to prevent ebullism above 19 km. Most spacesuits use only 20 kPa (150 Torr) of pure oxygen. This pressure is high enough to prevent ebullism, but decompression sickness and gas embolisms can still occur if decompression rates are not managed.

Rapid decompression can be much more dangerous than vacuum exposure itself. Even if the victim does not hold his or her breath, venting through the windpipe may be too slow to prevent the fatal rupture of the delicate alveoli of the lungs. Eardrums and sinuses may be ruptured by rapid decompression, soft tissues may bruise and seep blood, and the stress of shock will accelerate oxygen consumption leading to hypoxia. Injuries caused by rapid decompression are called barotrauma. A pressure drop of 13 kPa (100 Torr), which produces no symptoms if it is gradual, may be fatal if it occurs suddenly.

As described by the third of Newton's laws of motion of classical mechanics, all forces occur in pairs such that if one object exerts a force on another object, then the second object exerts an equal and opposite reaction force on the first. The third law is also more generally stated as: "To every action there is always opposed an equal reaction: or the mutual actions of two bodies upon each other are always equal, and directed to contrary parts." The attribution of which of the two forces is the action and which is the reaction is arbitrary. Either of the two can be considered the action, while the other is its associated reaction.

When something is exerting force on the ground, the ground will push back with equal force in the opposite direction. In certain fields of applied physics, such as biomechanics, this force by the ground is called 'ground reaction force'; the force by the object on the ground is viewed as the 'action'.

When someone wants to jump, he or she exerts additional downward force on the ground ('action'). Simultaneously, the ground exerts upward force on the person ('reaction'). If this upward force is greater than the person's weight, this will result in upward acceleration. When these forces are perpendicular to the ground, they are also called a normal force.

Likewise, the spinning wheels of a vehicle attempt to slide backward across the ground. If the ground is not too slippery, this results in a pair of friction forces: the 'action' by the wheel on the ground in backward direction, and the 'reaction' by the ground on the wheel in forward direction. This forward force propels the vehicle.

The Earth, among other planets, orbits the Sun because the Sun exerts a gravitational pull that acts as a centripetal force, holding the Earth to it, which would otherwise go shooting off into space. If the Sun's pull is considered an action, then Earth simultaneously exerts a reaction as a gravitational pull on the Sun. Earth's pull has the same amplitude as the Sun but in the opposite direction. Since the Sun's mass is so much larger than Earth's, the Sun does not generally appear to react to the pull of Earth, but in fact it does, as demonstrated in the animation (not to precise scale). A correct way of describing the combined motion of both objects (ignoring all other celestial bodies for the moment) is to say that they both orbit around the center of mass, referred to in astronomy as the barycenter, of the combined system.

Any mass on earth is pulled down by the gravitational force of the earth; this force is also called its weight. The corresponding 'reaction' is the gravitational force that mass exerts on the planet.

If the object is supported so that it remains at rest, for instance by a cable from which it is hanging, or by a surface underneath, or by a liquid on which it is floating, there is also a support force in upward direction (tension force, normal force, buoyant force, respectively). This support force is an 'equal and opposite' force; we know this not because of Newton's third law, but because the object remains at rest, so that the forces must be balanced.

To this support force there is also a 'reaction': the object pulls down on the supporting cable, or pushes down on the supporting surface or liquid. In this case, there are therefore four forces of equal magnitude:

F1. gravitational force by earth on object (downward)
F2. gravitational force by object on earth (upward)
F3. force by support on object (upward)
F4. force by object on support (downward)
Forces F1 and F2 are equal due of Newton's third law; the same is true for forces F3 and F4. Forces F1 and F3 are equal if and only if the object is in equilibrium, and no other forces are applied. (This has nothing to do with Newton's third law.)

If a mass is hanging from a spring, the same considerations apply as before. However, if this system is then perturbed (e.g., the mass is given a slight kick upwards or downwards, say), the mass starts to oscillate up and down. Because of these accelerations (and subsequent decelerations), we conclude from Newton's second law that a net force is responsible for the observed change in velocity. The gravitational force pulling down on the mass is no longer equal to the upward elastic force of the spring. In the terminology of the previous section, F1 and F3 are no longer equal.

However, it is still true that F1 = F2 and F3 = F4, as this is required by Newton's third law.

The terms 'action' and 'reaction' have the misleading suggestion of causality, as if the 'action' is the cause and 'reaction' is the effect. It is therefore easy to think of the second force as being there because of the first, and even happening some time after the first. This is incorrect; the forces are perfectly simultaneous, and are there for the same reason.

When the forces are caused by a person's volition (e.g. a soccer player kicks a ball), this volitional cause often leads to an asymmetric interpretation, where the force by the player on the ball is considered the 'action' and the force by the ball on the player, the 'reaction'. But physically, the situation is symmetric. The forces on ball and player are both explained by their nearness, which results in a pair of contact forces (ultimately due to electric repulsion). That this nearness is caused by a decision of the player has no bearing on the physical analysis. As far as the physics is concerned, the labels 'action' and 'reaction' can be flipped.

One problem frequently observed by physics educators is that students tend to apply Newton's third law to pairs of 'equal and opposite' forces acting on the same object. This is incorrect; the third law refers to forces on two different objects. In contrast, a book lying on a table is subject to a downward gravitational force (exerted by the earth) and to an upward normal force by the table, both forces acting on the same book. Since the book is not accelerating, these forces must be exactly balanced, according to Newton's second law. They are therefore 'equal and opposite', yet they are acting on the same object, hence they are not action-reaction forces in the sense of Newton's third law. The actual action-reaction forces in the sense of Newton's third law are the book pushing down on the table and the book's upward gravitational force on the earth. Moreover, the forces acting on the book are not always equally strong; they will be different if the book is pushed down by a third force, or if the table is slanted, or if the table-and-book system is in an accelerating elevator. The case of any number of forces acting on the same object is covered by considering the sum of all forces.

A possible cause of this problem is that the third law is often stated in an abbreviated form: For every action there is an equal and opposite reaction, without the details, namely that these forces act on two different objects. Moreover, there is a causal connection between the weight of something and the normal force: if an object had no weight, it would not experience support force from the table, and the weight dictates how strong the support force will be. This causal relationship is not due to the third law but to other physical relations in the system.

Another common mistake is to state that "the centrifugal force that an object experiences is the reaction to the centripetal force on that object."

If an object were simultaneously subject to both a centripetal force and an equal and opposite centrifugal force, the resultant force would vanish and the object could not experience a circular motion. The centrifugal force is sometimes called a fictitious force or pseudo force, to underscore the fact that such a force only appears when calculations or measurements are conducted in non-inertial reference frames.

A multistage rocket or step rocket is a launch vehicle that uses two or more rocket stages, each of which contains its own engines and propellant. A tandem or serial stage is mounted on top of another stage; a parallel stage is attached alongside another stage. The result is effectively two or more rockets stacked on top of or attached next to each other. Two-stage rockets are quite common, but rockets with as many as five separate stages have been successfully launched.

By jettisoning stages when they run out of propellant, the mass of the remaining rocket is decreased. Each successive stage can also be optimized for its specific operating conditions, such as decreased atmospheric pressure at higher altitudes. This staging allows the thrust of the remaining stages to more easily accelerate the rocket to its final speed and height.

In serial or tandem staging schemes, the first stage is at the bottom and is usually the largest, the second stage and subsequent upper stages are above it, usually decreasing in size. In parallel staging schemes solid or liquid rocket boosters are used to assist with launch. These are sometimes referred to as "stage 0". In the typical case, the first-stage and booster engines fire to propel the entire rocket upwards. When the boosters run out of fuel, they are detached from the rest of the rocket (usually with some kind of small explosive charge or explosive bolts) and fall away. The first stage then burns to completion and falls off. This leaves a smaller rocket, with the second stage on the bottom, which then fires. Known in rocketry circles as staging, this process is repeated until the desired final velocity is achieved. In some cases with serial staging, the upper stage ignites before the separation'97the interstage ring is designed with this in mind, and the thrust is used to help positively separate the two vehicles.

A multistage rocket is required to reach orbital speed. Single-stage-to-orbit designs are sought, but have not yet been demonstrated.

The reason multi-stage rockets are required is the limitation the laws of physics place on the maximum velocity achievable by a rocket of given fueled-to-dry mass ratio. This relation is given by the classical rocket equation:

where:

is delta-v of the vehicle (change of velocity plus losses due to gravity and atmospheric drag);
is the initial total (wet) mass, equal to final (dry) mass plus propellant;
is the final (dry) mass, after the propellant is expended;
is the effective exhaust velocity (determined by propellant, engine design and throttle condition);
is the natural logarithm function.
The delta v required to reach low Earth orbit (or the required velocity of a sufficiently heavy suborbital payload) requires a wet to dry mass ratio larger than can realistically be achieved in a single rocket stage. The multistage rocket overcomes this limit by splitting the delta-v into fractions. As each lower stage drops off and the succeeding stage fires, the rest of the rocket is still traveling near the burnout speed. Each lower stage's dry mass includes the propellant in the upper stages, and each succeeding upper stage has reduced its dry mass by discarding the useless dry mass of the spent lower stages.

A further advantage is that each stage can use a different type of rocket engine, each tuned for its particular operating conditions. Thus the lower-stage engines are designed for use at atmospheric pressure, while the upper stages can use engines suited to near vacuum conditions. Lower stages tend to require more structure than upper as they need to bear their own weight plus that of the stages above them. Optimizing the structure of each stage decreases the weight of the total vehicle and provides further advantage.

The advantage of staging comes at the cost of the lower stages lifting engines which are not yet being used, as well as making the entire rocket more complex and harder to build than a single stage. In addition, each staging event is a possible point of launch failure, due to separation failure, ignition failure, or stage collision. Nevertheless, the savings are so great that every rocket ever used to deliver a payload into orbit has had staging of some sort.

One of the most common measures of rocket efficiency is its specific impulse, which is defined as the thrust per flow rate (per second) of propellant consumption:

=
When rearranging the equation such that thrust is calculated as a result of the other factors, we have:

These equations show that a higher specific impulse means a more efficient rocket engine, capable of burning for longer periods of time. In terms of staging, the initial rocket stages usually have a lower specific impulse rating, trading efficiency for superior thrust in order to quickly push the rocket into higher altitudes. Later stages of the rocket usually have a higher specific impulse rating because the vehicle is further outside the atmosphere and the exhaust gas does not need to expand against as much atmospheric pressure.

When selecting the ideal rocket engine to use as an initial stage for a launch vehicle, a useful performance metric to examine is the thrust-to-weight ratio, and is calculated by the equation:

The common thrust-to-weight ratio of a launch vehicle is within the range of 1.3 to 2.0. Another performance metric to keep in mind when designing each rocket stage in a mission is the burn time, which is the amount of time the rocket engine will last before it has exhausted all of its propellant. For most non-final stages, thrust and specific impulse can be assumed constant, which allows the equation for burn time to be written as:

Where and are the initial and final masses of the rocket stage respectively. In conjunction with the burnout time, the burnout height and velocity are obtained using the same values, and are found by these two equations:

When dealing with the problem of calculating the total burnout velocity or time for the entire rocket system, the general procedure for doing so is as follows:

Partition the problem calculations into however many stages the rocket system comprises.
Calculate the initial and final mass for each individual stage.
Calculate the burnout velocity, and sum it with the initial velocity for each individual stage. Assuming each stage occurs immediately after the previous, the burnout velocity becomes the initial velocity for the following stage.
Repeat the previous two steps until the burnout time and/or velocity has been calculated for the final stage.
It is important to note that the burnout time does not define the end of the rocket stage's motion, as the vehicle will still have a velocity that will allow it to coast upward for a brief amount of time until the acceleration of the planet's gravity gradually changes it to a downward direction. The velocity and altitude of the rocket after burnout can be easily modeled using the basic physics equations of motion.

When comparing one rocket with another, it is impractical to directly compare the rocket's certain trait with the same trait of another because their individual attributes are often not independent of one another. For this reason, dimensionless ratios have been designed to enable a more meaningful comparison between rockets. The first is the initial to final mass ratio, which is the ratio between the rocket stage's full initial mass and the rocket stage's final mass once all of its fuel has been consumed. The equation for this ratio is:

Where is the empty mass of the stage, is the mass of the propellant, and is the mass of the payload. The second dimensionless performance quantity is the structural ratio, which is the ratio between the empty mass of the stage, and the combined empty mass and propellant mass as shown in this equation:

The last major dimensionless performance quantity is the payload ratio, which is the ratio between the payload mass and the combined mass of the empty rocket stage and the propellant:

After comparing the three equations for the dimensionless quantities, it is easy to see that they are not independent of each other, and in fact, the initial to final mass ratio can be rewritten in terms of structural ratio and payload ratio:

These performance ratios can also be used as references for how efficient a rocket system will be when performing optimizations and comparing varying configurations for a mission.

For initial sizing, the rocket equations can be used to derive the amount of propellant needed for the rocket based on the specific impulse of the engine and the total impulse required in N'b7s. The equation is:

where g is the gravity constant of Earth. This also enables the volume of storage required for the fuel to be calculated if the density of the fuel is known, which is almost always the case when designing the rocket stage. The volume is yielded when dividing the mass of the propellant by its density. Asides from the fuel required, the mass of the rocket structure itself must also be determined, which requires taking into account the mass of the required thrusters, electronics, instruments, power equipment, etc. These are known quantities for typical off the shelf hardware that should be considered in the mid to late stages of the design, but for preliminary and conceptual design, a simpler approach can be taken. Assuming one engine for a rocket stage provides all of the total impulse for that particular segment, a mass fraction can be used to determine the mass of the system. The mass of the stage transfer hardware such as initiators and safe-and-arm devices are very small by comparison and can be considered negligible.

For modern day solid rocket motors, it is a safe and reasonable assumption to say that 91 to 94 percent of the total mass is fuel. It is also important to note there is a small percentage of "residual" propellant that will be left stuck and unusable inside the tank, and should also be taken into consideration when determining amount of fuel for the rocket. A common initial estimate for this residual propellant is five percent. With this ratio and the mass of the propellant calculated, the mass of the empty rocket weight can be determined. Sizing rockets using a liquid bipropellant requires a slightly more involved approach because there are two separate tanks that are required: one for the fuel, and one for the oxidizer. The ratio of these two quantities is known as the mixture ratio, and is defined by the equation:

Where is the mass of the oxidizer and is the mass of the fuel. This mixture ratio not only governs the size of each tank, but also the specific impulse of the rocket. Determining the ideal mixture ratio is a balance of compromises between various aspects of the rocket being designed, and can vary depending on the type of fuel and oxidizer combination being used. For example, a mixture ratio of a bipropellant could be adjusted such that it may not have the optimal specific impulse, but will result in fuel tanks of equal size. This would yield simpler and cheaper manufacturing, packing, configuring, and integrating of the fuel systems with the rest of the rocket, and can become a benefit that could outweigh the drawbacks of a less efficient specific impulse rating. But suppose the defining constraint for the launch system is volume, and a low density fuel is required such as hydrogen. This example would be solved by using an oxidizer-rich mixture ratio, reducing efficiency and specific impulse rating, but will meet a smaller tank volume requirement.

The ultimate goal of optimal staging is to maximize the payload ratio (see ratios under performance), meaning the largest amount of payload is carried up to the required burnout velocity using the least amount of non-payload mass, which comprises everything else. Here are a few quick rules and guidelines to follow in order to reach optimal staging:

Initial stages should have lower , and later/final stages should have higher .
The stages with the lower should contribute less uc0u916 V.
The next stage is always a smaller size than the previous stage.
Similar stages should provide similar uc0u916 V.
The payload ratio can be calculated for each individual stage, and when multiplied together in sequence, will yield the overall payload ratio of the entire system. It is important to note that when computing payload ratio for individual stages, the payload includes the mass of all the stages after the current one. The overall payload ratio is:

Where n is the number of stages the rocket system comprises. Similar stages yielding the same payload ratio simplify this equation, however that is seldom the ideal solution for maximizing payload ratio, and uc0u916 V requirements may have to be partitioned unevenly as suggested in guideline tips 1 and 2 from above. Two common methods of determining this perfect u916 V partition between stages are either a technical algorithm that generates an analytical solution that can be implemented by a program, or simple trial and error. For the trial and error approach, it is best to begin with the final stage, calculating the initial mass which becomes the payload for the previous stage. From there it is easy to progress all the way down to the initial stage in the same manner, sizing all the stages of the rocket system.

Restricted rocket staging is based on the simplified assumption that each of the stages of the rocket system have the same specific impulse, structural ratio, and payload ratio, the only difference being the total mass of each increasing stage is less than that of the previous stage. Although this assumption may not be the ideal approach to yielding an efficient or optimal system, it greatly simplifies the equations for determining the burnout velocities, burnout times, burnout altitudes, and mass of each stage. This would make for a better approach to a conceptual design in a situation where a basic understanding of the system behavior is preferential to a detailed, accurate design. One important concept to understand when undergoing restricted rocket staging, is how the burnout velocity is affected by the number of stages that split up the rocket system. Increasing the number of stages for a rocket while keeping the specific impulse, payload ratios and structural ratios constant will always yield a higher burnout velocity than the same systems that use fewer stages. However, the law of diminishing returns is evident in that each increment in number of stages gives less of an improvement in burnout velocity than the previous increment. The burnout velocity gradually converges towards an asymptotic value as the number of stages increases towards a very high number. In addition to diminishing returns in burnout velocity improvement, the main reason why real world rockets seldom use more than three stages is because of increase of weight and complexity in the system for each added stage, ultimately yielding a higher cost for deployment.

A rocket system that implements tandem staging means that each individual stage runs in order one after the other. The rocket breaks free from the previous stage, then begins burning through the next stage in straight succession. On the other hand, a rocket that implements parallel staging has two or more different stages that are active at the same time. For example, the Space Shuttle has two Solid Rocket Boosters that burn simultaneously. Upon launch, the boosters ignite, and at the end of the stage, the two boosters are discarded while the external fuel tank is kept for another stage. Most quantitative approaches to the design of the rocket system's performance are focused on tandem staging, but the approach can be easily modified to include parallel staging. To begin with, the different stages of the rocket should be clearly defined. Continuing with the previous example, the end of the first stage which is sometimes referred to as 'stage 0', can be defined as when the side boosters separate from the main rocket. From there, the final mass of stage one can be considered the sum of the empty mass of stage one, the mass of stage two (the main rocket and the remaining unburned fuel) and the mass of the payload.

High-altitude and space-bound upper stages are designed to operate with little or no atmospheric pressure. This allows the use of lower pressure combustion chambers and engine nozzles with optimal vacuum expansion ratios. Some upper stages, especially those using hypergolic propellants like Delta-K or Ariane 5 ES second stage, are pressure fed, which eliminates the need for complex turbopumps. Other upper stages, such as the Centaur or DCSS, use liquid hydrogen expander cycle engines, or gas generator cycle engines like the Ariane 5 ECA's HM7B or the S-IVB's J-2. These stages are usually tasked with completing orbital injection and accelerating payloads into higher energy orbits such as GTO or to escape velocity. Upper stages, such as Fregat, used primarily to bring payloads from low Earth orbit to GTO or beyond are sometimes referred to as space tugs.

Each individual stage is generally assembled at its manufacturing site and shipped to the launch site; the term vehicle assembly refers to the mating of all rocket stage(s) and the spacecraft payload into a single assembly known as a space vehicle. Single-stage vehicles (suborbital), and multistage vehicles on the smaller end of the size range, can usually be assembled directly on the launch pad by lifting the stage(s) and spacecraft vertically in place by means of a crane.

This is generally not practical for larger space vehicles, which are assembled off the pad and moved into place on the launch site by various methods. NASA's Apollo/Saturn V manned Moon landing vehicle, and Space Shuttle, were assembled vertically onto mobile launcher platforms with attached launch umbilical towers, in a Vehicle Assembly Building, and then a special crawler-transporter moved the entire vehicle stack to the launch pad in an upright position. In contrast, vehicles such as the Russian Soyuz rocket and the SpaceX Falcon 9 are assembled horizontally in a processing hangar, transported horizontally, and then brought upright at the pad.

Spent upper stages of launch vehicles are a significant source of space debris remaining in orbit in a non-operational state for many years after use, and occasionally, large debris fields created from the breakup of a single upper stage while in orbit.

After the 1990s, spent upper stages are generally passivated after their use as a launch vehicle is complete in order to minimize risks while the stage remains derelict in orbit. Passivation means removing any sources of stored energy remaining on the vehicle, as by dumping fuel or discharging batteries.

Many early upper stages, in both the Soviet and U.S. space programs, were not passivated after mission completion. During the initial attempts to characterize the space debris problem, it became evident that a good proportion of all debris was due to the breaking up of rocket upper stages, particularly unpassivated upper-stage propulsion units.

An illustration and description in the 14th century Chinese Huolongjing by Jiao Yu and Liu Bowen shows the oldest known multistage rocket; this was the "fire-dragon issuing from the water" (
f1 'bb'f0'c1'fa'b3'f6'cb'ae
f0 , huuc0u466  l'f3ng chu363  shuu464 ), which was used mostly by the Chinese navy. It was a two-stage rocket that had booster rockets that would eventually burn out, yet before they did they automatically ignited a number of smaller rocket arrows that were shot out of the front end of the missile, which was shaped like a dragon's head with an open mouth. This multi-stage rocket may be considered the ancestor to the modern YingJi-62 ASCM. The British scientist and historian Joseph Needham points out that the written material and depicted illustration of this rocket come from the oldest stratum of the Huolongjing, which can be dated roughly 1300'961350 AD (from the book's part 1, chapter 3, page 23).

Another example of an early multistaged rocket is the Juhwa (
f1 'd7'df'bb'f0
f0 ) of Korean development. It was proposed by medieval Korean engineer, scientist and inventor Choe Museon and developed by the Firearms Bureau (
f1 'bb'f0uc0u15809 'b5'c0'b1'4f
f0 ) during the 14th century. The rocket had the length of 15 cm and 13 cm; the diameter was 2.2 cm. It was attached to an arrow 110 cm long; experimental records show that the first results were around 200m in range. There are records that show Korea kept developing this technology until it came to produce the Singijeon, or 'magical machine arrows' in the 16th century. The earliest experiments with multistage rockets in Europe were made in 1551 by Austrian Conrad Haas (1509'961576), the arsenal master of the town of Hermannstadt, Transylvania (now Sibiu/Hermannstadt, Romania). This concept was developed independently by at least five individuals:

Polish'96Lithuanian Kazimieras Simonaviuc0u269 ius (1600'961651)
Russian Konstantin Tsiolkovsky (1857'961935)
American Robert Goddard (1882'961945)
German Hermann Oberth (1894'961989)
French Louis Damblanc (1889'961969)
The first high-speed multistage rockets were the RTV-G-4 Bumper rockets tested at the White Sands Proving Ground and later at Cape Canaveral from 1948 to 1950. These consisted of a V-2 rocket and a WAC Corporal sounding rocket. The greatest altitude ever reached was 393 km, attained on February 24, 1949, at White Sands.

In 1947, the Soviet rocket engineer and scientist Mikhail Tikhonravov developed a theory of parallel stages, which he called "packet rockets". In his scheme, three parallel stages were fired from liftoff, but all three engines were fueled from the outer two stages, until they are empty and could be ejected. This is more efficient than sequential staging, because the second-stage engine is never just dead weight. In 1951, Soviet engineer and scientist Dmitry Okhotsimsky carried out a pioneering engineering study of general sequential and parallel staging, with and without the pumping of fuel between stages. The design of the R-7 Semyorka emerged from that study. The trio of rocket engines used in the first stage of the American Atlas I and Atlas II launch vehicles, arranged in a row, used parallel staging in a similar way: the outer pair of booster engines existed as a jettisonable pair which would, after they shut down, drop away with the lowermost outer skirt structure, leaving the central sustainer engine to complete the first stage's engine burn towards apogee or orbit.

Separation of each portion of a multistage rocket introduces additional risk into the success of the launch mission. Reducing the number of separation events results in a reduction in complexity. Separation events occur when stages or strap-on boosters separate after use, when the payload fairing separates prior to orbital insertion, or when used, a launch escape system which separates after the early phase of a launch. Pyrotechnic fasteners, or in some cases pneumatic systems like on the Falcon 9 Full Thrust, are typically used to separate rocket stages.

The three-stage-to-orbit launch system is a commonly used rocket system to attain Earth orbit. The spacecraft uses three distinct stages to provide propulsion consecutively in order to achieve orbital velocity. It is intermediate between a four-stage-to-orbit launcher and a two-stage-to-orbit launcher.

In celestial mechanics, escape velocity or escape speed is the minimum speed needed for a free, non-propelled object to escape from the gravitational influence of a primary body, thus reaching an infinite distance from it. It is typically stated as an ideal speed, ignoring atmospheric friction. Although the term "escape velocity" is common, it is more accurately described as a speed than a velocity because it is independent of direction; the escape speed increases with the mass of the primary body and decreases with the distance from the primary body. The escape speed thus depends on how far the object has already traveled, and its calculation at a given distance takes into account that without new acceleration it will slow down as it travels'97due to the massive body's gravity'97but it will never quite slow to a stop.

A rocket, continuously accelerated by its exhaust, can escape without ever reaching escape speed, since it continues to add kinetic energy from its engines. It can achieve escape at any speed, given sufficient propellant to provide new acceleration to the rocket to counter gravity's deceleration and thus maintain its speed.

More generally, escape velocity is the speed at which the sum of an object's kinetic energy and its gravitational potential energy is equal to zero; an object which has achieved escape velocity is neither on the surface, nor in a closed orbit (of any radius). With escape velocity in a direction pointing away from the ground of a massive body, the object will move away from the body, slowing forever and approaching, but never reaching, zero speed. Once escape velocity is achieved, no further impulse need be applied for it to continue in its escape. In other words, if given escape velocity, the object will move away from the other body, continually slowing, and will asymptotically approach zero speed as the object's distance approaches infinity, never to come back. Speeds higher than escape velocity retain a positive speed at infinite distance. Note that the minimum escape velocity assumes that there is no friction (e.g., atmospheric drag), which would increase the required instantaneous velocity to escape the gravitational influence, and that there will be no future acceleration or extraneous deceleration (for example from thrust or from gravity of other bodies), which would change the required instantaneous velocity.

Escape speed at a distance d from the center of a spherically symmetric primary body (such as a star or a planet) with mass M is given by the formula

where G is the universal gravitational constant (G uc0u8776  6.67'd710u8722 11 m3'b7kgu8722 1'b7su8722 2). The escape speed is independent of the mass of the escaping object. For example, the escape speed from Earth's surface is about 11.186 km/s (40,270 km/h; 25,020 mph; 36,700 ft/s).

When given an initial speed greater than the escape speed the object will asymptotically approach the hyperbolic excess speed satisfying the equation:

In these equations atmospheric friction (air drag) is not taken into account.

The existence of escape velocity is a consequence of conservation of energy and an energy field of finite depth. For an object with a given total energy, which is moving subject to conservative forces (such as a static gravity field) it is only possible for the object to reach combinations of locations and speeds which have that total energy; and places which have a higher potential energy than this cannot be reached at all. By adding speed (kinetic energy) to the object it expands the possible locations that can be reached, until, with enough energy, they become infinite.

For a given gravitational potential energy at a given position, the escape velocity is the minimum speed an object without propulsion needs to be able to "escape" from the gravity (i.e. so that gravity will never manage to pull it back). Escape velocity is actually a speed (not a velocity) because it does not specify a direction: no matter what the direction of travel is, the object can escape the gravitational field (provided its path does not intersect the planet).

An elegant way to derive the formula for escape velocity is to use the principle of conservation of energy (for another way, based on work, see below). For the sake of simplicity, unless stated otherwise, we assume that an object will escape the gravitational field of a uniform spherical planet by moving away from it and that the only significant force acting on the moving object is the planet's gravity. Imagine that a spaceship of mass m is initially at a distance r from the center of mass of the planet, whose mass is M, and its initial speed is equal to its escape velocity, . At its final state, it will be an infinite distance away from the planet, and its speed will be negligibly small. Kinetic energy K and gravitational potential energy Ug are the only types of energy that we will deal with (we will ignore the drag of the atmosphere), so by the conservation of energy,

We can set Kfinal = 0 because final velocity is arbitrarily small, and Ugfinal = 0 because final distance is infinity, so

where uc0u956  is the standard gravitational parameter.

The same result is obtained by a relativistic calculation, in which case the variable r represents the radial coordinate or reduced circumference of the Schwarzschild metric.

Defined a little more formally, "escape velocity" is the initial speed required to go from an initial point in a gravitational potential field to infinity and end at infinity with a residual speed of zero, without any additional acceleration. All speeds and velocities are measured with respect to the field. Additionally, the escape velocity at a point in space is equal to the speed that an object would have if it started at rest from an infinite distance and was pulled by gravity to that point.

In common usage, the initial point is on the surface of a planet or moon. On the surface of the Earth, the escape velocity is about 11.2 km/s, which is approximately 33 times the speed of sound (Mach 33) and several times the muzzle velocity of a rifle bullet (up to 1.7 km/s). However, at 9,000 km altitude in "space", it is slightly less than 7.1 km/s. Note that this escape velocity is relative to a non-rotating frame of reference, not relative to the moving surface of the planet or moon (see below).

The escape velocity is independent of the mass of the escaping object. It does not matter if the mass is 1 kg or 1,000 kg; what differs is the amount of energy required. For an object of mass the energy required to escape the Earth's gravitational field is GMm / r, a function of the object's mass (where r is radius of the Earth, nominally 6,371 kilometres (3,959 mi), G is the gravitational constant, and M is the mass of the Earth, M = 5.9736 'd7 1024 kg). A related quantity is the specific orbital energy which is essentially the sum of the kinetic and potential energy divided by the mass. An object has reached escape velocity when the specific orbital energy is greater than or equal to zero.

An alternative expression for the escape velocity particularly useful at the surface on the body is:

where r is the distance between the center of the body and the point at which escape velocity is being calculated and g is the gravitational acceleration at that distance (i.e., the surface gravity).

For a body with a spherically-symmetric distribution of mass, the escape velocity from the surface is proportional to the radius assuming constant density, and proportional to the square root of the average density uc0u961 .

where

Note that this escape velocity is relative to a non-rotating frame of reference, not relative to the moving surface of the planet or moon, as we now explain.

The escape velocity relative to the surface of a rotating body depends on direction in which the escaping body travels. For example, as the Earth's rotational velocity is 465 m/s at the equator, a rocket launched tangentially from the Earth's equator to the east requires an initial velocity of about 10.735 km/s relative to the moving surface at the point of launch to escape whereas a rocket launched tangentially from the Earth's equator to the west requires an initial velocity of about 11.665 km/s relative to that moving surface. The surface velocity decreases with the cosine of the geographic latitude, so space launch facilities are often located as close to the equator as feasible, e.g. the American Cape Canaveral (latitude 28'b028uc0u8242  N) and the French Guiana Space Centre (latitude 5'b014u8242  N).

In most situations it is impractical to achieve escape velocity almost instantly, because of the acceleration implied, and also because if there is an atmosphere, the hypersonic speeds involved (on Earth a speed of 11.2 km/s, or 40,320 km/h) would cause most objects to burn up due to aerodynamic heating or be torn apart by atmospheric drag. For an actual escape orbit, a spacecraft will accelerate steadily out of the atmosphere until it reaches the escape velocity appropriate for its altitude (which will be less than on the surface). In many cases, the spacecraft may be first placed in a parking orbit (e.g. a low Earth orbit at 160'962,000 km) and then accelerated to the escape velocity at that altitude, which will be slightly lower (about 11.0 km/s at a low Earth orbit of 200 km). The required additional change in speed, however, is far less because the spacecraft already has a significant orbital speed (in low Earth orbit speed is approximately 7.8 km/s, or 28,080 km/h).

The escape velocity at a given height is times the speed in a circular orbit at the same height, (compare this with the velocity equation in circular orbit). This corresponds to the fact that the potential energy with respect to infinity of an object in such an orbit is minus two times its kinetic energy, while to escape the sum of potential and kinetic energy needs to be at least zero. The velocity corresponding to the circular orbit is sometimes called the first cosmic velocity, whereas in this context the escape velocity is referred to as the second cosmic velocity.

For a body in an elliptical orbit wishing to accelerate to an escape orbit the required speed will vary, and will be greatest at periapsis when the body is closest to the central body. However, the orbital speed of the body will also be at its highest at this point, and the change in velocity required will be at its lowest, as explained by the Oberth effect.

Technically escape velocity can either be measured as a relative to the other, central body or relative to center of mass or barycenter of the system of bodies. Thus for systems of two bodies, the term escape velocity can be ambiguous, but it is usually intended to mean the barycentric escape velocity of the less massive body. In gravitational fields, escape velocity refers to the escape velocity of zero mass test particles relative to the barycenter of the masses generating the field. In most situations involving spacecraft the difference is negligible. For a mass equal to a Saturn V rocket, the escape velocity relative to the launch pad is 253.5 am/s (8 nanometers per year) faster than the escape velocity relative to the mutual center of mass.

Ignoring all factors other than the gravitational force between the body and the object, an object projected vertically at speed from the surface of a spherical body with escape velocity and radius will attain a maximum height satisfying the equation

which, solving for h results in

where is the ratio of the original speed to the escape velocity

Unlike escape velocity, the direction (vertically up) is important to achieve maximum height.

If an object attains exactly escape velocity, but is not directed straight away from the planet, then it will follow a curved path or trajectory. Although this trajectory does not form a closed shape, it can be referred to as an orbit. Assuming that gravity is the only significant force in the system, this object's speed at any point in the trajectory will be equal to the escape velocity at that point due to the conservation of energy, its total energy must always be 0, which implies that it always has escape velocity; see the derivation above. The shape of the trajectory will be a parabola whose focus is located at the center of mass of the planet. An actual escape requires a course with a trajectory that does not intersect with the planet, or its atmosphere, since this would cause the object to crash. When moving away from the source, this path is called an escape orbit. Escape orbits are known as C3 = 0 orbits. C3 is the characteristic energy, = uc0u8722 GM/2a, where a is the semi-major axis, which is infinite for parabolic trajectories.

If the body has a velocity greater than escape velocity then its path will form a hyperbolic trajectory and it will have an excess hyperbolic velocity, equivalent to the extra energy the body has. A relatively small extra delta-v above that needed to accelerate to the escape speed can result in a relatively large speed at infinity. Some orbital manoeuvres make use of this fact. For example, at a place where escape speed is 11.2 km/s, the addition of 0.4 km/s yields a hyperbolic excess speed of 3.02 km/s:

If a body in circular orbit (or at the periapsis of an elliptical orbit) accelerates along its direction of travel to escape velocity, the point of acceleration will form the periapsis of the escape trajectory. The eventual direction of travel will be at 90 degrees to the direction at the point of acceleration. If the body accelerates to beyond escape velocity the eventual direction of travel will be at a smaller angle, and indicated by one of the asymptotes of the hyperbolic trajectory it is now taking. This means the timing of the acceleration is critical if the intention is to escape in a particular direction.

If the speed at periapsis is v, then the eccentricity of the trajectory is given by:

This is valid for elliptical, parabolic, and hyperbolic trajectories. If the trajectory is hyperbolic or parabolic, it will asymptotically approach an angle from the direction at periapsis, with

The speed will asymptotically approach

In this table, the left-hand half gives the escape velocity from the visible surface (which may be gaseous as with Jupiter for example), relative to the centre of the planet or moon (that is, not relative to its moving surface). In the right-hand half, Ve refers to the speed relative to the central body (for example the sun), whereas Vte is the speed (at the visible surface of the smaller body) relative to the smaller body (planet or moon).

The last two columns will depend precisely where in orbit escape velocity is reached, as the orbits are not exactly circular (particularly Mercury and Pluto).

Let G be the gravitational constant and let M be the mass of the earth (or other gravitating body) and m be the mass of the escaping body or projectile. At a distance r from the centre of gravitation the body feels an attractive force

The work needed to move the body over a small distance dr against this force is therefore given by.

In mechanics, acceleration is the rate of change of the velocity of an object with respect to time. Accelerations are vector quantities (in that they have magnitude and direction). The orientation of an object's acceleration is given by the orientation of the net force acting on that object. The magnitude of an object's acceleration, as described by Newton's Second Law, is the combined effect of two causes:

the net balance of all external forces acting onto that object '97 magnitude is directly proportional to this net resulting force;
that object's mass, depending on the materials out of which it is made '97 magnitude is inversely proportional to the object's mass.
The SI unit for acceleration is metre per second squared (m
f2 uc0u8901 
f0 suc0u8722 2, ).

For example, when a vehicle starts from a standstill (zero velocity, in an inertial frame of reference) and travels in a straight line at increasing speeds, it is accelerating in the direction of travel. If the vehicle turns, an acceleration occurs toward the new direction and changes its motion vector. The acceleration of the vehicle in its current direction of motion is called a linear (or tangential during circular motions) acceleration, the reaction to which the passengers on board experience as a force pushing them back into their seats. When changing direction, the effecting acceleration is called radial (or orthogonal during circular motions) acceleration, the reaction to which the passengers experience as a centrifugal force. If the speed of the vehicle decreases, this is an acceleration in the opposite direction and mathematically a negative, sometimes called deceleration or retardation, and passengers experience the reaction to deceleration as an inertial force pushing them forward. Such negative accelerations are often achieved by retrorocket burning in spacecraft. Both acceleration and deceleration are treated the same, as they are both changes in velocity. Each of these accelerations (tangential, radial, deceleration) is felt by passengers until their relative (differential) velocity are neutralized in reference to the vehicle.

An object's average acceleration over a period of time is its change in velocity, , divided by the duration of the period, . Mathematically,

Instantaneous acceleration, meanwhile, is the limit of the average acceleration over an infinitesimal interval of time. In the terms of calculus, instantaneous acceleration is the derivative of the velocity vector with respect to time:

As acceleration is defined as the derivative of velocity, v, with respect to time t and velocity is defined as the derivative of position, x, with respect to time, acceleration can be thought of as the second derivative of x with respect to t:
(Here and elsewhere, if motion is in a straight line, vector quantities can be substituted by scalars in the equations.)

By the fundamental theorem of calculus, it can be seen that the integral of the acceleration function a(t) is the velocity function v(t); that is, the area under the curve of an acceleration vs. time (a vs. t) graph corresponds to the change of velocity.

Likewise, the integral of the jerk function j(t), the derivative of the acceleration function, can be used to find the change of acceleration at a certain time:

Acceleration has the dimensions of velocity (L/T) divided by time, i.e. L Tuc0u8722 2. The SI unit of acceleration is the metre per second squared (m su8722 2); or "metre per second per second", as the velocity in metres per second changes by the acceleration value, every second.

An object moving in a circular motion'97such as a satellite orbiting the Earth'97is accelerating due to the change of direction of motion, although its speed may be constant. In this case it is said to be undergoing centripetal (directed towards the center) acceleration.

Proper acceleration, the acceleration of a body relative to a free-fall condition, is measured by an instrument called an accelerometer.

In classical mechanics, for a body with constant mass, the (vector) acceleration of the body's center of mass is proportional to the net force vector (i.e. sum of all forces) acting on it (Newton'92s second law):

where F is the net force acting on the body, m is the mass of the body, and a is the center-of-mass acceleration. As speeds approach the speed of light, relativistic effects become increasingly large.
The velocity of a particle moving on a curved path as a function of time can be written as:

with v(t) equal to the speed of travel along the path, and

a unit vector tangent to the path pointing in the direction of motion at the chosen moment in time. Taking into account both the changing speed v(t) and the changing direction of ut, the acceleration of a particle moving on a curved path can be written using the chain rule of differentiation for the product of two functions of time as:

where un is the unit (inward) normal vector to the particle's trajectory (also called the principal normal), and r is its instantaneous radius of curvature based upon the osculating circle at time t. These components are called the tangential acceleration and the normal or radial acceleration (or centripetal acceleration in circular motion, see also circular motion and centripetal force).

Geometrical analysis of three-dimensional space curves, which explains tangent, (principal) normal and binormal, is described by the Frenet'96Serret formulas.

Uniform or constant acceleration is a type of motion in which the velocity of an object changes by an equal amount in every equal time period.

A frequently cited example of uniform acceleration is that of an object in free fall in a uniform gravitational field. The acceleration of a falling body in the absence of resistances to motion is dependent only on the gravitational field strength g (also called acceleration due to gravity). By Newton's Second Law the force acting on a body is given by:

Because of the simple analytic properties of the case of constant acceleration, there are simple formulas relating the displacement, initial and time-dependent velocities, and acceleration to the time elapsed:

where

is the elapsed time,
is the initial displacement from the origin,
is the displacement from the origin at time ,
is the initial velocity,
is the velocity at time , and
is the uniform rate of acceleration.
In particular, the motion can be resolved into two orthogonal parts, one of constant velocity and the other according to the above equations. As Galileo showed, the net result is parabolic motion, which describes, e. g., the trajectory of a projectile in a vacuum near the surface of Earth.

In uniform circular motion, that is moving with constant speed along a circular path, a particle experiences an acceleration resulting from the change of the direction of the velocity vector, while its magnitude remains constant. The derivative of the location of a point on a curve with respect to time, i.e. its velocity, turns out to be always exactly tangential to the curve, respectively orthogonal to the radius in this point. Since in uniform motion the velocity in the tangential direction does not change, the acceleration must be in radial direction, pointing to the center of the circle. This acceleration constantly changes the direction of the velocity to be tangent in the neighboring point, thereby rotating the velocity vector along the circle.

For a given speed , the magnitude of this geometrically caused acceleration (centripetal acceleration) is inversely proportional to the radius of the circle, and increases as the square of this speed:
Note that, for a given angular velocity , the centripetal acceleration is directly proportional to radius . This is due to the dependence of velocity on the radius .
Expressing centripetal acceleration vector in polar components, where is a vector from the centre of the circle to the particle with magnitude equal to this distance, and considering the orientation of the acceleration towards the center, yields

As usual in rotations, the speed of a particle may be expressed as an angular speed with respect to a point at the distance as

Thus

This acceleration and the mass of the particle determine the necessary centripetal force, directed toward the centre of the circle, as the net force acting on this particle to keep it in this uniform circular motion. The so-called 'centrifugal force', appearing to act outward on the body, is a so-called pseudo force experienced in the frame of reference of the body in circular motion, due to the body's linear momentum, a vector tangent to the circle of motion.

In a nonuniform circular motion, i.e., the speed along the curved path is changing, the acceleration has a non-zero component tangential to the curve, and is not confined to the principal normal, which directs to the center of the osculating circle, that determines the radius for the centripetal acceleration. The tangential component is given by the angular acceleration , i.e., the rate of change of the angular speed times the radius . That is,

The sign of the tangential component of the acceleration is determined by the sign of the angular acceleration (), and the tangent is always directed at right angles to the radius vector.

The special theory of relativity describes the behavior of objects traveling relative to other objects at speeds approaching that of light in a vacuum. Newtonian mechanics is exactly revealed to be an approximation to reality, valid to great accuracy at lower speeds. As the relevant speeds increase toward the speed of light, acceleration no longer follows classical equations.

As speeds approach that of light, the acceleration produced by a given force decreases, becoming infinitesimally small as light speed is approached; an object with mass can approach this speed asymptotically, but never reach it.

Unless the state of motion of an object is known, it is impossible to distinguish whether an observed force is due to gravity or to acceleration'97gravity and inertial acceleration have identical effects. Albert Einstein called this the equivalence principle, and said that only observers who feel no force at all'97including the force of gravity'97are justified in concluding that they are not accelerating.

In Newtonian mechanics, linear momentum, translational momentum, or simply momentum is the product of the mass and velocity of an object. It is a vector quantity, possessing a magnitude and a direction. If m is an object's mass and v is its velocity (also a vector quantity), then the object's momentum p is :

In the International System of Units (SI), the unit of measurement of momentum is the kilogram metre per second (kg
f2 uc0u8901 
f0 m/s), which is equivalent to the newton-second.

Newton's second law of motion states that the rate of change of a body's momentum is equal to the net force acting on it. Momentum depends on the frame of reference, but in any inertial frame it is a conserved quantity, meaning that if a closed system is not affected by external forces, its total linear momentum does not change. Momentum is also conserved in special relativity (with a modified formula) and, in a modified form, in electrodynamics, quantum mechanics, quantum field theory, and general relativity. It is an expression of one of the fundamental symmetries of space and time: translational symmetry.

Advanced formulations of classical mechanics, Lagrangian and Hamiltonian mechanics, allow one to choose coordinate systems that incorporate symmetries and constraints. In these systems the conserved quantity is generalized momentum, and in general this is different from the kinetic momentum defined above. The concept of generalized momentum is carried over into quantum mechanics, where it becomes an operator on a wave function. The momentum and position operators are related by the Heisenberg uncertainty principle.

In continuous systems such as electromagnetic fields, fluid dynamics and deformable bodies, a momentum density can be defined, and a continuum version of the conservation of momentum leads to equations such as the Navier'96Stokes equations for fluids or the Cauchy momentum equation for deformable solids or fluids.

Momentum is a vector quantity: it has both magnitude and direction. Since momentum has a direction, it can be used to predict the resulting direction and speed of motion of objects after they collide. Below, the basic properties of momentum are described in one dimension. The vector equations are almost identical to the scalar equations (see multiple dimensions).

The momentum of a particle is conventionally represented by the letter p. It is the product of two quantities, the particle's mass (represented by the letter m) and its velocity (v):

The unit of momentum is the product of the units of mass and velocity. In SI units, if the mass is in kilograms and the velocity is in meters per second then the momentum is in kilogram meters per second (kg
f2 uc0u8901 
f0 m/s). In cgs units, if the mass is in grams and the velocity in centimeters per second, then the momentum is in gram centimeters per second (g
f2 uc0u8901 
f0 cm/s).

Being a vector, momentum has magnitude and direction. For example, a 1 kg model airplane, traveling due north at 1 m/s in straight and level flight, has a momentum of 1 kg
f2 uc0u8901 
f0 m/s due north measured with reference to the ground.

The momentum of a system of particles is the vector sum of their momenta. If two particles have respective masses m1 and m2, and velocities v1 and v2, the total momentum is

The momenta of more than two particles can be added more generally with the following:

A system of particles has a center of mass, a point determined by the weighted sum of their positions:

If one or more of the particles is moving, the center of mass of the system will generally be moving as well (unless the system is in pure rotation around it). If the total mass of the particles is , and the center of mass is moving at velocity vcm, the momentum of the system is:

This is known as Euler's first law.

If the net force F applied to a particle is constant, and is applied for a time interval uc0u916 t, the momentum of the particle changes by an amount

In differential form, this is Newton's second law; the rate of change of the momentum of a particle is equal to the instantaneous force F acting on it,

If the net force experienced by a particle changes as a function of time, F(t), the change in momentum (or impulse J) between times t1 and t2 is

Impulse is measured in the derived units of the newton second (1 N
f2 uc0u8901 
f0 s = 1 kg
f2 uc0u8901 
f0 m/s) or dyne second (1 dyne
f2 uc0u8901 
f0 s = 1 g
f2 uc0u8901 
f0 cm/s)

Under the assumption of constant mass m, it is equivalent to write

hence the net force is equal to the mass of the particle times its acceleration.

Example: A model airplane of mass 1 kg accelerates from rest to a velocity of 6 m/s due north in 2 s. The net force required to produce this acceleration is 3 newtons due north. The change in momentum is 6 kg
f2 uc0u8901 
f0 m/s due north. The rate of change of momentum is 3 (kg
f2 uc0u8901 
f0 m/s)/s due north which is numerically equivalent to 3 newtons.

In a closed system (one that does not exchange any matter with its surroundings and is not acted on by external forces) the total momentum remains constant. This fact, known as the law of conservation of momentum, is implied by Newton's laws of motion. Suppose, for example, that two particles interact. As explained by the third law, the forces between them are equal in magnitude but opposite in direction. If the particles are numbered 1 and 2, the second law states that F1 = 
dp1
/
dt
 and F2 = 
dp2
/
dt
. Therefore,

with the negative sign indicating that the forces oppose. Equivalently,

If the velocities of the particles are u1 and u2 before the interaction, and afterwards they are v1 and v2, then

This law holds no matter how complicated the force is between particles. Similarly, if there are several particles, the momentum exchanged between each pair of particles adds to zero, so the total change in momentum is zero. This conservation law applies to all interactions, including collisions and separations caused by explosive forces. It can also be generalized to situations where Newton's laws do not hold, for example in the theory of relativity and in electrodynamics.

Momentum is a measurable quantity, and the measurement depends on the frame of reference. For example: if an aircraft of mass m kg is flying through the air at a speed of 50 m/s its momentum can be calculated to be 50m kg.m/s. If the aircraft is flying into a headwind of 5 m/s its speed relative to the surface of the Earth is only 45 m/s and its momentum can be calculated to be 45m kg.m/s. Both calculations are equally correct. In both frames of reference, any change in momentum will be found to be consistent with the relevant laws of physics.

Suppose a particle has position x in a stationary frame of reference. From the point of view of another frame of reference, moving at a uniform speed u, the position (represented by a primed coordinate) changes with time as

This is called a Galilean transformation. If the particle is moving at speed 
dx
/
dt
 = v in the first frame of reference, in the second, it is moving at speed

Since u does not change, the accelerations are the same:

Thus, momentum is conserved in both reference frames. Moreover, as long as the force has the same form, in both frames, Newton's second law is unchanged. Forces such as Newtonian gravity, which depend only on the scalar distance between objects, satisfy this criterion. This independence of reference frame is called Newtonian relativity or Galilean invariance.

A change of reference frame, can, often, simplify calculations of motion. For example, in a collision of two particles, a reference frame can be chosen, where, one particle begins at rest. Another, commonly used reference frame, is the center of mass frame '96 one that is moving with the center of mass. In this frame, the total momentum is zero.

If two particles, each of known momentum, collide and coalesce, the law of conservation of momentum can be used to determine the momentum of the coalesced body. If the outcome of the collision is that the two particles separate, the law is not sufficient to determine the momentum of each particle. If the momentum of one particle after the collision is known, the law can be used to determine the momentum of the other particle. Alternatively if the combined kinetic energy after the collision is known, the law can be used to determine the momentum of each particle after the collision. Kinetic energy is usually not conserved. If it is conserved, the collision is called an elastic collision; if not, it is an inelastic collision.

An elastic collision is one in which no kinetic energy is transformed into heat or some other form of energy. Perfectly elastic collisions can occur when the objects do not touch each other, as for example in atomic or nuclear scattering where electric repulsion keeps the objects apart. A slingshot maneuver of a satellite around a planet can also be viewed as a perfectly elastic collision. A collision between two pool balls is a good example of an almost totally elastic collision, due to their high rigidity, but when bodies come in contact there is always some dissipation.

A head-on elastic collision between two bodies can be represented by velocities in one dimension, along a line passing through the bodies. If the velocities are u1 and u2 before the collision and v1 and v2 after, the equations expressing conservation of momentum and kinetic energy are:

A change of reference frame can simplify analysis of a collision. For example, suppose there are two bodies of equal mass m, one stationary and one approaching the other at a speed v (as in the figure). The center of mass is moving at speed 
v
/
2
 and both bodies are moving towards it at speed 
v
/
2
. Because of the symmetry, after the collision both must be moving away from the center of mass at the same speed. Adding the speed of the center of mass to both, we find that the body that was moving is now stopped and the other is moving away at speed v. The bodies have exchanged their velocities. Regardless of the velocities of the bodies, a switch to the center of mass frame leads us to the same conclusion. Therefore, the final velocities are given by

In general, when the initial velocities are known, the final velocities are given by

If one body has much greater mass than the other, its velocity will be little affected by a collision while the other body will experience a large change.

In an inelastic collision, some of the kinetic energy of the colliding bodies is converted into other forms of energy (such as heat or sound). Examples include traffic collisions, in which the effect of loss of kinetic energy can be seen in the damage to the vehicles; electrons losing some of their energy to atoms (as in the Franck'96Hertz experiment); and particle accelerators in which the kinetic energy is converted into mass in the form of new particles.

In a perfectly inelastic collision (such as a bug hitting a windshield), both bodies have the same motion afterwards. A head-on inelastic collision between two bodies can be represented by velocities in one dimension, along a line passing through the bodies. If the velocities are u1 and u2 before the collision then in a perfectly inelastic collision both bodies will be travelling with velocity v after the collision. The equation expressing conservation of momentum is:

If one body is motionless to begin with (e.g. ), the equation for conservation of momentum is

so

In a different situation, if the frame of reference is moving at the final velocity such that , the objects would be brought to rest by a perfectly inelastic collision and 100% of the kinetic energy is converted to other forms of energy. In this instance the initial velocities of the bodies would be non-zero, or the bodies would have to be massless.

One measure of the inelasticity of the collision is the coefficient of restitution CR, defined as the ratio of relative velocity of separation to relative velocity of approach. In applying this measure to a ball bouncing from a solid surface, this can be easily measured using the following formula:

The momentum and energy equations also apply to the motions of objects that begin together and then move apart. For example, an explosion is the result of a chain reaction that transforms potential energy stored in chemical, mechanical, or nuclear form into kinetic energy, acoustic energy, and electromagnetic radiation. Rockets also make use of conservation of momentum: propellant is thrust outward, gaining momentum, and an equal and opposite momentum is imparted to the rocket.

Real motion has both direction and velocity and must be represented by a vector. In a coordinate system with x, y, z axes, velocity has components vx in the x-direction, vy in the y-direction, vz in the z-direction. The vector is represented by a boldface symbol:

Similarly, the momentum is a vector quantity and is represented by a boldface symbol:

The equations in the previous sections, work in vector form if the scalars p and v are replaced by vectors p and v. Each vector equation represents three scalar equations. For example,

represents three equations:

The kinetic energy equations are exceptions to the above replacement rule. The equations are still one-dimensional, but each scalar represents the magnitude of the vector, for example,

Each vector equation represents three scalar equations. Often coordinates can be chosen so that only two components are needed, as in the figure. Each component can be obtained separately and the results combined to produce a vector result.

A simple construction involving the center of mass frame can be used to show that if a stationary elastic sphere is struck by a moving sphere, the two will head off at right angles after the collision (as in the figure).

The concept of momentum plays a fundamental role in explaining the behavior of variable-mass objects such as a rocket ejecting fuel or a star accreting gas. In analyzing such an object, one treats the object's mass as a function that varies with time: m(t). The momentum of the object at time t is therefore p(t) = m(t)v(t). One might then try to invoke Newton's second law of motion by saying that the external force F on the object is related to its momentum p(t) by F = 
dp
/
dt
, but this is incorrect, as is the related expression found by applying the product rule to 
d(mv)
/
dt
:

(incorrect)
This equation does not correctly describe the motion of variable-mass objects. The correct equation is

where u is the velocity of the ejected/accreted mass as seen in the object's rest frame. This is distinct from v, which is the velocity of the object itself as seen in an inertial frame.

This equation is derived by keeping track of both the momentum of the object as well as the momentum of the ejected/accreted mass (dm). When considered together, the object and the mass (dm) constitute a closed system in which total momentum is conserved.

Newtonian physics assumes that absolute time and space exist outside of any observer; this gives rise to Galilean invariance. It also results in a prediction that the speed of light can vary from one reference frame to another. This is contrary to observation. In the special theory of relativity, Einstein keeps the postulate that the equations of motion do not depend on the reference frame, but assumes that the speed of light c is invariant. As a result, position and time in two reference frames are related by the Lorentz transformation instead of the Galilean transformation.

Consider, for example, one reference frame moving relative to another at velocity v in the x direction. The Galilean transformation gives the coordinates of the moving frame as

while the Lorentz transformation gives

where uc0u947  is the Lorentz factor:

Newton's second law, with mass fixed, is not invariant under a Lorentz transformation. However, it can be made invariant by making the inertial mass m of an object a function of velocity:

m0 is the object's invariant mass.

The modified momentum,

obeys Newton's second law:

Within the domain of classical mechanics, relativistic momentum closely approximates Newtonian momentum: at low velocity, uc0u947 m0v is approximately equal to m0v, the Newtonian expression for momentum.

In the theory of special relativity, physical quantities are expressed in terms of four-vectors that include time as a fourth coordinate along with the three space coordinates. These vectors are generally represented by capital letters, for example R for position. The expression for the four-momentum depends on how the coordinates are expressed. Time may be given in its normal units or multiplied by the speed of light so that all the components of the four-vector have dimensions of length. If the latter scaling is used, an interval of proper time, uc0u964 , defined by

is invariant under Lorentz transformations (in this expression and in what follows the (+ uc0u8722  u8722  u8722 ) metric signature has been used, different authors use different conventions). Mathematically this invariance can be ensured in one of two ways: by treating the four-vectors as Euclidean vectors and multiplying time by u8730 u8722 1; or by keeping time a real quantity and embedding the vectors in a Minkowski space. In a Minkowski space, the scalar product of two four-vectors U = (U0, U1, U2, U3) and V = (V0, V1, V2, V3) is defined as

In all the coordinate systems, the (contravariant) relativistic four-velocity is defined by

and the (contravariant) four-momentum is

where m0 is the invariant mass. If R = (ct, x, y, z) (in Minkowski space), then

Using Einstein's mass-energy equivalence, E = mc2, this can be rewritten as

Thus, conservation of four-momentum is Lorentz-invariant and implies conservation of both mass and energy.

The magnitude of the momentum four-vector is equal to m0c:

and is invariant across all reference frames.

The relativistic energy'96momentum relationship holds even for massless particles such as photons; by setting m0 = 0 it follows that

In a game of relativistic "billiards", if a stationary particle is hit by a moving particle in an elastic collision, the paths formed by the two afterwards will form an acute angle. This is unlike the non-relativistic case where they travel at right angles.

The four-momentum of a planar wave can be related to a wave four-vector

For a particle, the relationship between temporal components, E = uc0u295  u969 , is the Planck'96Einstein relation, and the relation between spatial components, p = u295  k, describes a de Broglie matter wave.

Newton's laws can be difficult to apply to many kinds of motion because the motion is limited by constraints. For example, a bead on an abacus is constrained to move along its wire and a pendulum bob is constrained to swing at a fixed distance from the pivot. Many such constraints can be incorporated by changing the normal Cartesian coordinates to a set of generalized coordinates that may be fewer in number. Refined mathematical methods have been developed for solving mechanics problems in generalized coordinates. They introduce a generalized momentum, also known as the canonical or conjugate momentum, that extends the concepts of both linear momentum and angular momentum. To distinguish it from generalized momentum, the product of mass and velocity is also referred to as mechanical, kinetic or kinematic momentum. The two main methods are described below.

In Lagrangian mechanics, a Lagrangian is defined as the difference between the kinetic energy T and the potential energy V:

If the generalized coordinates are represented as a vector q = (q1, q2, ... , qN) and time differentiation is represented by a dot over the variable, then the equations of motion (known as the Lagrange or Euler'96Lagrange equations) are a set of N equations:

If a coordinate qi is not a Cartesian coordinate, the associated generalized momentum component pi does not necessarily have the dimensions of linear momentum. Even if qi is a Cartesian coordinate, pi will not be the same as the mechanical momentum if the potential depends on velocity. Some sources represent the kinematic momentum by the symbol uc0u928 .

In this mathematical framework, a generalized momentum is associated with the generalized coordinates. Its components are defined as

Each component pj is said to be the conjugate momentum for the coordinate qj.

Now if a given coordinate qi does not appear in the Lagrangian (although its time derivative might appear), then

This is the generalization of the conservation of momentum.

Even if the generalized coordinates are just the ordinary spatial coordinates, the conjugate momenta are not necessarily the ordinary momentum coordinates. An example is found in the section on electromagnetism.

In Hamiltonian mechanics, the Lagrangian (a function of generalized coordinates and their derivatives) is replaced by a Hamiltonian that is a function of generalized coordinates and momentum. The Hamiltonian is defined as

where the momentum is obtained by differentiating the Lagrangian as above. The Hamiltonian equations of motion are

As in Lagrangian mechanics, if a generalized coordinate does not appear in the Hamiltonian, its conjugate momentum component is conserved.

Conservation of momentum is a mathematical consequence of the homogeneity (shift symmetry) of space (position in space is the canonical conjugate quantity to momentum). That is, conservation of momentum is a consequence of the fact that the laws of physics do not depend on position; this is a special case of Noether's theorem. For systems that do not have this symmetry, it may not be possible to define conservation of momentum. Examples where conservation of momentum does not apply include curved spacetimes in general relativity or time crystals in condensed matter physics.

In Maxwell's equations, the forces between particles are mediated by electric and magnetic fields. The electromagnetic force (Lorentz force) on a particle with charge q due to a combination of electric field E and magnetic field B is

(in SI units). It has an electric potential uc0u966 (r, t) and magnetic vector potential A(r, t). In the non-relativistic regime, its generalized momentum is

while in relativistic mechanics this becomes

The quantity is sometimes called the potential momentum. It is the momentum due to the interaction of the particle with the electromagnetic fields. The name is an analogy with the potential energy , which is the energy due to the interaction of the particle with the electromagnetic fields. These quantities form a four-vector, so the analogy is consistent; besides, the concept of potential momentum is important in explaining the so-called hidden-momentum of the electromagnetic fields

In Newtonian mechanics, the law of conservation of momentum can be derived from the law of action and reaction, which states that every force has a reciprocating equal and opposite force. Under some circumstances, moving charged particles can exert forces on each other in non-opposite directions. Nevertheless, the combined momentum of the particles and the electromagnetic field is conserved.

The Lorentz force imparts a momentum to the particle, so by Newton's second law the particle must impart a momentum to the electromagnetic fields.

In a vacuum, the momentum per unit volume is

where uc0u956 0 is the vacuum permeability and c is the speed of light. The momentum density is proportional to the Poynting vector S which gives the directional rate of energy transfer per unit area:

If momentum is to be conserved over the volume V over a region Q, changes in the momentum of matter through the Lorentz force must be balanced by changes in the momentum of the electromagnetic field and outflow of momentum. If Pmech is the momentum of all the particles in Q, and the particles are treated as a continuum, then Newton's second law gives

The electromagnetic momentum is

and the equation for conservation of each component i of the momentum is

The term on the right is an integral over the surface area uc0u931  of the surface u963  representing momentum flow into and out of the volume, and nj is a component of the surface normal of S. The quantity Tij is called the Maxwell stress tensor, defined as

The above results are for the microscopic Maxwell equations, applicable to electromagnetic forces in a vacuum (or on a very small scale in media). It is more difficult to define momentum density in media because the division into electromagnetic and mechanical is arbitrary. The definition of electromagnetic momentum density is modified to

where the H-field H is related to the B-field and the magnetization M by

The electromagnetic stress tensor depends on the properties of the media.

In quantum mechanics, momentum is defined as a self-adjoint operator on the wave function. The Heisenberg uncertainty principle defines limits on how accurately the momentum and position of a single observable system can be known at once. In quantum mechanics, position and momentum are conjugate variables.

For a single particle described in the position basis the momentum operator can be written as

where uc0u8711  is the gradient operator, u295  is the reduced Planck constant, and i is the imaginary unit. This is a commonly encountered form of the momentum operator, though the momentum operator in other bases can take other forms. For example, in momentum space the momentum operator is represented as

where the operator p acting on a wave function uc0u968 (p) yields that wave function multiplied by the value p, in an analogous fashion to the way that the position operator acting on a wave function u968 (x) yields that wave function multiplied by the value x.

For both massive and massless objects, relativistic momentum is related to the phase constant by

Electromagnetic radiation (including visible light, ultraviolet light, and radio waves) is carried by photons. Even though photons (the particle aspect of light) have no mass, they still carry momentum. This leads to applications such as the solar sail. The calculation of the momentum of light within dielectric media is somewhat controversial (see Abraham'96Minkowski controversy).

In fields such as fluid dynamics and solid mechanics, it is not feasible to follow the motion of individual atoms or molecules. Instead, the materials must be approximated by a continuum in which there is a particle or fluid parcel at each point that is assigned the average of the properties of atoms in a small region nearby. In particular, it has a density uc0u961  and velocity v that depend on time t and position r. The momentum per unit volume is u961 v.

Consider a column of water in hydrostatic equilibrium. All the forces on the water are in balance and the water is motionless. On any given drop of water, two forces are balanced. The first is gravity, which acts directly on each atom and molecule inside. The gravitational force per unit volume is uc0u961 g, where g is the gravitational acceleration. The second force is the sum of all the forces exerted on its surface by the surrounding water. The force from below is greater than the force from above by just the amount needed to balance gravity. The normal force per unit area is the pressure p. The average force per unit volume inside the droplet is the gradient of the pressure, so the force balance equation is

If the forces are not balanced, the droplet accelerates. This acceleration is not simply the partial derivative 
uc0u8706 v
/
uc0u8706 t
 because the fluid in a given volume changes with time. Instead, the material derivative is needed:

Applied to any physical quantity, the material derivative includes the rate of change at a point and the changes due to advection as fluid is carried past the point. Per unit volume, the rate of change in momentum is equal to uc0u961 
Dv
/
Dt
. This is equal to the net force on the droplet.

Forces that can change the momentum of a droplet include the gradient of the pressure and gravity, as above. In addition, surface forces can deform the droplet. In the simplest case, a shear stress uc0u964 , exerted by a force parallel to the surface of the droplet, is proportional to the rate of deformation or strain rate. Such a shear stress occurs if the fluid has a velocity gradient because the fluid is moving faster on one side than another. If the speed in the x direction varies with z, the tangential force in direction x per unit area normal to the z direction is

where uc0u956  is the viscosity. This is also a flux, or flow per unit area, of x-momentum through the surface.

Including the effect of viscosity, the momentum balance equations for the incompressible flow of a Newtonian fluid are

These are known as the Navier'96Stokes equations.

The momentum balance equations can be extended to more general materials, including solids. For each surface with normal in direction i and force in direction j, there is a stress component uc0u963 ij. The nine components make up the Cauchy stress tensor u963 , which includes both pressure and shear. The local conservation of momentum is expressed by the Cauchy momentum equation:

where f is the body force.

The Cauchy momentum equation is broadly applicable to deformations of solids and liquids. The relationship between the stresses and the strain rate depends on the properties of the material (see Types of viscosity).

A disturbance in a medium gives rise to oscillations, or waves, that propagate away from their source. In a fluid, small changes in pressure p can often be described by the acoustic wave equation:

where c is the speed of sound. In a solid, similar equations can be obtained for propagation of pressure (P-waves) and shear (S-waves).

The flux, or transport per unit area, of a momentum component uc0u961 vj by a velocity vi is equal to u961  vjvj. In the linear approximation that leads to the above acoustic equation, the time average of this flux is zero. However, nonlinear effects can give rise to a nonzero average. It is possible for momentum flux to occur even though the wave itself does not have a mean momentum.

In about 530 AD, working in Alexandria, Byzantine philosopher John Philoponus developed a concept of momentum in his commentary to Aristotle's Physics. Aristotle claimed that everything that is moving must be kept moving by something. For example, a thrown ball must be kept moving by motions of the air. Most writers continued to accept Aristotle's theory until the time of Galileo, but a few were skeptical. Philoponus pointed out the absurdity in Aristotle's claim that motion of an object is promoted by the same air that is resisting its passage. He proposed instead that an impetus was imparted to the object in the act of throwing it. Ibn Suc0u299 nu257  (also known by his Latinized name Avicenna) read Philoponus and published his own theory of motion in The Book of Healing in 1020. He agreed that an impetus is imparted to a projectile by the thrower; but unlike Philoponus, who believed that it was a temporary virtue that would decline even in a vacuum, he viewed it as a persistent, requiring external forces such as air resistance to dissipate it. The work of Philoponus, and possibly that of Ibn Su299 nu257 , was read and refined by the European philosophers Peter Olivi and Jean Buridan. Buridan, who in about 1350 was made rector of the University of Paris, referred to impetus being proportional to the weight times the speed. Moreover, Buridan's theory was different from his predecessor's in that he did not consider impetus to be self-dissipating, asserting that a body would be arrested by the forces of air resistance and gravity which might be opposing its impetus.

Ren'e9 Descartes believed that the total "quantity of motion" (Latin: quantitas motus) in the universe is conserved, where the quantity of motion is understood as the product of size and speed. This should not be read as a statement of the modern law of momentum, since he had no concept of mass as distinct from weight and size, and more important, he believed that it is speed rather than velocity that is conserved. So for Descartes if a moving object were to bounce off a surface, changing its direction but not its speed, there would be no change in its quantity of motion. Galileo, in his Two New Sciences, used the Italian word impeto to similarly describe Descartes' quantity of motion.

Leibniz, in his "Discourse on Metaphysics", gave an argument against Descartes' construction of the conservation of the "quantity of motion" using an example of dropping blocks of different sizes different distances. He points out that force is conserved but quantity of motion, construed as the product of size and speed of an object, is not conserved.

Christiaan Huygens concluded quite early that Descartes's laws for the elastic collision of two bodies must be wrong, and he formulated the correct laws. An important step was his recognition of the Galilean invariance of the problems. His views then took many years to be circulated. He passed them on in person to William Brouncker and Christopher Wren in London, in 1661. What Spinoza wrote to Henry Oldenburg about them, in 1666 which was during the Second Anglo-Dutch War, was guarded. Huygens had actually worked them out in a manuscript De motu corporum ex percussione in the period 1652'966. The war ended in 1667, and Huygens announced his results to the Royal Society in 1668. He published them in the Journal des s'e7avans in 1669.

The first correct statement of the law of conservation of momentum was by English mathematician John Wallis in his 1670 work, Mechanica sive De Motu, Tractatus Geometricus: "the initial state of the body, either of rest or of motion, will persist" and "If the force is greater than the resistance, motion will result". Wallis used momentum for quantity of motion, and vis for force. Newton's Philosophi'e6 Naturalis Principia Mathematica, when it was first published in 1687, showed a similar casting around for words to use for the mathematical momentum. His Definition II defines quantitas motus, "quantity of motion", as "arising from the velocity and quantity of matter conjointly", which identifies it as momentum. Thus when in Law II he refers to mutatio motus, "change of motion", being proportional to the force impressed, he is generally taken to mean momentum and not motion. It remained only to assign a standard term to the quantity of motion. The first use of "momentum" in its proper mathematical sense is not clear but by the time of Jennings's Miscellanea in 1721, five years before the final edition of Newton's Principia Mathematica, momentum M or "quantity of motion" was being defined for students as "a rectangle", the product of Q and V, where Q is "quantity of material" and V is "velocity", 

In 1728, the Cyclopedia states:

"The Momentum, Impetus, or Quantity of Motion of any Body, is the Factum [i.e., product] of its Velocity, (or the Space it moves in a given Time, see Motion) multiplied into its Mass."

An airfoil (American English) or aerofoil (British English) is the cross-sectional shape of an object whose motion through a gas is capable of generating significant lift, such as a wing, a sail, or the blades of propeller, rotor, or turbine.

A solid body moving through a fluid produces an aerodynamic force. The component of this force perpendicular to the relative freestream velocity is called lift. The component parallel to the relative freestream velocity is called drag. An airfoil is a streamlined shape that is capable of generating significantly more lift than drag. Airfoils can be designed for use at different speeds by modifying their geometry: those for subsonic flight generally have a rounded leading edge, while those designed for supersonic flight tend to be slimmer with a sharp leading edge. All have a sharp trailing edge. Foils of similar function designed with water as the working fluid are called hydrofoils.

The lift on an airfoil is primarily the result of its angle of attack. When oriented at a suitable angle, the airfoil deflects the oncoming air (for fixed-wing aircraft, a downward force), resulting in a force on the airfoil in the direction opposite to the deflection. This force is known as aerodynamic force and can be resolved into two components: lift and drag. Most foil shapes require a positive angle of attack to generate lift, but cambered airfoils can generate lift at zero angle of attack. This "turning" of the air in the vicinity of the airfoil creates curved streamlines, resulting in lower pressure on one side and higher pressure on the other. This pressure difference is accompanied by a velocity difference, via Bernoulli's principle, so the resulting flowfield about the airfoil has a higher average velocity on the upper surface than on the lower surface. In some situations (e.g. inviscid potential flow) the lift force can be related directly to the average top/bottom velocity difference without computing the pressure by using the concept of circulation and the Kutta'96Joukowski theorem.

The wings and stabilizers of fixed-wing aircraft, as well as helicopter rotor blades, are built with airfoil-shaped cross sections. Airfoils are also found in propellers, fans, compressors and turbines. Sails are also airfoils, and the underwater surfaces of sailboats, such as the centerboard, rudder, and keel, are similar in cross-section and operate on the same principles as airfoils. Swimming and flying creatures and even many plants and sessile organisms employ airfoils/hydrofoils: common examples being bird wings, the bodies of fish, and the shape of sand dollars. An airfoil-shaped wing can create downforce on an automobile or other motor vehicle, improving traction.

When the wind is obstructed by an object such as a flat plate, a building, or the deck of a bridge, the object will experience drag and also an aerodynamic force perpendicular to the wind. This does not mean the object qualifies as an airfoil. Airfoils are highly-efficient lifting shapes, able to generate more lift than similarly sized flat plates of the same area, and able to generate lift with significantly less drag. Airfoils are used in the design of aircraft, propellers, rotor blades, wind turbines and other applications of aeronautical engineering.

A lift and drag curve obtained in wind tunnel testing is shown on the right. The curve represents an airfoil with a positive camber so some lift is produced at zero angle of attack. With increased angle of attack, lift increases in a roughly linear relation, called the slope of the lift curve. At about 18 degrees this airfoil stalls, and lift falls off quickly beyond that. The drop in lift can be explained by the action of the upper-surface boundary layer, which separates and greatly thickens over the upper surface at and past the stall angle. The thickened boundary layer's displacement thickness changes the airfoil's effective shape, in particular it reduces its effective camber, which modifies the overall flow field so as to reduce the circulation and the lift. The thicker boundary layer also causes a large increase in pressure drag, so that the overall drag increases sharply near and past the stall point.

Airfoil design is a major facet of aerodynamics. Various airfoils serve different flight regimes. Asymmetric airfoils can generate lift at zero angle of attack, while a symmetric airfoil may better suit frequent inverted flight as in an aerobatic airplane. In the region of the ailerons and near a wingtip a symmetric airfoil can be used to increase the range of angles of attack to avoid spin'96stall. Thus a large range of angles can be used without boundary layer separation. Subsonic airfoils have a round leading edge, which is naturally insensitive to the angle of attack. The cross section is not strictly circular, however: the radius of curvature is increased before the wing achieves maximum thickness to minimize the chance of boundary layer separation. This elongates the wing and moves the point of maximum thickness back from the leading edge.

Supersonic airfoils are much more angular in shape and can have a very sharp leading edge, which is very sensitive to angle of attack. A supercritical airfoil has its maximum thickness close to the leading edge to have a lot of length to slowly shock the supersonic flow back to subsonic speeds. Generally such transonic airfoils and also the supersonic airfoils have a low camber to reduce drag divergence. Modern aircraft wings may have different airfoil sections along the wing span, each one optimized for the conditions in each section of the wing.

Movable high-lift devices, flaps and sometimes slats, are fitted to airfoils on almost every aircraft. A trailing edge flap acts similarly to an aileron; however, it, as opposed to an aileron, can be retracted partially into the wing if not used.

A laminar flow wing has a maximum thickness in the middle camber line. Analyzing the Navier'96Stokes equations in the linear regime shows that a negative pressure gradient along the flow has the same effect as reducing the speed. So with the maximum camber in the middle, maintaining a laminar flow over a larger percentage of the wing at a higher cruising speed is possible. However, some surface contamination will disrupt the laminar flow, making it turbulent. For example, with rain on the wing, the flow will be turbulent. Under certain conditions, insect debris on the wing will cause the loss of small regions of laminar flow as well. Before NASA's research in the 1970s and 1980s the aircraft design community understood from application attempts in the WW II era that laminar flow wing designs were not practical using common manufacturing tolerances and surface imperfections. That belief changed after new manufacturing methods were developed with composite materials (e.g. laminar-flow airfoils developed by Professor Franz Wortmann for use with wings made of fibre-reinforced plastic). Machined metal methods were also introduced. NASA's research in the 1980s revealed the practicality and usefulness of laminar flow wing designs and opened the way for laminar-flow applications on modern practical aircraft surfaces, from subsonic general aviation aircraft to transonic large transport aircraft, to supersonic designs.

Schemes have been devised to define airfoils '96 an example is the NACA system. Various airfoil generation systems are also used. An example of a general purpose airfoil that finds wide application, and pre'96dates the NACA system, is the Clark-Y. Today, airfoils can be designed for specific functions by the use of computer programs.

The various terms related to airfoils are defined below:

The suction surface (a.k.a. upper surface) is generally associated with higher velocity and lower static pressure.
The pressure surface (a.k.a. lower surface) has a comparatively higher static pressure than the suction surface. The pressure gradient between these two surfaces contributes to the lift force generated for a given airfoil.
The geometry of the airfoil is described with a variety of terms :

The leading edge is the point at the front of the airfoil that has maximum curvature (minimum radius).
The trailing edge is defined similarly as the point of maximum curvature at the rear of the airfoil.
The chord line is the straight line connecting leading and trailing edges. The chord length, or simply chord, , is the length of the chord line. That is the reference dimension of the airfoil section.
The shape of the airfoil is defined using the following geometrical parameters:

The mean camber line or mean line is the locus of points midway between the upper and lower surfaces. Its shape depends on the thickness distribution along the chord;
The thickness of an airfoil varies along the chord. It may be measured in either of two ways:
Thickness measured perpendicular to the camber line. This is sometimes described as the "American convention";
Thickness measured perpendicular to the chord line. This is sometimes described as the "British convention".
Some important parameters to describe an airfoil's shape are its camber and its thickness. For example, an airfoil of the NACA 4-digit series such as the NACA 2415 (to be read as 2 '96 4 '96 15) describes an airfoil with a camber of 0.02 chord located at 0.40 chord, with 0.15 chord of maximum thickness.

Finally, important concepts used to describe the airfoil's behaviour when moving through a fluid are:

The aerodynamic center, which is the chord-wise location about which the pitching moment is independent of the lift coefficient and the angle of attack.
The center of pressure, which is the chord-wise location about which the pitching moment is momentarily zero. On a cambered airfoil, the center of pressure is not a fixed location as it moves in response to changes in angle of attack and lift coefficient.
Thin airfoil theory is a simple theory of airfoils that relates angle of attack to lift for incompressible, inviscid flows. It was devised by German mathematician Max Munk and further refined by British aerodynamicist Hermann Glauert and others in the 1920s. The theory idealizes the flow around an airfoil as two-dimensional flow around a thin airfoil. It can be imagined as addressing an airfoil of zero thickness and infinite wingspan.

Thin airfoil theory was particularly notable in its day because it provided a sound theoretical basis for the following important properties of airfoils in two-dimensional inviscid flow:

on a symmetric airfoil, the center of pressure and aerodynamic center are coincident and lie exactly one quarter of the chord behind the leading edge.
on a cambered airfoil, the aerodynamic center lies exactly one quarter of the chord behind the leading edge, but the position of the center of pressure moves when the angle of attack changes.
the slope of the lift coefficient versus angle of attack line is units per radian.
As a consequence of (3), the section lift coefficient of a symmetric airfoil of infinite wingspan is:

where is the section lift coefficient,
is the angle of attack in radians, measured relative to the chord line.
(The above expression is also applicable to a cambered airfoil where is the angle of attack measured relative to the zero-lift line instead of the chord line.)

Also as a consequence of (3), the section lift coefficient of a cambered airfoil of infinite wingspan is:

where is the section lift coefficient when the angle of attack is zero.
Thin airfoil theory does not account for the stall of the airfoil, which usually occurs at an angle of attack between 10'b0 and 15'b0 for typical airfoils. In the mid-late 2000s, however, a theory predicting the onset of leading-edge stall was proposed by Wallace J. Morris II in his doctoral thesis. Morris's subsequent refinements contain the details on the current state of theoretical knowledge on the leading-edge stall phenomenon. Morris's theory predicts the critical angle of attack for leading-edge stall onset as the condition at which a global separation zone is predicted in the solution for the inner flow. Morris's theory demonstrates that a subsonic flow about a thin airfoil can be described in terms of an outer region, around most of the airfoil chord, and an inner region, around the nose, that asymptotically match each other. As the flow in the outer region is dominated by classical thin airfoil theory, Morris's equations exhibit many components of thin airfoil theory.

The airfoil is modeled as a thin lifting mean-line (camber line). The mean-line, y(x), is considered to produce a distribution of vorticity along the line, s. By the Kutta condition, the vorticity is zero at the trailing edge. Since the airfoil is thin, x (chord position) can be used instead of s, and all angles can be approximated as small.

From the Biot'96Savart law, this vorticity produces a flow field where

is the location where induced velocity is produced, is the location of the vortex element producing the velocity and is the chord length of the airfoil.

Since there is no flow normal to the curved surface of the airfoil, balances that from the component of main flow , which is locally normal to the plate '96 the main flow is locally inclined to the plate by an angle . That is:

This integral equation can be solved for , after replacing x by

as a Fourier series in with a modified lead term .

That is

(These terms are known as the Glauert integral).

The coefficients are given by

and

By the Kutta'96Joukowski theorem, the total lift force F is proportional to

and its moment M about the leading edge to

The calculated Lift coefficient depends only on the first two terms of the Fourier series, as

The moment M about the leading edge depends only on and , as

The moment about the 1/4 chord point will thus be

From this it follows that the center of pressure is aft of the 'quarter-chord' point 0.25uc0u8201 c, by

The aerodynamic center, AC, is at the quarter-chord point. The AC is where the pitching moment Muc0u8242  does not vary with a change in lift coefficient, i.e.,

A reaction control system (RCS) is a spacecraft system that uses thrusters and reaction control wheels to provide attitude control, and sometimes propulsion. Use of diverted engine thrust to provide stable attitude control of a short-or-vertical takeoff and landing aircraft below conventional winged flight speeds, such as with the Harrier "jump jet", may also be referred to as a reaction control system.

An RCS is capable of providing small amounts of thrust in any desired direction or combination of directions. An RCS is also capable of providing torque to allow control of rotation (roll, pitch, and yaw).

Reaction control systems often use combinations of large and small (vernier) thrusters, to allow different levels of response.

Spacecraft reaction control systems are used for:

attitude control during re-entry;
stationkeeping in orbit;
close maneuvering during docking procedures;
control of orientation, or "pointing the nose" of the craft;
a backup means of deorbiting;
ullage motors to prime the fuel system for a main engine burn.
Because spacecraft only contain a finite amount of fuel and there is little chance to refill them, alternative reaction control systems have been developed so that fuel can be conserved. For stationkeeping, some spacecraft (particularly those in geosynchronous orbit) use high-specific impulse engines such as arcjets, ion thrusters, or Hall effect thrusters. To control orientation, a few spacecraft, including the ISS, use momentum wheels which spin to control rotational rates on the vehicle.

The Mercury space capsule and Gemini reentry module both used groupings of nozzles to provide attitude control. The thrusters were located off their center of mass, thus providing a torque to rotate the capsule. The Gemini capsule was also capable of adjusting its reentry course by rolling, which directed its off-center lifting force. The Mercury thrusters used a hydrogen peroxide monopropellant which turned to steam when forced through a tungsten screen, and the Gemini thrusters used hypergolic mono-methyl hydrazine fuel oxidized with nitrogen tetroxide.

The Gemini spacecraft was also equipped with a hypergolic Orbit Attitude and Maneuvering System, which made it the first crewed spacecraft with translation as well as rotation capability. In-orbit attitude control was achieved by firing pairs of eight 25-pound-force (110 N) thrusters located around the circumference of its adapter module at the extreme aft end. Lateral translation control was provided by four 100-pound-force (440 N) thrusters around the circumference at the forward end of the adaptor module (close to the spacecraft's center of mass). Two forward-pointing 85-pound-force (380 N) thrusters at the same location, provided aft translation, and two 100-pound-force (440 N) thrusters located in the aft end of the adapter module provided forward thrust, which could be used to change the craft's orbit. The Gemini reentry module also had a separate Reentry Control System of sixteen thrusters located at the base of its nose, to provide rotational control during reentry.

The Apollo Command Module had a set of twelve hypergolic thrusters for attitude control, and directional reentry control similar to Gemini.

The Apollo Service Module and Lunar Module each had a set of sixteen R-4D hypergolic thrusters, grouped into external clusters of four, to provide both translation and attitude control. The clusters were located near the craft's average centers of mass, and were fired in pairs in opposite directions for attitude control.

A pair of translation thrusters are located at the rear of the Soyuz spacecraft; the counter-acting thrusters are similarly paired in the middle of the spacecraft (near the center of mass) pointing outwards and forward. These act in pairs to prevent the spacecraft from rotating. The thrusters for the lateral directions are mounted close to the center of mass of the spacecraft, in pairs as well.

The suborbital X-15 and a companion training aero-spacecraft, the NF-104 AST, both intended to travel to an altitude that rendered their aerodynamic control surfaces unusable, established a convention for locations for thrusters on winged vehicles not intended to dock in space; that is, those that only have attitude control thrusters. Those for pitch and yaw are located in the nose, forward of the cockpit, and replace a standard radar system. Those for roll are located at the wingtips. The X-20, which would have gone into orbit, continued this pattern.

Unlike these, the Space Shuttle Orbiter had many more thrusters, which were required to control vehicle attitude in both orbital flight and during the early part of atmospheric entry, as well as carry out rendezvous and docking maneuvers in orbit. Shuttle thrusters were grouped in the nose of the vehicle and on each of the two aft Orbital Maneuvering System pods. No nozzles interrupted the heat shield on the underside of the craft; instead, the nose RCS nozzles which control positive pitch were mounted on the side of the vehicle, and were canted downward. The downward-facing negative pitch thrusters were located in the OMS pods mounted in the tail/afterbody.

The International Space Station uses electrically powered control moment gyroscopes (CMG) for primary attitude control, with RCS thruster systems as backup and augmentation systems.

Gimbaled thrust is the system of thrust vectoring used in most rockets, including the Space Shuttle, the Saturn V lunar rockets, and the Falcon 9.

In a gimbaled thrust system, the engine or just the exhaust nozzle of the rocket can be swiveled on two axes (pitch and yaw ) from side to side. As the nozzle is moved, the direction of the thrust is changed relative to the center of gravity of the rocket.

The diagram illustrates three cases. The middle rocket shows the straight-line flight configuration in which the direction of thrust is along the center line of the rocket and through the center of gravity of the rocket. On the rocket at the left, the nozzle has been deflected to the left and the thrust line is now inclined to the rocket center line at an angle called the gimbal angle. Since the thrust no longer passes through the center of gravity, a torque is generated about the center of gravity and the nose of the rocket turns to the left. If the nozzle is gimbaled back along the center line, the rocket will move to the left. On the rocket at the right, the nozzle has been deflected to the right and the nose is moved to the right.

A reaction wheel (RW) is used primarily by spacecraft for three-axis attitude control, and does not require rockets or external applicators of torque. They provide a high pointing accuracy, and are particularly useful when the spacecraft must be rotated by very small amounts, such as keeping a telescope pointed at a star.

A reaction wheel is sometimes operated as (and referred to as) a momentum wheel, by operating it at a constant (or near-constant) rotation speed, to provide a satellite with a large amount of stored angular momentum. Doing so alters the spacecraft's rotational dynamics so that disturbance torques perpendicular to one axis of the satellite (the axis parallel to the wheel's spin axis) do not result directly in spacecraft angular motion about the same axis as the disturbance torque; instead, they result in (generally smaller) angular motion (precession) of that spacecraft axis about a perpendicular axis. This has the effect of tending to stabilize that spacecraft axis to point in a nearly-fixed direction, allowing for a less-complicated attitude control system. Satellites using this "momentum-bias" stabilization approach include SCISAT-1; by orienting the momentum wheel's axis to be parallel to the orbit-normal vector, this satellite is in a "pitch momentum bias" configuration.

A control moment gyroscope (CMG) is a related but different type of attitude actuator, generally consisting of a momentum wheel mounted in a one-axis or two-axis gimbal. When mounted to a rigid spacecraft, applying a constant torque to the wheel using one of the gimbal motors causes the spacecraft to develop a constant angular velocity about a perpendicular axis, thus allowing control of the spacecraft's pointing direction. CMGs are generally able to produce larger sustained torques than RWs with less motor heating, and are preferentially used in larger or more-agile (or both) spacecraft, including Skylab, Mir, and the International Space Station.

Reaction wheels are used to control the attitude of a satellite without the use of thrusters, which reduces the mass fraction needed for fuel.

They work by equipping the spacecraft with an electric motor attached to a flywheel, which, when its rotation speed is changed, causes the spacecraft to begin to counter-rotate proportionately through conservation of angular momentum. Reaction wheels can rotate a spacecraft only around its center of mass (see torque); they are not capable of moving the spacecraft from one place to another (see translational force).

For three-axis control, reaction wheels must be mounted along at least three directions, with extra wheels providing redundancy to the attitude control system. A redundant mounting configuration could consist of four wheels along tetrahedral axes, or a spare wheel carried in addition to a three axis configuration. Changes in speed (in either direction) are controlled electronically by computer. The strength of the materials used in a reaction wheel determine the speed at which the wheel would come apart, and therefore how much angular momentum it can store.

Since the reaction wheel is a small fraction of the spacecraft's total mass, easily controlled, temporary changes in its speed result in small changes in angle. The wheels therefore permit very precise changes in a spacecraft's attitude. For this reason, reaction wheels are often used to aim spacecraft carrying cameras or telescopes.

Over time, reaction wheels may build up enough stored momentum to exceed the maximum speed of the wheel, called saturation, which will need to be canceled. Designers therefore supplement reaction wheel systems with other attitude control mechanisms. In the presence of a magnetic field (as in low Earth orbit), a spacecraft can employ magnetorquers (better known as torque rods) to transfer angular momentum to Earth through its planetary magnetic field. In the absence of a magnetic field, the most efficient practice is to use either high-efficiency attitude jets such as ion thrusters, or small, lightweight solar sails placed in locations away from the spacecraft's center of mass, such as on solar cell arrays or projecting masts.

Beresheet was launched on a Falcon 9 rocket on 22 February 2019 1:45 UTC, with the goal of landing on the moon. Beresheet uses the low-energy transfer technique to save fuel. Since its fourth maneuver in its elliptical orbit, to prevent shakes when the amount of liquid fuel ran low, there was a need to use a reaction wheel.

The James Webb Space Telescope has six reaction wheels built by Rockwell Collins Deutschland.

LightSail 2 was launched on 25 June 2019, focused around the concept of a solar sail. LightSail 2 uses a reaction wheel system to change orientation by very small amounts, allowing it to receive different amounts of momentum from the light across the sail, resulting in a higher altitude.

The failure of one or more reaction wheels can cause a spacecraft to lose its ability to maintain attitude (orientation) and thus potentially cause a mission failure. Recent studies conclude that these failures can be correlated with space weather effects. These events probably caused failures by inducing electrostatic discharge in the steel ball bearings of Ithaco wheels, compromising the smoothness of the mechanism.

Two servicing missions to the Hubble Space Telescope have replaced a reaction wheel. In February 1997, the Second Servicing Mission (STS-82) replaced one after 'electrical anomalies', rather than any mechanical problem. Study of the returned mechanism provided a rare opportunity to study equipment that had undergone long-term service (seven years) in space, particularly for the effects of vacuum on lubricants. The lubricating compound was found to be in 'excellent condition'. In 2002, during Servicing Mission 3B (STS-109), astronauts from the shuttle Columbia replaced another reaction wheel. Neither of these wheels had failed and Hubble was designed with four redundant wheels, and maintained pointing ability so long as three were functional.

In 2004, during the mission of the Hayabusa spacecraft, an X-axis reaction wheel failed. The Y-axis wheel failed in 2005, causing the craft to rely on chemical thrusters to maintain attitude control.

From July 2012 to May 11, 2013, two out of the four reaction wheels in the Kepler telescope failed. This loss severely affected Kepler's ability to maintain a sufficiently precise orientation to continue its original mission. On August 15, 2013, engineers concluded that Kepler's reaction wheels cannot be recovered and that planet-searching using the transit method (measuring changes in star brightness caused by orbiting planets) could not continue. Although the failed reaction wheels still function, they are experiencing friction exceeding acceptable levels, and consequently hindering the ability of the telescope to properly orient itself. The Kepler telescope was returned to its "point rest state", a stable configuration that uses small amounts of thruster fuel to compensate for the failed reaction wheels, while the Kepler team considered alternative uses for Kepler that do not require the extreme accuracy in its orientation needed by the original mission. On May 16, 2014, NASA extended the Kepler mission to a new mission named K2, which uses Kepler differently, but allows it to continue searching for exoplanets. On October 30, 2018, NASA announced the end of the Kepler mission after it was determined that the fuel supply had been exhausted.

The NASA space probe Dawn had excess friction in one reaction wheel in June 2010. It was originally scheduled to depart Vesta and begin its two-and-a-half-year journey to Ceres on August 26, 2012; however, a problem with another of the spacecraft's reaction wheels forced Dawn to briefly delay its departure from Vesta's gravity until September 5, 2012, and it planned to use thruster jets instead of the reaction wheels during the three-year journey to Ceres. The loss of the reaction wheels limited the camera observations on the approach to Ceres.

On the evening of Tuesday, January 18, 2022, a possible failure of one of Swift Observatory's reaction wheels caused the mission control team to power off the suspected wheel, putting the observatory in safe mode as a precaution. This is the first time a reaction wheel has experienced a failure in Swift's 17 years of operations.

Thrust vectoring, also known as thrust vector control (TVC), is the ability of an aircraft, rocket, or other vehicle to manipulate the direction of the thrust from its engine(s) or motor(s) to control the attitude or angular velocity of the vehicle.

In rocketry and ballistic missiles that fly outside the atmosphere, aerodynamic control surfaces are ineffective, so thrust vectoring is the primary means of attitude control. Exhaust vanes and gimbaled engines were used in the 1930s by Robert Goddard.

For aircraft, the method was originally envisaged to provide upward vertical thrust as a means to give aircraft vertical (VTOL) or short (STOL) takeoff and landing ability. Subsequently, it was realized that using vectored thrust in combat situations enabled aircraft to perform various maneuvers not available to conventional-engined planes. To perform turns, aircraft that use no thrust vectoring must rely on aerodynamic control surfaces only, such as ailerons or elevator; aircraft with vectoring must still use control surfaces, but to a lesser extent.

In missile literature originating from Russian sources, thrust vectoring is often referred to as gas-dynamic steering or gas-dynamic control.

Nominally, the line of action of the thrust vector of a rocket nozzle passes through the vehicle's center of mass, generating zero net moment about the mass center. It is possible to generate pitch and yaw moments by deflecting the main rocket thrust vector so that it does not pass through the mass center. Because the line of action is generally oriented nearly parallel to the roll axis, roll control usually requires the use of two or more separately hinged nozzles or a separate system altogether, such as fins, or vanes in the exhaust plume of the rocket engine, deflecting the main thrust. Thrust vector control (TVC) is only possible when the propulsion system is creating thrust; separate mechanisms are required for attitude and flight path control during other stages of flight.

Thrust vectoring can be achieved by four basic means:

Gimbaled engine(s) or nozzle(s)
Reactive fluid injection
Auxiliary "Vernier" thrusters
Exhaust vanes, also known as jet vanes
Thrust vectoring for many liquid rockets is achieved by gimbaling the whole engine. This involves moving the entire combustion chamber and outer engine bell as on the Titan II's twin first-stage motors, or even the entire engine assembly including the related fuel and oxidizer pumps. The Saturn V and the Space Shuttle used gimbaled engines.

A later method developed for solid propellant ballistic missiles achieves thrust vectoring by deflecting only the nozzle of the rocket using electric actuators or hydraulic cylinders. The nozzle is attached to the missile via a ball joint with a hole in the center, or a flexible seal made of a thermally resistant material, the latter generally requiring more torque and a higher power actuation system. The Trident C4 and D5 systems are controlled via hydraulically actuated nozzle. The STS SRBs used gimbaled nozzles.

Another method of thrust vectoring used on solid propellant ballistic missiles is liquid injection, in which the rocket nozzle is fixed, but a fluid is introduced into the exhaust flow from injectors mounted around the aft end of the missile. If the liquid is injected on only one side of the missile, it modifies that side of the exhaust plume, resulting in different thrust on that side and an asymmetric net force on the missile. This was the control system used on the Minuteman II and the early SLBMs of the United States Navy.

An effect similar to thrust vectoring can be produced with multiple vernier thrusters, small auxiliary combustion chambers which lack their own turbopumps and can gimbal on one axis. These were used on the Atlas and R-7 missiles and are still used on the Soyuz rocket, which is descended from the R-7, but are seldom used on new designs due to their complexity and weight. These are distinct from reaction control system thrusters, which are fixed and independent rocket engines used for maneuvering in space.

One of the earliest methods of thrust vectoring in rocket engines was to place vanes in the engine's exhaust stream. These exhaust vanes or jet vanes allow the thrust to be deflected without moving any parts of the engine, but reduce the rocket's efficiency. They have the benefit of allowing roll control with only a single engine, which nozzle gimbaling does not. The V-2 used graphite exhaust vanes and aerodynamic vanes, as did the Redstone, derived from the V-2. The Sapphire and Nexo rockets of the amateur group Copenhagen Suborbitals provide a modern example of jet vanes. Jet vanes must be made of a refractory material or actively cooled to prevent them from melting. Sapphire used solid copper vanes for copper's high heat capacity and thermal conductivity, and Nexo used graphite for its high melting point, but unless actively cooled, jet vanes will undergo significant erosion. This, combined with jet vanes' inefficiency, mostly precludes their use in new rockets.

Some smaller sized atmospheric tactical missiles, such as the AIM-9X Sidewinder, eschew flight control surfaces and instead use mechanical vanes to deflect rocket motor exhaust to one side.

By using mechanical vanes to deflect the exhaust of the missile's rocket motor, a missile can steer itself even shortly after being launched (when the missile is moving slowly, before it has reached a high speed). This is because even though the missile is moving at a low speed, the rocket motor's exhaust has a high enough speed to provide sufficient forces on the mechanical vanes. Thus, thrust vectoring can reduce a missile's minimum range. For example, anti-tank missiles such as the Eryx and the PARS 3 LR use thrust vectoring for this reason.

Some other projectiles that use thrust-vectoring:

9M330
Strix mortar round uses twelve midsection lateral thruster rockets to provide terminal course corrections
AAD uses jet vanes
Astra (missile)
Akash (missile)
BrahMos
QRSAM uses jet vanes
MPATGM uses jet vanes
Barak 8 uses jet vanes
A-Darter uses jet vanes
ASRAAM uses jet vanes
R-73 (missile) uses jet vanes
HQ-9 uses jet vanes
PL-10 (ASR) uses jet vanes
MICA (missile) uses jet vanes
PARS 3 LR uses jet vanes
Aster missile family combines aerodynamic control and the direct thrust vector control called "PIF-PAF"
AIM-9X uses four jet vanes inside the exhaust, that move as the fins move.
9M96E uses a gas-dynamic control system enables maneuver at altitudes of up to 35km at forces of over 20g, which permits engagement of non-strategic ballistic missiles.
9K720 Iskander is controlled during the whole flight with gas-dynamic and aerodynamic control surfaces.
Dongfeng subclasses/JL-2/JL-3 ballistic missiles (allegedly fitted with TVC control)
Most currently operational vectored thrust aircraft use turbofans with rotating nozzles or vanes to deflect the exhaust stream. This method can successfully deflect thrust through as much as 90 degrees, relative to the aircraft centerline. However, the engine must be sized for vertical lift, rather than normal flight, which results in a weight penalty. Afterburning (or Plenum Chamber Burning, PCB, in the bypass stream) is difficult to incorporate and is impractical for take-off and landing thrust vectoring, because the very hot exhaust can damage runway surfaces. Without afterburning it is hard to reach supersonic flight speeds. A PCB engine, the Bristol Siddeley BS100, was cancelled in 1965.

Tiltrotor aircraft vector thrust via rotating turboprop engine nacelles. The mechanical complexities of this design are quite troublesome, including twisting flexible internal components and driveshaft power transfer between engines. Most current tiltrotor designs feature two rotors in a side-by-side configuration. If such a craft is flown in a way where it enters a vortex ring state, one of the rotors will always enter slightly before the other, causing the aircraft to perform a drastic and unplanned roll.

Thrust vectoring is also used as a control mechanism for airships. An early application was the British Army airship Delta, which first flew in 1912. It was later used on HMA (His Majesty's Airship) No. 9r, a British rigid airship that first flew in 1916 and the twin 1930s-era U.S. Navy rigid airships USS Akron and USS Macon that were used as airborne aircraft carriers, and a similar form of thrust vectoring is also particularly valuable today for the control of modern non-rigid airships. In this use, most of the load is usually supported by buoyancy and vectored thrust is used to control the motion of the aircraft. The first airship that used a control system based on pressurized air was Enrico Forlanini's Omnia Dir in 1930s.

A design for a jet incorporating thrust vectoring was submitted in 1949 to the British Air Ministry by Percy Walwyn; Walwyn's drawings are preserved at the National Aerospace Library at Farnborough. Official interest was curtailed when it was realised that the designer was a patient in a mental hospital.

Now being researched, Fluidic Thrust Vectoring (FTV) diverts thrust via secondary fluidic injections. Tests show that air forced into a jet engine exhaust stream can deflect thrust up to 15 degrees. Such nozzles are desirable for their lower mass and cost (up to 50% less), inertia (for faster, stronger control response), complexity (mechanically simpler, fewer or no moving parts or surfaces, less maintenance), and radar cross section for stealth. This will likely be used in many unmanned aerial vehicle (UAVs), and 6th generation fighter aircraft.

Thrust-vectoring flight control (TVFC) is obtained through deflection of the aircraft jets in some or all of the pitch, yaw and roll directions. In the extreme, deflection of the jets in yaw, pitch and roll creates desired forces and moments enabling complete directional control of the aircraft flight path without the implementation of the conventional aerodynamic flight controls (CAFC). TVFC can also be used to hold stationary flight in areas of the flight envelope where the main aerodynamic surfaces are stalled. TVFC includes control of STOVL aircraft during the hover and during the transition between hover and forward speeds below 50 knots where aerodynamic surfaces are ineffective.

When vectored thrust control uses a single propelling jet, as with a single-engined aircraft, the ability to produce rolling moments may not be possible. An example is an afterburning supersonic nozzle where nozzle functions are throat area, exit area, pitch vectoring and yaw vectoring. These functions are controlled by four separate actuators. A simpler variant using only three actuators would not have independent exit area control.

When TVFC is implemented to complement CAFC, agility and safety of the aircraft are maximized. Increased safety may occur in the event of malfunctioning CAFC as a result of battle damage.

To implement TVFC a variety of nozzles both mechanical and fluidic may be applied. This includes convergent and convergent-divergent nozzles that may be fixed or geometrically variable. It also includes variable mechanisms within a fixed nozzle, such as rotating cascades and rotating exit vanes. Within these aircraft nozzles, the geometry itself may vary from two-dimensional (2-D) to axisymmetric or elliptic. The number of nozzles on a given aircraft to achieve TVFC can vary from one on a CTOL aircraft to a minimum of four in the case of STOVL aircraft.

It is necessary to clarify some definitions used in thrust-vectoring nozzle design.

Axisymmetric
Nozzles with circular exits.
Conventional aerodynamic flight control (CAFC)
Pitch, yaw-pitch, yaw-pitch-roll or any other combination of aircraft control through aerodynamic deflection using rudders, flaps, elevators and/or ailerons.
Converging-diverging nozzle (C-D)
Generally used on supersonic jet aircraft where nozzle pressure ratio (npr) > 3. The engine exhaust is expanded through a converging section to achieve Mach 1 and then expanded through a diverging section to achieve supersonic speed at the exit plane, or less at low npr.
Converging nozzle
Generally used on subsonic and transonic jet aircraft where npr < 3. The engine exhaust is expanded through a converging section to achieve Mach 1 at the exit plane, or less at low npr.
Effective Vectoring Angle
The average angle of deflection of the jet stream centerline at any given moment in time.
Fixed nozzle
A thrust-vectoring nozzle of invariant geometry or one of variant geometry maintaining a constant geometric area ratio, during vectoring. This will also be referred to as a civil aircraft nozzle and represents the nozzle thrust vectoring control applicable to passenger, transport, cargo and other subsonic aircraft.
Fluidic thrust vectoring
The manipulation or control of the exhaust flow with the use of a secondary air source, typically bleed air from the engine compressor or fan.
Geometric vectoring angle
Geometric centerline of the nozzle during vectoring. For those nozzles vectored at the geometric throat and beyond, this can differ considerably from the effective vectoring angle.
Three-bearing swivel duct nozzle (3BSD)
Three angled segments of engine exhaust duct rotate relative to one another about duct centreline to produce nozzle thrust axis pitch and yaw.
Three-dimensional (3-D)
Nozzles with multi-axis or pitch and yaw control.
Thrust vectoring (TV)
The deflection of the jet away from the body-axis through the implementation of a flexible nozzle, flaps, paddles, auxiliary fluid mechanics or similar methods.
Thrust-vectoring flight control (TVFC)
Pitch, yaw-pitch, yaw-pitch-roll, or any other combination of aircraft control through deflection of thrust generally issuing from an air-breathing turbofan engine.
Two-dimensional (2-D)
Nozzles with square or rectangular exits. In addition to the geometrical shape 2-D can also refer to the degree-of-freedom (DOF) controlled which is single axis, or pitch-only, in which case round nozzles are included.
Two-dimensional converging-diverging (2-D C-D)
Square, rectangular, or round supersonic nozzles on fighter aircraft with pitch-only control.
Variable nozzle
A thrust-vectoring nozzle of variable geometry maintaining a constant, or allowing a variable, effective nozzle area ratio, during vectoring. This will also be referred to as a military aircraft nozzle as it represents the nozzle thrust vectoring control applicable to fighter and other supersonic aircraft with afterburning. The convergent section may be fully controlled with the divergent section following a pre-determined relationship to the convergent throat area. Alternatively, the throat area and the exit area may be controlled independently, to allow the divergent section to match the exact flight condition.
Geometric area ratios
Maintaining a fixed geometric area ratio from the throat to the exit during vectoring. The effective throat is constricted as the vectoring angle increases.
Effective area ratios
Maintaining a fixed effective area ratio from the throat to the exit during vectoring. The geometric throat is opened as the vectoring angle increases.
Differential area ratios
Maximizing nozzle expansion efficiency generally through predicting the optimal effective area as a function of the mass flow rate.
Type I
Nozzles whose baseframe mechanically is rotated before the geometrical throat.
Type II
Nozzles whose baseframe is mechanically rotated at the geometrical throat.
Type III
Nozzles whose baseframe is not rotated. Rather, the addition of mechanical deflection post-exit vanes or paddles enables jet deflection.
Type IV
Jet deflection through counter-flowing or co-flowing (by shock-vector control or throat shifting) auxiliary jet streams. Fluid-based jet deflection using secondary fluidic injection.
Additional type
Nozzles whose upstream exhaust duct consists of wedge-shaped segments which rotate relative to each other about the duct centreline.
An example of 2D thrust vectoring is the Rolls-Royce Pegasus engine used in the Hawker Siddeley Harrier, as well as in the AV-8B Harrier II variant.

Widespread use of thrust vectoring for enhanced maneuverability in Western production-model fighter aircraft didn't occur until the deployment of the Lockheed Martin F-22 Raptor fifth-generation jet fighter in 2005, with its afterburning, 2D thrust-vectoring Pratt & Whitney F119 turbofan.

While the Lockheed Martin F-35 Lightning II uses a conventional afterburning turbofan (Pratt & Whitney F135) to facilitate supersonic operation, its F-35B variant, developed for joint usage by the US Marine Corps, Royal Air Force, Royal Navy, and Italian Navy, also incorporates a vertically mounted, low-pressure shaft-driven remote fan, which is driven through a clutch during landing from the engine. Both the exhaust from this fan and the main engine's fan are deflected by thrust vectoring nozzles, to provide the appropriate combination of lift and propulsive thrust. It is not conceived for enhanced maneuverability in combat, only for VTOL operation, and the F-35A and F-35C do not use thrust vectoring at all.

The Sukhoi Su-30MKI, produced by India under license at Hindustan Aeronautics Limited, is in active service with the Indian Air Force. The TVC makes the aircraft highly maneuverable, capable of near-zero airspeed at high angles of attack without stalling, and dynamic aerobatics at low speeds. The Su-30MKI is powered by two Al-31FP afterburning turbofans. The TVC nozzles of the MKI are mounted 32 degrees outward to longitudinal engine axis (i.e. in the horizontal plane) and can be deflected 'b115 degrees in the vertical plane. This produces a corkscrew effect, greatly enhancing the turning capability of the aircraft.

A few computerized studies add thrust vectoring to extant passenger airliners, like the Boeing 727 and 747, to prevent catastrophic failures, while the experimental X-48C may be jet-steered in the future.

Examples of rockets and missiles which use thrust vectoring include both large systems such as the Space Shuttle Solid Rocket Booster (SRB), S-300P (SA-10) surface-to-air missile, UGM-27 Polaris nuclear ballistic missile and RT-23 (SS-24) ballistic missile and smaller battlefield weapons such as Swingfire.

The principles of air thrust vectoring have been recently adapted to military sea applications in the form of fast water-jet steering that provide super-agility. Examples are the fast patrol boat Dvora Mk-III, the Hamina class missile boat and the US Navy's Littoral combat ships.

Thrust vectoring can convey two main benefits: VTOL/STOL, and higher maneuverability. Aircraft are usually optimized to maximally exploit one benefit, though will gain in the other.

Spin-stabilisation is the method of stabilizing a satellite or launch vehicle by means of spin. For most satellite applications this approach has been superseded by three-axis stabilisation. It is also used in non-satellite applications such as rifle and artillery.

Despinning can be achieved by various techniques, including yo-yo de-spin.

On rockets with a solid motor upper stage, spin stabilization is used to keep the motor from drifting off course as they don't have their own thrusters. Usually small rockets are used to spin up the spacecraft and rocket then fire the rocket and send the craft off.

Some rockets, like the Jupiter-C, Delta II, Minotaur V and the satellite Aryabhata are spin-stabilised.

The Pioneer 4 spacecraft, the second object sent on a lunar flyby in 1959, maintained its attitude using spin-stabilization.[1]

The Schiaparelli EDM lander was spun up to 2.5 RPM before being ejected from the ExoMars Trace Gas Orbiter prior to its attempted landing on Mars in October 2016.[2]

Another spin-stabilized spacecraft is Juno, which arrived at Jupiter orbit in 2016.

In operation as a third stage, the Star 48 rocket booster sits on top of spin table, and before it is separated it is spun up to stabilize it during the separation from the previous stage.

In physics, gravity (from Latin gravitas 'weight') is a fundamental interaction which causes mutual attraction between all things with mass or energy. Gravity is by far the weakest of the four fundamental interactions, approximately 1038 times weaker than the strong interaction, 1036 times weaker than the electromagnetic force and 1029 times weaker than the weak interaction. As a result, it has no significant influence at the level of subatomic particles. However, gravity is the most significant interaction between objects at the macroscopic scale, and it determines the motion of planets, stars, galaxies, and even light.

On Earth, gravity gives weight to physical objects, and the Moon's gravity causes tides in the oceans. Gravity also has many important biological functions, helping to guide the growth of plants through the process of gravitropism and influencing the circulation of fluids in multicellular organisms. Investigation into the effects of weightlessness has shown that gravity may play a role in immune system function and cell differentiation within the human body.

The gravitational attraction between the original gaseous matter in the Universe allowed it to coalesce and form stars which eventually condensed into galaxies, so gravity is responsible for many of the large-scale structures in the Universe. Gravity has an infinite range, although its effects become weaker as objects get farther away.

Gravity is most accurately described by the general theory of relativity (proposed by Albert Einstein in 1915), which describes gravity not as a force, but as the curvature of spacetime, caused by the uneven distribution of mass, and causing masses to move along geodesic lines. The most extreme example of this curvature of spacetime is a black hole, from which nothing'97not even light'97can escape once past the black hole's event horizon. However, for most applications, gravity is well approximated by Newton's law of universal gravitation, which describes gravity as a force causing any two bodies to be attracted toward each other, with magnitude proportional to the product of their masses and inversely proportional to the square of the distance between them.

Current models of particle physics imply that the earliest instance of gravity in the Universe, possibly in the form of quantum gravity, supergravity or a gravitational singularity, along with ordinary space and time, developed during the Planck epoch (up to 10uc0u8722 43 seconds after the birth of the Universe), possibly from a primeval state, such as a false vacuum, quantum vacuum or virtual particle, in a currently unknown manner. Scientists are currently working to develop a theory of gravity consistent with quantum mechanics, a quantum gravity theory, which would allow gravity to be united in a common mathematical framework (a theory of everything) with the other three fundamental interactions of physics.

The nature and mechanism of gravity was explored by a wide range of ancient scholars. In Greece, Aristotle believed that objects fell towards the Earth because the Earth was the center of the Universe and attracted all of the mass in the Universe towards it. He also thought that the speed of a falling object should increase with its weight, a conclusion which was later shown to be false. While Aristotle's view was widely accepted throughout Ancient Greece, there were other thinkers such as Plutarch who correctly predicted that the attraction of gravity was not unique to the Earth.

Although he didn't understand gravity as a force, the ancient Greek philosopher Archimedes discovered the center of gravity of a triangle. He also postulated that if two equal weights did not have the same center of gravity, the center of gravity of the two weights together would be in the middle of the line that joins their centers of gravity.

In India, the mathematician-astronomer Aryabhata first identified gravity to explain why objects are not driven away from the Earth by the centrifugal force of the planet's rotation. Later, in the seventh century CE, Brahmagupta proposed the idea that gravity is an attractive force which draws objects to the Earth and used the term gurutvuc0u257 karu7779 au7751  to describe it.

In the ancient Middle East, gravity was a topic of fierce debate. The Persian intellectual Al-Biruni believed that the force of gravity was not unique to the Earth, and he correctly assumed that other heavenly bodies should exert a gravitational attraction as well. In contrast, Al-Khazini held the same position as Aristotle that all matter in the Universe is attracted to the center of the Earth.

In the mid-16th century, various European scientists experimentally disproved the Aristotelian notion that heavier objects fall at a faster rate. In particular, the Spanish Dominican priest Domingo de Soto wrote in 1551 that bodies in free fall uniformly accelerate. De Soto may have been influenced by earlier experiments conducted by other Dominican priests in Italy, including those by Benedetto Varchi, Francesco Beato, Luca Ghini, and Giovan Bellaso which contradicted Aristotle's teachings on the fall of bodies. The mid-16th century Italian physicist Giambattista Benedetti published papers claiming that, due to specific gravity, objects made of the same material but with different masses would fall at the same speed. With the 1586 Delft tower experiment, the Flemish physicist Simon Stevin observed that two cannonballs of differing sizes and weights fell at the same rate when dropped from a tower. Finally, in the late 16th century, Galileo Galilei performed his famous Leaning Tower of Pisa experiment in order to show once again that balls of different weights would fall at the same speed. Combining this knowledge with careful measurements of balls rolling down inclines, Galileo firmly established that gravitational acceleration is the same for all objects. Galileo postulated that air resistance is the reason that objects with a low density and high surface area fall more slowly in an atmosphere.

In 1604, Galileo correctly hypothesized that the distance of a falling object is proportional to the square of the time elapsed. This was later confirmed by Italian scientists Jesuits Grimaldi and Riccioli between 1640 and 1650. They also calculated the magnitude of the Earth's gravity by measuring the oscillations of a pendulum.

In 1684, Newton sent a manuscript to Edmond Halley titled De motu corporum in gyrum ('On the motion of bodies in an orbit'), which provided a physical justification for Kepler's laws of planetary motion. Halley was impressed by the manuscript and urged Newton to expand on it, and a few years later Newton published a groundbreaking book called Philosophi'e6 Naturalis Principia Mathematica (Mathematical Principles of Natural Philosophy). In this book, Newton described gravitation as a universal force, and claimed that "the forces which keep the planets in their orbs must [be] reciprocally as the squares of their distances from the centers about which they revolve." This statement was later condensed into the following inverse-square law:

where F is the force, m1 and m2 are the masses of the objects interacting, r is the distance between the centers of the masses and G is the gravitational constant.
Newton's Principia was well-received by the scientific community, and his law of gravitation quickly spread across the European world. More than a century later, in 1821, his theory of gravitation rose to even greater prominence when it was used to predict the existence of Neptune. In that year, the French astronomer Alexis Bouvard used this theory to create a table modeling the orbit of Uranus, which was shown to differ significantly from the planet's actual trajectory. In order to explain this discrepancy, many astronomers speculated that there might be a large object beyond the orbit of Uranus which was disrupting its orbit. In 1846, the astronomers John Couch Adams and Urbain Le Verrier independently used Newton's law to predict Neptune's location in the night sky, and the planet was discovered there within a day.

General relativity
Spacetime curvature schematic
IntroductionHistory
Mathematical formulation
Tests
show
Fundamental concepts
show
Phenomena
show
EquationsFormalisms
show
Solutions
show
Scientists
icon Physics portal  Category
vte
Eventually, astronomers noticed an eccentricity in the orbit of the planet Mercury which could not be explained by Newton's theory: the perihelion of the orbit was increasing by about 42.98 arcseconds per century. The most obvious explanation for this discrepancy was an as-yet-undiscovered celestial body (such as a planet orbiting the Sun even closer than Mercury), but all efforts to find such a body turned out to be fruitless. Finally, in 1915, Albert Einstein developed a theory of general relativity which was able to accurately model Mercury's orbit.

In general relativity, the effects of gravitation are ascribed to spacetime curvature instead of a force. Einstein began to toy with this idea in the form of the equivalence principle, a discovery which he later described as "the happiest thought of my life." In this theory, free fall is considered to be equivalent to inertial motion, meaning that free-falling inertial objects are accelerated relative to non-inertial observers on the ground. In contrast to Newtonian physics, Einstein believed that it was possible for this acceleration to occur without any force being applied to the object.

Einstein proposed that spacetime is curved by matter, and that free-falling objects are moving along locally straight paths in curved spacetime. These straight paths are called geodesics. As in Newton's first law of motion, Einstein believed that a force applied to an object would cause it to deviate from a geodesic. For instance, people standing on the surface of the Earth are prevented from following a geodesic path because the mechanical resistance of the Earth exerts an upward force on them. This explains why moving along the geodesics in spacetime is considered inertial.

Einstein's description of gravity was quickly accepted by the majority of physicists, as it was able to explain a wide variety of previously baffling experimental results. In the coming years, a wide range of experiments provided additional support for the idea of general relativity. Today, Einstein's theory of relativity is used for all gravitational calculations where absolute precision is desired, although Newton's inverse-square law continues to be a useful and fairly accurate approximation.

In modern physics, general relativity remains the framework for the understanding of gravity. Physicists continue to work to find solutions to the Einstein field equations that form the basis of general relativity, while some scientists have speculated that general relativity may not be applicable at all in certain scenarios.

The Einstein field equations are a system of 10 partial differential equations which describe how matter effects the curvature of spacetime. The system is often expressed in the following simplified form:

where Guc0u956 u957  is the Einstein tensor, gu956 u957  is the metric tensor, Tu956 u957  is the stress'96energy tensor, u923  is the cosmological constant, is Newton's gravitational constant and is the speed of light. The term is sometimes referred to as the Einstein gravitational constant .

A major area of research is the discovery of exact solutions to the Einstein field equations. Solving these equations amounts to calculating a precise value for the metric tensor (which defines the curvature and geometry of spacetime) under certain physical conditions. There is no formal definition for what constitutes such solutions, but most scientists agree that they should be expressable using elementary functions or linear differential equations. Some of the most notable solutions of the equations include:

The Schwarzschild solution, which describes spacetime surrounding a spherically symmetric non-rotating uncharged massive object. For compact enough objects, this solution generated a black hole with a central singularity. At points far away from the central mass, the accelerations predicted by the Schwarzschild solution are practically identical to those predicted by Newton's theory of gravity.
The Reissner-Nordstr'f6m solution, which analyzes a non-rotating spherically symmetric object with charge and was independently discovered by several different researchers between 1916 and 1921. In some cases, this solution can predict the existence of black holes with double event horizons.
The Kerr solution, which generalizes the Schwarzchild solution to rotating massive objects. Because of the difficulty of factoring in the effects of rotation into Einstein's field equations, this solution was not discovered until 1963.
The Kerr-Newman solution for charged, rotating massive objects. This solution was derived in 1964, using the same technique of complex coordinate transformation that was used for the Kerr solution.
The cosmological Friedmann-Lema'eetre-Robertson-Walker solution, discovered in 1922 by Alexander Friedmann and then confirmed in 1927 by Georges Lema'eetre. This solution was revolutionary for predicting the expansion of the Universe, which was confirmed seven years later after a series of measurements by Edwin Hubble. It even showed that general relativity was incompatible with a static universe, and Einstein later conceded that he had been wrong to design his field equations to account for a Universe that was not expanding.
Today, there remain many important situations in which the Einstein field equations have not been solved. Chief among these is the two-body problem, which concerns the geometry of spacetime around two mutually interacting massive objects (such as the Sun and the Earth, or the two stars in a binary star system). The situation gets even more complicated when considering the interactions of three or more massive bodies (the "n-body problem"), and some scientists suspect that the Einstein field equations will never be solved in this context. However, it is still possible to construct an approximate solution to the field equations in the n-body problem by using the technique of post-Newtonian expansion. In general, the extreme nonlinearity of Einstein's field equations makes it difficult to solve them in all but the most specific cases.

Despite its success in predicting the effects of gravity at large scales, general relativity is ultimately a classical theory which is incompatible with quantum mechanics. In most cases, this doesn't matter, because gravity is usually only relevant at large scales while quantum physics is only relevant at very small ones. However, in order to understand environments such as the singularity of a black hole (where an extremely large amount of mass is compressed into a small space), many physicists have begun to search for a theory that could unite both gravity and quantum mechanics under a more general framework.

One path is to describe gravity in the framework of quantum field theory, which has been successful to accurately describe the other fundamental interactions. The electromagnetic force arises from an exchange of virtual photons, where the QFT description of gravity is that there is an exchange of virtual gravitons. This description reproduces general relativity in the classical limit. However, this approach fails at short distances of the order of the Planck length, where a more complete theory of quantum gravity (or a new approach to quantum mechanics) is required.

It is difficult to test the theory of general relativity on Earth, because for small masses and velocities its predictions are almost identical to that of Newtonian gravity. Still, since its development, an ongoing series of experimental results have provided support for the theory:

In 1919, the British astrophysicist Arthur Eddington was able to confirm the predicted gravitational lensing of light during that year's solar eclipse. Eddington measured starlight deflections twice those predicted by Newtonian corpuscular theory, in accordance with the predictions of general relativity. Although Eddington's analysis was later disputed, this experiment made Einstein famous almost overnight and caused general relativity to become widely accepted in the scientific community.
In 1959, American physicists Robert Pound and Glen Rebka performed an experiment in which they used gamma rays to confirm the prediction of gravitational time dilation. By sending the rays down a 74-foot tower and measuring their frequency at the bottom, the scientists confirmed that light is redshifted as it moves towards a source of gravity. The observed redshift also supported the idea that time runs more slowly in the presence of a gravitational field.
The time delay of light passing close to a massive object was first identified by Irwin I. Shapiro in 1964 in interplanetary spacecraft signals.
In 1971, scientists discovered the first-ever black hole in the galaxy Cygnus. The black hole was detected because it was emitting bursts of x-rays as it consumed a smaller star, and it came to be known as Cygnus X-1. This discovery confirmed yet another prediction of general relativity, because Einstein's equations implied that light could not escape from a sufficiently large and compact object.
General relativity states that gravity acts on light and matter equally, meaning that a sufficiently massive object could warp light around it and create a gravitational lens. This phenomenon was observed for the first time in 1979, when the Hubble telescope saw two mirror images of the same quasar whose light had been bent around the galaxy YGKOW G1.
Frame dragging, the idea that a rotating massive object should twist spacetime around it, was confirmed by Gravity Probe B results in 2011.
In 2015, the LIGO observatory detected faint gravitational waves, the existence of which had been predicted by general relativity. Scientists believe that the waves emanated from a black hole merger that occurred 1.5 billion light-years away.
Every planetary body (including the Earth) is surrounded by its own gravitational field, which can be conceptualized with Newtonian physics as exerting an attractive force on all objects. Assuming a spherically symmetrical planet, the strength of this field at any given point above the surface is proportional to the planetary body's mass and inversely proportional to the square of the distance from the center of the body.

The strength of the gravitational field is numerically equal to the acceleration of objects under its influence. The rate of acceleration of falling objects near the Earth's surface varies very slightly depending on latitude, surface features such as mountains and ridges, and perhaps unusually high or low sub-surface densities. For purposes of weights and measures, a standard gravity value is defined by the International Bureau of Weights and Measures, under the International System of Units (SI).

The force of gravity on Earth is the resultant (vector sum) of two forces: (a) The gravitational attraction in accordance with Newton's universal law of gravitation, and (b) the centrifugal force, which results from the choice of an earthbound, rotating frame of reference. The force of gravity is weakest at the equator because of the centrifugal force caused by the Earth's rotation and because points on the equator are furthest from the center of the Earth. The force of gravity varies with latitude and increases from about 9.780 m/s2 at the Equator to about 9.832 m/s2 at the poles.

The earliest gravity (possibly in the form of quantum gravity, supergravity or a gravitational singularity), along with ordinary space and time, developed during the Planck epoch (up to 10uc0u8722 43 seconds after the birth of the Universe), possibly from a primeval state (such as a false vacuum, quantum vacuum or virtual particle), in a currently unknown manner.

General relativity predicts that energy can be transported out of a system through gravitational radiation. The first indirect evidence for gravitational radiation was through measurements of the Hulse'96Taylor binary in 1973. This system consists of a pulsar and neutron star in orbit around one another. Its orbital period has decreased since its initial discovery due to a loss of energy, which is consistent for the amount of energy loss due to gravitational radiation. This research was awarded the Nobel Prize in Physics in 1993.

The first direct evidence for gravitational radiation was measured on 14 September 2015 by the LIGO detectors. The gravitational waves emitted during the collision of two black holes 1.3 billion-light years from Earth were measured. This observation confirms the theoretical predictions of Einstein and others that such waves exist. It also opens the way for practical observation and understanding of the nature of gravity and events in the Universe including the Big Bang. Neutron star and black hole formation also create detectable amounts of gravitational radiation. This research was awarded the Nobel Prize in physics in 2017.

In December 2012, a research team in China announced that it had produced measurements of the phase lag of Earth tides during full and new moons which seem to prove that the speed of gravity is equal to the speed of light. This means that if the Sun suddenly disappeared, the Earth would keep orbiting the vacant point normally for 8 minutes, which is the time light takes to travel that distance. The team's findings were released in the Chinese Science Bulletin in February 2013.

In October 2017, the LIGO and Virgo detectors received gravitational wave signals within 2 seconds of gamma ray satellites and optical telescopes seeing signals from the same direction. This confirmed that the speed of gravitational waves was the same as the speed of light.

The Space Age is a period encompassing the activities related to the Space Race, space exploration, space technology, and the cultural developments influenced by these events, beginning with the launch of Sputnik 1 during 1957, and continuing to the present.

The Space Age began with the development of several technologies that converged with the October 4, 1957 launch of Sputnik 1 by the Soviet Union. This was the world's first artificial satellite, orbiting the Earth in 96.17 minutes and weighing 83 kg (183 lb). The launch of Sputnik 1 ushered in a new era of political, scientific and technological achievements that became known as the Space Age, by the rapid development of new technology and a race for achievement, mostly between the United States and the Soviet Union. Rapid advances were made in rocketry, materials science, and other areas. Much of the technology originally developed for space applications has been spun off and found additional uses. One such example is memory foam.

Prior to human spaceflights being attempted, various animals were flown to space to identify potential detrimental effects of microgravity and radiation exposure at high altitudes.

The Space Age reached its peak with the Apollo program that captured the imagination of much of the world's population. The landing of Apollo 11 was watched by over 500 million people around the world and is widely recognized as one of the defining moments of the 20th century. Since then, public attention has largely moved to other areas.

In the United States, the Space Shuttle Challenger disaster in 1986 marked a significant decline in crewed Shuttle launches. Following the disaster, NASA grounded all Shuttles for safety concerns until 1988. During the 1990s funding for space-related programs fell sharply as the remaining structures of the now-dissolved Soviet Union disintegrated and NASA no longer had any direct competition.

Since then, participation in space launches has increasingly widened to include more governments and commercial interests. Since the 1990s, the public perception of space exploration and space-related technologies has been that such endeavors are increasingly commonplace.

NASA permanently grounded all U.S. Space Shuttles in 2011. NASA has since relied on Russia and SpaceX to take American astronauts to and from the International Space Station.

In the early 21st century, the Ansari X Prize competition was set up to help jump-start private spaceflight. The winner, Space Ship One in 2004, became the first spaceship not funded by a government agency.

Several countries now have space programs; from related technology ventures to full-fledged space programs with launch facilities. There are many scientific and commercial satellites in use today, with thousands of satellites in orbit, and several countries have plans to send humans into space. Some of the countries joining this new race are France, India, China, Israel and the United Kingdom, all of which have employed surveillance satellites. There are several other countries with less extensive space programs, including Brazil, Germany, Ukraine, and Spain.

As for the United States space program, NASA is currently constructing a deep-space crew capsule named the Orion. NASA's goal with this new space capsule is to carry humans to Mars. The Orion spacecraft is due to be completed in the early 2020s. NASA is hoping that this mission will '93usher in a new era of space exploration.'94

Another major factor affecting the current Space Age is the privatization of space flight. A significant private spaceflight company is SpaceX which became the proprietor of one of world's most capable operational launch vehicle when they launched their current largest rocket, the Falcon Heavy in 2018. Elon Musk, the founder and CEO of SpaceX, has put forward the goal of establishing a colony of one million people on Mars and the company is developing its Starship launch vehicle to facilitate this. Since the Demo-2 mission for NASA in 2020 in which SpaceX launched astronauts for the first time to the International Space Station, the company has maintained an orbital human spaceflight capability. Blue Origin, a private company founded by Amazon.com founder Jeff Bezos, is developing rockets for use in space tourism, commercial satellite launches, and eventual missions to the Moon and beyond. Richard Branson's company Virgin Galactic is concentrating on launch vehicles for space tourism. A spinoff company, Virgin Orbit, air-launches small satellites with their LauncherOne rocket. Another small-satellite launcher, Rocket Lab, has developed the Electron rocket and the Photon satellite bus for sending spacecraft further into the Solar System.

History of technology
show
By technological eras
show
By historical regions
show
By type of technology
show
Technology timelines
show
Article indices
vte
The Space Age might also be considered to have begun much earlier than October 4, 1957, because in June 1944, a German V-2 rocket became the first manmade object to enter space, albeit only briefly. Some even consider March 1926 as the beginning of the Space Age, when American rocket pioneer Robert H. Goddard launched the world's first liquid fuel rocket, though his rocket did not reach outer space. Also already in the 1920s the world's first large-scale experimental rocket program, Opel-RAK, was initiated under the leadership of Fritz von Opel and Max Valier. Speed records for ground and rail vehicles were achieved in 1928 and von Opel piloted the world's first public flight of a rocket plane, Opel RAK.1. The Great Depression put an end to the Opel-RAK program but it nevertheless had a strong and long-lasting impact on later spaceflight pioneers, in particular on Wernher von Braun who would eventually head the Nazi era V2 program.

Since the V-2 rocket flight was undertaken in secrecy, it was not public knowledge for many years afterward. Further, the German launches, as well as the subsequent sounding rocket tests performed in both the United States and the Soviet Union during the late 1940s and early 1950s, were not considered significant enough to start a new age because they did not reach orbit. Having a rocket powerful enough to reach orbit meant that a nation could place a payload anywhere on the planet, or to use another term, possessed an intercontinental ballistic missile. The fact that after such a development nowhere on Earth was safe from a nuclear warhead is why the orbital standard is commonly used to define when the space age began.

Apollo 11 (July 16'9624, 1969) was the American spaceflight that first landed humans on the Moon. Commander Neil Armstrong and lunar module pilot Buzz Aldrin landed the Apollo Lunar Module Eagle on July 20, 1969, at 20:17 UTC, and Armstrong became the first person to step onto the Moon's surface six hours and 39 minutes later, on July 21 at 02:56 UTC. Aldrin joined him 19 minutes later, and they spent about two and a quarter hours together exploring the site they had named Tranquility Base upon landing. Armstrong and Aldrin collected 47.5 pounds (21.5 kg) of lunar material to bring back to Earth as pilot Michael Collins flew the Command Module Columbia in lunar orbit, and were on the Moon's surface for 21 hours, 36 minutes before lifting off to rejoin Columbia.

Apollo 11 was launched by a Saturn V rocket from Kennedy Space Center on Merritt Island, Florida, on July 16 at 13:32 UTC, and it was the fifth crewed mission of NASA's Apollo program. The Apollo spacecraft had three parts: a command module (CM) with a cabin for the three astronauts, the only part that returned to Earth; a service module (SM), which supported the command module with propulsion, electrical power, oxygen, and water; and a lunar module (LM) that had two stages'97a descent stage for landing on the Moon and an ascent stage to place the astronauts back into lunar orbit.

After being sent to the Moon by the Saturn V's third stage, the astronauts separated the spacecraft from it and traveled for three days until they entered lunar orbit. Armstrong and Aldrin then moved into Eagle and landed in the Sea of Tranquility on July 20. The astronauts used Eagle's ascent stage to lift off from the lunar surface and rejoin Collins in the command module. They jettisoned Eagle before they performed the maneuvers that propelled Columbia out of the last of its 30 lunar orbits onto a trajectory back to Earth. They returned to Earth and splashed down in the Pacific Ocean on July 24 after more than eight days in space.

Armstrong's first step onto the lunar surface was broadcast on live TV to a worldwide audience. He described the event as "one small step for [a] man, one giant leap for mankind." Apollo 11 effectively proved US victory in the Space Race to demonstrate spaceflight superiority, by fulfilling a national goal proposed in 1961 by President John F. Kennedy, "before this decade is out, of landing a man on the Moon and returning him safely to the Earth."

In the late 1950s and early 1960s, the United States was engaged in the Cold War, a geopolitical rivalry with the Soviet Union. On October 4, 1957, the Soviet Union launched Sputnik 1, the first artificial satellite. This surprise success fired fears and imaginations around the world. It demonstrated that the Soviet Union had the capability to deliver nuclear weapons over intercontinental distances, and challenged American claims of military, economic and technological superiority. This precipitated the Sputnik crisis, and triggered the Space Race to prove which superpower would achieve superior spaceflight capability. President Dwight D. Eisenhower responded to the Sputnik challenge by creating the National Aeronautics and Space Administration (NASA), and initiating Project Mercury, which aimed to launch a man into Earth orbit. But on April 12, 1961, Soviet cosmonaut Yuri Gagarin became the first person in space, and the first to orbit the Earth. Nearly a month later, on May 5, 1961, Alan Shepard became the first American in space, completing a 15-minute suborbital journey. After being recovered from the Atlantic Ocean, he received a congratulatory telephone call from Eisenhower's successor, John F. Kennedy.

Since the Soviet Union had higher lift capacity launch vehicles, Kennedy chose, from among options presented by NASA, a challenge beyond the capacity of the existing generation of rocketry, so that the US and Soviet Union would be starting from a position of equality. A crewed mission to the Moon would serve this purpose.

On May 25, 1961, Kennedy addressed the United States Congress on "Urgent National Needs" and declared:

I believe that this nation should commit itself to achieving the goal, before this decade [1960s] is out, of landing a man on the Moon and returning him safely to the Earth. No single space project in this period will be more impressive to mankind, or more important for the long-range exploration of space; and none will be so difficult or expensive to accomplish. We propose to accelerate the development of the appropriate lunar space craft. We propose to develop alternate liquid and solid fuel boosters, much larger than any now being developed, until certain which is superior. We propose additional funds for other engine development and for unmanned explorations'97explorations which are particularly important for one purpose which this nation will never overlook: the survival of the man who first makes this daring flight. But in a very real sense, it will not be one man going to the Moon'97if we make this judgment affirmatively, it will be an entire nation. For all of us must work to put him there.

'97uc0u8201 Kennedy's speech to Congress
On September 12, 1962, Kennedy delivered another speech before a crowd of about 40,000 people in the Rice University football stadium in Houston, Texas. A widely quoted refrain from the middle portion of the speech reads as follows:

There is no strife, no prejudice, no national conflict in outer space as yet. Its hazards are hostile to us all. Its conquest deserves the best of all mankind, and its opportunity for peaceful cooperation may never come again. But why, some say, the Moon? Why choose this as our goal? And they may well ask, why climb the highest mountain? Why, 35 years ago, fly the Atlantic? Why does Rice play Texas? We choose to go to the Moon! We choose to go to the Moon ... We choose to go to the Moon in this decade and do the other things, not because they are easy, but because they are hard; because that goal will serve to organize and measure the best of our energies and skills, because that challenge is one that we are willing to accept, one we are unwilling to postpone, and one we intend to win, and the others, too.
In spite of that, the proposed program faced the opposition of many Americans and was dubbed a "moondoggle" by Norbert Wiener, a mathematician at the Massachusetts Institute of Technology. The effort to land a man on the Moon already had a name: Project Apollo. When Kennedy met with Nikita Khrushchev, the Premier of the Soviet Union in June 1961, he proposed making the Moon landing a joint project, but Khrushchev did not take up the offer. Kennedy again proposed a joint expedition to the Moon in a speech to the United Nations General Assembly on September 20, 1963. The idea of a joint Moon mission was abandoned after Kennedy's death.

An early and crucial decision was choosing lunar orbit rendezvous over both direct ascent and Earth orbit rendezvous. A space rendezvous is an orbital maneuver in which two spacecraft navigate through space and meet up. In July 1962 NASA head James Webb announced that lunar orbit rendezvous would be used and that the Apollo spacecraft would have three major parts: a command module (CM) with a cabin for the three astronauts, and the only part that returned to Earth; a service module (SM), which supported the command module with propulsion, electrical power, oxygen, and water; and a lunar module (LM) that had two stages'97a descent stage for landing on the Moon, and an ascent stage to place the astronauts back into lunar orbit. This design meant the spacecraft could be launched by a single Saturn V rocket that was then under development.

Technologies and techniques required for Apollo were developed by Project Gemini. The Apollo project was enabled by NASA's adoption of new advances in semiconductor electronic technology, including metal-oxide-semiconductor field-effect transistors (MOSFETs) in the Interplanetary Monitoring Platform (IMP) and silicon integrated circuit (IC) chips in the Apollo Guidance Computer (AGC).

Project Apollo was abruptly halted by the Apollo 1 fire on January 27, 1967, in which astronauts Gus Grissom, Ed White, and Roger B. Chaffee died, and the subsequent investigation. In October 1968, Apollo 7 evaluated the command module in Earth orbit, and in December Apollo 8 tested it in lunar orbit. In March 1969, Apollo 9 put the lunar module through its paces in Earth orbit, and in May Apollo 10 conducted a "dress rehearsal" in lunar orbit. By July 1969, all was in readiness for Apollo 11 to take the final step onto the Moon.

The Soviet Union appeared to be winning the Space Race by beating the US to firsts, but its early lead was overtaken by the US Gemini program and Soviet failure to develop the N1 launcher, which would have been comparable to the Saturn V. The Soviets tried to beat the US to return lunar material to the Earth by means of uncrewed probes. On July 13, three days before Apollo 11's launch, the Soviet Union launched Luna 15, which reached lunar orbit before Apollo 11. During descent, a malfunction caused Luna 15 to crash in Mare Crisium about two hours before Armstrong and Aldrin took off from the Moon's surface to begin their voyage home. The Nuffield Radio Astronomy Laboratories radio telescope in England recorded transmissions from Luna 15 during its descent, and these were released in July 2009 for the 40th anniversary of Apollo 11.

The initial crew assignment of Commander Neil Armstrong, Command Module Pilot (CMP) Jim Lovell, and Lunar Module Pilot (LMP) Buzz Aldrin on the backup crew for Apollo 9 was officially announced on November 20, 1967. Lovell and Aldrin had previously flown together as the crew of Gemini 12. Due to design and manufacturing delays in the LM, Apollo 8 and Apollo 9 swapped prime and backup crews, and Armstrong's crew became the backup for Apollo 8. Based on the normal crew rotation scheme, Armstrong was then expected to command Apollo 11.

There would be one change. Michael Collins, the CMP on the Apollo 8 crew, began experiencing trouble with his legs. Doctors diagnosed the problem as a bony growth between his fifth and sixth vertebrae, requiring surgery. Lovell took his place on the Apollo 8 crew, and when Collins recovered he joined Armstrong's crew as CMP. In the meantime, Fred Haise filled in as backup LMP, and Aldrin as backup CMP for Apollo 8. Apollo 11 was the second American mission where all the crew members had prior spaceflight experience, the first being Apollo 10. The next was STS-26 in 1988.

Deke Slayton gave Armstrong the option to replace Aldrin with Lovell, since some thought Aldrin was difficult to work with. Armstrong had no issues working with Aldrin but thought it over for a day before declining. He thought Lovell deserved to command his own mission (eventually Apollo 13).

The Apollo 11 prime crew had none of the close cheerful camaraderie characterized by that of Apollo 12. Instead, they forged an amiable working relationship. Armstrong in particular was notoriously aloof, but Collins, who considered himself a loner, confessed to rebuffing Aldrin's attempts to create a more personal relationship. Aldrin and Collins described the crew as "amiable strangers". Armstrong did not agree with the assessment, and said "... all the crews I was on worked very well together."

The backup crew consisted of Lovell as Commander, William Anders as CMP, and Haise as LMP. Anders had flown with Lovell on Apollo 8. In early 1969, he accepted a job with the National Aeronautics and Space Council effective August 1969, and announced he would retire as an astronaut at that time. Ken Mattingly was moved from the support crew into parallel training with Anders as backup CMP in case Apollo 11 was delayed past its intended July launch date, at which point Anders would be unavailable.

By the normal crew rotation in place during Apollo, Lovell, Mattingly, and Haise were scheduled to fly on Apollo 14 after backing up for Apollo 11. Later, Lovell's crew was forced to switch places with Alan Shepard's tentative Apollo 13 crew to give Shepard more training time.

During Projects Mercury and Gemini, each mission had a prime and a backup crew. For Apollo, a third crew of astronauts was added, known as the support crew. The support crew maintained the flight plan, checklists and mission ground rules, and ensured the prime and backup crews were apprised of changes. They developed procedures, especially those for emergency situations, so these were ready for when the prime and backup crews came to train in the simulators, allowing them to concentrate on practicing and mastering them. For Apollo 11, the support crew consisted of Ken Mattingly, Ronald Evans and Bill Pogue.

The capsule communicator (CAPCOM) was an astronaut at the Mission Control Center in Houston, Texas, who was the only person who communicated directly with the flight crew. For Apollo 11, the CAPCOMs were: Charles Duke, Ronald Evans, Bruce McCandless II, James Lovell, William Anders, Ken Mattingly, Fred Haise, Don L. Lind, Owen K. Garriott and Harrison Schmitt.

The flight directors for this mission were:

Other key personnel who played important roles in the Apollo 11 mission include the following.

The Apollo 11 mission emblem was designed by Collins, who wanted a symbol for "peaceful lunar landing by the United States". At Lovell's suggestion, he chose the bald eagle, the national bird of the United States, as the symbol. Tom Wilson, a simulator instructor, suggested an olive branch in its beak to represent their peaceful mission. Collins added a lunar background with the Earth in the distance. The sunlight in the image was coming from the wrong direction; the shadow should have been in the lower part of the Earth instead of the left. Aldrin, Armstrong and Collins decided the Eagle and the Moon would be in their natural colors, and decided on a blue and gold border. Armstrong was concerned that "eleven" would not be understood by non-English speakers, so they went with "Apollo 11", and they decided not to put their names on the patch, so it would "be representative of everyone who had worked toward a lunar landing".

An illustrator at the Manned Spacecraft Center (MSC) did the artwork, which was then sent off to NASA officials for approval. The design was rejected. Bob Gilruth, the director of the MSC felt the talons of the eagle looked "too warlike". After some discussion, the olive branch was moved to the talons. When the Eisenhower dollar coin was released in 1971, the patch design provided the eagle for its reverse side. The design was also used for the smaller Susan B. Anthony dollar unveiled in 1979.

After the crew of Apollo 10 named their spacecraft Charlie Brown and Snoopy, assistant manager for public affairs Julian Scheer wrote to George Low, the Manager of the Apollo Spacecraft Program Office at the MSC, to suggest the Apollo 11 crew be less flippant in naming their craft. The name Snowcone was used for the CM and Haystack was used for the LM in both internal and external communications during early mission planning.

The LM was named Eagle after the motif which was featured prominently on the mission insignia. At Scheer's suggestion, the CM was named Columbia after Columbiad, the giant cannon that launched a spacecraft (also from Florida) in Jules Verne's 1865 novel From the Earth to the Moon. It also referred to Columbia, a historical name of the United States. In Collins' 1976 book, he said Columbia was in reference to Christopher Columbus.

The astronauts had personal preference kits (PPKs), small bags containing personal items of significance they wanted to take with them on the mission. Five 0.5-pound (0.23 kg) PPKs were carried on Apollo 11: three (one for each astronaut) were stowed on Columbia before launch, and two on Eagle.

Neil Armstrong's LM PPK contained a piece of wood from the Wright brothers' 1903 Wright Flyer's left propeller and a piece of fabric from its wing, along with a diamond-studded astronaut pin originally given to Slayton by the widows of the Apollo 1 crew. This pin had been intended to be flown on that mission and given to Slayton afterwards, but following the disastrous launch pad fire and subsequent funerals, the widows gave the pin to Slayton. Armstrong took it with him on Apollo 11.

NASA's Apollo Site Selection Board announced five potential landing sites on February 8, 1968. These were the result of two years' worth of studies based on high-resolution photography of the lunar surface by the five uncrewed probes of the Lunar Orbiter program and information about surface conditions provided by the Surveyor program. The best Earth-bound telescopes could not resolve features with the resolution Project Apollo required. The landing site had to be close to the lunar equator to minimize the amount of propellant required, clear of obstacles to minimize maneuvering, and flat to simplify the task of the landing radar. Scientific value was not a consideration.

Areas that appeared promising on photographs taken on Earth were often found to be totally unacceptable. The original requirement that the site be free of craters had to be relaxed, as no such site was found. Five sites were considered: Sites 1 and 2 were in the Sea of Tranquility (Mare Tranquillitatis); Site 3 was in the Central Bay (Sinus Medii); and Sites 4 and 5 were in the Ocean of Storms (Oceanus Procellarum). The final site selection was based on seven criteria:

The site needed to be smooth, with relatively few craters;
with approach paths free of large hills, tall cliffs or deep craters that might confuse the landing radar and cause it to issue incorrect readings;
reachable with a minimum amount of propellant;
allowing for delays in the launch countdown;
providing the Apollo spacecraft with a free-return trajectory, one that would allow it to coast around the Moon and safely return to Earth without requiring any engine firings should a problem arise on the way to the Moon;
with good visibility during the landing approach, meaning the Sun would be between 7 and 20 degrees behind the LM; and
a general slope of less than two degrees in the landing area.
The requirement for the Sun angle was particularly restrictive, limiting the launch date to one day per month. A landing just after dawn was chosen to limit the temperature extremes the astronauts would experience. The Apollo Site Selection Board selected Site 2, with Sites 3 and 5 as backups in the event of the launch being delayed. In May 1969, Apollo 10's lunar module flew to within 15 kilometers (9.3 mi) of Site 2, and reported it was acceptable.

During the first press conference after the Apollo 11 crew was announced, the first question was, "Which one of you gentlemen will be the first man to step onto the lunar surface?" Slayton told the reporter it had not been decided, and Armstrong added that it was "not based on individual desire".

One of the first versions of the egress checklist had the lunar module pilot exit the spacecraft before the commander, which matched what had been done on Gemini missions, where the commander had never performed the spacewalk. Reporters wrote in early 1969 that Aldrin would be the first man to walk on the Moon, and Associate Administrator George Mueller told reporters he would be first as well. Aldrin heard that Armstrong would be the first because Armstrong was a civilian, which made Aldrin livid. Aldrin attempted to persuade other lunar module pilots he should be first, but they responded cynically about what they perceived as a lobbying campaign. Attempting to stem interdepartmental conflict, Slayton told Aldrin that Armstrong would be first since he was the commander. The decision was announced in a press conference on April 14, 1969.

For decades, Aldrin believed the final decision was largely driven by the lunar module's hatch location. Because the astronauts had their spacesuits on and the spacecraft was so small, maneuvering to exit the spacecraft was difficult. The crew tried a simulation in which Aldrin left the spacecraft first, but he damaged the simulator while attempting to egress. While this was enough for mission planners to make their decision, Aldrin and Armstrong were left in the dark on the decision until late spring. Slayton told Armstrong the plan was to have him leave the spacecraft first, if he agreed. Armstrong said, "Yes, that's the way to do it."

The media accused Armstrong of exercising his commander's prerogative to exit the spacecraft first. Chris Kraft revealed in his 2001 autobiography that a meeting occurred between Gilruth, Slayton, Low, and himself to make sure Aldrin would not be the first to walk on the Moon. They argued that the first person to walk on the Moon should be like Charles Lindbergh, a calm and quiet person. They made the decision to change the flight plan so the commander was the first to egress from the spacecraft.

The ascent stage of LM-5 Eagle arrived at the Kennedy Space Center on January 8, 1969, followed by the descent stage four days later, and CSM-107 Columbia on January 23. There were several differences between Eagle and Apollo 10's LM-4 Snoopy; Eagle had a VHF radio antenna to facilitate communication with the astronauts during their EVA on the lunar surface; a lighter ascent engine; more thermal protection on the landing gear; and a package of scientific experiments known as the Early Apollo Scientific Experiments Package (EASEP). The only change in the configuration of the command module was the removal of some insulation from the forward hatch. The CSM was mated on January 29, and moved from the Operations and Checkout Building to the Vehicle Assembly Building on April 14.

The S-IVB third stage of Saturn V AS-506 had arrived on January 18, followed by the S-II second stage on February 6, S-IC first stage on February 20, and the Saturn V Instrument Unit on February 27. At 12:30 on May 20, the 5,443-tonne (5,357-long-ton; 6,000-short-ton) assembly departed the Vehicle Assembly Building atop the crawler-transporter, bound for Launch Pad 39A, part of Launch Complex 39, while Apollo 10 was still on its way to the Moon. A countdown test commenced on June 26, and concluded on July 2. The launch complex was floodlit on the night of July 15, when the crawler-transporter carried the mobile service structure back to its parking area. In the early hours of the morning, the fuel tanks of the S-II and S-IVB stages were filled with liquid hydrogen. Fueling was completed by three hours before launch. Launch operations were partly automated, with 43 programs written in the ATOLL programming language.

Slayton roused the crew shortly after 04:00, and they showered, shaved, and had the traditional pre-flight breakfast of steak and eggs with Slayton and the backup crew. They then donned their space suits and began breathing pure oxygen. At 06:30, they headed out to Launch Complex 39. Haise entered Columbia about three hours and ten minutes before launch time. Along with a technician, he helped Armstrong into the left-hand couch at 06:54. Five minutes later, Collins joined him, taking up his position on the right-hand couch. Finally, Aldrin entered, taking the center couch. Haise left around two hours and ten minutes before launch. The closeout crew sealed the hatch, and the cabin was purged and pressurized. The closeout crew then left the launch complex about an hour before launch time. The countdown became automated at three minutes and twenty seconds before launch time. Over 450 personnel were at the consoles in the firing room.

An estimated one million spectators watched the launch of Apollo 11 from the highways and beaches in the vicinity of the launch site. Dignitaries included the Chief of Staff of the United States Army, General William Westmoreland, four cabinet members, 19 state governors, 40 mayors, 60 ambassadors and 200 congressmen. Vice President Spiro Agnew viewed the launch with former president Lyndon B. Johnson and his wife Lady Bird Johnson. Around 3,500 media representatives were present. About two-thirds were from the United States; the rest came from 55 other countries. The launch was televised live in 33 countries, with an estimated 25 million viewers in the United States alone. Millions more around the world listened to radio broadcasts. President Richard Nixon viewed the launch from his office in the White House with his NASA liaison officer, Apollo astronaut Frank Borman.

Saturn V AS-506 launched Apollo 11 on July 16, 1969, at 13:32:00 UTC (9:32:00 EDT). At 13.2 seconds into the flight, the launch vehicle began to roll into its flight azimuth of 72.058'b0. Full shutdown of the first-stage engines occurred about 2 minutes and 42 seconds into the mission, followed by separation of the S-IC and ignition of the S-II engines. The second stage engines then cut off and separated at about 9 minutes and 8 seconds, allowing the first ignition of the S-IVB engine a few seconds later.

Apollo 11 entered a near-circular Earth orbit at an altitude of 100.4 nautical miles (185.9 km) by 98.9 nautical miles (183.2 km), twelve minutes into its flight. After one and a half orbits, a second ignition of the S-IVB engine pushed the spacecraft onto its trajectory toward the Moon with the trans-lunar injection (TLI) burn at 16:22:13 UTC. About 30 minutes later, with Collins in the left seat and at the controls, the transposition, docking, and extraction maneuver was performed. This involved separating Columbia from the spent S-IVB stage, turning around, and docking with Eagle still attached to the stage. After the LM was extracted, the combined spacecraft headed for the Moon, while the rocket stage flew on a trajectory past the Moon. This was done to avoid the third stage colliding with the spacecraft, the Earth, or the Moon. A slingshot effect from passing around the Moon threw it into an orbit around the Sun.

On July 19 at 17:21:50 UTC, Apollo 11 passed behind the Moon and fired its service propulsion engine to enter lunar orbit. In the thirty orbits that followed, the crew saw passing views of their landing site in the southern Sea of Tranquility about 12 miles (19 km) southwest of the crater Sabine D. The site was selected in part because it had been characterized as relatively flat and smooth by the automated Ranger 8 and Surveyor 5 landers and the Lunar Orbiter mapping spacecraft, and because it was unlikely to present major landing or EVA challenges. It lay about 25 kilometers (16 mi) southeast of the Surveyor 5 landing site, and 68 kilometers (42 mi) southwest of Ranger 8's crash site.

At 12:52:00 UTC on July 20, Aldrin and Armstrong entered Eagle, and began the final preparations for lunar descent. At 17:44:00 Eagle separated from Columbia. Collins, alone aboard Columbia, inspected Eagle as it pirouetted before him to ensure the craft was not damaged, and that the landing gear was correctly deployed. Armstrong exclaimed: "The Eagle has wings!"

As the descent began, Armstrong and Aldrin found themselves passing landmarks on the surface two or three seconds early, and reported that they were "long"; they would land miles west of their target point. Eagle was traveling too fast. The problem could have been mascons'97concen'adtra'adtions of high mass in a region or regions of the Moon's crust that contains a gravitational anomaly, potentially altering Eagle's trajectory. Flight Director Gene Kranz speculated that it could have resulted from extra air pressure in the docking tunnel. Or it could have been the result of Eagle's pirouette maneuver.

Five minutes into the descent burn, and 6,000 feet (1,800 m) above the surface of the Moon, the LM guidance computer (LGC) distracted the crew with the first of several unexpected 1201 and 1202 program alarms. Inside Mission Control Center, computer engineer Jack Garman told Guidance Officer Steve Bales it was safe to continue the descent, and this was relayed to the crew. The program alarms indicated "executive overflows", meaning the guidance computer could not complete all its tasks in real-time and had to postpone some of them. Margaret Hamilton, the Director of Apollo Flight Computer Programming at the MIT Charles Stark Draper Laboratory later recalled:

To blame the computer for the Apollo 11 problems is like blaming the person who spots a fire and calls the fire department. Actually, the computer was programmed to do more than recognize error conditions. A complete set of recovery programs was incorporated into the software. The software's action, in this case, was to eliminate lower priority tasks and re-establish the more important ones. The computer, rather than almost forcing an abort, prevented an abort. If the computer hadn't recognized this problem and taken recovery action, I doubt if Apollo 11 would have been the successful Moon landing it was.
During the mission, the cause was diagnosed as the rendezvous radar switch being in the wrong position, causing the computer to process data from both the rendezvous and landing radars at the same time. Software engineer Don Eyles concluded in a 2005 Guidance and Control Conference paper that the problem was due to a hardware design bug previously seen during testing of the first uncrewed LM in Apollo 5. Having the rendezvous radar on (so it was warmed up in case of an emergency landing abort) should have been irrelevant to the computer, but an electrical phasing mismatch between two parts of the rendezvous radar system could cause the stationary antenna to appear to the computer as dithering back and forth between two positions, depending upon how the hardware randomly powered up. The extra spurious cycle stealing, as the rendezvous radar updated an involuntary counter, caused the computer alarms.

When Armstrong again looked outside, he saw that the computer's landing target was in a boulder-strewn area just north and east of a 300-foot-diameter (91 m) crater (later determined to be West crater), so he took semi-automatic control. Armstrong considered landing short of the boulder field so they could collect geological samples from it, but could not since their horizontal velocity was too high. Throughout the descent, Aldrin called out navigation data to Armstrong, who was busy piloting Eagle. Now 107 feet (33 m) above the surface, Armstrong knew their propellant supply was dwindling and was determined to land at the first possible landing site.

Armstrong found a clear patch of ground and maneuvered the spacecraft towards it. As he got closer, now 250 feet (76 m) above the surface, he discovered his new landing site had a crater in it. He cleared the crater and found another patch of level ground. They were now 100 feet (30 m) from the surface, with only 90 seconds of propellant remaining. Lunar dust kicked up by the LM's engine began to impair his ability to determine the spacecraft's motion. Some large rocks jutted out of the dust cloud, and Armstrong focused on them during his descent so he could determine the spacecraft's speed.

A light informed Aldrin that at least one of the 67-inch (170 cm) probes hanging from Eagle's footpads had touched the surface a few moments before the landing and he said: "Contact light!" Armstrong was supposed to immediately shut the engine down, as the engineers suspected the pressure caused by the engine's own exhaust reflecting off the lunar surface could make it explode, but he forgot. Three seconds later, Eagle landed and Armstrong shut the engine down. Aldrin immediately said "Okay, engine stop. ACA'97out of detent." Armstrong acknowledged: "Out of detent. Auto." Aldrin continued: "Mode control'97both auto. Descent engine command override off. Engine arm'97off. 413 is in."

ACA was the Attitude Control Assembly'97the LM's control stick. Output went to the LGC to command the reaction control system (RCS) jets to fire. "Out of Detent" meant the stick had moved away from its centered position; it was spring-centered like the turn indicator in a car. LGC address 413 contained the variable that indicated the LM had landed.

Eagle landed at 20:17:40 UTC on Sunday July 20 with 216 pounds (98 kg) of usable fuel remaining. Information available to the crew and mission controllers during the landing showed the LM had enough fuel for another 25 seconds of powered flight before an abort without touchdown would have become unsafe, but post-mission analysis showed that the real figure was probably closer to 50 seconds. Apollo 11 landed with less fuel than most subsequent missions, and the astronauts encountered a premature low fuel warning. This was later found to be the result of the propellant sloshing more than expected, uncovering a fuel sensor. On subsequent missions, extra anti-slosh baffles were added to the tanks to prevent this.

Armstrong acknowledged Aldrin's completion of the post-landing checklist with "Engine arm is off", before responding to the CAPCOM, Charles Duke, with the words, "Houston, Tranquility Base here. The Eagle has landed." Armstrong's unrehearsed change of call sign from "Eagle" to "Tranquility Base" emphasized to listeners that landing was complete and successful. Duke mispronounced his reply as he expressed the relief at Mission Control: "Roger, Twan'97Tranquility, we copy you on the ground. You got a bunch of guys about to turn blue. We're breathing again. Thanks a lot."

Two and a half hours after landing, before preparations began for the EVA, Aldrin radioed to Earth:

This is the LM pilot. I'd like to take this opportunity to ask every person listening in, whoever and wherever they may be, to pause for a moment and contemplate the events of the past few hours and to give thanks in his or her own way.
He then took communion privately. At this time NASA was still fighting a lawsuit brought by atheist Madalyn Murray O'Hair (who had objected to the Apollo 8 crew reading from the Book of Genesis) demanding that their astronauts refrain from broadcasting religious activities while in space. For this reason, Aldrin chose to refrain from directly mentioning taking communion on the Moon. Aldrin was an elder at the Webster Presbyterian Church, and his communion kit was prepared by the pastor of the church, Dean Woodruff. Webster Presbyterian possesses the chalice used on the Moon and commemorates the event each year on the Sunday closest to July 20. The schedule for the mission called for the astronauts to follow the landing with a five-hour sleep period, but they chose to begin preparations for the EVA early, thinking they would be unable to sleep.

Preparations for Neil Armstrong and Buzz Aldrin to walk on the Moon began at 23:43. These took longer than expected; three and a half hours instead of two. During training on Earth, everything required had been neatly laid out in advance, but on the Moon the cabin contained a large number of other items as well, such as checklists, food packets, and tools. Six hours and thirty-nine minutes after landing Armstrong and Aldrin were ready to go outside, and Eagle was depressurized.

Eagle's hatch was opened at 02:39:33. Armstrong initially had some difficulties squeezing through the hatch with his portable life support system (PLSS). Some of the highest heart rates recorded from Apollo astronauts occurred during LM egress and ingress. At 02:51 Armstrong began his descent to the lunar surface. The remote control unit on his chest kept him from seeing his feet. Climbing down the nine-rung ladder, Armstrong pulled a D-ring to deploy the modular equipment stowage assembly (MESA) folded against Eagle's side and activate the TV camera.

Apollo 11 used slow-scan television (TV) incompatible with broadcast TV, so it was displayed on a special monitor and a conventional TV camera viewed this monitor (thus, a broadcast of a broadcast), significantly reducing the quality of the picture. The signal was received at Goldstone in the United States, but with better fidelity by Honeysuckle Creek Tracking Station near Canberra in Australia. Minutes later the feed was switched to the more sensitive Parkes radio telescope in Australia. Despite some technical and weather difficulties, ghostly black and white images of the first lunar EVA were received and broadcast to at least 600 million people on Earth. Copies of this video in broadcast format were saved and are widely available, but recordings of the original slow scan source transmission from the lunar surface were likely destroyed during routine magnetic tape re-use at NASA.

After describing the surface dust as "very fine-grained" and "almost like a powder", at 02:56:15, six and a half hours after landing, Armstrong stepped off Eagle's footpad and declared: "That's one small step for [a] man, one giant leap for mankind."

Armstrong intended to say "That's one small step for a man", but the word "a" is not audible in the transmission, and thus was not initially reported by most observers of the live broadcast. When later asked about his quote, Armstrong said he believed he said "for a man", and subsequent printed versions of the quote included the "a" in square brackets. One explanation for the absence may be that his accent caused him to slur the words "for a" together; another is the intermittent nature of the audio and video links to Earth, partly because of storms near Parkes Observatory. A more recent digital analysis of the tape claims to reveal the "a" may have been spoken but obscured by static. Other analysis points to the claims of static and slurring as "face-saving fabrication", and that Armstrong himself later admitted to misspeaking the line.

About seven minutes after stepping onto the Moon's surface, Armstrong collected a contingency soil sample using a sample bag on a stick. He then folded the bag and tucked it into a pocket on his right thigh. This was to guarantee there would be some lunar soil brought back in case an emergency required the astronauts to abandon the EVA and return to the LM. Twelve minutes after the sample was collected, he removed the TV camera from the MESA and made a panoramic sweep, then mounted it on a tripod. The TV camera cable remained partly coiled and presented a tripping hazard throughout the EVA. Still photography was accomplished with a Hasselblad camera that could be operated hand held or mounted on Armstrong's Apollo space suit. Aldrin joined Armstrong on the surface. He described the view with the simple phrase: "Magnificent desolation."

Armstrong said moving in the lunar gravity, one-sixth of Earth's, was "even perhaps easier than the simulations ... It's absolutely no trouble to walk around." Aldrin joined him on the surface and tested methods for moving around, including two-footed kangaroo hops. The PLSS backpack created a tendency to tip backward, but neither astronaut had serious problems maintaining balance. Loping became the preferred method of movement. The astronauts reported that they needed to plan their movements six or seven steps ahead. The fine soil was quite slippery. Aldrin remarked that moving from sunlight into Eagle's shadow produced no temperature change inside the suit, but the helmet was warmer in sunlight, so he felt cooler in shadow. The MESA failed to provide a stable work platform and was in shadow, slowing work somewhat. As they worked, the moonwalkers kicked up gray dust, which soiled the outer part of their suits.

The astronauts planted the Lunar Flag Assembly containing a flag of the United States on the lunar surface, in clear view of the TV camera. Aldrin remembered, "Of all the jobs I had to do on the Moon the one I wanted to go the smoothest was the flag raising." But the astronauts struggled with the telescoping rod and could only jam the pole about 2 inches (5 cm) into the hard lunar surface. Aldrin was afraid it might topple in front of TV viewers. But he gave "a crisp West Point salute". Before Aldrin could take a photo of Armstrong with the flag, President Richard Nixon spoke to them through a telephone-radio transmission, which Nixon called "the most historic phone call ever made from the White House." Nixon originally had a long speech prepared to read during the phone call, but Frank Borman, who was at the White House as a NASA liaison during Apollo 11, convinced Nixon to keep his words brief.

Nixon: Hello, Neil and Buzz. I'm talking to you by telephone from the Oval Room at the White House. And this certainly has to be the most historic telephone call ever made from the White House. I just can't tell you how proud we all are of what you have done. For every American, this has to be the proudest day of our lives. And for people all over the world, I am sure that they too join with Americans in recognizing what an immense feat this is. Because of what you have done, the heavens have become a part of man's world. And as you talk to us from the Sea of Tranquility, it inspires us to redouble our efforts to bring peace and tranquility to Earth. For one priceless moment in the whole history of man, all the people on this Earth are truly one: one in their pride in what you have done, and one in our prayers that you will return safely to Earth.

Armstrong: Thank you, Mr. President. It's a great honor and privilege for us to be here, representing not only the United States, but men of peace of all nations, and with interest and a curiosity, and men with a vision for the future. It's an honor for us to be able to participate here today.
They deployed the EASEP, which included a passive seismic experiment package used to measure moonquakes and a retroreflector array used for the lunar laser ranging experiment. Then Armstrong walked 196 feet (60 m) from the LM to snap photos at the rim of Little West Crater while Aldrin collected two core samples. He used the geologist's hammer to pound in the tubes'97the only time the hammer was used on Apollo 11'97but was unable to penetrate more than 6 inches (15 cm) deep. The astronauts then collected rock samples using scoops and tongs on extension handles. Many of the surface activities took longer than expected, so they had to stop documenting sample collection halfway through the allotted 34 minutes. Aldrin shoveled 6 kilograms (13 lb) of soil into the box of rocks in order to pack them in tightly. Two types of rocks were found in the geological samples: basalt and breccia. Three new minerals were discovered in the rock samples collected by the astronauts: armalcolite, tranquillityite, and pyroxferroite. Armalcolite was named after Armstrong, Aldrin, and Collins. All have subsequently been found on Earth.

While on the surface, Armstrong uncovered a plaque mounted on the LM ladder, bearing two drawings of Earth (of the Western and Eastern Hemispheres), an inscription, and signatures of the astronauts and President Nixon. The inscription read:

Here men from the planet Earth first set foot upon the Moon July 1969, A. D. We came in peace for all mankind.
At the behest of the Nixon administration to add a reference to God, NASA included the vague date as a reason to include A.D., which stands for Anno Domini, "in the year of our Lord" (although it should have been placed before the year, not after).

Mission Control used a coded phrase to warn Armstrong his metabolic rates were high, and that he should slow down. He was moving rapidly from task to task as time ran out. As metabolic rates remained generally lower than expected for both astronauts throughout the walk, Mission Control granted the astronauts a 15-minute extension. In a 2010 interview, Armstrong explained that NASA limited the first moonwalk's time and distance because there was no empirical proof of how much cooling water the astronauts' PLSS backpacks would consume to handle their body heat generation while working on the Moon.

Aldrin entered Eagle first. With some difficulty the astronauts lifted film and two sample boxes containing 21.55 kilograms (47.5 lb) of lunar surface material to the LM hatch using a flat cable pulley device called the Lunar Equipment Conveyor (LEC). This proved to be an inefficient tool, and later missions preferred to carry equipment and samples up to the LM by hand. Armstrong reminded Aldrin of a bag of memorial items in his sleeve pocket, and Aldrin tossed the bag down. Armstrong then jumped onto the ladder's third rung, and climbed into the LM. After transferring to LM life support, the explorers lightened the ascent stage for the return to lunar orbit by tossing out their PLSS backpacks, lunar overshoes, an empty Hasselblad camera, and other equipment. The hatch was closed again at 05:11:13. They then pressurized the LM and settled down to sleep.

Presidential speech writer William Safire had prepared an In Event of Moon Disaster announcement for Nixon to read in the event the Apollo 11 astronauts were stranded on the Moon. The remarks were in a memo from Safire to Nixon's White House Chief of Staff H. R. Haldeman, in which Safire suggested a protocol the administration might follow in reaction to such a disaster. According to the plan, Mission Control would "close down communications" with the LM, and a clergyman would "commend their souls to the deepest of the deep" in a public ritual likened to burial at sea. The last line of the prepared text contained an allusion to Rupert Brooke's First World War poem, "The Soldier".

While moving inside the cabin, Aldrin accidentally damaged the circuit breaker that would arm the main engine for liftoff from the Moon. There was a concern this would prevent firing the engine, stranding them on the Moon. A felt-tip pen was sufficient to activate the switch.

After more than 21+1uc0u8260 2 hours on the lunar surface, in addition to the scientific instruments, the astronauts left behind: an Apollo 1 mission patch in memory of astronauts Roger Chaffee, Gus Grissom, and Edward White, who died when their command module caught fire during a test in January 1967; two memorial medals of Soviet cosmonauts Vladimir Komarov and Yuri Gagarin, who died in 1967 and 1968 respectively; a memorial bag containing a gold replica of an olive branch as a traditional symbol of peace; and a silicon message disk carrying the goodwill statements by Presidents Eisenhower, Kennedy, Johnson, and Nixon along with messages from leaders of 73 countries around the world. The disk also carries a listing of the leadership of the US Congress, a listing of members of the four committees of the House and Senate responsible for the NASA legislation, and the names of NASA's past and then-current top management.

After about seven hours of rest, the crew was awakened by Houston to prepare for the return flight. Two and a half hours later, at 17:54:00 UTC, they lifted off in Eagle's ascent stage to rejoin Collins aboard Columbia in lunar orbit. Film taken from the LM ascent stage upon liftoff from the Moon reveals the American flag, planted some 25 feet (8 m) from the descent stage, whipping violently in the exhaust of the ascent stage engine. Aldrin looked up in time to witness the flag topple: "The ascent stage of the LM separated ... I was concentrating on the computers, and Neil was studying the attitude indicator, but I looked up long enough to see the flag fall over." Subsequent Apollo missions planted their flags farther from the LM.

During his day flying solo around the Moon, Collins never felt lonely. Although it has been said "not since Adam has any human known such solitude", Collins felt very much a part of the mission. In his autobiography he wrote: "this venture has been structured for three men, and I consider my third to be as necessary as either of the other two". In the 48 minutes of each orbit when he was out of radio contact with the Earth while Columbia passed round the far side of the Moon, the feeling he reported was not fear or loneliness, but rather "awareness, anticipation, satisfaction, confidence, almost exultation".

One of Collins' first tasks was to identify the lunar module on the ground. To give Collins an idea where to look, Mission Control radioed that they believed the lunar module landed about 4 miles (6.4 km) off target. Each time he passed over the suspected lunar landing site, he tried in vain to find the module. On his first orbits on the back side of the Moon, Collins performed maintenance activities such as dumping excess water produced by the fuel cells and preparing the cabin for Armstrong and Aldrin to return.

Just before he reached the dark side on the third orbit, Mission Control informed Collins there was a problem with the temperature of the coolant. If it became too cold, parts of Columbia might freeze. Mission Control advised him to assume manual control and implement Environmental Control System Malfunction Procedure 17. Instead, Collins flicked the switch on the system from automatic to manual and back to automatic again, and carried on with normal housekeeping chores, while keeping an eye on the temperature. When Columbia came back around to the near side of the Moon again, he was able to report that the problem had been resolved. For the next couple of orbits, he described his time on the back side of the Moon as "relaxing". After Aldrin and Armstrong completed their EVA, Collins slept so he could be rested for the rendezvous. While the flight plan called for Eagle to meet up with Columbia, Collins was prepared for a contingency in which he would fly Columbia down to meet Eagle.

Eagle rendezvoused with Columbia at 21:24 UTC on July 21, and the two docked at 21:35. Eagle's ascent stage was jettisoned into lunar orbit at 23:41. Just before the Apollo 12 flight, it was noted that Eagle was still likely to be orbiting the Moon. Later NASA reports mentioned that Eagle's orbit had decayed, resulting in it impacting in an "uncertain location" on the lunar surface. In 2021, however, some calculations show that the lander may still be in orbit.

On July 23, the last night before splashdown, the three astronauts made a television broadcast in which Collins commented:

 ... The Saturn V rocket which put us in orbit is an incredibly complicated piece of machinery, every piece of which worked flawlessly ... We have always had confidence that this equipment will work properly. All this is possible only through the blood, sweat, and tears of a number of people ... All you see is the three of us, but beneath the surface are thousands and thousands of others, and to all of those, I would like to say, "Thank you very much."
Aldrin added:

This has been far more than three men on a mission to the Moon; more, still, than the efforts of a government and industry team; more, even, than the efforts of one nation. We feel that this stands as a symbol of the insatiable curiosity of all mankind to explore the unknown ... Personally, in reflecting on the events of the past several days, a verse from Psalms comes to mind. "When I consider the heavens, the work of Thy fingers, the Moon and the stars, which Thou hast ordained; What is man that Thou art mindful of him?"
Armstrong concluded:

The responsibility for this flight lies first with history and with the giants of science who have preceded this effort; next with the American people, who have, through their will, indicated their desire; next with four administrations and their Congresses, for implementing that will; and then, with the agency and industry teams that built our spacecraft, the Saturn, the Columbia, the Eagle, and the little EMU, the spacesuit and backpack that was our small spacecraft out on the lunar surface. We would like to give special thanks to all those Americans who built the spacecraft; who did the construction, design, the tests, and put their hearts and all their abilities into those craft. To those people tonight, we give a special thank you, and to all the other people that are listening and watching tonight, God bless you. Good night from Apollo 11.
On the return to Earth, a bearing at the Guam tracking station failed, potentially preventing communication on the last segment of the Earth return. A regular repair was not possible in the available time but the station director, Charles Force, had his ten-year-old son Greg use his small hands to reach into the housing and pack it with grease. Greg was later thanked by Armstrong.

The aircraft carrier USS Hornet, under the command of Captain Carl J. Seiberlich, was selected as the primary recovery ship (PRS) for Apollo 11 on June 5, replacing its sister ship, the LPH USS Princeton, which had recovered Apollo 10 on May 26. Hornet was then at her home port of Long Beach, California. On reaching Pearl Harbor on July 5, Hornet embarked the Sikorsky SH-3 Sea King helicopters of HS-4, a unit which specialized in recovery of Apollo spacecraft, specialized divers of UDT Detachment Apollo, a 35-man NASA recovery team, and about 120 media representatives. To make room, most of Hornet's air wing was left behind in Long Beach. Special recovery equipment was also loaded, including a boilerplate command module used for training.

On July 12, with Apollo 11 still on the launch pad, Hornet departed Pearl Harbor for the recovery area in the central Pacific, in the vicinity of 10'b036uc0u8242 N 172'b024u8242 E. A presidential party consisting of Nixon, Borman, Secretary of State William P. Rogers and National Security Advisor Henry Kissinger flew to Johnston Atoll on Air Force One, then to the command ship USS Arlington in Marine One. After a night on board, they would fly to Hornet in Marine One for a few hours of ceremonies. On arrival aboard Hornet, the party was greeted by the Commander-in-Chief, Pacific Command (CINCPAC), Admiral John S. McCain Jr., and NASA Administrator Thomas O. Paine, who flew to Hornet from Pago Pago in one of Hornet's carrier onboard delivery aircraft.

Weather satellites were not yet common, but US Air Force Captain Hank Brandli had access to top-secret spy satellite images. He realized that a storm front was headed for the Apollo recovery area. Poor visibility which could make locating the capsule difficult, and strong upper-level winds which "would have ripped their parachutes to shreds" according to Brandli, posed a serious threat to the safety of the mission. Brandli alerted Navy Captain Willard S. Houston Jr., the commander of the Fleet Weather Center at Pearl Harbor, who had the required security clearance. On their recommendation, Rear Admiral Donald C. Davis, commander of Manned Spaceflight Recovery Forces, Pacific, advised NASA to change the recovery area, each man risking his career. A new location was selected 215 nautical miles (398 km) northeast.

This altered the flight plan. A different sequence of computer programs was used, one never before attempted. In a conventional entry, trajectory event P64 was followed by P67. For a skip-out re-entry, P65 and P66 were employed to handle the exit and entry parts of the skip. In this case, because they were extending the re-entry but not actually skipping out, P66 was not invoked and instead, P65 led directly to P67. The crew were also warned they would not be in a full-lift (heads-down) attitude when they entered P67. The first program's acceleration subjected the astronauts to 6.5 standard gravities (64 m/s2); the second, to 6.0 standard gravities (59 m/s2).

Before dawn on July 24, Hornet launched four Sea King helicopters and three Grumman E-1 Tracers. Two of the E-1s were designated as "air boss" while the third acted as a communications relay aircraft. Two of the Sea Kings carried divers and recovery equipment. The third carried photographic equipment, and the fourth carried the decontamination swimmer and the flight surgeon. At 16:44 UTC (05:44 local time) Columbia's drogue parachutes were deployed. This was observed by the helicopters. Seven minutes later Columbia struck the water forcefully 2,660 km (1,440 nmi) east of Wake Island, 380 km (210 nmi) south of Johnston Atoll, and 24 km (13 nmi) from Hornet, at 13'b019uc0u8242 N 169'b09u8242 W. 82 'b0F (28 'b0C) with 6 feet (1.8 m) seas and winds at 17 knots (31 km/h; 20 mph) from the east were reported under broken clouds at 1,500 feet (460 m) with visibility of 10 nautical miles (19 km; 12 mi) at the recovery site. Reconnaissance aircraft flying to the original splashdown location reported the conditions Brandli and Houston had predicted.

During splashdown, Columbia landed upside down but was righted within ten minutes by flotation bags activated by the astronauts. A diver from the Navy helicopter hovering above attached a sea anchor to prevent it from drifting. More divers attached flotation collars to stabilize the module and positioned rafts for astronaut extraction.

The divers then passed biological isolation garments (BIGs) to the astronauts, and assisted them into the life raft. The possibility of bringing back pathogens from the lunar surface was considered remote, but NASA took precautions at the recovery site. The astronauts were rubbed down with a sodium hypochlorite solution and Columbia wiped with Povidone-iodine to remove any lunar dust that might be present. The astronauts were winched on board the recovery helicopter. BIGs were worn until they reached isolation facilities on board Hornet. The raft containing decontamination materials was intentionally sunk.

After touchdown on Hornet at 17:53 UTC, the helicopter was lowered by the elevator into the hangar bay, where the astronauts walked the 30 feet (9.1 m) to the Mobile quarantine facility (MQF), where they would begin the Earth-based portion of their 21 days of quarantine. This practice would continue for two more Apollo missions, Apollo 12 and Apollo 14, before the Moon was proven to be barren of life, and the quarantine process dropped. Nixon welcomed the astronauts back to Earth. He told them: "[A]s a result of what you've done, the world has never been closer together before."

After Nixon departed, Hornet was brought alongside the 5-short-ton (4.5 t) Columbia, which was lifted aboard by the ship's crane, placed on a dolly and moved next to the MQF. It was then attached to the MQF with a flexible tunnel, allowing the lunar samples, film, data tapes and other items to be removed. Hornet returned to Pearl Harbor, where the MQF was loaded onto a Lockheed C-141 Starlifter and airlifted to the Manned Spacecraft Center. The astronauts arrived at the Lunar Receiving Laboratory at 10:00 UTC on July 28. Columbia was taken to Ford Island for deactivation, and its pyrotechnics made safe. It was then taken to Hickham Air Force Base, from whence it was flown to Houston in a Douglas C-133 Cargomaster, reaching the Lunar Receiving Laboratory on July 30.

In accordance with the Extra-Terrestrial Exposure Law, a set of regulations promulgated by NASA on July 16 to codify its quarantine protocol, the astronauts continued in quarantine. After three weeks in confinement (first in the Apollo spacecraft, then in their trailer on Hornet, and finally in the Lunar Receiving Laboratory), the astronauts were given a clean bill of health. On August 10, 1969, the Interagency Committee on Back Contamination met in Atlanta and lifted the quarantine on the astronauts, on those who had joined them in quarantine (NASA physician William Carpentier and MQF project engineer John Hirasaki), and on Columbia itself. Loose equipment from the spacecraft remained in isolation until the lunar samples were released for study.

On August 13, the three astronauts rode in ticker-tape parades in their honor in New York and Chicago, with an estimated six million attendees. On the same evening in Los Angeles there was an official state dinner to celebrate the flight, attended by members of Congress, 44 governors, Chief Justice of the United States Warren E. Burger and his predecessor, Earl Warren, and ambassadors from 83 nations at the Century Plaza Hotel. Nixon and Agnew honored each astronaut with a presentation of the Presidential Medal of Freedom.

The three astronauts spoke before a joint session of Congress on September 16, 1969. They presented two US flags, one to the House of Representatives and the other to the Senate, that they had carried with them to the surface of the Moon. The flag of American Samoa on Apollo 11 is on display at the Jean P. Haydon Museum in Pago Pago, the capital of American Samoa.

This celebration began a 38-day world tour that brought the astronauts to 22 foreign countries and included visits with the leaders of many countries. The crew toured from September 29 to November 5. Many nations honored the first human Moon landing with special features in magazines or by issuing Apollo 11 commemorative postage stamps or coins.

Humans walking on the Moon and returning safely to Earth accomplished Kennedy's goal set eight years earlier. In Mission Control during the Apollo 11 landing, Kennedy's speech flashed on the screen, followed by the words "TASK ACCOMPLISHED, July 1969". The success of Apollo 11 demonstrated the United States' technological superiority; and with the success of Apollo 11, America had won the Space Race.

New phrases permeated into the English language. "If they can send a man to the Moon, why can't they ...?" became a common saying following Apollo 11. Armstrong's words on the lunar surface also spun off various parodies.

While most people celebrated the accomplishment, disenfranchised Americans saw it as a symbol of the divide in America, evidenced by protesters led by Ralph Abernathy outside of Kennedy Space Center the day before Apollo 11 launched. NASA Administrator Thomas Paine met with Abernathy at the occasion, both hoping that the space program can spur progress also in other regards, such as poverty in the US. Paine was then asked, and agreed, to host protesters as spectators at the launch, and Abernathy, awestruck by the spectacle, prayed for the astronauts. Racial and financial inequalities frustrated citizens who wondered why money spent on the Apollo program was not spent taking care of humans on Earth. A poem by Gil Scott-Heron called "Whitey on the Moon" (1970) illustrated the racial inequality in the United States that was highlighted by the Space Race. The poem starts with:

A rat done bit my sister Nell.
(with Whitey on the moon)
Her face and arms began to swell.
(and Whitey's on the moon)
I can't pay no doctor bill.
(but Whitey's on the moon)
Ten years from now I'll be paying still.
(while Whitey's on the moon)
[...]

Twenty percent of the world's population watched humans walk on the Moon for the first time. While Apollo 11 sparked the interest of the world, the follow-on Apollo missions did not hold the interest of the nation. One possible explanation was the shift in complexity. Landing someone on the Moon was an easy goal to understand; lunar geology was too abstract for the average person. Another is that Kennedy's goal of landing humans on the Moon had already been accomplished. A well-defined objective helped Project Apollo accomplish its goal, but after it was completed it was hard to justify continuing the lunar missions.

While most Americans were proud of their nation's achievements in space exploration, only once during the late 1960s did the Gallup Poll indicate that a majority of Americans favored "doing more" in space as opposed to "doing less". By 1973, 59 percent of those polled favored cutting spending on space exploration. The Space Race had been won, and Cold War tensions were easing as the US and Soviet Union entered the era of d'e9tente. This was also a time when inflation was rising, which put pressure on the government to reduce spending. What saved the space program was that it was one of the few government programs that had achieved something great. Drastic cuts, warned Caspar Weinberger, the deputy director of the Office of Management and Budget, might send a signal that "our best years are behind us".

After the Apollo 11 mission, officials from the Soviet Union said landing humans on the Moon was dangerous and unnecessary. At the time the Soviet Union was attempting to retrieve lunar samples robotically. The Soviets publicly denied there was a race to the Moon, and indicated they were not making an attempt. Mstislav Keldysh said in July 1969, "We are concentrating wholly on the creation of large satellite systems." It was revealed in 1989 that the Soviets had tried to send people to the Moon, but were unable due to technological difficulties. The public's reaction in the Soviet Union was mixed. The Soviet government limited the release of information about the lunar landing, which affected the reaction. A portion of the populace did not give it any attention, and another portion was angered by it.

The Apollo 11 landing is referenced in the songs "Armstrong, Aldrin and Collins" by The Byrds on the 1969 album Ballad of Easy Rider and "Coon on the Moon" by Howlin' Wolf on the 1973 album The Back Door Wolf.

The command module Columbia went on a tour of the United States, visiting 49 state capitals, the District of Columbia, and Anchorage, Alaska. In 1971, it was transferred to the Smithsonian Institution, and was displayed at the National Air and Space Museum (NASM) in Washington, DC. It was in the central Milestones of Flight exhibition hall in front of the Jefferson Drive entrance, sharing the main hall with other pioneering flight vehicles such as the Wright Flyer, Spirit of St. Louis, Bell X-1, North American X-15 and Friendship 7.

Columbia was moved in 2017 to the NASM Mary Baker Engen Restoration Hangar at the Steven F. Udvar-Hazy Center in Chantilly, Virginia, to be readied for a four-city tour titled Destination Moon: The Apollo 11 Mission. This included Space Center Houston from October 14, 2017, to March 18, 2018, the Saint Louis Science Center from April 14 to September 3, 2018, the Senator John Heinz History Center in Pittsburgh from September 29, 2018, to February 18, 2019, and its last location at Museum of Flight in Seattle from March 16 to September 2, 2019. Continued renovations at the Smithsonian allowed time for an additional stop for the capsule, and it was moved to the Cincinnati Museum Center. The ribbon cutting ceremony was on September 29, 2019.

For 40 years Armstrong's and Aldrin's space suits were displayed in the museum's Apollo to the Moon exhibit, until it permanently closed on December 3, 2018, to be replaced by a new gallery which was scheduled to open in 2022. A special display of Armstrong's suit was unveiled for the 50th anniversary of Apollo 11 in July 2019. The quarantine trailer, the flotation collar and the flotation bags are in the Smithsonian's Steven F. Udvar-Hazy Center annex near Washington Dulles International Airport in Chantilly, Virginia, where they are on display along with a test lunar module.

The descent stage of the LM Eagle remains on the Moon. In 2009, the Lunar Reconnaissance Orbiter (LRO) imaged the various Apollo landing sites on the surface of the Moon, for the first time with sufficient resolution to see the descent stages of the lunar modules, scientific instruments, and foot trails made by the astronauts. The remains of the ascent stage lie at an unknown location on the lunar surface, after being abandoned and impacting the Moon. The location is uncertain because Eagle ascent stage was not tracked after it was jettisoned, and the lunar gravity field is sufficiently non-uniform to make the orbit of the spacecraft unpredictable after a short time.

In March 2012 a team of specialists financed by Amazon founder Jeff Bezos located the F-1 engines from the S-IC stage that launched Apollo 11 into space. They were found on the Atlantic seabed using advanced sonar scanning. His team brought parts of two of the five engines to the surface. In July 2013, a conservator discovered a serial number under the rust on one of the engines raised from the Atlantic, which NASA confirmed was from Apollo 11. The S-IVB third stage which performed Apollo 11's trans-lunar injection remains in a solar orbit near to that of Earth.

The main repository for the Apollo Moon rocks is the Lunar Sample Laboratory Facility at the Lyndon B. Johnson Space Center in Houston, Texas. For safekeeping, there is also a smaller collection stored at White Sands Test Facility near Las Cruces, New Mexico. Most of the rocks are stored in nitrogen to keep them free of moisture. They are handled only indirectly, using special tools. Over 100 research laboratories around the world conduct studies of the samples, and approximately 500 samples are prepared and sent to investigators every year.

In November 1969, Nixon asked NASA to make up about 250 presentation Apollo 11 lunar sample displays for 135 nations, the fifty states of the United States and its possessions, and the United Nations. Each display included Moon dust from Apollo 11. The rice-sized particles were four small pieces of Moon soil weighing about 50 mg and were enveloped in a clear acrylic button about as big as a United States half dollar coin. This acrylic button magnified the grains of lunar dust. The Apollo 11 lunar sample displays were given out as goodwill gifts by Nixon in 1970.

The Passive Seismic Experiment ran until the command uplink failed on August 25, 1969. The downlink failed on December 14, 1969. As of 2018, the Lunar Laser Ranging experiment remains operational.

Armstrong's Hasselblad camera was thought to be lost or left on the Moon surface.

In 2015, after Armstrong died in 2012, his widow contacted the National Air and Space Museum to inform them she had found a white cloth bag in one of Armstrong's closets. The bag contained various items, which should have been left behind in the lunar module, including the 16mm Data Acquisition Camera that had been used to capture images of the first Moon landing. The camera is currently on display at the National Air and Space Museum.

On July 15, 2009, Life.com released a photo gallery of previously unpublished photos of the astronauts taken by Life photographer Ralph Morse prior to the Apollo 11 launch. From July 16 to 24, 2009, NASA streamed the original mission audio on its website in real time 40 years to the minute after the events occurred. It is in the process of restoring the video footage and has released a preview of key moments. In July 2010, air-to-ground voice recordings and film footage shot in Mission Control during the Apollo 11 powered descent and landing was re-synchronized and released for the first time. The John F. Kennedy Presidential Library and Museum set up an Adobe Flash website that rebroadcasts the transmissions of Apollo 11 from launch to landing on the Moon.

On July 20, 2009, Armstrong, Aldrin, and Collins met with US President Barack Obama at the White House. "We expect that there is, as we speak, another generation of kids out there who are looking up at the sky and are going to be the next Armstrong, Collins, and Aldrin", Obama said. "We want to make sure that NASA is going to be there for them when they want to take their journey." On August 7, 2009, an act of Congress awarded the three astronauts a Congressional Gold Medal, the highest civilian award in the United States. The bill was sponsored by Florida Senator Bill Nelson and Florida Representative Alan Grayson.

A group of British scientists interviewed as part of the anniversary events reflected on the significance of the Moon landing:

It was carried out in a technically brilliant way with risks taken ... that would be inconceivable in the risk-averse world of today ... The Apollo programme is arguably the greatest technical achievement of mankind to date ... nothing since Apollo has come close [to] the excitement that was generated by those astronauts'97Armstrong, Aldrin and the 10 others who followed them.
On June 10, 2015, Congressman Bill Posey introduced resolution H.R. 2726 to the 114th session of the United States House of Representatives directing the United States Mint to design and sell commemorative coins in gold, silver and clad for the 50th anniversary of the Apollo 11 mission. On January 24, 2019, the Mint released the Apollo 11 Fiftieth Anniversary commemorative coins to the public on its website.

A documentary film, Apollo 11, with restored footage of the 1969 event, premiered in IMAX on March 1, 2019, and broadly in theaters on March 8.

The Smithsonian Institute's National Air and Space Museum and NASA sponsored the "Apollo 50 Festival" on the National Mall in Washington DC. The three day (July 18 to 20, 2019) outdoor festival featured hands-on exhibits and activities, live performances, and speakers such as Adam Savage and NASA scientists.

As part of the festival, a projection of the 363-foot (111 m) tall Saturn V rocket was displayed on the east face of the 555-foot (169 m) tall Washington Monument from July 16 through the 20th from 9:30 pm until 11:30 pm (EDT). The program also included a 17-minute show that combined full-motion video projected on the Washington Monument to recreate the assembly and launch of the Saturn V rocket. The projection was joined by a 40-foot (12 m) wide recreation of the Kennedy Space Center countdown clock and two large video screens showing archival footage to recreate the time leading up to the moon landing. There were three shows per night on July 19'9620, with the last show on Saturday, delayed slightly so the portion where Armstrong first set foot on the Moon would happen exactly 50 years to the second after the actual event.

On July 19, 2019, the Google Doodle paid tribute to the Apollo 11 Moon Landing, complete with a link to an animated YouTube video with voiceover by astronaut Michael Collins.

Aldrin, Collins, and Armstrong's sons were hosted by President Donald Trump in the Oval Office.

Footprints on the Moon, a 1969 documentary film by Bill Gibson and Barry Coe, about the Apollo 11 mission
Moonwalk One, a 1971 documentary film by Theo Kamecke
Apollo 11: As it Happened, a 1994 six-hour documentary on ABC News' coverage of the event
Apollo 11, a 2019 documentary film by Todd Douglas Miller with restored footage of the 1969 event
Chasing the Moon, a July 2019 PBS three-night six-hour documentary, directed by Robert Stone, examined the events leading up to the Apollo 11 mission. An accompanying book of the same name was also released.
8 Days: To the Moon and Back, a PBS and BBC Studios 2019 documentary film by Anthony Philipson re-enacting major portions of the Apollo 11 mission using mission audio recordings, new studio footage, NASA and news archives, and computer-generated imagery.
Apollo 11 insignia.png
Part of a series on
Apollo 11
Crew
Neil Armstrong 'b7 Buzz Aldrin 'b7 Michael Collins
Spacecraft
CM-107 Columbia 'b7 LM-5 Eagle
Landing site
Tranquility Base
Recovery vessels
Helicopter 66 'b7 USS Hornet
Commemoration
Anniversaries 'b7 50th Anniversary commemorative coins 'b7 Eisenhower dollar 'b7 Anthony dollar
Related
British TV coverage 'b7 Goodwill messages 'b7 In popular culture 'b7 Lunar sample display 'b7 Missing tapes
vte
Apollo in Real Time '96 Interactive website of Apollo 11, 13, and 17
Exploration of the Moon '96 Various missions to the Moon
List of missions to the Moon
Moon landing conspiracy theories
^ Jump up to: a b Eric Jones of the Apollo Lunar Surface Journal explains that the indefinite article "a" was intended, whether or not it was said; the intention was to contrast a man (an individual's action) and mankind (as a species).
In some of the following sources, times are shown in the format hours:minutes:seconds (e.g. 109:24:15), referring to the mission's Ground Elapsed Time (GET), based on the official launch time of July 16, 1969, 13:32:00 UTC (000:00:00 GET).

Fireworks are a class of low explosive pyrotechnic devices used for aesthetic and entertainment purposes. The most common use of a firework is as part of a fireworks display (also called a fireworks show or pyrotechnics), a display of the effects produced by firework devices.

Fireworks take many forms to produce the four primary effects: noise, light, smoke, as well as floating materials (confetti most notably). They may be designed to burn with colored flames and sparks including red, orange, yellow, green, blue, purple and silver. Displays are common throughout the world and are the focal point of many cultural and religious celebrations.

Fireworks are generally classified as to where they perform, either as a ground or aerial firework. In the latter case they may provide their own propulsion (skyrocket) or be shot into the air by a mortar (aerial shell).

The most common feature of fireworks is a paper or pasteboard tube or casing filled with the combustible material, often pyrotechnic stars. A number of these tubes or cases are often combined so as to make when kindled, a great variety of sparkling shapes, often variously colored. A skyrocket is a common form of firework, although the first skyrockets were used in warfare. The aerial shell, however, is the backbone of today's commercial aerial display, and a smaller version for consumer use is known as the festival ball in the United States.

Fireworks were originally invented in China. Cultural events and festivities such as the Chinese New Year and the Mid-Autumn Moon Festival were and still are times when fireworks are guaranteed sights. China is the largest manufacturer and exporter of fireworks in the world.

Silent fireworks are becoming popular for providing all the beauty without the added explosive sounds imitating artillery and warfare that traumatize pets, wildlife, and many humans. The Italian town of Collecchio switched to silent fireworks in 2015, mandating the switch.

The earliest fireworks came from China during the Song dynasty (960'961279). Fireworks were used to accompany many festivities. The art and science of firework making has developed into an independent profession. In China, pyrotechnicians were respected for their knowledge of complex techniques in mounting firework displays.

During the Han dynasty (202 BC '96 220 AD), people threw bamboo stems into a fire to produce an explosion with a loud sound. In later times, gunpowder packed into small containers was used to mimic the sounds of burning bamboo. Exploding bamboo stems and gunpowder firecrackers were interchangeably known as baozhu (
f1 'b1'ac'd6'f1
f0 ) or baogan (
f1 'b1'ac'b8'cd
f0 ). It was during the Song dynasty that people manufactured the first firecrackers comprising tubes made from rolled sheets of paper containing gunpowder and a fuse. They also strung these firecrackers together into large clusters, known as bian (lit. "whip") or bianpao (lit. "whip cannon"), so the firecrackers could be set off one by one in close sequence. By the 12th and possibly the 11th century, the term baozhang (
f1 'b1'ac'd5'cc
f0 ) was used to specifically refer to gunpowder firecrackers.

During the Song dynasty, many of the common people could purchase various kinds of fireworks from market vendors. Grand displays of fireworks were also known to be held. In 1110, a large fireworks display in a martial demonstration was held to entertain Emperor Huizong of Song (r. 1100'961125) and his court. A record from 1264 states that a rocket-propelled firework went off near the Empress Dowager Gong Sheng and startled her during a feast held in her honor by her son Emperor Lizong of Song (r. 1224'961264). Rocket propulsion was common in warfare, as evidenced by the Huolongjing compiled by Liu Bowen (1311'961375) and Jiao Yu (fl. c. 1350'961412). In 1240 the Arabs acquired knowledge of gunpowder and its uses from China. A Syrian named Hasan al-Rammah wrote of rockets, fireworks, and other incendiaries, using terms that suggested he derived his knowledge from Chinese sources, such as his references to fireworks as "Chinese flowers".

In regards to colored fireworks, this was derived and developed from earlier (possibly Han dynasty or soon thereafter) Chinese application of chemical substances to create colored smoke and fire. Such application appears in the Huolongjing (14th century) and Wubeizhi (preface of 1621, printed 1628), which describes recipes, several of which used low-nitrate gunpowder, to create military signal smokes with various colors. In the Wubei Huolongjing (
f1 'ce'e4'82'e4'bb'f0'fd'88'bd'9b
f0 ; Ming, completed after 1628), two formulas appears for firework-like signals, the sanzhangju (
f1 'c8'fd'd5'c9'be'd5
f0 ) and baizhanglian (
f1 'b0'd9'd5'c9'c9'8f
f0 ), that produces silver sparkles in the smoke. In the Huoxil'fce (
f1 'bb'f0'91'f2'c2'd4
f0 ; 1753) by Zhao Xuemin (
f1 'da'77'8c'57'c3'f4
f0 ), there are several recipes with low-nitrate gunpowder and other chemical substances to tint flames and smoke. These included, for instance, arsenical sulphide for yellow, copper acetate (verdigris) for green, lead carbonate for lilac-white, and mercurous chloride (calomel) for white. The Chinese pyrotechnics were described by the French author Antoine Caillot (1818): "It is certain that the variety of colours which the Chinese have the secret of giving to flame is the greatest mystery of their fireworks." Similarly, the English geographer Sir John Barrow (ca. 1797) wrote "The diversity of colours indeed with which the Chinese have the secret of cloathing fire seems to be the chief merit of their pyrotechny."

Fireworks were produced in Europe by the 14th century, becoming popular by the 17th century. Lev Izmailov, ambassador of Peter the Great, once reported from China: "They make such fireworks that no one in Europe has ever seen." In 1758, the Jesuit missionary Pierre Nicolas le Ch'e9ron d'Incarville, living in Beijing, wrote about the methods and composition on how to make many types of Chinese fireworks to the Paris Academy of Sciences, which revealed and published the account five years later. Am'e9d'e9e-Fran'e7ois Fr'e9zier published his revised work Trait'e9 des feux d'artice pour le spectacle (Treatise on Fireworks) in 1747 (originally 1706), covering the recreational and ceremonial uses of fireworks, rather than their military uses. Music for the Royal Fireworks was composed by George Frideric Handel in 1749 to celebrate the Peace treaty of Aix-la-Chapelle, which had been declared the previous year.

"Prior to the nineteenth century and the advent of modern chemistry they [fireworks] must have been relatively dull and unexciting." Bertholet in 1786 discovered that oxidations with potassium chlorate resulted in a violet emission. Subsequent developments revealed that oxidations with the chlorates of barium, strontium, copper, and sodium result in intense emission of bright colors. The isolation of metallic magnesium and aluminium marked another breakthrough as these metals burn with an intense silvery light.

Improper use of fireworks may be dangerous, both to the person operating them (risks of burns and wounds) and to bystanders; in addition, they may start fires after landing on flammable material. For this reason, the use of fireworks is generally legally restricted. Display fireworks are restricted by law for use by professionals; consumer items, available to the public, are smaller versions containing limited amounts of explosive material to reduce potential danger.

Fireworks are also a problem for animals, both domestic and wild, which can be frightened by their noise, leading to them running away, often into danger, or hurting themselves on fences or in other ways in an attempt to escape. Frightened birds also may abandon nests and not return to complete rearing their young.

Pyrotechnical competitions involving fireworks are held in many countries. The most prestigious fireworks competition is the Montreal Fireworks Festival, an annual competition held in Montreal, Quebec, Canada. Another magnificent competition is Le Festival d'Art Pyrotechnique held in the summer annually at the Bay of Cannes in C'f4te d'Azur, France. The World Pyro Olympics is an annual competition amongst the top fireworks companies in the world. It is held in Manila, Philippines. The event is one of the largest and most intense international fireworks competitions.

Enthusiasts in the United States have formed clubs which unite hobbyists and professionals. The groups provide safety instruction and organize meetings and private "shoots" at remote premises where members shoot commercial fireworks as well as fire pieces of their own manufacture. Clubs secure permission to fire items otherwise banned by state or local ordinances. Competition among members and between clubs, demonstrating everything from single shells to elaborate displays choreographed to music, are held. One of the oldest clubs is Crackerjacks, Inc., organized in 1976 in the Eastern Seaboard region of the U.S.

The Pyrotechnics Guild International, Inc. or PGI, founded in 1969, is an independent worldwide nonprofit organization of amateur and professional fireworks enthusiasts. It is notable for its large number of members, around 3,500 in total. The PGI exists solely to further the safe usage and enjoyment of both professional grade and consumer grade fireworks while both advancing the art and craft of pyrotechnics and preserving its historical aspects. Each August the PGI conducts its annual week-long convention, where some the world's biggest and best fireworks displays occur. Vendors, competitors, and club members come from around the US and from various parts of the globe to enjoy the show and to help out at this all-volunteer event. Aside from the nightly firework shows, the competition is a highlight of the convention. This is a completely unique event where individual classes of hand-built fireworks are competitively judged, ranging from simple fireworks rockets to extremely large and complex aerial shells. Some of the biggest, best, most intricate fireworks displays in the United States take place during the convention week.

Amateur and professional members can come to the convention to purchase fireworks, paper goods, novelty items, non-explosive chemical components and much more at the PGI trade show. Before the nightly fireworks displays and competitions, club members have a chance to enjoy open shooting of any and all legal consumer or professional grade fireworks, as well as testing and display of hand-built fireworks. The week ends with the Grand Public Display on Friday night, which gives the chosen display company a chance to strut their stuff in front of some of the world's biggest fireworks aficionados. The stakes are high and much planning is put into the show. In 1994 a shell of 36 inches (914 mm) in diameter was fired during the convention, more than twice as large as the largest shell usually seen in the US, and shells as large as 24 inches (610 mm) are frequently fired.

Canada
Both fireworks and firecrackers are a popular tradition during Halloween in Vancouver, although apparently this is not the custom elsewhere in Canada.

Ireland
In the Republic of Ireland and Northern Ireland there are many fireworks displays, during the Halloween season. The largest are in the cities of Belfast, Derry, and Dublin. The 2010 Derry Halloween fireworks attracted an audience of more than 20,000 people. The sale of fireworks is strongly restricted in the Republic of Ireland, although many illegal fireworks are sold throughout October or smuggled from Northern Ireland. In the Republic the maximum punishment for possessing fireworks without a licence, or lighting fireworks in a public place, is a '8010,000 fine and a five-year prison sentence.

United States
Two firework displays on All Hallows' Eve in the United States are the annual "Happy Hallowishes" show at Walt Disney World's Magic Kingdom "Mickey's Not-So-Scary Halloween Party" event, which began in 2005, and the "Halloween Screams" at Disneyland Park, which began in 2009.

In Australia, fireworks displays are used in the public celebration of major events such as New Year's Eve and Australia Day. Notable annual fireworks events include the Sydney New Year's Eve Midnight Fireworks show and the City of Perth Skyworks.

In France, fireworks are traditionally displayed on the eve of Bastille day (14 July) to commemorate the French revolution and the storming of the Bastille on that same day in 1789. Every city in France lights up the sky for the occasion with a special mention to Paris that offers a spectacle around the Eiffel Tower.

In Hungary fireworks are used on 20 August, which is a national celebration day

Indians throughout the world celebrate with fireworks as part of their popular "festival of lights" (Diwali) in Oct-Nov every year.

During the summer in Japan, fireworks festivals (
f1 'bb'a8'bb'f0'b4'f3'bb'e1
f0 , hanabi taikai) are held nearly every day someplace in the country, in total numbering more than 200 during August. The festivals consist of large fireworks shows, the largest of which use between 100,000 and 120,000 rounds (Tondabayashi, Osaka), and can attract more than 800,000 spectators. Street vendors set up stalls to sell various drinks and staple Japanese food (such as yakisoba, okonomiyaki, takoyaki, kakiguc0u333 ri (shaved ice), and traditionally held festival games, such as kingyo-sukui, or goldfish scooping.

Even today, men and women attend these events wearing the traditional yukata, summer kimono, or jinbei, and gather in large social circles of family or friends to sit picnic-like, eating and drinking, while watching the show.

The first fireworks festival in Japan was held in 1733.

Sumidagawa Fireworks Festival is one of the many being celebrated annually throughout Japan in summer.

Fireworks have been used in Malta for hundreds of years. When the islands were ruled by the Order of St John, fireworks were used on special occasions such as the election of a new Grand Master, the appointment of a new Pope or the birth of a prince.

Nowadays, fireworks are used in village feasts throughout the summer. The Malta International Fireworks Festival is also held annually.

Pyrotechnics experts from around the world have competed in Monte Carlo, Monaco since 1966. The festival runs from July to August every year, and the winner returns in 18 November for the fireworks display on the night before the National Day of Monaco. The event is held in Port Hercule, beginning at around 9:30pm every night, depending on the sunset.

The Singapore Fireworks Celebrations (previously the Singapore Fireworks Festival) is an annual event held in Singapore as part of its National Day celebrations. The festival features local and foreign teams which launch displays on different nights. While currently non-competitive in nature, the organizer has plans to introduce a competitive element in the future.

The annual festival has grown in magnitude, from 4,000 rounds used in 2004, to 6,000 in 2005, to more than 9,100 in 2006.

Busan International Fireworks Festival is one of the most significant fireworks festivals in Asia.

In Switzerland fireworks are often used on 1 August, which is a national celebration day.

One of the biggest occasions for fireworks in the UK is Guy Fawkes Night held each year on 5 November, to celebrate the foiling of the Catholic Gunpowder Plot on 5 November 1605, an attempt to kill King James I. The Guardian newspaper said in 2008 that Britain's biggest Guy Fawkes night events were:

After Dark fireworks, Sheffield homepage
Bangers on the Beach (Holyhead Round Table charity fireworks), Holyhead homepage
Battel Bonfire in Battle, East Sussex homepage
Blackheath Fireworks, London homepage
Bught Park fireworks, Inverness homepage
Fireworks with Vikings, Tutbury, Staffordshire homepage
Flaming Tar Barrels, Ottery St Mary homepage
Glasgow Green fireworks homepage
Halloween Happening fireworks, Derry homepage
Midsummer Common, Cambridge homepage
Sparks in the Park (Cardiff Round Table charity fireworks), Cardiff homepage
The main firework celebrations in the UK are by the public who buy from many suppliers.

America's earliest settlers brought their enthusiasm for fireworks to the United States. Fireworks and black ash were used to celebrate important events long before the American Revolutionary War. The very first celebration of Independence Day was in 1777, six years before Americans knew whether or not the new nation would survive the war; fireworks were a part of all festivities. In 1789, George Washington's inauguration was accompanied by a fireworks display.. George Marshall was an American naval hero during the War of 1812 and other campaigns. He was a Master Gunner and pyrotechnics specialist who wrote Marshall's Practical Marine Gunnery in 1822. The book outlines chemical formulas for the composition of fireworks. This early fascination with fireworks' noise and color continues today with fireworks displays commonly included in Independence Day celebrations.

In 2004, Disneyland, in Anaheim, California, pioneered the commercial use of aerial fireworks launched with compressed air rather than gunpowder. The display shell explodes in the air using an electronic timer. The advantages of compressed air launch are a reduction in fumes, and much greater accuracy in height and timing. The Walt Disney Company is now the largest consumer of fireworks in the world.

In addition to large public displays, people often buy small quantities of fireworks for their own celebrations. Fireworks on general sale are usually less powerful than professional fireworks. Types include firecrackers, rockets, cakes (multishot aerial fireworks), and smoke balls.

Fireworks can also be used in an agricultural capacity as to frighten away birds.

Colors in fireworks are usually generated by pyrotechnic stars'97usually just called stars'97which produce intense light when ignited. Stars contain four basic types of ingredients.

A fuel
An oxidizer'97a compound that combines with the fuel to produce intense heat
Color-producing salts (when the fuel itself is not the colorant)
A binder which holds the pellet together.
Some of the more common color-producing compounds are tabulated here. The color of a compound in a firework will be the same as its color in a flame test (shown at right). Not all compounds that produce a colored flame are appropriate for coloring fireworks, however. Ideal colorants will produce a pure, intense color when present in moderate concentration.

The color of sparks is limited to red/orange, yellow/gold and white/silver. This is explained by light emission from an incandescent solid particle in contrast to the element-specific emission from the vapor phase of a flame. Light emitted from a solid particle is defined by black-body radiation. Low boiling metals can form sparks with an intensively colored glowing shell surrounding the basic particle. This is caused by vapor phase combustion of the metal.

The brightest stars, often called Mag Stars, are fueled by aluminium. Magnesium is rarely used in the fireworks industry due to its lack of ability to form a protective oxide layer. Often an alloy of both metals called magnalium is used.

Many of the chemicals used in the manufacture of fireworks are non-toxic, while many more have some degree of toxicity, can cause skin sensitivity, or exist in dust form and are thereby inhalation hazards. Still others are poisons if directly ingested or inhaled.

The following table lists the principal elements used in modern pyrotechnics. Some elements are used in their elemental form such as particles of titanium, aluminium, iron, zirconium, and magnesium. These elements burn in the presence of air (O2) or oxidants (perchlorate, chlorate). Most elements in pyrotechnics are in the form of salts.

A cake is a cluster of individual tubes linked by fuse that fires a series of aerial effects. Tube diameters can range in size from 1uc0u8260 4'964 inches (6.4'96101.6 mm), and a single cake can have more than 1,000 shots. The variety of effects within individual cakes is often such that they defy descriptive titles and are instead given cryptic names such as "Bermuda Triangle", "Pyro Glyphics", "Waco Wakeup", and "Poisonous Spider", to name a few. Others are simply quantities of 2.5'964 in (64'96102 mm) shells fused together in single-shot tubes.

A shell containing several large stars that travel a short distance before breaking apart into smaller stars, creating a crisscrossing grid-like effect. Strictly speaking, a crossette star should split into 4 pieces which fly off symmetrically, making a cross. Once limited to silver or gold effects, colored crossettes such as red, green, or white are now very common.

A spherical break of colored stars, similar to a peony, but with stars that leave a visible trail of sparks.

Essentially the same as a peony shell, but with fewer and larger stars. These stars travel a longer-than-usual distance from the shell break before burning out. For instance, if a 3 in (76 mm) peony shell is made with a star size designed for a 6 in (152 mm) shell, it is then considered a dahlia. Some dahlia shells are cylindrical rather than spherical to allow for larger stars.

A type of Chrysanthemum or Peony, with a center cluster of non-moving stars, normally of a contrasting color or effect.

Inserts that propel themselves rapidly away from the shell burst, often resembling fish swimming away.

Named for the shape of its break, this shell features heavy long-burning tailed stars that only travel a short distance from the shell burst before free-falling to the ground. Also known as a waterfall shell. Sometimes there is a glittering through the "waterfall".

Kamuro is a Japanese word meaning "boys haircut", which is what this shell resembles when fully exploded in the air. It is a dense burst of glittering silver or gold stars which leave a heavy glitter trail and shine bright in the night's sky.

A mine (a.k.a. pot 'e0 feu) is a ground firework that expels stars and/or other garnitures into the sky. Shot from a mortar like a shell, a mine consists of a canister with the lift charge on the bottom with the effects placed on top. Mines can project small reports, serpents, small shells, as well as just stars. Although mines up to 12 inches (305 mm) diameter appear on occasion, they are usually between 3'965 inches (76'96127 mm), in diameter.

A large shell containing several smaller shells of various sizes and types. The initial burst scatters the shells across the sky before they explode. Also called a bouquet shell. When a shell contains smaller shells of the same size and type, the effect is usually referred to as "Thousands". Very large bouquet shells (up to 48 inches [1,219 mm]) are frequently used in Japan.

Bang
The bang is the most common effect in fireworks and sounds like artillery cannon being fired; technically a "report". Silent fireworks have all of the visual effects, however. The "salute" effect is even more pronounced and sometimes is banned.
Crackle
The firework produces a crackling sound.
Hummer
Tiny tube fireworks that are ejected into the air spinning with such force that they shred their outer coating, in doing so they whizz and hum.
Whistle
High pitched often very loud screaming and screeching created by the resonance of gas. This is caused by a very fast strobing (on/off burning stage) of the fuel. The rapid bursts of gas from the fuel vibrate the air many hundreds of times per second causing the familiar whistling sound. It is not, as is commonly thought, made in the conventional way that musical instruments are using specific tube shapes or apertures. Common whistle fuels contain benzoate or salicylate compounds and a suitable oxidizer such as potassium perchlorate.
A shell containing a relatively few large comet stars arranged in such a way as to burst with large arms or tendrils, producing a palm tree-like effect. Proper palm shells feature a thick rising tail that displays as the shell ascends, thereby simulating the tree trunk to further enhance the "palm tree" effect. One might also see a burst of color inside the palm burst (given by a small insert shell) to simulate coconuts.

A spherical break of colored stars that burn without a tail effect. The peony is the most commonly seen shell type.

A shell with stars specially arranged so as to create a ring. Variations include smiley faces, hearts, and clovers.

A Roman candle is a long tube containing several large stars which fire at a regular interval. These are commonly arranged in fan shapes or crisscrossing shapes, at a closer proximity to the audience. Some larger Roman candles contain small shells (bombettes) rather than stars.

A shell intended to produce a loud report rather than a visual effect. Salute shells usually contain flash powder, producing a quick flash followed by a very loud report resembling military artillery. Titanium may be added to the flash powder mix to produce a cloud of bright sparks around the flash. Salutes are commonly used in large quantities during finales to create intense noise and brightness. They are often cylindrical in shape to allow for a larger payload of flash powder, but ball shapes are common and cheaper as well. Salutes are also called Maroons.

A shell containing a fast burning tailed or charcoal star that is burst very hard so that the stars travel in a straight and flat trajectory before slightly falling and burning out. This appears in the sky as a series of radial lines much like the legs of a spider.

An effect created by large, slow-burning stars within a shell that leave a trail of large glittering sparks behind and make a sizzling noise. The "time" refers to the fact that these stars burn away gradually, as opposed to the standard brocade "rain" effect where a large amount of glitter material is released at once.

Similar to a chrysanthemum, but with long-burning silver or gold stars that produce a soft, dome-shaped weeping willow-like effect.

Farfalle is an effect in Italian fireworks with spinning silver sprays in the air.

Similar to a Farfalle but has spinning stars

Fireworks pose risks of injury to people, and of damage, largely as a fire hazard. The explosions added to fireworks may frighten and traumatize animals and people. Wildlife may die while fleeing in a panic and in affected areas birds may abandon forever nests containing their young.

Fireworks produce smoke and dust that may contain residues of heavy metals, sulfur-coal compounds and some low concentration toxic chemicals. These by-products of fireworks combustion will vary depending on the mix of ingredients of a particular firework. (The color green, for instance, may be produced by adding the various compounds and salts of barium, some of which are toxic, and some of which are not.) Some fishers have noticed and reported to environmental authorities that firework residues can hurt fish and other water-life because some may contain toxic compounds (such as antimony sulfide or arsenic). This is a subject of much debate due to the fact that large-scale pollution from other sources makes it difficult to measure the amount of pollution that comes specifically from fireworks. The possible toxicity of any fallout may also be affected by the amount of black powder used, type of oxidizer, colors produced and launch method.

Perchlorate salts, when in solid form, dissolve and move rapidly in groundwater and surface water. Even in low concentrations in drinking water supplies, perchlorate ions are known to inhibit the uptake of iodine by the thyroid gland. As of 2010, there are no federal drinking water standards for perchlorates in the United States, but the US Environmental Protection Agency has studied the impacts of perchlorates on the environment as well as drinking water.

Several U.S. states have enacted drinking water standard for perchlorates, including Massachusetts in 2006. California's legislature enacted AB 826, the Perchlorate Contamination Prevention Act of 2003, requiring California's Department of Toxic Substance Control (DTSC) to adopt regulations specifying best management practices for perchlorate-containing substances. The Perchlorate Best Management Practices were adopted on 31 December 2005 and became operative on 1 July 2006. California issued drinking water standards in 2007. Several other states, including Arizona, Maryland, Nevada, New Mexico, New York, and Texas have established non-enforceable, advisory levels for perchlorates.

The courts have also taken action with regard to perchlorate contamination. For example, in 2003, a federal district court in California found that Comprehensive Environmental Response, Compensation and Liability Act (CERCLA) applied because perchlorate is ignitable and therefore a "characteristic" hazardous waste.

Pollutants from fireworks raise concerns because of potential health risks associated with hazardous by-products. For most people the effects of exposure to low levels of toxins from many sources over long periods are unknown. For persons with asthma or multiple chemical sensitivity the smoke from fireworks may aggravate existing health problems.

Pollution is also a concern because fireworks often contain heavy metals as source of color. However, gunpowder smoke and the solid residues are basic, and as such the net effect of fireworks on acid rain is debatable. What is not disputed is that most consumer fireworks leave behind a considerable amount of solid debris, including both readily biodegradable components as well as nondegradable plastic items. Concerns over pollution, consumer safety, and debris have restricted the sale and use of consumer fireworks in many countries. Professional displays, on the other hand, remain popular around the world.

Others argue that alleged concern over pollution from fireworks constitutes a red herring, since the amount of contamination from fireworks is minuscule in comparison to emissions from sources such as the burning of fossil fuels. In the US, some states and local governments restrict the use of fireworks in accordance with the Clean Air Act which allows laws relating to the prevention and control of outdoor air pollution to be enacted. Few governmental entities, by contrast, effectively limit pollution from burning fossil fuels such as diesel fuel or coal. Coal-fueled electricity generation alone is a much greater source of heavy metal contamination in the environment than fireworks.

Some companies within the U.S. fireworks industry claim they are working with Chinese manufacturers to reduce and ultimately hope to eliminate of the pollutant perchlorate.

Fireworks are illegal in most Australian states and territories, unless part of a display by a licensed pyrotechnician and with a permit. However Tasmania, ACT and Northern Territory allow consumer use with a permit (dependent on calendar date and circumstances). On 1 July for Territory Day you can freely use fireworks without a permit in the Northern Territory.

Small novelties such as party poppers and sparklers are legal for consumers across Australia.

On 24 August 2009, the ACT Government announced a complete ban on backyard fireworks.

The use, storage and sale of commercial-grade fireworks in Canada is licensed by Natural Resources Canada's Explosive Regulatory Division (ERD). Unlike their consumer counterpart, commercial-grade fireworks function differently, and come in a wide range of sizes from 50 mm (2 inches) up to 300 mm (11+13uc0u8260 16 inches) or more in diameter. Commercial grade fireworks require a Fireworks Operator Certificate (FOC), obtained from the ERD by completing a one-day safety course. There are two categories of FOC: one for pyrotechnics (those used on stage and in movies) and another for display fireworks (those used in dedicated fireworks shows). Each requires completion of its own course, although there are special categories of FOC which allow visiting operators to run their shows with the assistance of a Canadian supervisor.

The display fireworks FOC has 2 levels: assistant (which allows you to work under a qualified supervisor until you are familiar with the basics), and fully licensed. A fully licensed display fireworks operator can also be further endorsed for marine launch, flying saucers, and other more technically demanding fireworks displays.

The pyrotechnician FOC has 3 levels: pyrotechnician (which allows work under a supervisor), supervising pyrotechnician, and special effects pyrotechnician (which allows the fabrication of certain types of pyrotechnic devices). Additionally, a special effects pyrotechnician can be endorsed for the use of detonating cord.

Since commercial-grade fireworks are shells which are loaded into separate mortars by hand, there is danger in every stage of the setup. Setup of these fireworks involves the placement and securing of mortars on wooden or wire racks; loading of the shells; and if electronically firing, wiring and testing. The mortars are generally made of FRE (fiber-reinforced epoxy) or HDPE (high-density polyethylene). Older mortars made of sheet steel have been banned by most countries due to the problem of shrapnel produced during a misfire.

Setup of mortars in Canada for an oblong firing site require that a mortar be configured at an angle of 10 to 15 degrees down-range with a safety distance of at least 200 meters (660 ft) down-range and 100 meters (330 ft) surrounding the mortars, plus distance adjustments for wind speed and direction. In June 2007, the ERD approved circular firing sites for use with vertically fired mortars with a safety distance of at least 175-meter (574 ft) radius, plus distance adjustments for wind speed and direction.

Loading of shells is a delicate process, and must be done with caution, and a loader must ensure not only the mortar is clean, but also make sure that no part of their body is directly over the mortar in case of a premature fire. Wiring the shells is a painstaking process; whether the shells are being fired manually or electronically, any "chain fusing" or wiring of electrical ignitors, care must be taken to prevent the fuse (an electrical match, often incorrectly called a squib) from igniting. If the setup is wired electrically, the electrical matches are usually plugged into a "firing rail" or "breakout box" that runs back to the main firing board; from there, the Firing Board is simply hooked up to a car battery, and can proceed with firing the show when ready.

Since commercial-grade fireworks are so much larger and more powerful, setup, and firing crews are always under great pressure to ensure they safely set up, fire, and clean up after a show.

In Chile, the manufacture, importation, possession and use of fireworks is prohibited to unauthorized individuals; only certified firework companies can legally use fireworks. As they are considered a type of explosive, offenders can in principle be tried before military courts, although this is unusual in practice.

The European Union's policy is aimed at harmonising and standardising the EU member states' policies on the regulation of production, transportation, sale, consumption and overall safety of fireworks across Europe.

In Belgium, each municipality can decide how to regulate fireworks. During New Year's Eve, lighting fireworks without a licence is allowed in 35% of the 308 Flemish municipalities, in around 50% a permit from the burgemeester (mayor) is required, and around 14% of municipalities have banned consumer fireworks altogether.

In Finland those under 18 years old haven't been allowed to buy any fireworks since 2009. Safety goggles are required. The use of fireworks is generally allowed on the evening and night of New Year's Eve, 31 December. In some municipalities of Western Finland it is allowed to use fireworks without a fire station's permission on the last weekend of August. With the fire station's permission, fireworks can be used year-round.

In Germany, amateurs over 18 years old are allowed to buy and ignite fireworks of Category F2 for several hours on 31 December and 1 January; each German municipality is authorised to limit the number of hours this may last locally. The sale of Category F3 and F4 fireworks to consumers is prohibited. Lighting fireworks is forbidden near churches, hospitals, retirement homes and wooden or thatch-roofed buildings. All major German cities organise professional fireworks shows.

In addition to the previously existing regulations, there was a nationwide ban on the sale of category F2 fireworks to consumers on New Year's Eve 2020/2021 during the COVID-19 pandemic, with the aim to relieve the burden on hospitals by reducing the number of emergencies due to injuries caused by fireworks on New Year's Eve.

In 2015, the Italian town of Collecchio mandated silent fireworks, being among the first to make the switch without losing the beauty of the visual displays.

In the Netherlands, fireworks cannot be sold to anyone under the age of 16. It may only be sold during a period of three days before a new year. If one of these days is a Sunday, that day is excluded from sale and sale may commence one day earlier.

In the Republic of Ireland, fireworks are illegal and possession is punishable by huge fines and/or prison. However, around Halloween a large amount of fireworks are set off, due to the ease of being able to purchase from Northern Ireland.

In Sweden, fireworks can only be purchased and used by people 18 or older. Fire crackers used to be banned, but are now allowed under European Union fireworks policy.

In Iceland, the Icelandic law states that anyone may purchase and use fireworks during a certain period around New Year's Eve. Most places that sell fireworks in Iceland make their own rules about age of buyers, usually it is around 16. The people of Reykjav'edk spend enormous sums of money on fireworks, most of which are fired as midnight approaches on 31 December. As a result, every New Year's Eve the city is lit up with fireworks displays.

Fireworks in New Zealand are available from 2 to 5 November, around Guy Fawkes Day, and may be purchased only by those 18 years of age and older (up from 14 years pre-2007). Despite the restriction on when fireworks may be sold, there is no restriction regarding when fireworks may be used. The types of fireworks available to the public are multi-shot "cakes", Roman candles, single shot shooters, ground and wall spinners, fountains, cones, sparklers, and various novelties, such as smoke bombs and Pharaoh's serpents. Consumer fireworks are also not allowed to be louder than 90 decibels.

In Norway, fireworks can only be purchased and used by people 18 or older. Sale is restricted to a few days before New Year's Eve. Rockets are not allowed.

Fireworks in the UK have become more strictly regulated since 1997. Since 2005, the law has been harmonised gradually, in accordance with other EU member state laws.

Fireworks are mostly used in England, Scotland and Wales around Diwali, in late October or early November, and Guy Fawkes Night, 5 November. In the UK, responsibility for the safety of firework displays is shared between the Health and Safety Executive, fire brigades and local authorities. Currently, there is no national system of licensing for fireworks operators, but in order to purchase display fireworks, operators must have licensed explosives storage and public liability insurance.

Fireworks cannot be sold to people under the age of 18 and are not permitted to be set off between 11pm and 7am with exceptions only for:

Bonfire Night (5 November) (permitted until midnight)
The Chinese New Year (permitted until 1am)
Diwali (permitted until 1am)
New Year (permitted until midnight New Year's Eve, and continuing to be permitted until 1am)
The maximum legal NEC (net explosive content) of a UK firework available to the public is two kilograms. Jumping jacks, strings of firecrackers, shell firing tubes, bangers and mini-rockets were all banned during the late 1990s. In 2004, single-shot air bombs and bottle rockets were banned, and rocket sizes were limited. From March 2008 any firework with more than 5% flashpowder per tube has been classified 1.3G. The aim of these measures was to eliminate "pocket money" fireworks, and to limit the disruptive effects of loud bangs.

In the United States, the laws governing fireworks vary widely from state to state, or from county to county. Federal, state, and local authorities govern the use of display fireworks in the United States. At the federal level, the Consumer Product Safety Commission (CPSC) regulates consumer fireworks through the Federal Hazardous Substances Act (FHSA). The National Fire Protection Association (NFPA) sets forth a set of codes that give the minimum standards of display fireworks use and safety in the U.S. Both state and local jurisdictions can further add restrictions on the use and safety requirements of display fireworks. There are currently 46 states in the United States in which fireworks are legal for consumer use.

In military terminology, a missile is a guided airborne ranged weapon capable of self-propelled flight usually by a jet engine or rocket motor. Missiles are thus also called guided missiles or guided rockets (when in rocket form). Missiles have five system components: targeting, guidance system, flight system, engine and warhead. Missiles come in types adapted for different purposes: surface-to-surface and air-to-surface missiles (ballistic, cruise, anti-ship, anti-tank, etc.), surface-to-air missiles (and anti-ballistic), air-to-air missiles, and anti-satellite weapons.

Airborne explosive devices without propulsion are referred to as shells if fired by an artillery piece and bombs if dropped by an aircraft. Unguided jet- or rocket-propelled weapons are usually described as rocket artillery.

Historically, the word missile referred to any projectile that is thrown, shot or propelled towards a target; this usage is still recognized today.

The first missiles to be used operationally were a series of missiles developed by Nazi Germany in World War II. Most famous of these are the V-1 flying bomb and V-2 rocket, both of which used a mechanical autopilot to keep the missile flying along a pre-chosen route. Less well known were a series of Anti-Ship and Anti-aircraft missiles, typically based on a simple radio control (command guidance) system directed by the operator. However, these early systems in World War II were only built in small numbers.

Guided missiles have a number of different system components:

Guidance system
Targeting system
Flight system
Engine
Warhead
The most common method of guidance is to use some form of radiation, such as infrared, lasers, or radio waves, to guide the missile onto its target. This radiation may emanate from the target (such as the heat of an engine or the radio waves from an enemy radar), it may be provided by the missile itself (such as radar), or it may be provided by a friendly third party (such as the radar of the launch vehicle/platform, or a laser designator operated by friendly infantry). The first two are often known as fire-and-forget as they need no further support or control from the launch vehicle/platform in order to function. Another method is to use TV guidance, with visible light or infrared pictures produced in order to see the target. The pictures may be used either by a human operator who steers the missile onto its target or by a computer doing much the same job. One of the more bizarre guidance methods instead used a pigeon to steer a missile to its target. Some missiles also have a home-on-jam capability to guide itself to a radar-emitting source. Many missiles use a combination of two or more methods to improve accuracy and the chances of successful engagement.

Another method is to target the missile by knowing the location of the target and using a guidance system such as INS, TERCOM, or satellite guidance. This guidance system guides the missile by knowing the missile's current position and the position of the target and then calculating a course between them. This job can also be performed somewhat crudely by a human operator who can see the target and the missile and guide it using either cable- or radio-based remote control, or by an automatic system that can simultaneously track the target and the missile. Furthermore, some missiles use initial targeting, sending them to a target area, where they will switch to primary targeting, using either radar or IR targeting to acquire the target.

Whether a guided missile uses a targeting system, a guidance system or both, it needs a flight system. The flight system uses the data from the targeting or guidance system to maneuver the missile in flight, allowing it to counter inaccuracies in the missile or to follow a moving target. There are two main systems: vectored thrust (for missiles that are powered throughout the guidance phase of their flight) and aerodynamic maneuvering (wings, fins, canard (aeronautics), etc.).

Missiles are powered by an engine, generally either a type of rocket engine or jet engine. Rockets are generally of the solid-propellant type for ease of maintenance and fast deployment, although some larger ballistic missiles use liquid-propellant rockets. Jet engines are generally used in cruise missiles, most commonly of the turbojet type, due to its relative simplicity and low frontal area. Turbofans and ramjets are the only other common forms of jet engine propulsion, although any type of engine could theoretically be used. Long-range missiles may have multiple engine stages, particularly in those launched from the surface. These stages may all be of similar types or may include a mix of engine types uc0u8722  for example, surface-launched cruise missiles often have a rocket booster for launching and a jet engine for sustained flight.

Some missiles may have additional propulsion from another source at launch; for example, the V1 was launched by a catapult, and the MGM-51 Shillelagh was fired out of a tank gun (using a smaller charge than would be used for a shell).

Missiles generally have one or more explosive warheads, although other weapon types may also be used. The warheads of a missile provide its primary destructive power (many missiles have extensive secondary destructive power due to the high kinetic energy of the weapon and unburnt fuel that may be on board). Warheads are most commonly of the high explosive type, often employing shaped charges to exploit the accuracy of a guided weapon to destroy hardened targets. Other warhead types include submunitions, incendiaries, nuclear weapons, chemical, biological or radiological weapons or kinetic energy penetrators. Warheadless missiles are often used for testing and training purposes.

Missiles are generally categorized by their launch platform and intended target. In broadest terms, these will either be surface (ground or water) or air, and then sub-categorized by range and the exact target type (such as anti-tank or anti-ship). Many weapons are designed to be launched from both surface or the air, and a few are designed to attack either surface or air targets (such as the ADATS missile). Most weapons require some modification in order to be launched from the air or surface, such as adding boosters to the surface-launched version.

After the boost stage, ballistic missiles follow a trajectory mainly determined by ballistics. The guidance is for relatively small deviations from that.

Ballistic missiles are largely used for land attack missions. Although normally associated with nuclear weapons, some conventionally armed ballistic missiles are in service, such as MGM-140 ATACMS. The V2 had demonstrated that a ballistic missile could deliver a warhead to a target city with no possibility of interception, and the introduction of nuclear weapons meant it could efficiently do damage when it arrived. The accuracy of these systems was fairly poor, but post-war development by most military forces improved the basic Inertial navigation system concept to the point where it could be used as the guidance system on Intercontinental ballistic missiles flying thousands of kilometers. Today, the ballistic missile represents the only strategic deterrent in most military forces; however, some ballistic missiles are being adapted for conventional roles, such as the Russian Iskander or the Chinese DF-21D anti-ship ballistic missile. Ballistic missiles are primarily surface-launched from mobile launchers, silos, ships or submarines, with air launch being theoretically possible with a weapon such as the cancelled Skybolt missile.

The Russian Topol M (SS-27 Sickle B) is the fastest (7,320 m/s) missile currently in service.

The V1 had been successfully intercepted during World War II, but this did not make the cruise missile concept entirely useless. After the war, the US deployed a small number of nuclear-armed cruise missiles in Germany, but these were considered to be of limited usefulness. Continued research into much longer-ranged and faster versions led to the US's SM-64 Navaho and its Soviet counterparts, the Burya and Buran cruise missile. However, these were rendered largely obsolete by the ICBM, and none were used operationally. Shorter-range developments have become widely used as highly accurate attack systems, such as the US Tomahawk missile and Russian Kh-55. Cruise missiles are generally further divided into subsonic or supersonic weapons - supersonic weapons such as BrahMos (India, Russia) are difficult to shoot down, whereas subsonic weapons tend to be much lighter and cheaper, allowing more to be fired.

Cruise missiles are generally associated with land-attack operations, but also have an important role as anti-shipping weapons. They are primarily launched from air, sea or submarine platforms in both roles, although land-based launchers also exist.

Another major German missile development project was the anti-shipping class (such as the Fritz X and Henschel Hs 293), intended to stop any attempt at a cross-channel invasion. However, the British were able to render their systems useless by jamming their radios, and missiles with wire guidance were not ready by D-Day. After the war, the anti-shipping class slowly developed and became a major class in the 1960s with the introduction of the low-flying jet- or rocket-powered cruise missiles known as "sea-skimmers". These became famous during the Falklands War, when an Argentine Exocet missile disabled a Royal Navy destroyer.

A number of anti-submarine missiles also exist; these generally use the missile in order to deliver another weapon system such as a torpedo or depth charge to the location of the submarine, at which point the other weapon will conduct the underwater phase of the mission.

By the end of WWII, all forces had widely introduced unguided rockets using high-explosive anti-tank warheads as their major anti-tank weapon (see Panzerfaust, Bazooka). However, these had a limited useful range of 100 m or so, and the Germans were looking to extend this with the use of a missile using wire guidance, the X-7. After the war, this became a major design class in the later 1950s and, by the 1960s, had developed into practically the only non-tank anti-tank system in general use. During the 1973 Yom Kippur War between Israel and Egypt, the 9M14 Malyutka (aka Sagger) man-portable anti-tank missile proved potent against Israeli tanks. While other guidance systems have been tried, the basic reliability of wire guidance means this will remain the primary means to control anti-tank missiles in the near future. Anti-tank missiles may be launched from aircraft, vehicles or by ground troops in the case of smaller weapons.

By 1944, US and British air forces were sending huge air fleets over occupied Europe, increasing the pressure on the Luftwaffe day and night fighter forces. The Germans were keen to get some sort of useful ground-based anti-aircraft system into operation. Several systems were under development, but none had reached operational status before the war's end. The US Navy also started missile research to deal with the Kamikaze threat. By 1950, systems based on this early research started to reach operational service, including the US Army's MIM-3 Nike Ajax and the Navy's "3T's" (Talos, Terrier, Tartar), soon followed by the Soviet S-25 Berkut and S-75 Dvina and French and British systems. Anti-aircraft weapons exist for virtually every possible launch platform, with surface-launched systems ranging from huge, self-propelled or ship-mounted launchers to man-portable systems. Subsurface-to-air missiles are usually launched from below water (usually from submarines).

Like most missiles, the S-300, S-400, Advanced Air Defence and MIM-104 Patriot are for defense against short-range missiles and carry explosive warheads.

In the case of a large closing speed, a projectile without explosives is used; just a collision is sufficient to destroy the target. See Missile Defense Agency for the following systems being developed:

Arrow 3
Kinetic Energy Interceptor (KEI)
Aegis Ballistic Missile Defense System (Aegis BMD) - an SM-3 missile with a Lightweight Exo-Atmospheric Projectile (LEAP) Kinetic Warhead (KW)
Used for the first time by Soviet pilots in the summer of 1939 during the Battle of Khalkhin Gol. On August 20, 1939, the Japanese Nakajima Ki-27 fighter was attacked by the Soviet Polikarpov I-16 fighter of Captain N. Zvonarev. He fired a rocket salvo from a distance of about a kilometer, after which the Ki-27 crashed to the ground. A group of Polikarpov I-16 fighters under command of Captain N. Zvonarev were using RS-82 rockets against Japanese aircraft, shooting down 16 fighters and 3 bombers in total.

German experience in World War II demonstrated that destroying a large aircraft was quite difficult, and they had invested considerable effort into air-to-air missile systems to do this. Their Messerschmitt Me 262's jets often carried R4M rockets, and other types of "bomber destroyer" aircraft had unguided rockets as well. In the post-war period, the R4M served as the pattern for a number of similar systems, used by almost all interceptor aircraft during the 1940s and 1950s. Most rockets (except for the AIR-2 Genie, due to its nuclear warhead with a large blast radius) had to be carefully aimed at relatively close range to hit the target successfully. The United States Navy and U.S. Air Force began deploying guided missiles in the early 1950s, most famous being the US Navy's AIM-9 Sidewinder and the USAF's AIM-4 Falcon. These systems have continued to advance, and modern air warfare consists almost entirely of missile firing. In the Falklands War, less powerful British Harriers were able to defeat faster Argentinian opponents using American AIM-9L missiles. The latest heat-seeking designs can lock onto a target from various angles, not just from behind, where the heat signature from the engines is strongest. Other types rely on radar guidance (either on board or "painted" by the launching aircraft). Air-to-air missiles also have a wide range of sizes, ranging from helicopter-launched self-defense weapons with a range of a few kilometers, to long-range weapons designed for interceptor aircraft such as the R-37 (missile).

In the 1950s and 1960s, Soviet designers started work on an anti-satellite weapon as part of the Istrebitel Sputnikov program ("istrebitel sputnikov" literally means "destroyer of satellites"). After a lengthy development process of roughly twenty years, it was finally decided that the testing of these weapons be canceled. This was when the United States started testing their own systems. The Brilliant Pebbles defense system proposed during the 1980s would have used kinetic energy collisions without explosives. Anti-satellite weapons may be launched either by an aircraft or a surface platform, depending on the design. To date, only a few known tests have occurred. As of 2019, only 4 countries - China, India, United States, and Russia have operational anti-satellite weapons.

In aircraft, an ejection seat or ejector seat is a system designed to rescue the pilot or other crew of an aircraft (usually military) in an emergency. In most designs, the seat is propelled out of the aircraft by an explosive charge or rocket motor, carrying the pilot with it. The concept of an ejectable escape crew capsule has also been tried. Once clear of the aircraft, the ejection seat deploys a parachute. Ejection seats are common on certain types of military aircraft.

A bungee-assisted escape from an aircraft took place in 1910. In 1916, Everard Calthrop, an early inventor of parachutes, patented an ejector seat using compressed air.

The modern layout for an ejection seat was first introduced by Romanian inventor Anastase Dragomir in the late 1920s. The design featured a parachuted cell (a dischargeable chair from an aircraft or other vehicle). It was successfully tested on 25 August 1929 at the Paris-Orly Airport near Paris and in October 1929 at Buc0u259 neasa, near Bucharest. Dragomir patented his "catapult-able cockpit" at the French Patent Office.

The design was perfected during World War II. Prior to this, the only means of escape from an incapacitated aircraft was to jump clear ("bail out"), and in many cases this was difficult due to injury, the difficulty of egress from a confined space, g forces, the airflow past the aircraft, and other factors.

The first ejection seats were developed independently during World War II by Heinkel and SAAB. Early models were powered by compressed air and the first aircraft to be fitted with such a system was the Heinkel He 280 prototype jet-engined fighter in 1940. One of the He 280 test pilots, Helmut Schenk, became the first person to escape from a stricken aircraft with an ejection seat on 13 January 1942 after his control surfaces iced up and became inoperative. The fighter had been being used in tests of the Argus As 014 impulse jets for Fieseler Fi 103 missile development. It had its usual Heinkek HeS 8A turbojets removed, and was towed aloft from the Erprobungsstelle Rechlin central test facility of the Luftwaffe in Germany by a pair of Messerschmitt Bf 110C tugs in a heavy snow-shower. At 7,875 ft (2,400 m), Schenk found he had no control, jettisoned his towline, and ejected. The He 280 was never put into production status. The first operational type built anywhere to provide ejection seats for the crew was the Heinkel He 219 Uhu night fighter in 1942.

In Sweden, a version using compressed air was tested in 1941. A gunpowder ejection seat was developed by Bofors and tested in 1943 for the Saab 21. The first test in the air was on a Saab 17 on 27 February 1944, and the first real use occurred by Lt. Bengt Johansson on 29 July 1946 after a mid-air collision between a J 21 and a J 22.

As the first operational military jet in late 1944 to ever feature one, the winner of the German Volksj'e4ger "people's fighter" home defense jet fighter design competition; the lightweight Heinkel He 162A Spatz, featured a new type of ejection seat, this time fired by an explosive cartridge. In this system, the seat rode on wheels set between two pipes running up the back of the cockpit. When lowered into position, caps at the top of the seat fitted over the pipes to close them. Cartridges, basically identical to shotgun shells, were placed in the bottom of the pipes, facing upward. When fired, the gases would fill the pipes, "popping" the caps off the end, and thereby forcing the seat to ride up the pipes on its wheels and out of the aircraft. By the end of the war, the Dornier Do 335 Pfeil '97 primarily from it having a rear-mounted engine (of the twin engines powering the design) powering a pusher propeller located at the aft end of the fuselage presenting a hazard to a normal "bailout" escape '97 and a few late-war prototype aircraft were also fitted with ejection seats.

After World War II, the need for such systems became pressing, as aircraft speeds were getting ever higher, and it was not long before the sound barrier was broken. Manual escape at such speeds would be impossible. The United States Army Air Forces experimented with downward-ejecting systems operated by a spring, but it was the work of James Martin and his company Martin-Baker that proved crucial.

The first live flight test of the Martin-Baker system took place on 24 July 1946, when fitter Bernard Lynch ejected from a Gloster Meteor Mk III jet. Shortly afterward, on 17 August 1946, 1st Sgt. Larry Lambert was the first live U.S. ejectee. Lynch demonstrated the ejection seat at the Daily Express Air Pageant in 1948, ejecting from a Meteor. Martin-Baker ejector seats were fitted to prototype and production aircraft from the late 1940s, and the first emergency use of such a seat occurred in 1949 during testing of the jet-powered Armstrong Whitworth A.W.52 experimental flying wing.

Early seats used a solid propellant charge to eject the pilot and seat by igniting the charge inside a telescoping tube attached to the seat. As aircraft speeds increased still further, this method proved inadequate to get the pilot sufficiently clear of the airframe. Increasing the amount of propellant risked damaging the occupant's spine, so experiments with rocket propulsion began. In 1958, the Convair F-102 Delta Dagger was the first aircraft to be fitted with a rocket-propelled seat. Martin-Baker developed a similar design, using multiple rocket units feeding a single nozzle. The greater thrust from this configuration had the advantage of being able to eject the pilot to a safe height even if the aircraft was on or very near the ground.

In the early 1960s, deployment of rocket-powered ejection seats designed for use at supersonic speeds began in such planes as the Convair F-106 Delta Dart. Six pilots have ejected at speeds exceeding 700 knots (1,300 km/h; 810 mph). The highest altitude at which a Martin-Baker seat was deployed was 57,000 ft (17,400 m) (from a Canberra bomber in 1958). Following an accident on 30 July 1966 in the attempted launch of a D-21 drone, two Lockheed M-21 crew members ejected at Mach 3.25 at an altitude of 80,000 ft (24,000 m). The pilot was recovered successfully, but the launch control officer drowned after a water landing. Despite these records, most ejections occur at fairly low speeds and altitudes, when the pilot can see that there is no hope of regaining aircraft control before impact with the ground.

Late in the Vietnam War, the U.S. Air Force and U.S. Navy became concerned about its pilots ejecting over hostile territory and those pilots either being captured or killed and the losses in men and aircraft in attempts to rescue them. Both services began a program titled Air Crew Escape/Rescue Capability or Aerial Escape and Rescue Capability (AERCAB) ejection seats (both terms have been used by the US military and defence industry), where after the pilot ejected, the ejection seat would fly them to a location far enough away from where they ejected to where they could safely be picked up. A Request for Proposals for concepts for AERCAB ejection seats were issued in the late 1960s. Three companies submitted papers for further development: A Rogallo wing design by Bell Systems; a gyrocopter design by Kaman Aircraft; and a mini-conventional fixed wing aircraft employing a Princeton Wing (i.e. a wing made of flexible material that rolls out and then becomes rigid by means of internal struts or supports etc. deploying) by Fairchild Hiller. All three, after ejection, would be propelled by small turbojet engine developed for target drones. With the exception of the Kaman design, the pilot would still be required to parachute to the ground after reaching a safety-point for rescue. The AERCAB project was terminated in the 1970s with the end of the Vietnam War. The Kaman design, in early 1972, was the only one which was to reach the hardware stage. It came close to being tested with a special landing-gear platform attached to the AERCAB ejection seat for first-stage ground take offs and landings with a test pilot.

The purpose of an ejection seat is pilot survival. The pilot typically experiences an acceleration of about 12'9614g. Western seats usually impose lighter loads on the pilots; 1960s'9670s era Soviet technology often goes up to 20'9622 g (with SM-1 and KM-1 gunbarrel-type ejection seats). Compression fractures of vertebrae are a recurrent side effect of ejection.

It was theorised early on that ejection at supersonic speeds would be unsurvivable; extensive tests, including Project Whoosh with chimpanzee test subjects, were undertaken to determine that it was feasible.

The capabilities of the NPP Zvezda K-36 were unintentionally demonstrated at the Fairford Air Show on 24 July 1993 when the pilots of two MiG-29 fighters ejected after a mid-air collision.

The minimal ejection altitude for ACES II seat in inverted flight is about 140 feet (43 m) above ground level at 150 KIAS, while the Russian counterpart '96 K-36DM has the minimal ejection altitude from inverted flight of 100 feet (30 m) AGL. When an aircraft is equipped with the NPP Zvezda K-36DM ejection seat and the pilot is wearing the uc0u1050 u1054 -15 protective gear, they are able to eject at airspeeds from 0 to 1,400 kilometres per hour (870 mph) and altitudes of 0 to 25 km (16 mi or about 82,000 ft). The K-36DM ejection seat features drag chutes and a small shield that rises between the pilot's legs to deflect air around the pilot.

Pilots have successfully ejected from underwater in a handful of instances, after being forced to ditch in water. Documented evidence exists that pilots of the US and Indian navies have performed this feat.

As of 20 June 2011 '96 when two Spanish Air Force pilots ejected over San Javier airport '96 the number of lives saved by Martin-Baker products was 7,402 from 93 air forces. The company runs a club called the "Ejection Tie Club" and gives survivors a unique tie and lapel pin. The total figure for all types of ejection seats is unknown, but may be considerably higher.

Early models of the ejection seat were equipped with only an overhead ejection handle which doubled in function by forcing the pilot to assume the right posture and by having them pull a screen down to protect both their face and oxygen mask from the subsequent air blast. Martin Baker added a secondary handle in the front of the seat to allow ejection even when pilots weren't able to reach upwards because of high g-force. Later (e.g. in Martin Baker's MK9) the top handle was discarded because the lower handle had proven easier to operate and the technology of helmets had advanced to also protect from the air blast.

The "standard" ejection system operates in two stages. First, the entire canopy or hatch above the aviator is opened, shattered, or jettisoned, and the seat and occupant are launched through the opening. In most earlier aircraft this required two separate actions by the aviator, while later egress system designs, such as the Advanced Concept Ejection Seat model 2 (ACES II), perform both functions as a single action.

The ACES II ejection seat is used in most American-built fighters. The A-10 uses connected firing handles that activate both the canopy jettison systems, followed by the seat ejection. The F-15 has the same connected system as the A-10 seat. Both handles accomplish the same task, so pulling either one suffices. The F-16 has only one handle located between the pilot's knees, since the cockpit is too narrow for side-mounted handles.

Non-standard egress systems include Downward Track (used for some crew positions in bomber aircraft, including the B-52 Stratofortress), Canopy Destruct (CD) and Through-Canopy Penetration (TCP), Drag Extraction, Encapsulated Seat, and even Crew Capsule.

Early models of the F-104 Starfighter were equipped with a Downward Track ejection seat due to the hazard of the T-tail. In order to make this work, the pilot was equipped with "spurs" which were attached to cables that would pull the legs inward so the pilot could be ejected. Following this development, some other egress systems began using leg retractors as a way to prevent injuries to flailing legs, and to provide a more stable center of gravity. Some models of the F-104 were equipped with upward-ejecting seats.

Similarly, two of the six ejection seats on the B-52 Stratofortress fire downward, through hatch openings on the bottom of the aircraft; the downward hatches are released from the aircraft by a thruster that unlocks the hatch, while gravity and wind remove the hatch and arm the seat. The four seats on the forward upper deck (two of them, EWO and Gunner, facing the rear of the airplane) fire upwards as usual. Any such downward-firing system is of no use on or near the ground if aircraft is in level flight at the time of the ejection.

Aircraft designed for low-level use sometimes have ejection seats which fire through the canopy, as waiting for the canopy to be ejected is too slow. Many aircraft types (e.g., the BAE Hawk and the Harrier line of aircraft) use Canopy Destruct systems, which have an explosive cord (MDC '96 Miniature Detonation Cord or FLSC '96 Flexible Linear Shaped Charge) embedded within the acrylic plastic of the canopy. The MDC is initiated when the eject handle is pulled, and shatters the canopy over the seat a few milliseconds before the seat is launched. This system was developed for the Hawker Siddeley Harrier family of VTOL aircraft as ejection may be necessary while the aircraft was in the hover, and jettisoning the canopy might result in the pilot and seat striking it. This system is also used in the T-6 Texan II and F-35 Lightning II.

Through-Canopy Penetration is similar to Canopy Destruct, but a sharp spike on the top of the seat, known as the "shell tooth", strikes the underside of the canopy and shatters it. The A-10 Thunderbolt II is equipped with canopy breakers on either side of its headrest in the event that the canopy fails to jettison. The T-6 is also equipped with such breakers if the MDC fails to detonate. In ground emergencies, a ground crewman or pilot can use a breaker knife attached to the inside of the canopy to shatter the transparency. The A-6 Intruder and EA-6B Prowler seats were capable of ejecting through the canopy, with canopy jettison a separate option if there is enough time.

CD and TCP systems cannot be used with canopies made of flexible materials, such as the Lexan polycarbonate canopy used on the F-16.

Soviet VTOL naval fighter planes such as the Yakovlev Yak-38 were equipped with ejection seats which were automatically activated during at least some part of the flight envelope.

Drag Extraction is the lightest and simplest egress system available, and has been used on many experimental aircraft. Halfway between simply "bailing out" and using explosive-eject systems, Drag Extraction uses the airflow past the aircraft (or spacecraft) to move the aviator out of the cockpit and away from the stricken craft on a guide rail. Some operate like a standard ejector seat, by jettisoning the canopy, then deploying a drag chute into the airflow. That chute pulls the occupant out of the aircraft, either with the seat or following release of the seat straps, who then rides off the end of a rail extending far enough out to help clear the structure. In the case of the Space Shuttle, the astronauts would have ridden a long, curved rail, blown by the wind against their bodies, then deployed their chutes after free-falling to a safe altitude.

Encapsulated Seat egress systems were developed for use in the B-58 Hustler and B-70 Valkyrie supersonic bombers. These seats were enclosed in an air-operated clamshell, which permitted the aircrew to escape at airspeeds and altitudes high enough to otherwise cause bodily harm. These seats were designed to allow the pilot to control the plane even with the clamshell closed, and the capsule would float in case of water landings.

Some aircraft designs, such as the General Dynamics F-111, do not have individual ejection seats, but instead, the entire section of the airframe containing the crew can be ejected as a single capsule. In this system, very powerful rockets are used, and multiple large parachutes are used to bring the capsule down, in a manner similar to the Launch Escape System of the Apollo spacecraft. On landing, an airbag system is used to cushion the landing, and this also acts as a flotation device if the Crew Capsule lands in water.

A zero-zero ejection seat is designed to safely extract upward and land its occupant from a grounded stationary position (i.e., zero altitude and zero airspeed), specifically from aircraft cockpits. The zero-zero capability was developed to help aircrews escape upward from unrecoverable emergencies during low-altitude and/or low-speed flight, as well as ground mishaps. Parachutes require a minimum altitude for opening, to give time for deceleration to a safe landing speed. Thus, prior to the introduction of zero-zero capability, ejections could only be performed above minimum altitudes and airspeeds. If the seat was to work from zero (aircraft) altitude, the seat would have to lift itself to a sufficient altitude.

These early seats were fired from the aircraft with a cannon, providing the high impulse needed over the very short length on the cannon barrel within the seat. This limited the total energy, and thus the additional height possible, as otherwise the high forces needed would crush the pilot.

Modern zero-zero technology use small rockets to propel the seat upward to an adequate altitude and a small explosive charge to open the parachute canopy quickly for a successful parachute descent, so that proper deployment of the parachute no longer relies on airspeed and altitude. The seat cannon clears the seat from the aircraft, then the under-seat rocket pack fires to lift the seat to altitude. As the rockets fire for longer than the cannon, they do not require the same high forces. Zero-zero rocket seats also reduced forces on the pilot during any ejection, reducing injuries and spinal compression.

The Kamov Ka-50, which entered limited service with Russian forces in 1995, was the first production helicopter with an ejection seat. The system is similar to that of a conventional fixed-wing aircraft; however the main rotors are equipped with explosive bolts to jettison the blades moments before the seat is fired.

The only commercial jetliner ever fitted with ejection seats was the Soviet Tupolev Tu-144. However, the seats were present in the prototype only, and were only available for the crew and not the passengers. The Tu-144 that crashed at the Paris Air Show in 1973 was a production model, and did not have ejection seats.

The Lunar Landing Research Vehicle, (LLRV) and its successor Lunar Landing Training Vehicle (LLTV), used ejection seats. Neil Armstrong ejected on 6 May 1968; following Joe Algranti and Stuart M. Present.

The only spacecraft ever flown with installed ejection seats were Vostok, Gemini, and the Space Shuttle.

Early flights of the Space Shuttle, which used Columbia, were with a crew of two, both provided with ejector seats (STS-1 to STS-4), but the seats were disabled and then removed as the crew size was increased. Columbia and Enterprise were the only two Space Shuttle orbiters fitted with ejection seats. The Buran-class orbiters were planned to be fitted with K-36RB (K-36M-11F35) seats, but as the program was canceled, the seats were never used.

No real life land vehicle has ever been fitted with an ejection seat, though it is a common trope in fiction. A notable example is the Aston Martin DB5 from the James Bond films, which had an ejecting passenger seat.


Part of a series on
Spaceflight
SpaceX Crew Dragon (More cropped).jpg
History
Space Race Timeline of spaceflight Space probes Lunar missions
Applications
Earth observation satellites Spy satellites Communications satellites Military satellite Satellite navigation Space telescopes Space exploration Space tourism Space colonization
Spacecraft
Robotic spacecraft
Satellite Space probe Cargo spacecraft Human spaceflight
Space capsule Apollo Lunar Module Space Shuttle Space station Spaceplane
Space launch
Spaceport Launch pad Expendable and reusable launch vehicles Escape velocity Non-rocket spacelaunch
Spaceflight types
Sub-orbital Orbital Interplanetary Interstellar Intergalactic
List of space organizations
Space agencies Space forces Companies
 Spaceflight portal
vte
A launch vehicle or carrier rocket is a rocket-propelled vehicle used to carry a payload from Earth's surface to space, usually to Earth orbit or beyond. A launch system includes the launch vehicle, launch pad, vehicle assembly and fuelling systems, range safety, and other related infrastructure.

Orbital launch vehicles can be grouped based on many different factors, most notably payload mass, although price points are a major concern for some users. Most launch vehicles have been developed by or for national space programs, with considerable national prestige attached to spaceflight accomplishments. Payloads include crewed spacecraft, satellites, robotic spacecraft, scientific probes, landers, rovers, and many more.

Orbital spaceflight is difficult and expensive, with progress limited by the underlying technology as much as human and societal factors.

Launch vehicles are classed by NASA according to low Earth orbit payload capability:

Small-lift launch vehicle: < 2,000 kilograms (4,400 lb) - e.g. Vega
Medium-lift launch vehicle: 2,000 to 20,000 kilograms (4,400 to 44,100 lb) - e.g. Soyuz ST
Heavy-lift launch vehicle: > 20,000 to 50,000 kilograms (44,000 to 110,000 lb) - e.g. Ariane 5
Super-heavy lift vehicle: > 50,000 kilograms (110,000 lb) - e.g. Saturn V
Sounding rockets are similar to small-lift launch vehicles, however they are usually even smaller and do not place payloads into orbit. A modified SS-520 sounding rocket was used to place a 4-kilogram payload (TRICOM-1R) into orbit in 2018.

Orbital spaceflight requires a satellite or spacecraft payload to be accelerated to very high velocity. In the vacuum of space, reaction forces must be provided by the ejection of mass, resulting in the rocket equation. The physics of spaceflight are such that rocket stages are typically required to achieve the desired orbit.

Expendable launch vehicles are designed for one-time use, with boosters that usually separate from their payload and disintegrate during atmospheric reentry or on contact with the ground. In contrast, reusable launch vehicle boosters are designed to be recovered intact and launched again. The Falcon 9 is an example reusable launch vehicle.

For example, the European Space Agency is responsible for the Ariane V, and the United Launch Alliance manufactures and launches the Delta IV and Atlas V rockets.

Launchpads can be located on land (spaceport), on a fixed ocean platform (San Marco), on a mobile ocean platform (Sea Launch), and on a submarine. Launch vehicles can also be launched from the air.

A launch vehicle will start off with its payload at some location on the surface of the Earth. To reach orbit, the vehicle must travel vertically to leave the atmosphere and horizontally to prevent re-contacting the ground. The required velocity varies depending on the orbit but will always be extreme when compared to velocities encountered in normal life.

Launch vehicles provide varying degrees of performance. For example, a satellite bound for Geostationary orbit (GEO) can either be directly inserted by the upper stage of the launch vehicle or launched to a geostationary transfer orbit (GTO). A direct insertion places greater demands on the launch vehicle, while GTO is more demanding of the spacecraft. Once in orbit, launch vehicle upper stages and satellites can have overlapping capabilities, although upper stages tend to have orbital lifetimes measured in hours or days while spacecraft can last decades.

Distributed launch involves the accomplishment of a goal with multiple spacecraft launches. A large spacecraft such as the International Space Station can be constructed by assembling modules in orbit, or in-space propellant transfer conducted to greatly increase the delta-V capabilities of a cislunar or deep space vehicle. Distributed launch enables space missions that are not possible with single launch architectures.

Mission architectures for distributed launch were explored in the 2000s and launch vehicles with integrated distributed launch capability built in began development in 2017 with the Starship design. The standard Starship launch architecture is to refuel the spacecraft in low Earth orbit to enable the craft to send high-mass payloads on much more energetic missions.

After 1980, but before the 2010s, two orbital launch vehicles developed the capability to return to the launch site (RTLS). Both the US Space Shuttle'97with one of its abort modes'97and the Soviet Buran had a designed-in capability to return a part of the launch vehicle to the launch site via the mechanism of horizontal-landing of the spaceplane portion of the launch vehicle. In both cases, the main vehicle thrust structure and the large propellant tank were expendable, as had been the standard procedure for all orbital launch vehicles flown prior to that time. Both were subsequently demonstrated on actual orbital nominal flights, although both also had an abort mode during launch that could conceivably allow the crew to land the spaceplane following an off-nominal launch.

In the 2000s, both SpaceX and Blue Origin have privately developed a set of technologies to support vertical landing of the booster stage of a launch vehicle. After 2010, SpaceX undertook a development program to acquire the ability to bring back and vertically land a part of the Falcon 9 orbital launch vehicle: the first stage. The first successful landing was done in December 2015, since then several additional rocket stages landed either at a landing pad adjacent to the launch site or on a landing platform at sea, some distance away from the launch site. The Falcon Heavy is similarly designed to reuse the three cores comprising its first stage. On its first flight in February 2018, the two outer cores successfully returned to the launch site landing pads while the center core targeted the landing platform at sea but did not successfully land on it.

Blue Origin developed similar technologies for bringing back and landing their suborbital New Shepard, and successfully demonstrated return in 2015, and successfully reused the same booster on a second suborbital flight in January 2016. By October 2016, Blue had reflown, and landed successfully, that same launch vehicle a total of five times. It must however be noted that the launch trajectories of both vehicles are very different, with New Shepard going straight up and down, whereas Falcon 9 has to cancel substantial horizontal velocity and return from a significant distance downrange.

Both Blue Origin and SpaceX also have additional reusable launch vehicles under development. Blue is developing the first stage of the orbital New Glenn LV to be reusable, with first flight planned for no earlier than 2020. SpaceX has a new super-heavy launch vehicle under development for missions to interplanetary space. The Big Falcon Rocket (BFR) is designed to support RTLS, vertical-landing and full reuse of both the booster stage and the integrated second-stage/large-spacecraft that are designed for use with the BFR. First launch is expected in the early 2020s.

A satellite is an object that is intentionally placed into orbit. These objects are called artificial satellites to distinguish them from natural satellites such as Earth's Moon.

On 4 October 1957, the Soviet Union launched the world's first artificial satellite, Sputnik 1. Since then, about 8,900 satellites from more than 40 countries have been launched. According to a 2018 estimate, about 5,000 remained in orbit. Of those, about 1,900 were operational, while the rest had exceeded their useful lives and become space debris. Approximately 63% of operational satellites are in low Earth orbit, 6% are in medium-Earth orbit (at 20,000 km), 29% are in geostationary orbit (at 36,000 km) and the remaining 2% are in various elliptical orbits. In terms of countries with the most satellites, the United States has the most with 2,944 satellites, China is second with 499, and Russia third with 169. A few large space stations, including the International Space Station, have been launched in parts and assembled in orbit. Over a dozen space probes have been placed into orbit around other bodies and become artificial satellites of the Moon, Mercury, Venus, Mars, Jupiter, Saturn, a few asteroids, a comet and the Sun.

Satellites are used for many purposes. Among several other applications, they can be used to make star maps and maps of planetary surfaces, and also take pictures of planets they are launched into. Common types include military and civilian Earth observation satellites, communications satellites, navigation satellites, weather satellites, and space telescopes. Space stations and human spacecraft in orbit are also satellites.

Satellites can operate by themselves or as part of a larger system, a satellite formation or satellite constellation.

Satellite orbits have a large range depending on the purpose of the satellite, and are classified in a number of ways. Well-known (overlapping) classes include low Earth orbit, polar orbit, and geostationary orbit.

A launch vehicle is a rocket that places a satellite into orbit. Usually, it lifts off from a launch pad on land. Some are launched at sea from a submarine or a mobile maritime platform, or aboard a plane (see air launch to orbit).

Satellites are usually semi-independent computer-controlled systems. Satellite subsystems attend many tasks, such as power generation, thermal control, telemetry, attitude control, scientific instrumentation, communication, etc.

The first published mathematical study of the possibility of an artificial satellite was Newton's cannonball, a thought experiment by Isaac Newton to explain the motion of natural satellites, in his Philosophi'e6 Naturalis Principia Mathematica (1687). The first fictional depiction of a satellite being launched into orbit was a short story by Edward Everett Hale, "The Brick Moon" (1869). The idea surfaced again in Jules Verne's The Begum's Fortune (1879).

In 1903, Konstantin Tsiolkovsky (1857'961935) published Exploring Space Using Jet Propulsion Devices, which is the first academic treatise on the use of rocketry to launch spacecraft. He calculated the orbital speed required for a minimal orbit, and that a multi-stage rocket fueled by liquid propellants could achieve this.

In 1928, Herman Potouc0u269 nik (1892'961929) published his sole book, The Problem of Space Travel '96 The Rocket Motor. He described the use of orbiting spacecraft for observation of the ground and described how the special conditions of space could be useful for scientific experiments.

In a 1945 Wireless World article, the English science fiction writer Arthur C. Clarke described in detail the possible use of communications satellites for mass communications. He suggested that three geostationary satellites would provide coverage over the entire planet.

In May 1946, the United States Air Force's Project RAND released the Preliminary Design of an Experimental World-Circling Spaceship, which stated that "A satellite vehicle with appropriate instrumentation can be expected to be one of the most potent scientific tools of the Twentieth Century." The United States had been considering launching orbital satellites since 1945 under the Bureau of Aeronautics of the United States Navy. Project RAND eventually released the report, but considered the satellite to be a tool for science, politics, and propaganda, rather than a potential military weapon.

In 1946, American theoretical astrophysicist Lyman Spitzer proposed an orbiting space telescope.

In February 1954 Project RAND released "Scientific Uses for a Satellite Vehicle", written by R.R. Carhart. This expanded on potential scientific uses for satellite vehicles and was followed in June 1955 with "The Scientific Use of an Artificial Satellite", by H.K. Kallmann and W.W. Kellogg.

In the context of activities planned for the International Geophysical Year (1957'9658), the White House announced on 29 July 1955 that the U.S. intended to launch satellites by the spring of 1958. This became known as Project Vanguard. On 31 July, the Soviets announced that they intended to launch a satellite by the fall of 1957.

The first artificial satellite was Sputnik 1, launched by the Soviet Union on 4 October 1957 under the Sputnik program, with Sergei Korolev as chief designer. Sputnik 1 helped to identify the density of high atmospheric layers through measurement of its orbital change and provided data on radio-signal distribution in the ionosphere. The unanticipated announcement of Sputnik 1's success precipitated the Sputnik crisis in the United States and ignited the so-called Space Race within the Cold War.

Sputnik 2 was launched on 3 November 1957 and carried the first living passenger into orbit, a dog named Laika.

In early 1955, following pressure by the American Rocket Society, the National Science Foundation, and the International Geophysical Year, the Army and Navy were working on Project Orbiter with two competing programs. The army used the Jupiter C rocket, while the civilian/Navy program used the Vanguard rocket to launch a satellite. Explorer 1 became the United States' first artificial satellite on 31 January 1958.

In June 1961, three-and-a-half years after the launch of Sputnik 1, the United States Space Surveillance Network cataloged 115 Earth-orbiting satellites.

Early satellites were constructed to unique designs. With advancements in technology, multiple satellites began to be built on single model platforms called satellite buses. The first standardized satellite bus design was the HS-333 geosynchronous (GEO) communication satellite launched in 1972. Beginning in 1997, FreeFlyer is a commercial off-the-shelf software application for satellite mission analysis, design and operations.

Currently the largest artificial satellite ever is the International Space Station.

Herman Potouc0u269 nik explored the idea of using orbiting spacecraft for detailed peaceful and military observation of the ground in his 1928 book, The Problem of Space Travel. He described how the special conditions of space could be useful for scientific experiments. The book described geostationary satellites (first put forward by Konstantin Tsiolkovsky) and discussed communication between them and the ground using radio, but fell short of the idea of using satellites for mass broadcasting and as telecommunications relays.

Satellites can be tracked from Earth stations and also from other satellites.

The United States Space Surveillance Network (SSN), a division of the United States Strategic Command, has been tracking objects in Earth's orbit since 1957 when the Soviet Union opened the Space Age with the launch of Sputnik I. Since then, the SSN has tracked more than 26,000 objects. The SSN currently tracks more than 8,000-artificial orbiting objects. The rest have re-entered Earth's atmosphere and disintegrated, or survived re-entry and impacted the Earth. The SSN tracks objects that are 10 centimeters in diameter or larger; those now orbiting Earth range from satellites weighing several tons to pieces of spent rocket bodies weighing only 10 pounds. About seven percent are operational satellites (i.e. ~560 satellites), the rest are space debris. The United States Strategic Command is primarily interested in the active satellites, but also tracks space debris which upon reentry might otherwise be mistaken for incoming missiles.

There are three basic categories of (non-military) satellite services:

Fixed satellite services handle hundreds of billions of voice, data, and video transmission tasks across all countries and continents between certain points on the Earth's surface.

Mobile satellite systems help connect remote regions, vehicles, ships, people and aircraft to other parts of the world and/or other mobile or stationary communications units, in addition to serving as navigation systems.

Scientific research satellites provide meteorological information, land survey data (e.g. remote sensing), Amateur (HAM) Radio, and other different scientific research applications such as earth science, marine science, and atmospheric research.

Astronomical satellites are satellites used for observation of distant planets, galaxies, and other outer space objects.
Biosatellites are satellites designed to carry living organisms, generally for scientific experimentation.
Communication satellites are satellites stationed in space for the purpose of telecommunications. Modern communications satellites typically use geosynchronous orbits, Molniya orbits or Low Earth orbits.
Earth observation satellites are satellites intended for non-military uses such as environmental monitoring, meteorology, map making etc. (See especially Earth Observing System.)
Navigational satellites are satellites that use radio time signals transmitted to enable mobile receivers on the ground to determine their exact location. The relatively clear line of sight between the satellites and receivers on the ground, combined with ever-improving electronics, allows satellite navigation systems to measure location to accuracies on the order of a few meters in real time.
Killer satellites are satellites that are designed to destroy enemy warheads, satellites, and other space assets.
Crewed spacecraft (spaceships) are large satellites able to put humans into (and beyond) an orbit, and return them to Earth. (The Lunar Module of the U.S. Apollo program was an exception, in that it did not have the capability of returning human occupants to Earth.) Spacecraft including spaceplanes of reusable systems have major propulsion or landing facilities. They can be used as transport to and from the orbital stations.
Miniaturized satellites are satellites of unusually low masses and small sizes. New classifications are used to categorize these satellites: minisatellite (500'961000 kg), microsatellite (below 100 kg), nanosatellite (below 10 kg).
Reconnaissance satellites are Earth observation satellite or communications satellite deployed for military or intelligence applications. Very little is known about the full power of these satellites, as governments who operate them usually keep information pertaining to their reconnaissance satellites classified.
Recovery satellites are satellites that provide a recovery of reconnaissance, biological, space-production and other payloads from orbit to Earth.
Space-based solar power satellites are proposed satellites that would collect energy from sunlight and transmit it for use on Earth or other places.
Space stations are artificial orbital structures that are designed for human beings to live on in outer space. A space station is distinguished from other crewed spacecraft by its lack of major propulsion or landing facilities. Space stations are designed for medium-term living in orbit, for periods of weeks, months, or even years.
Tether satellites are satellites that are connected to another satellite by a thin cable called a tether.
Weather satellites are primarily used to monitor Earth's weather and climate.
The first satellite, Sputnik 1, was put into orbit around Earth and was therefore in geocentric orbit. This is the most common type of orbit by far, with approximately 3,372 active artificial satellites orbiting the Earth. Geocentric orbits may be further classified by their altitude, inclination and eccentricity.

The commonly used altitude classifications of geocentric orbit are Low Earth orbit (LEO), Medium Earth orbit (MEO) and High Earth orbit (HEO). Low Earth orbit is any orbit below 2,000 km. Medium Earth orbit is any orbit between 2,000 and 35,786 km. High Earth orbit is any orbit higher than 35,786 km.

Galactocentric orbit: An orbit around the centre of a galaxy. The Sun follows this type of orbit about the galactic centre of the Milky Way.
Heliocentric orbit: An orbit around the Sun. In our Solar System, all planets, comets, and asteroids are in such orbits, as are many artificial satellites and pieces of space debris. Moons by contrast are not in a heliocentric orbit but rather orbit their parent planet.
Geocentric orbit: An orbit around the planet Earth, such as the Moon or artificial satellites. Currently there are over 6,542 active artificial satellites orbiting the Earth.
Areocentric orbit: An orbit around the planet Mars, such as by moons or artificial satellites.
Low Earth orbit (LEO): Geocentric orbits ranging in altitude from 180 km '96 2,000 km (1,200 mi)
Medium Earth orbit (MEO): Geocentric orbits ranging in altitude from 2,000 km (1,200 mi) '96 35,786 km (22,236 mi). Also known as an intermediate circular orbit.
Geosynchronous orbit (GEO): Geocentric circular orbit with an altitude of 35,786 kilometres (22,236 mi). The period of the orbit equals one sidereal day, coinciding with the rotation period of the Earth. The speed is 3,075 metres per second (10,090 ft/s).
High Earth orbit (HEO): Geocentric orbits above the altitude of geosynchronous orbit 35,786 km (22,236 mi).
Inclined orbit: An orbit whose inclination in reference to the equatorial plane is not zero degrees.
Polar orbit: An orbit that passes above or nearly above both poles of the planet on each revolution. Therefore, it has an inclination of (or very close to) 90 degrees.
Polar sun synchronous orbit: A nearly polar orbit that takes advantage of nodal precession such that a satellite in such an orbit passes the equator at the same local time on every pass. Useful for image taking satellites because shadows will be nearly the same on every pass, and for solar observation satellites because they can have a continuous view of the Sun throughout the year.
Circular orbit: An orbit that has an eccentricity of 0 and whose path traces a circle.
Hohmann transfer orbit: An orbit that moves a spacecraft from one approximately circular orbit, usually the orbit of a planet, to another, using two engine impulses. The perihelion of the transfer orbit is at the same distance from the Sun as the radius of one planet's orbit, and the aphelion is at the other. The two rocket burns change the spacecraft's path from one circular orbit to the transfer orbit, and later to the other circular orbit. This maneuver was named after Walter Hohmann.
Elliptic orbit: An orbit with an eccentricity greater than 0 and less than 1 whose orbit traces the path of an ellipse.
Geosynchronous transfer orbit: An elliptic orbit where the perigee is at the altitude of a Low Earth orbit (LEO) and the apogee at the altitude of a geosynchronous orbit. Satellites use this orbit to transfer to a geostationary orbit.
Geostationary transfer orbit: A geosynchronous transfer orbit that is used to transfer to a geostationary orbit.
Molniya orbit: A highly eccentric orbit with inclination of 63.4'b0 and orbital period of half of a sidereal day (roughly 12 hours). Such a satellite spends most of its time over two designated areas of the planet (usually Russia and North America).
Tundra orbit: A highly eccentric orbit with inclination of 63.4'b0 and orbital period of one sidereal day (roughly 24 hours). Such a satellite spends most of its time over a single designated area of the planet.
Synchronous orbit: An orbit where the satellite has an orbital period equal to the average rotational period (earth's is: 23 hours, 56 minutes, 4.091 seconds) of the body being orbited and in the same direction of rotation as that body. To a ground observer such a satellite would trace an analemma (figure 8) in the sky.
Semi-synchronous orbit (SSO): An orbit with an altitude of approximately 20,200 km (12,600 mi) and an orbital period equal to one-half of the average rotational period (Earth's is approximately 12 hours) of the body being orbited
Geosynchronous orbit (GSO): Orbits with an altitude of approximately 35,786 km (22,236 mi). Such a satellite would trace an analemma (figure 8) in the sky.
Geostationary orbit (GEO): A geosynchronous orbit with an inclination of zero. To an observer on the ground this satellite would appear as a fixed point in the sky.
Clarke orbit: Another name for a geostationary orbit. Named after scientist and writer Arthur C. Clarke.
Supersynchronous orbit: A disposal / storage orbit above GSO/GEO. Satellites will drift west. Also a synonym for Disposal orbit.
Subsynchronous orbit: A drift orbit close to but below GSO/GEO. Satellites will drift east.
Graveyard orbit: An orbit a few hundred kilometers above geosynchronous that satellites are moved into at the end of their operation.
Disposal orbit: A synonym for graveyard orbit.
Junk orbit: A synonym for graveyard orbit.
Areosynchronous orbit: A synchronous orbit around the planet Mars with an orbital period equal in length to Mars' sidereal day, 24.6229 hours.
Areostationary orbit (ASO): A circular areosynchronous orbit on the equatorial plane and about 17000 km (10557 miles) above the surface. To an observer on the ground this satellite would appear as a fixed point in the sky.
Heliosynchronous orbit: A heliocentric orbit about the Sun where the satellite's orbital period matches the Sun's period of rotation. These orbits occur at a radius of 24,360 Gm (0.1628 AU) around the Sun, a little less than half of the orbital radius of Mercury.
Sun-synchronous orbit: An orbit which combines altitude and inclination in such a way that the satellite passes over any given point of the planets' surface at the same local solar time. Such an orbit can place a satellite in constant sunlight and is useful for imaging, spy, and weather satellites.
Moon orbit: The orbital characteristics of Earth's Moon. Average altitude of 384,403 kilometers (238,857 mi), elliptical'96inclined orbit.
Horseshoe orbit: An orbit that appears to a ground observer to be orbiting a certain planet but is actually in co-orbit with the planet. See asteroids 3753 (Cruithne) and 2002 AA29.
Suborbital spaceflight: A maneuver where a spacecraft approaches the height of orbit but lacks the velocity to sustain it.
Lunar transfer orbit (LTO)
Prograde orbit: An orbit with an inclination of less than 90'b0. Or rather, an orbit that is in the same direction as the rotation of the primary.
Retrograde orbit: An orbit with an inclination of more than 90'b0. Or rather, an orbit counter to the direction of rotation of the planet. Apart from those in sun-synchronous orbit, few satellites are launched into retrograde orbit because the quantity of fuel required to launch them is much greater than for a prograde orbit. This is because when the rocket starts out on the ground, it already has an eastward component of velocity equal to the rotational velocity of the planet at its launch latitude.
Halo orbit and Lissajous orbit: Orbits "around" Lagrangian points.
The satellite's functional versatility is embedded within its technical components and its operations characteristics. Looking at the "anatomy" of a typical satellite, one discovers two modules. Note that some novel architectural concepts such as Fractionated spacecraft somewhat upset this taxonomy.

The bus module consists of the following subsystems:

The structural subsystem provides the mechanical base structure with adequate stiffness to withstand stress and vibrations experienced during launch, maintain structural integrity and stability while on station in orbit, and shields the satellite from extreme temperature changes and micro-meteorite damage.

The telemetry subsystem (aka Command and Data Handling, C&DH) monitors the on-board equipment operations, transmits equipment operation data to the earth control station, and receives the earth control station's commands to perform equipment operation adjustments.

The power subsystem may consist of solar panels to convert solar energy into electrical power, regulation and distribution functions, and batteries that store power and supply the satellite when it passes into the Earth's shadow. Nuclear power sources (Radioisotope thermoelectric generator) have also been used in several successful satellite programs including the Nimbus program (1964'961978).

The thermal control subsystem helps protect electronic equipment from extreme temperatures due to intense sunlight or the lack of sun exposure on different sides of the satellite's body (e.g. optical solar reflector)

The attitude and orbit control subsystem consists of sensors to measure vehicle orientation, control laws embedded in the flight software, and actuators (reaction wheels, thrusters). These apply the torques and forces needed to re-orient the vehicle to the desired altitude, keep the satellite in the correct orbital position, and keep antennas pointed in the right directions.

The second major module is the communication payload, which is made up of transponders. A transponder is capable of :

Receiving uplinked radio signals from earth satellite transmission stations (antennas).
Amplifying received radio signals
Sorting the input signals and directing the output signals through input/output signal multiplexers to the proper downlink antennas for retransmission to earth satellite receiving stations (antennas).
When satellites reach the end of their mission (this normally occurs within 3 or 4 years after launch), satellite operators have the option of de-orbiting the satellite, leaving the satellite in its current orbit or moving the satellite to a graveyard orbit. Historically, due to budgetary constraints at the beginning of satellite missions, satellites were rarely designed to be de-orbited. One example of this practice is the satellite Vanguard 1. Launched in 1958, Vanguard 1, the 4th artificial satellite to be put in Geocentric orbit, was still in orbit as of February 2022, as well as the upper stage of its launch rocket.

Instead of being de-orbited, most satellites in the first six decades of spaceflight were either left in their current orbit or moved to a graveyard orbit. As of 2002, the FCC requires all geostationary satellites to commit to moving to a graveyard orbit at the end of their operational life prior to launch.

In cases of uncontrolled de-orbiting, the major variable is the solar flux, and minor variables are the components and form factor of the satellite itself, as well as gravitational perturbations generated by the Sun and the Moon. The nominal breakup altitude due to aerodynamic forces and temperatures is 78 km, with a range between 72 and 84 km. Solar panels, however, are destroyed before any other component at altitudes between 90 and 95 km.

After the late 2010s, and especially after the advent and operational fielding of large satellite internet constellations'97where on-orbit active satellites more than doubled over a period of five years'97the companies building the constellations began to propose regular planned deorbiting of the older satellites that reach end of life, as a part of the regulatory process of obtaining a launch license.

By the early 2000s, and particularly after the advent of CubeSats and increased launches of microsats'97frequently launched to the lower altitudes of low Earth orbit (LEO)'97satellites began to more frequently be designed to demise, or breakup and burnup entirely in the atmosphere. For example, SpaceX Starlink satellites, the first large satellite internet constellation to exceed 1000 active satellites on orbit in ~2020, are designed to be 100% demisable and burn up completely on atmospheric reentry at end of life, or in the event of an early satellite failure.

This list includes countries with an independent capability to place satellites in orbit, including production of the necessary launch vehicle. Note: many more countries have the capability to design and build satellites but are unable to launch them, instead relying on foreign launch services. This list does not consider those numerous countries, but only lists those capable of launching satellites indigenously, and the date this capability was first demonstrated. The list does not include the European Space Agency, a multi-national state organization, nor private consortiums.



The United States tried in 1957 to launch the first satellite using its own launcher before successfully completing a launch in 1958.
Japan tried four times in 1966'961969 to launch a satellite with its own launcher before successfully completing a launch in 1970.
China tried in 1969 to launch the first satellite using its own launcher before successfully completing a launch in 1970.
India, after launching its first national satellite using a foreign launcher in 1975, tried in 1979 to launch the first satellite using its own launcher before succeeding in 1980.
Iraq have claimed an orbital launch of a warhead in 1989, but this claim was later disproved.
Brazil, after launching its first national satellite using a foreign launcher in 1985, tried to launch a satellite using its own VLS 1 launcher three times in 1997, 1999, and 2003, but all attempts were unsuccessful.
North Korea claimed a launch of Kwangmyuc0u335 ngsu335 ng-1 and Kwangmyu335 ngsu335 ng-2 satellites in 1998 and 2009, but U.S., Russian and other officials and weapons experts later reported that the rockets failed to send a satellite into orbit, if that was the goal. The United States, Japan and South Korea believe this was actually a ballistic missile test, which was a claim also made after North Korea's 1998 satellite launch, and later rejected. The first (April 2012) launch of Kwangmyu335 ngsu335 ng-3 was unsuccessful, a fact publicly recognized by the DPRK. However, the December 2012 launch of the "second version" of Kwangmyu335 ngsu335 ng-3 was successful, putting the DPRK's first confirmed satellite into orbit.
South Korea (Korea Aerospace Research Institute), after launching their first national satellite by foreign launcher in 1992, unsuccessfully tried to launch its own launcher, the KSLV (Naro)-1, (created with the assistance of Russia) in 2009 and 2010 until success was achieved in 2013 by Naro-3.
The First European multi-national state organization ELDO tried to make the orbital launches at Europa I and Europa II rockets in 1968'961970 and 1971 but stopped operation after failures.
^ Russia and Ukraine were parts of the Soviet Union and thus inherited their launch capability without the need to develop it indigenously. Through the Soviet Union they are also on the number one position in this list of accomplishments.
France, the United Kingdom, and Ukraine launched their first satellites by own launchers from foreign spaceports.
Some countries such as South Africa, Spain, Italy, Germany, Canada, Australia, Argentina, Egypt and private companies such as OTRAG, have developed their own launchers, but have not had a successful launch.
Only twelve, countries from the list below (USSR, USA, France, Japan, China, UK, India, Russia, Ukraine, Israel, Iran and North Korea) and one regional organization (the European Space Agency, ESA) have independently launched satellites on their own indigenously developed launch vehicles.
Several other countries, including Brazil, Argentina, Pakistan, Romania, Taiwan, Indonesia, Australia, Malaysia, Turkey and Switzerland are at various stages of development of their own small-scale launcher capabilities.
Orbital Sciences Corporation launched a satellite into orbit on the Pegasus in 1990. SpaceX launched a satellite into orbit on the Falcon 1 in 2008. Rocket Lab launched three cubesats into orbit on the Electron in 2018.

While Canada was the third country to build a satellite which was launched into space, it was launched aboard an American rocket from an American spaceport. The same goes for Australia, who launched first satellite involved a donated U.S. Redstone rocket and American support staff as well as a joint launch facility with the United Kingdom. The first Italian satellite San Marco 1 launched on 15 December 1964 on a U.S. Scout rocket from Wallops Island (Virginia, United States) with an Italian launch team trained by NASA. By similar occasions, almost all further first national satellites was launched by foreign rockets.

United States tried unsuccessfully to launch its first satellite in 1957; they were successful in 1958.
China tried unsuccessfully to launch its first satellite in 1969; they were successful in 1970.
Chile tried unsuccessfully in 1995 to launch its first satellite FASat-Alfa by foreign rocket; in 1998 they were successful.'86
North Korea has tried in 1998, 2009, 2012 to launch satellites, first successful launch on 12 December 2012.
Libya since 1996 developed its own national Libsat satellite project with the goal of providing telecommunication and remote sensing services that was postponed after the fall of Gaddafi.
Belarus tried unsuccessfully in 2006 to launch its first satellite BelKA by foreign rocket.'86
'86-note: Both Chile and Belarus used Russian companies as principal contractors to build their satellites, they used Russian-Ukrainian manufactured rockets and launched either from Russia or Kazakhstan.

Armenia founded ArmCosmos in 2012 and announced an intention to create and launch the countries first telecommunication satellite, named ArmSat. The investment estimate is $250 million and potential contractors for building the satellite includes Russia, China and Canada.
Cambodia's Royal Group plans to purchase for $250'96350 million and launch in the beginning of 2013 the telecommunication satellite.
Cayman Islands's Global IP Cayman private company plans to launch GiSAT-1 geostationary communications satellite in 2018.
Democratic Republic of Congo ordered at November 2012 in China (Academy of Space Technology (CAST) and Great Wall Industry Corporation (CGWIC)) the first telecommunication satellite CongoSat-1 which will be built on DFH-4 satellite bus platform and will be launched in China till the end of 2015.
Croatia has a goal to construct a satellite by 2013'962014. Launch into Earth orbit would be done by a foreign provider.
Ireland's team of Dublin Institute of Technology intends to launch the first Irish satellite within European University program CubeSat QB50.
Republic of Moldova's first remote sensing satellite plans to start in 2013 by Space centre at national Technical University.
Myanmar plans to purchase for $200 million their own telecommunication satellite.
Nicaragua ordered for $254 million at November 2013 in China the first telecommunication satellite Nicasat-1 (to be built at DFH-4 satellite bus platform by CAST and CGWIC), that planning to launch in China at 2016.
Paraguay under new Agencia Espacial del Paraguay '96- AEP airspace agency plans first Eart observation satellite.
Serbia's first satellite Tesla-1 was designed, developed and assembled by nongovernmental organisations in 2009 but still remains unlaunched.
Sri Lanka has a goal to construct two satellites beside of rent the national SupremeSAT payload in Chinese satellites. Sri Lankan Telecommunications Regulatory Commission has signed an agreement with Surrey Satellite Technology Ltd to get relevant help and resources. Launch into Earth orbit would be done by a foreign provider.
Syrian Space Research Center developing CubeSat-like small first national satellite since 2008.
Tunisia is developing its first satellite, ERPSat01. Consisting of a CubeSat of 1 kg mass, it will be developed by the Sfax School of Engineering. ERPSat satellite is planned to be launched into orbit in 2013.
Uzbekistan's State Space Research Agency (UzbekCosmos) announced in 2001 about intention of launch in 2002 first remote sensing satellite. Later in 2004 was stated that two satellites (remote sensing and telecommunication) will be built by Russia for $60'9670 million each
Bangladesh launched Bangabandhu-1, its first satellite, for geostationary communications and broadcasting. It was manufactured by Thales Alenia Space and launched on 12 May 2018 and launched by Falcon 9 Block 5 of SpaceX.
Since the mid-2000s, satellites have been hacked by militant organizations to broadcast propaganda and to pilfer classified information from military communication networks.

For testing purposes, satellites in low earth orbit have been destroyed by ballistic missiles launched from earth. Russia, United States, China and India have demonstrated the ability to eliminate satellites. In 2007 the Chinese military shot down an aging weather satellite, followed by the US Navy shooting down a defunct spy satellite in February 2008. On 27 March 2019 India shot down a live test satellite at 300 km altitude in 3 minutes. India became the fourth country to have the capability to destroy live satellites.

Due to the low received signal strength of satellite transmissions, they are prone to jamming by land-based transmitters. Such jamming is limited to the geographical area within the transmitter's range. GPS satellites are potential targets for jamming, but satellite phone and television signals have also been subjected to jamming.

Also, it is very easy to transmit a carrier radio signal to a geostationary satellite and thus interfere with the legitimate uses of the satellite's transponder. It is common for Earth stations to transmit at the wrong time or on the wrong frequency in commercial satellite space, and dual-illuminate the transponder, rendering the frequency unusable. Satellite operators now have sophisticated monitoring that enables them to pinpoint the source of any carrier and manage the transponder space effectively.

During the last five decades, space agencies have sent thousands of space crafts, space capsules, or satellites to the universe. In fact, weather forecasters make predictions on the weather and natural calamities based on observations from these satellites.

The National Aeronautics and Space Administration (NASA) requested the National Academies to publish a report, "Earth Observations from Space; The First 50 Years of Scientific Achievements", in 2008. It described how the capability to view the whole globe simultaneously from satellite observations revolutionized studies about the planet Earth. This development brought about a new age of combined Earth sciences. The National Academies report concluded that continuing Earth observations from the galaxy are necessary to resolve scientific and social challenges in the future.

The NASA introduced an Earth Observing System (EOS) composed of several satellites, science component, and data system described as the Earth Observing System Data and Information System (EOSDIS). It disseminates numerous science data products as well as services designed for interdisciplinary education. EOSDIS data can be accessed online and accessed through File Transfer Protocol (FTP) and Hyper Text Transfer Protocol Secure (HTTPS). Scientists and researchers perform EOSDIS science operations within a distributed platform of multiple interconnected nodes or Science Investigator-led Processing Systems (SIPS) and discipline-specific Distributed Active Archive Centers (DACCs).

The European Space Agency have been operating Earth Observation satellites since the launch of Meteosat 1 in November 1977. ESA currently has plans to launch a satellite equipped with an artificial intelligence (AI) processor that will allow the spacecraft to make decisions on images to capture and data to transmit to the Earth. BrainSat will use the Intel Myriad X vision processing unit (VPU). The launching will be scheduled in 2019. ESA director for Earth Observation Programs Josef Aschbacher made the announcement during the PhiWeek in November 2018. This is the five-day meet that focused on the future of Earth observation. The conference was held at the ESA Center for Earth Observation in Frascati, Italy. ESA also launched the PhiLab, referring to the future-focused team that works to harness the potentials of AI and other disruptive innovations. Meanwhile, the ESA also announced that it expects to commence the qualification flight of the Space Rider space plane in 2021. This will come after several demonstration missions. Space Rider is the sequel of the Agency's Intermediate Experimental vehicle (IXV) which was launched in 2015. It has the capacity payload of 800 kilograms for orbital missions that will last a maximum of two months.

Issues like space debris, radio and light pollution are increasing in magnitude and at the same time lack progress in national or international regulation. Space debris poses dangers to spacecraft (including satellites) in or crossing geocentric orbits and have the potential to drive a Kessler syndrome which could potentially curtail humanity from conducting space endeavors in the future by making such nearly impossible.

With the increase in numbers of satellite constellations, like SpaceX Starlink, the astronomical community, such as the IAU, report that orbital pollution is getting increased significantly. A report from the SATCON1 workshop in 2020 concluded that the effects of large satellite constellations can severely affect some astronomical research efforts and lists six ways to mitigate harm to astronomy. The IAU is establishing a center (CPS) to coordinate or aggregate measures to mitigate such detrimental effects.

Some notable satellite failures that polluted and dispersed radioactive materials are Kosmos 954, Kosmos 1402 and the Transit 5-BN-3.

Generally liability has been covered by the Liability Convention. Using wood as an alternative material has been posited in order to reduce pollution and debris from satellites that reenter the atmosphere.

Several open source satellites both in terms of open source hardware and open source software were flown or are in development. The satellites have usually form of a CubeSat or PocketQube. In 2013 an amateur radio satellite OSSI-1 was launched and remained in orbit for about 2 months. In 2017 UPSat created by the Greek University of Patras and Libre Space Foundation remained in orbit for 18 months. In 2019 FossaSat-1 was launched. As of February 2021 the Portland State Aerospace Society is developing two open source satellites called OreSat and the Libre Space Foundation also has ongoing satellite projects.

An Earth observation satellite or Earth remote sensing satellite is a satellite used or designed for Earth observation (EO) from orbit, including spy satellites and similar ones intended for non-military uses such as environmental monitoring, meteorology, cartography and others. The most common type are Earth imaging satellites, that take satellite images, analogous to aerial photographs; some EO satellites may perform remote sensing without forming pictures, such as in GNSS radio occultation.

The first occurrence of satellite remote sensing can be dated to the launch of the first artificial satellite, Sputnik 1, by the Soviet Union on October 4, 1957. Sputnik 1 sent back radio signals, which scientists used to study the ionosphere. The United States Army Ballistic Missile Agency launched the first American satellite, Explorer 1, for NASA'92s Jet Propulsion Laboratory on January 31, 1958. The information sent back from its radiation detector led to the discovery of the Earth's Van Allen radiation belts. The TIROS-1 spacecraft, launched on April 1, 1960 as part of NASA's Television Infrared Observation Satellite (TIROS) program, sent back the first television footage of weather patterns to be taken from space.

In 2008, more than 150 Earth observation satellites were in orbit, recording data with both passive and active sensors and acquiring more than 10 terabits of data daily. By 2021, that total had grown to over 950, with the largest number of satellites operated by US-based company Planet Labs.

Most Earth observation satellites carry instruments that should be operated at a relatively low altitude. Most orbit at altitudes above 500 to 600 kilometers (310 to 370 mi). Lower orbits have significant air-drag, which makes frequent orbit reboost maneuvers necessary. The Earth observation satellites ERS-1, ERS-2 and Envisat of European Space Agency as well as the MetOp spacecraft of EUMETSAT are all operated at altitudes of about 800 km (500 mi). The Proba-1, Proba-2 and SMOS spacecraft of European Space Agency are observing the Earth from an altitude of about 700 km (430 mi). The Earth observation satellites of UAE, DubaiSat-1 & DubaiSat-2 are also placed in Low Earth Orbits (LEO) orbits and providing satellite imagery of various parts of the Earth.

To get (nearly) global coverage with a low orbit, a polar orbit is used. A low orbit will have an orbital period of roughly 100 minutes and the Earth will rotate around its polar axis about 25'b0 between successive orbits. The ground track moves towards the west 25'b0 each orbit, allowing a different section of the globe to be scanned with each orbit. Most are in Sun-synchronous orbits.

A geostationary orbit, at 36,000 km (22,000 mi), allows a satellite to hover over a constant spot on the earth since the orbital period at this altitude is 24 hours. This allows uninterrupted coverage of more than 1/3 of the Earth per satellite, so three satellites, spaced 120'b0 apart, can cover the whole Earth except the extreme polar regions. This type of orbit is mainly used for meteorological satellites.

A weather satellite is a type of satellite that is primarily used to monitor the weather and climate of the Earth. These meteorological satellites, however, see more than clouds and cloud systems. City lights, fires, effects of pollution, auroras, sand and dust storms, snow cover, ice mapping, boundaries of ocean currents, energy flows, etc., are other types of environmental information collected using weather satellites.

Weather satellite images helped in monitoring the volcanic ash cloud from Mount St. Helens and activity from other volcanoes such as Mount Etna. Smoke from fires in the western United States such as Colorado and Utah have also been monitored.

Other environmental satellites can assist environmental monitoring by detecting changes in the Earth's vegetation, atmospheric trace gas content, sea state, ocean color, and ice fields. By monitoring vegetation changes over time, droughts can be monitored by comparing the current vegetation state to its long term average. For example, the 2002 oil spill off the northwest coast of Spain was watched carefully by the European ENVISAT, which, though not a weather satellite, flies an instrument (ASAR) which can see changes in the sea surface. Anthropogenic emissions can be monitored by evaluating data of tropospheric NO2 and SO2.

These types of satellites are almost always in Sun-synchronous and "frozen" orbits. A sun-synchronous orbit passes over each spot on the ground at the same time of day, so that observations from each pass can be more easily compared, since the sun is in the same spot in each observation. A "frozen" orbit is the closest possible orbit to a circular orbit that is undisturbed by the oblateness of the Earth, gravitational attraction from the sun and moon, solar radiation pressure, and air drag.

Terrain can be mapped from space with the use of satellites, such as Radarsat-1 and TerraSAR-X.

According to the International Telecommunication Union (ITU), Earth exploration-satellite service (also: Earth exploration-satellite radiocommunication service) is '96 according to Article 1.51 of the ITU Radio Regulations (RR) '96 defined as:
